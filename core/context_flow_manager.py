#!/usr/bin/env python3
"""
ğŸ”„ ä¸Šä¸‹æ–‡æµç®¡ç†å™¨ (Context Flow Manager)
è§£å†³å·¥å…·é—´ä¿¡æ¯å­¤å²›é—®é¢˜ï¼Œç¡®ä¿æ•°æ®æœ‰æ•ˆä¼ é€’å’Œæ•´åˆ
"""

import logging
import json
import re
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum

logger = logging.getLogger(__name__)


class DataType(Enum):
    """æ•°æ®ç±»å‹æšä¸¾"""
    TEXT = "text"
    JSON = "json"
    TABLE = "table"
    URL = "url"
    CODE = "code"
    NUMBER = "number"
    FILE_PATH = "file_path"
    ERROR = "error"


class DataRelevance(Enum):
    """æ•°æ®ç›¸å…³æ€§æšä¸¾"""
    HIGH = "high"      # é«˜åº¦ç›¸å…³ï¼Œå¿…é¡»ä¼ é€’
    MEDIUM = "medium"  # ä¸­ç­‰ç›¸å…³ï¼Œå»ºè®®ä¼ é€’
    LOW = "low"        # ä½ç›¸å…³æ€§ï¼Œå¯é€‰ä¼ é€’
    IRRELEVANT = "irrelevant"  # ä¸ç›¸å…³


@dataclass
class ContextData:
    """ä¸Šä¸‹æ–‡æ•°æ®é¡¹"""
    data_id: str
    content: Any
    data_type: DataType
    source_tool: str
    source_step: int
    timestamp: datetime
    relevance: DataRelevance = DataRelevance.MEDIUM
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        result = asdict(self)
        result['timestamp'] = self.timestamp.isoformat()
        result['data_type'] = self.data_type.value
        result['relevance'] = self.relevance.value
        return result


@dataclass
class DataFlow:
    """æ•°æ®æµå®šä¹‰"""
    from_tool: str
    to_tool: str
    data_mapping: Dict[str, str]  # æºå­—æ®µ -> ç›®æ ‡å­—æ®µ
    transformation_rule: Optional[str] = None
    required: bool = True


class ContextFlowManager:
    """
    ğŸ”„ ä¸Šä¸‹æ–‡æµç®¡ç†å™¨
    
    åŠŸèƒ½ï¼š
    1. æ™ºèƒ½æå–å’Œåˆ†ç±»å·¥å…·è¾“å‡ºæ•°æ®
    2. å»ºç«‹å·¥å…·é—´æ•°æ®ä¼ é€’æ˜ å°„
    3. ç¡®ä¿å…³é”®ä¿¡æ¯åœ¨æ­¥éª¤é—´æœ‰æ•ˆä¼ é€’
    4. æä¾›æ•°æ®ç›¸å…³æ€§åˆ†æå’Œè¿‡æ»¤
    """
    
    def __init__(self, max_context_size: int = 1000):
        """åˆå§‹åŒ–ä¸Šä¸‹æ–‡æµç®¡ç†å™¨"""
        self.context_store: Dict[str, ContextData] = {}
        self.data_flows: List[DataFlow] = []
        self.step_outputs: Dict[int, List[str]] = {}  # æ­¥éª¤ -> æ•°æ®IDs
        self.max_context_size = max_context_size
        
        # å·¥å…·é—´å¸¸è§æ•°æ®æµæ¨¡å¼
        self.common_flows = self._initialize_common_flows()
        
        # æ•°æ®æå–æ¨¡å¼
        self.extraction_patterns = self._initialize_extraction_patterns()
        
        logger.info("ğŸ”„ ContextFlowManager initialized")
    
    def _initialize_common_flows(self) -> List[DataFlow]:
        """åˆå§‹åŒ–å¸¸è§çš„å·¥å…·é—´æ•°æ®æµ"""
        return [
            # æœç´¢ -> ä»£ç æ‰§è¡Œ
            DataFlow(
                from_tool="deepsearch", 
                to_tool="microsandbox",
                data_mapping={"search_results": "input_data"},
                transformation_rule="extract_key_facts"
            ),
            DataFlow(
                from_tool="browser_use", 
                to_tool="microsandbox",
                data_mapping={"page_content": "input_data", "extracted_data": "raw_data"},
                transformation_rule="parse_web_data"
            ),
            
            # ä»£ç æ‰§è¡Œ -> åˆ†æ
            DataFlow(
                from_tool="microsandbox",
                to_tool="deepsearch", 
                data_mapping={"calculation_result": "query_context", "error_message": "problem_context"},
                transformation_rule="format_for_search"
            ),
            
            # æœç´¢ -> æµè§ˆå™¨
            DataFlow(
                from_tool="deepsearch",
                to_tool="browser_use",
                data_mapping={"urls": "target_urls", "search_results": "context"},
                transformation_rule="extract_urls"
            )
        ]
    
    def _initialize_extraction_patterns(self) -> Dict[str, List[str]]:
        """åˆå§‹åŒ–æ•°æ®æå–æ¨¡å¼"""
        return {
            "numbers": [
                r'\b\d+\.?\d*\b',  # æ•°å­—
                r'\$\d+\.?\d*',    # ä»·æ ¼
                r'\d+%',           # ç™¾åˆ†æ¯”
            ],
            "urls": [
                r'https?://[^\s<>"{}|\\^`[\]]+',
                r'www\.[^\s<>"{}|\\^`[\]]+',
            ],
            "dates": [
                r'\d{4}-\d{2}-\d{2}',
                r'\d{1,2}/\d{1,2}/\d{4}',
                r'\b\d{1,2}\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{4}\b',
            ],
            "codes": [
                r'```[\s\S]*?```',  # ä»£ç å—
                r'`[^`]+`',         # å†…è”ä»£ç 
            ],
            "key_facts": [
                r'[A-Z][^.!?]*(?:\d+[^.!?]*)?[.!?]',  # åŒ…å«æ•°å­—çš„å…³é”®å¥å­
                r'(?:ç»“æœ|ç»“è®º|å‘ç°|æ˜¾ç¤º|è¡¨æ˜)[:ï¼š][^.!?]*[.!?]',  # ç»“è®ºæ€§è¯­å¥
            ]
        }
    
    def extract_context_data(self, tool_output: str, source_tool: str, 
                           step_number: int) -> List[ContextData]:
        """
        ğŸ” ä»å·¥å…·è¾“å‡ºä¸­æå–ä¸Šä¸‹æ–‡æ•°æ®
        
        Args:
            tool_output: å·¥å…·è¾“å‡ºå†…å®¹
            source_tool: æºå·¥å…·åç§°
            step_number: æ­¥éª¤ç¼–å·
            
        Returns:
            List[ContextData]: æå–çš„ä¸Šä¸‹æ–‡æ•°æ®åˆ—è¡¨
        """
        extracted_data = []
        current_time = datetime.now()
        
        # 1. æŒ‰æ•°æ®ç±»å‹æå–
        for data_type, patterns in self.extraction_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, tool_output, re.MULTILINE | re.IGNORECASE)
                for i, match in enumerate(matches):
                    data_id = f"{source_tool}_{step_number}_{data_type}_{i}"
                    
                    # ç¡®å®šæ•°æ®ç±»å‹å’Œç›¸å…³æ€§
                    extracted_type = self._determine_data_type(match.group(), data_type)
                    relevance = self._assess_relevance(match.group(), source_tool, data_type)
                    
                    context_data = ContextData(
                        data_id=data_id,
                        content=match.group().strip(),
                        data_type=extracted_type,
                        source_tool=source_tool,
                        source_step=step_number,
                        timestamp=current_time,
                        relevance=relevance,
                        metadata={
                            "pattern_type": data_type,
                            "match_start": match.start(),
                            "match_end": match.end(),
                            "context": tool_output[max(0, match.start()-50):match.end()+50]
                        }
                    )
                    
                    extracted_data.append(context_data)
        
        # 2. ç‰¹æ®Šæ•°æ®æå–
        special_data = self._extract_special_data(tool_output, source_tool, step_number)
        extracted_data.extend(special_data)
        
        # 3. å­˜å‚¨åˆ°ä¸Šä¸‹æ–‡ä»“åº“
        for data in extracted_data:
            self.context_store[data.data_id] = data
        
        # 4. è®°å½•æ­¥éª¤è¾“å‡º
        if step_number not in self.step_outputs:
            self.step_outputs[step_number] = []
        self.step_outputs[step_number].extend([data.data_id for data in extracted_data])
        
        logger.info(f"ğŸ” ä» {source_tool} æ­¥éª¤{step_number} æå–äº† {len(extracted_data)} ä¸ªæ•°æ®é¡¹")
        return extracted_data
    
    def _extract_special_data(self, output: str, source_tool: str, 
                            step_number: int) -> List[ContextData]:
        """æå–ç‰¹æ®Šç±»å‹çš„æ•°æ®"""
        special_data = []
        current_time = datetime.now()
        
        # é”™è¯¯ä¿¡æ¯æå–
        error_patterns = [
            r'error[:ï¼š]\s*([^\n]+)',
            r'failed[:ï¼š]\s*([^\n]+)',
            r'exception[:ï¼š]\s*([^\n]+)',
            r'é”™è¯¯[:ï¼š]\s*([^\n]+)',
        ]
        
        for pattern in error_patterns:
            matches = re.finditer(pattern, output, re.IGNORECASE)
            for i, match in enumerate(matches):
                data_id = f"{source_tool}_{step_number}_error_{i}"
                
                special_data.append(ContextData(
                    data_id=data_id,
                    content=match.group(1).strip(),
                    data_type=DataType.ERROR,
                    source_tool=source_tool,
                    source_step=step_number,
                    timestamp=current_time,
                    relevance=DataRelevance.HIGH,
                    metadata={"error_type": "execution_error"}
                ))
        
        # JSONæ•°æ®æå–
        try:
            # å°è¯•è§£ææ•´ä¸ªè¾“å‡ºä¸ºJSON
            json_data = json.loads(output)
            data_id = f"{source_tool}_{step_number}_json_0"
            
            special_data.append(ContextData(
                data_id=data_id,
                content=json_data,
                data_type=DataType.JSON,
                source_tool=source_tool,
                source_step=step_number,
                timestamp=current_time,
                relevance=DataRelevance.HIGH,
                metadata={"json_keys": list(json_data.keys()) if isinstance(json_data, dict) else []}
            ))
        except (json.JSONDecodeError, TypeError):
            # æŸ¥æ‰¾åµŒå…¥çš„JSONç‰‡æ®µ
            json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
            matches = re.finditer(json_pattern, output)
            for i, match in enumerate(matches):
                try:
                    json_obj = json.loads(match.group())
                    data_id = f"{source_tool}_{step_number}_json_{i}"
                    
                    special_data.append(ContextData(
                        data_id=data_id,
                        content=json_obj,
                        data_type=DataType.JSON,
                        source_tool=source_tool,
                        source_step=step_number,
                        timestamp=current_time,
                        relevance=DataRelevance.MEDIUM,
                        metadata={"embedded_json": True}
                    ))
                except json.JSONDecodeError:
                    continue
        
        return special_data
    
    def _determine_data_type(self, content: str, pattern_type: str) -> DataType:
        """ç¡®å®šæ•°æ®ç±»å‹"""
        type_mapping = {
            "numbers": DataType.NUMBER,
            "urls": DataType.URL,
            "dates": DataType.TEXT,
            "codes": DataType.CODE,
            "key_facts": DataType.TEXT,
        }
        return type_mapping.get(pattern_type, DataType.TEXT)
    
    def _assess_relevance(self, content: str, source_tool: str, pattern_type: str) -> DataRelevance:
        """è¯„ä¼°æ•°æ®ç›¸å…³æ€§"""
        # é«˜ç›¸å…³æ€§æŒ‡æ ‡
        high_indicators = [
            "price", "cost", "result", "error", "failure", "success",
            "ä»·æ ¼", "æˆæœ¬", "ç»“æœ", "é”™è¯¯", "å¤±è´¥", "æˆåŠŸ"
        ]
        
        # ä¸­ç­‰ç›¸å…³æ€§æŒ‡æ ‡
        medium_indicators = [
            "data", "information", "value", "number", "count",
            "æ•°æ®", "ä¿¡æ¯", "å€¼", "æ•°é‡", "è®¡æ•°"
        ]
        
        content_lower = content.lower()
        
        if any(indicator in content_lower for indicator in high_indicators):
            return DataRelevance.HIGH
        elif any(indicator in content_lower for indicator in medium_indicators):
            return DataRelevance.MEDIUM
        elif pattern_type in ["numbers", "urls", "key_facts"]:
            return DataRelevance.MEDIUM
        else:
            return DataRelevance.LOW
    
    def get_relevant_context(self, target_tool: str, current_step: int, 
                           relevance_threshold: DataRelevance = DataRelevance.MEDIUM) -> Dict[str, Any]:
        """
        ğŸ“Š è·å–ä¸ç›®æ ‡å·¥å…·ç›¸å…³çš„ä¸Šä¸‹æ–‡æ•°æ®
        
        Args:
            target_tool: ç›®æ ‡å·¥å…·åç§°
            current_step: å½“å‰æ­¥éª¤ç¼–å·
            relevance_threshold: ç›¸å…³æ€§é˜ˆå€¼
            
        Returns:
            Dict[str, Any]: ç›¸å…³çš„ä¸Šä¸‹æ–‡æ•°æ®
        """
        relevant_data = {}
        
        # 1. è·å–å‰å‡ æ­¥çš„é«˜ç›¸å…³æ€§æ•°æ®
        relevance_order = [DataRelevance.HIGH, DataRelevance.MEDIUM, DataRelevance.LOW]
        threshold_index = relevance_order.index(relevance_threshold)
        
        for step in range(max(0, current_step - 5), current_step):
            if step in self.step_outputs:
                for data_id in self.step_outputs[step]:
                    if data_id in self.context_store:
                        data = self.context_store[data_id]
                        if relevance_order.index(data.relevance) <= threshold_index:
                            category = f"step_{step}_{data.data_type.value}"
                            if category not in relevant_data:
                                relevant_data[category] = []
                            relevant_data[category].append({
                                "content": data.content,
                                "source": data.source_tool,
                                "relevance": data.relevance.value,
                                "metadata": data.metadata
                            })
        
        # 2. åº”ç”¨ç‰¹å®šå·¥å…·çš„æ•°æ®æµè§„åˆ™
        tool_specific_data = self._get_tool_specific_context(target_tool, current_step)
        relevant_data.update(tool_specific_data)
        
        # 3. é™åˆ¶ä¸Šä¸‹æ–‡å¤§å°
        if len(str(relevant_data)) > self.max_context_size:
            relevant_data = self._truncate_context(relevant_data)
        
        logger.info(f"ğŸ“Š ä¸º {target_tool} è·å–äº† {len(relevant_data)} ç±»ç›¸å…³ä¸Šä¸‹æ–‡")
        return relevant_data
    
    def _get_tool_specific_context(self, target_tool: str, current_step: int) -> Dict[str, Any]:
        """è·å–ç‰¹å®šå·¥å…·çš„ç›¸å…³ä¸Šä¸‹æ–‡"""
        tool_context = {}
        
        # æŸ¥æ‰¾é€‚ç”¨çš„æ•°æ®æµ
        applicable_flows = [flow for flow in self.common_flows if flow.to_tool == target_tool]
        
        for flow in applicable_flows:
            # å¯»æ‰¾æ¥æºå·¥å…·çš„è¾“å‡º
            for step in range(max(0, current_step - 3), current_step):
                if step in self.step_outputs:
                    for data_id in self.step_outputs[step]:
                        if data_id in self.context_store:
                            data = self.context_store[data_id]
                            if data.source_tool == flow.from_tool:
                                # åº”ç”¨æ•°æ®æ˜ å°„
                                for source_field, target_field in flow.data_mapping.items():
                                    if target_field not in tool_context:
                                        tool_context[target_field] = []
                                    
                                    processed_content = self._apply_transformation(
                                        data.content, flow.transformation_rule
                                    )
                                    
                                    tool_context[target_field].append({
                                        "content": processed_content,
                                        "original_content": data.content,
                                        "source_tool": data.source_tool,
                                        "transformation": flow.transformation_rule
                                    })
        
        return tool_context
    
    def _apply_transformation(self, content: Any, transformation_rule: Optional[str]) -> Any:
        """åº”ç”¨æ•°æ®è½¬æ¢è§„åˆ™"""
        if not transformation_rule:
            return content
        
        content_str = str(content)
        
        try:
            if transformation_rule == "extract_key_facts":
                # æå–å…³é”®äº‹å®
                sentences = re.split(r'[.!?]+', content_str)
                key_sentences = []
                for sentence in sentences:
                    if any(keyword in sentence.lower() for keyword in 
                          ["price", "cost", "result", "shows", "indicates", "ä»·æ ¼", "æˆæœ¬", "ç»“æœ", "æ˜¾ç¤º", "è¡¨æ˜"]):
                        key_sentences.append(sentence.strip())
                return " ".join(key_sentences[:3])  # æœ€å¤š3ä¸ªå…³é”®å¥å­
            
            elif transformation_rule == "parse_web_data":
                # è§£æç½‘é¡µæ•°æ®
                numbers = re.findall(r'\$?\d+\.?\d*', content_str)
                urls = re.findall(r'https?://[^\s<>"{}|\\^`[\]]+', content_str)
                return {
                    "numbers": numbers[:5],  # æœ€å¤š5ä¸ªæ•°å­—
                    "urls": urls[:3],        # æœ€å¤š3ä¸ªURL
                    "summary": content_str[:200]  # 200å­—ç¬¦æ‘˜è¦
                }
            
            elif transformation_rule == "format_for_search":
                # æ ¼å¼åŒ–ç”¨äºæœç´¢
                if isinstance(content, (dict, list)):
                    return f"Based on calculation: {json.dumps(content)}"
                else:
                    return f"Related to: {str(content)[:100]}"
            
            elif transformation_rule == "extract_urls":
                # æå–URL
                urls = re.findall(r'https?://[^\s<>"{}|\\^`[\]]+', content_str)
                return urls[:5]  # æœ€å¤š5ä¸ªURL
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ•°æ®è½¬æ¢å¤±è´¥ ({transformation_rule}): {e}")
            return content
        
        return content
    
    def _truncate_context(self, context_data: Dict[str, Any]) -> Dict[str, Any]:
        """æˆªæ–­ä¸Šä¸‹æ–‡ä»¥æ§åˆ¶å¤§å°"""
        truncated = {}
        total_size = 0
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆé«˜ç›¸å…³æ€§ä¼˜å…ˆï¼‰
        sorted_items = sorted(
            context_data.items(), 
            key=lambda x: self._get_category_priority(x[0]), 
            reverse=True
        )
        
        for category, items in sorted_items:
            if total_size >= self.max_context_size:
                break
            
            truncated_items = []
            for item in items:
                item_size = len(str(item))
                if total_size + item_size <= self.max_context_size:
                    truncated_items.append(item)
                    total_size += item_size
                else:
                    break
            
            if truncated_items:
                truncated[category] = truncated_items
        
        logger.info(f"ğŸ”„ ä¸Šä¸‹æ–‡æˆªæ–­: {len(context_data)} -> {len(truncated)} ç±»åˆ«")
        return truncated
    
    def _get_category_priority(self, category: str) -> int:
        """è·å–ç±»åˆ«ä¼˜å…ˆçº§"""
        if "error" in category:
            return 100
        elif "number" in category:
            return 90
        elif "json" in category:
            return 80
        elif "url" in category:
            return 70
        elif "code" in category:
            return 60
        else:
            return 50
    
    def generate_context_prompt(self, target_tool: str, current_step: int, 
                              task_description: str) -> str:
        """
        ğŸ“ ç”ŸæˆåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æç¤º
        
        Args:
            target_tool: ç›®æ ‡å·¥å…·
            current_step: å½“å‰æ­¥éª¤
            task_description: ä»»åŠ¡æè¿°
            
        Returns:
            str: åŒ…å«ä¸Šä¸‹æ–‡çš„æç¤ºæ–‡æœ¬
        """
        relevant_context = self.get_relevant_context(target_tool, current_step)
        
        if not relevant_context:
            return ""
        
        prompt_parts = [
            "ğŸ”„ **Previous Step Results (Use this information in your current action):**\n"
        ]
        
        for category, items in relevant_context.items():
            if items:
                prompt_parts.append(f"\n**{category.replace('_', ' ').title()}:**")
                for i, item in enumerate(items[:3]):  # æœ€å¤šæ˜¾ç¤º3ä¸ªé¡¹ç›®
                    content = item["content"]
                    if isinstance(content, str) and len(content) > 150:
                        content = content[:150] + "..."
                    prompt_parts.append(f"  {i+1}. {content}")
        
        # æ·»åŠ ä½¿ç”¨æŒ‡å¯¼
        prompt_parts.extend([
            "\nğŸ¯ **IMPORTANT**: You MUST reference and use the above information in your tool call.",
            "Do not ignore previous results or generate new simulated data when real data is available.",
            f"Current task context: {task_description[:100]}...\n"
        ])
        
        return "\n".join(prompt_parts)
    
    def get_context_summary(self, steps_range: Optional[Tuple[int, int]] = None) -> Dict[str, Any]:
        """
        ğŸ“ˆ è·å–ä¸Šä¸‹æ–‡æ•°æ®æ‘˜è¦
        
        Args:
            steps_range: æ­¥éª¤èŒƒå›´ (start, end)ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰æ­¥éª¤
            
        Returns:
            Dict[str, Any]: ä¸Šä¸‹æ–‡æ‘˜è¦
        """
        if steps_range:
            start_step, end_step = steps_range
            relevant_steps = range(start_step, end_step + 1)
        else:
            relevant_steps = self.step_outputs.keys()
        
        summary = {
            "total_data_items": 0,
            "data_by_type": {},
            "data_by_relevance": {},
            "data_by_tool": {},
            "recent_errors": [],
            "key_numbers": [],
            "important_urls": []
        }
        
        for step in relevant_steps:
            if step in self.step_outputs:
                for data_id in self.step_outputs[step]:
                    if data_id in self.context_store:
                        data = self.context_store[data_id]
                        summary["total_data_items"] += 1
                        
                        # æŒ‰ç±»å‹ç»Ÿè®¡
                        data_type = data.data_type.value
                        summary["data_by_type"][data_type] = summary["data_by_type"].get(data_type, 0) + 1
                        
                        # æŒ‰ç›¸å…³æ€§ç»Ÿè®¡
                        relevance = data.relevance.value
                        summary["data_by_relevance"][relevance] = summary["data_by_relevance"].get(relevance, 0) + 1
                        
                        # æŒ‰å·¥å…·ç»Ÿè®¡
                        tool = data.source_tool
                        summary["data_by_tool"][tool] = summary["data_by_tool"].get(tool, 0) + 1
                        
                        # æ”¶é›†ç‰¹æ®Šæ•°æ®
                        if data.data_type == DataType.ERROR:
                            summary["recent_errors"].append({
                                "step": step,
                                "tool": data.source_tool,
                                "error": str(data.content)[:100]
                            })
                        elif data.data_type == DataType.NUMBER and data.relevance == DataRelevance.HIGH:
                            summary["key_numbers"].append({
                                "step": step,
                                "tool": data.source_tool,
                                "number": data.content
                            })
                        elif data.data_type == DataType.URL:
                            summary["important_urls"].append({
                                "step": step,
                                "tool": data.source_tool,
                                "url": data.content
                            })
        
        return summary
    
    def clear_old_context(self, keep_steps: int = 10):
        """
        ğŸ§¹ æ¸…ç†æ—§çš„ä¸Šä¸‹æ–‡æ•°æ®
        
        Args:
            keep_steps: ä¿ç•™çš„æ­¥éª¤æ•°é‡
        """
        if not self.step_outputs:
            return
        
        max_step = max(self.step_outputs.keys())
        cutoff_step = max_step - keep_steps
        
        removed_count = 0
        for step in list(self.step_outputs.keys()):
            if step < cutoff_step:
                for data_id in self.step_outputs[step]:
                    if data_id in self.context_store:
                        del self.context_store[data_id]
                        removed_count += 1
                del self.step_outputs[step]
        
        logger.info(f"ğŸ§¹ æ¸…ç†äº† {removed_count} ä¸ªæ—§ä¸Šä¸‹æ–‡æ•°æ®é¡¹")