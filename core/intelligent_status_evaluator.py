"""
æ™ºèƒ½çŠ¶æ€è¯„ä¼°å™¨ - åŸºäºç»“æœé©±åŠ¨çš„è¯­ä¹‰åˆ¤å®šé€»è¾‘

æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š
1. ä»"æ ¼å¼é©±åŠ¨"è½¬å‘"ç»“æœé©±åŠ¨"
2. å¼•å…¥LLMè¯­ä¹‰ç†è§£èƒ½åŠ›
3. è¯†åˆ«Agentè‡ªæˆ‘çº æ­£è¿‡ç¨‹
4. å…³æ³¨ä»»åŠ¡å®é™…äº§å‡ºè€Œéæ ‡ç­¾æ ¼å¼
"""

import json
import logging
import re
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

from core.interfaces import TaskExecutionConstants

logger = logging.getLogger(__name__)


class TaskOutcomeType(Enum):
    """ä»»åŠ¡ç»“æœç±»å‹"""
    CLEAR_SUCCESS = "clear_success"         # æ˜ç¡®æˆåŠŸ
    CORRECTED_SUCCESS = "corrected_success" # ç»è¿‡çº æ­£çš„æˆåŠŸ
    PARTIAL_SUCCESS = "partial_success"     # éƒ¨åˆ†æˆåŠŸ
    PROCESS_FAILURE = "process_failure"     # è¿‡ç¨‹å¤±è´¥ä½†å¯æ¢å¤
    COMPLETE_FAILURE = "complete_failure"   # å®Œå…¨å¤±è´¥


@dataclass
class OutcomeEvidence:
    """ç»“æœè¯æ®"""
    evidence_type: str
    content: str
    confidence: float
    source_step: int
    timestamp: str


@dataclass
class TaskEvaluation:
    """ä»»åŠ¡è¯„ä¼°ç»“æœ"""
    outcome_type: TaskOutcomeType
    confidence_score: float
    primary_evidence: List[OutcomeEvidence]
    final_output: str
    correction_detected: bool
    semantic_reasoning: str
    

class IntelligentStatusEvaluator:
    """
    æ™ºèƒ½çŠ¶æ€è¯„ä¼°å™¨
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. è¯­ä¹‰ç†è§£ï¼šä½¿ç”¨LLMåˆ†æä»»åŠ¡å®Œæˆæƒ…å†µ
    2. è‡ªæˆ‘çº æ­£è¯†åˆ«ï¼šæ£€æµ‹Agentçš„é”™è¯¯ä¿®å¤è¿‡ç¨‹
    3. ç»“æœé©±åŠ¨ï¼šåŸºäºå®é™…äº§å‡ºè€Œéæ ¼å¼æ ‡ç­¾
    4. ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼šè€ƒè™‘å®Œæ•´çš„ä»»åŠ¡æ‰§è¡Œä¸Šä¸‹æ–‡
    """
    
    def __init__(self, llm_client):
        self.llm_client = llm_client
        self.evaluation_prompt_template = self._create_evaluation_prompt()
        
    def _create_evaluation_prompt(self) -> str:
        """åˆ›å»ºè¯„ä¼°æç¤ºæ¨¡æ¿"""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»»åŠ¡æ‰§è¡Œè¯„ä¼°ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹Agentä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ï¼Œåˆ¤æ–­ä»»åŠ¡æ˜¯å¦çœŸæ­£å®Œæˆã€‚

**è¯„ä¼°åŸåˆ™**ï¼š
1. å…³æ³¨æœ€ç»ˆå®é™…äº§å‡ºï¼Œè€Œéä¸­é—´è¿‡ç¨‹é”™è¯¯
2. è¯†åˆ«Agentçš„è‡ªæˆ‘çº æ­£è¡Œä¸ºï¼ˆé”™è¯¯â†’å‘ç°â†’ä¿®æ­£â†’æˆåŠŸï¼‰
3. é‡è§†å·¥å…·æ‰§è¡Œçš„å®é™…ç»“æœ
4. è€ƒè™‘ä»»åŠ¡çš„å®é™…å®Œæˆåº¦ï¼Œè€Œéæ ¼å¼å®Œæ•´æ€§

**åˆ†æå†…å®¹**ï¼š
ä»»åŠ¡è¦æ±‚ï¼š{task_input}
æ‰§è¡Œè½¨è¿¹ï¼š{trajectory_summary}
æœ€ç»ˆè¾“å‡ºï¼š{final_output}
å·¥å…·æ‰§è¡Œæƒ…å†µï¼š{tool_execution_summary}

**è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼ˆç”¨JSONæ ¼å¼ï¼‰**ï¼š
1. ä»»åŠ¡æ˜¯å¦å®é™…å®Œæˆï¼Ÿï¼ˆè€ƒè™‘æœ€ç»ˆç»“æœï¼‰
2. æ˜¯å¦æ£€æµ‹åˆ°è‡ªæˆ‘çº æ­£è¿‡ç¨‹ï¼Ÿ
3. ä¸»è¦æˆåŠŸè¯æ®æ˜¯ä»€ä¹ˆï¼Ÿ
4. ç½®ä¿¡åº¦è¯„åˆ†ï¼ˆ0-1ï¼‰
5. ä¸€å¥è¯è¯„ä¼°ç†ç”±

è¾“å‡ºæ ¼å¼ï¼š
{{
    "task_completed": true/false,
    "self_correction_detected": true/false,
    "success_evidence": "å…·ä½“è¯æ®æè¿°",
    "confidence_score": 0.95,
    "reasoning": "è¯„ä¼°ç†ç”±"
}}"""

    async def evaluate_task_completion(
        self,
        task_input: str,
        trajectory: List[Dict[str, Any]],
        final_output: str,
        tool_results: List[Dict[str, Any]]
    ) -> TaskEvaluation:
        """
        æ™ºèƒ½è¯„ä¼°ä»»åŠ¡å®ŒæˆçŠ¶æ€
        
        Args:
            task_input: åŸå§‹ä»»åŠ¡è¾“å…¥
            trajectory: å®Œæ•´æ‰§è¡Œè½¨è¿¹
            final_output: æœ€ç»ˆè¾“å‡ºå†…å®¹
            tool_results: å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨
            
        Returns:
            TaskEvaluation: è¯„ä¼°ç»“æœ
        """
        try:
            # 1. é¢„å¤„ç†å’Œç‰¹å¾æå–
            trajectory_summary = self._summarize_trajectory(trajectory)
            tool_summary = self._summarize_tool_execution(tool_results)
            
            # 2. æ£€æµ‹è‡ªæˆ‘çº æ­£æ¨¡å¼
            correction_evidence = self._detect_self_correction(trajectory)
            
            # 3. LLMè¯­ä¹‰è¯„ä¼°
            semantic_evaluation = await self._llm_semantic_evaluation(
                task_input, trajectory_summary, final_output, tool_summary
            )
            
            # 4. åŸºäºè§„åˆ™çš„è¾…åŠ©åˆ¤å®š
            rule_based_signals = self._rule_based_evaluation(
                trajectory, final_output, tool_results
            )
            
            # 5. ç»¼åˆåˆ¤å®š
            final_evaluation = self._synthesize_evaluation(
                semantic_evaluation, 
                rule_based_signals, 
                correction_evidence
            )
            
            logger.info(f"ğŸ§  æ™ºèƒ½çŠ¶æ€è¯„ä¼°å®Œæˆ: {final_evaluation.outcome_type.value}, "
                       f"ç½®ä¿¡åº¦: {final_evaluation.confidence_score:.2f}")
            
            return final_evaluation
            
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½çŠ¶æ€è¯„ä¼°å¤±è´¥: {e}")
            # é™çº§åˆ°ç®€å•è§„åˆ™åˆ¤å®š
            return self._fallback_evaluation(final_output, tool_results)

    def _summarize_trajectory(self, trajectory: List[Dict[str, Any]]) -> str:
        """æ€»ç»“æ‰§è¡Œè½¨è¿¹çš„å…³é”®ä¿¡æ¯"""
        if not trajectory:
            return "æ— æ‰§è¡Œè½¨è¿¹"
            
        summary_parts = []
        step_count = len(trajectory)
        
        # ç»Ÿè®¡å·¥å…·ä½¿ç”¨æƒ…å†µ
        tools_used = set()
        success_steps = 0
        error_steps = 0
        
        for step in trajectory:
            step_content = str(step)
            
            # æ£€æµ‹å·¥å…·ä½¿ç”¨
            if 'tool_name' in step or any(tool in step_content.lower() 
                                        for tool in ['microsandbox', 'deepsearch', 'browser_use']):
                if 'tool_name' in step:
                    tools_used.add(step.get('tool_name', 'unknown'))
                    
            # ç»Ÿè®¡æˆåŠŸ/å¤±è´¥æ­¥éª¤
            if any(indicator in step_content.lower() 
                   for indicator in ['success', 'æˆåŠŸ', 'completed', 'å®Œæˆ']):
                success_steps += 1
            elif any(indicator in step_content.lower() 
                    for indicator in ['error', 'failed', 'é”™è¯¯', 'å¤±è´¥']):
                error_steps += 1
        
        summary_parts.append(f"æ€»æ­¥æ•°: {step_count}")
        summary_parts.append(f"ä½¿ç”¨å·¥å…·: {', '.join(tools_used) if tools_used else 'æ— '}")
        summary_parts.append(f"æˆåŠŸæ­¥éª¤: {success_steps}, é”™è¯¯æ­¥éª¤: {error_steps}")
        
        # æ·»åŠ æœ€åå‡ æ­¥çš„å†…å®¹æ‘˜è¦
        if len(trajectory) > 0:
            last_steps = trajectory[-min(3, len(trajectory)):]
            last_content = " | ".join([str(step)[:100] + "..." 
                                     for step in last_steps])
            summary_parts.append(f"æœ€åæ­¥éª¤: {last_content}")
        
        return " ; ".join(summary_parts)

    def _summarize_tool_execution(self, tool_results: List[Dict[str, Any]]) -> str:
        """æ€»ç»“å·¥å…·æ‰§è¡Œæƒ…å†µ"""
        if not tool_results:
            return "æ— å·¥å…·æ‰§è¡Œ"
            
        tool_summary = {}
        total_success = 0
        total_failure = 0
        
        for result in tool_results:
            tool_name = result.get('tool_name', 'unknown')
            is_success = result.get('success', False)
            
            if tool_name not in tool_summary:
                tool_summary[tool_name] = {'success': 0, 'failure': 0}
                
            if is_success:
                tool_summary[tool_name]['success'] += 1
                total_success += 1
            else:
                tool_summary[tool_name]['failure'] += 1
                total_failure += 1
        
        summary_parts = []
        for tool, stats in tool_summary.items():
            summary_parts.append(f"{tool}: {stats['success']}æˆåŠŸ/{stats['failure']}å¤±è´¥")
            
        summary_parts.append(f"æ€»è®¡: {total_success}æˆåŠŸ/{total_failure}å¤±è´¥")
        
        return " ; ".join(summary_parts)

    def _detect_self_correction(self, trajectory: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ£€æµ‹è‡ªæˆ‘çº æ­£æ¨¡å¼"""
        correction_patterns = {
            'error_to_correction': [],  # é”™è¯¯â†’çº æ­£æ¨¡å¼
            'retry_patterns': [],       # é‡è¯•æ¨¡å¼
            'strategy_adjustment': []   # ç­–ç•¥è°ƒæ•´æ¨¡å¼
        }
        
        for i, step in enumerate(trajectory):
            step_content = str(step).lower()
            
            # æ¨¡å¼1: é”™è¯¯è¯†åˆ«å’Œçº æ­£
            if any(error_word in step_content for error_word in 
                   ['error', 'mistake', 'é”™è¯¯', 'å¤±è´¥', 'incorrect']):
                # æ£€æŸ¥åç»­æ­¥éª¤æ˜¯å¦æœ‰çº æ­£è¡Œä¸º
                if i + 1 < len(trajectory):
                    next_step = str(trajectory[i + 1]).lower()
                    if any(fix_word in next_step for fix_word in 
                           ['fix', 'correct', 'ä¿®æ­£', 'çº æ­£', 'retry', 'é‡è¯•']):
                        correction_patterns['error_to_correction'].append({
                            'error_step': i,
                            'correction_step': i + 1,
                            'error_content': step_content[:200],
                            'correction_content': next_step[:200]
                        })
            
            # æ¨¡å¼2: é‡è¯•æ¨¡å¼
            if any(retry_word in step_content for retry_word in 
                   ['retry', 'try again', 'é‡è¯•', 'å†æ¬¡å°è¯•']):
                correction_patterns['retry_patterns'].append({
                    'step': i,
                    'content': step_content[:200]
                })
            
            # æ¨¡å¼3: ç­–ç•¥è°ƒæ•´
            if any(adjust_word in step_content for adjust_word in 
                   ['different approach', 'alternative', 'æ¢ä¸ªæ–¹æ³•', 'è°ƒæ•´ç­–ç•¥']):
                correction_patterns['strategy_adjustment'].append({
                    'step': i,
                    'content': step_content[:200]
                })
        
        return correction_patterns

    async def _llm_semantic_evaluation(
        self, 
        task_input: str, 
        trajectory_summary: str, 
        final_output: str, 
        tool_summary: str
    ) -> Dict[str, Any]:
        """ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰è¯„ä¼°"""
        
        prompt = self.evaluation_prompt_template.format(
            task_input=task_input[:500],  # é™åˆ¶é•¿åº¦é¿å…è¶…å‡ºtokené™åˆ¶
            trajectory_summary=trajectory_summary,
            final_output=final_output[:1000],
            tool_execution_summary=tool_summary
        )
        
        try:
            # ä½¿ç”¨LLMClientçš„_call_apiæ–¹æ³•
            response_data = await self.llm_client._call_api(
                messages=[{"role": "user", "content": prompt}],
                timeout=30  # å¿«é€Ÿå“åº”
            )
            
            # ğŸ”§ å…¼å®¹æ–°çš„è¿”å›æ ¼å¼ï¼šæå–contentå­—æ®µ
            if isinstance(response_data, dict):
                response = response_data.get('content', '')
            else:
                response = str(response_data)
            
            # å°è¯•è§£æJSONå“åº”
            try:
                # æå–JSONéƒ¨åˆ†
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    json_content = json_match.group(0)
                    evaluation = json.loads(json_content)
                    logger.debug(f"ğŸ§  LLMè¯­ä¹‰è¯„ä¼°ç»“æœ: {evaluation}")
                    return evaluation
                else:
                    logger.warning("âŒ LLMå“åº”ä¸­æœªæ‰¾åˆ°JSONæ ¼å¼")
                    return self._default_semantic_evaluation()
            except json.JSONDecodeError as e:
                logger.warning(f"âŒ JSONè§£æå¤±è´¥: {e}, å“åº”: {response[:200]}")
                return self._default_semantic_evaluation()
                
        except Exception as e:
            logger.error(f"âŒ LLMè¯­ä¹‰è¯„ä¼°è°ƒç”¨å¤±è´¥: {e}")
            return self._default_semantic_evaluation()

    def _default_semantic_evaluation(self) -> Dict[str, Any]:
        """é»˜è®¤è¯­ä¹‰è¯„ä¼°ç»“æœ"""
        return {
            "task_completed": False,
            "self_correction_detected": False,
            "success_evidence": "è¯­ä¹‰è¯„ä¼°ä¸å¯ç”¨",
            "confidence_score": 0.5,
            "reasoning": "LLMè¯­ä¹‰è¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åŸºå‡†"
        }

    def _rule_based_evaluation(
        self, 
        trajectory: List[Dict[str, Any]], 
        final_output: str, 
        tool_results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„è¾…åŠ©è¯„ä¼°"""
        
        evaluation = {
            'has_meaningful_output': False,
            'tool_success_rate': 0.0,
            'contains_answer_tags': False,
            'error_indicators_count': 0,
            'success_indicators_count': 0
        }
        
        # 1. æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ„ä¹‰çš„è¾“å‡º
        evaluation['has_meaningful_output'] = (
            len(final_output.strip()) > 20 and
            final_output.strip() != TaskExecutionConstants.NO_ACTION_PERFORMED
        )
        
        # 2. è®¡ç®—å·¥å…·æˆåŠŸç‡
        if tool_results:
            successful_tools = sum(1 for result in tool_results 
                                 if result.get('success', False))
            evaluation['tool_success_rate'] = successful_tools / len(tool_results)
        
        # 3. æ£€æŸ¥ç­”æ¡ˆæ ‡ç­¾
        answer_tag = TaskExecutionConstants.XML_TAGS['ANSWER']
        evaluation['contains_answer_tags'] = (
            f"<{answer_tag}>" in final_output or 
            f"</{answer_tag}>" in final_output or
            "\\boxed{" in final_output
        )
        
        # 4. ç»Ÿè®¡é”™è¯¯å’ŒæˆåŠŸæŒ‡ç¤ºè¯
        final_output_lower = final_output.lower()
        
        evaluation['error_indicators_count'] = sum(
            1 for indicator in TaskExecutionConstants.FAILURE_INDICATORS
            if indicator.lower() in final_output_lower
        )
        
        evaluation['success_indicators_count'] = sum(
            1 for indicator in TaskExecutionConstants.SUCCESS_INDICATORS
            if indicator.lower() in final_output_lower
        )
        
        return evaluation

    def _synthesize_evaluation(
        self,
        semantic_eval: Dict[str, Any],
        rule_eval: Dict[str, Any],
        correction_evidence: Dict[str, Any]
    ) -> TaskEvaluation:
        """ç»¼åˆæ‰€æœ‰è¯„ä¼°ä¿¡æ¯å¾—å‡ºæœ€ç»ˆåˆ¤å®š"""
        
        # åŸºç¡€ä¿¡æ¯
        llm_says_completed = semantic_eval.get('task_completed', False)
        llm_confidence = semantic_eval.get('confidence_score', 0.5)
        self_correction_detected = (
            semantic_eval.get('self_correction_detected', False) or
            any(len(patterns) > 0 for patterns in correction_evidence.values())
        )
        
        # è§„åˆ™æŒ‡æ ‡
        has_meaningful_output = rule_eval['has_meaningful_output']
        tool_success_rate = rule_eval['tool_success_rate']
        has_answer_tags = rule_eval['contains_answer_tags']
        error_count = rule_eval['error_indicators_count']
        success_count = rule_eval['success_indicators_count']
        
        # ç»¼åˆå†³ç­–é€»è¾‘
        if llm_says_completed and llm_confidence > 0.8:
            # LLMé«˜ç½®ä¿¡åº¦è®¤ä¸ºå®Œæˆ
            if self_correction_detected:
                outcome_type = TaskOutcomeType.CORRECTED_SUCCESS
                confidence = min(0.95, llm_confidence + 0.1)
            else:
                outcome_type = TaskOutcomeType.CLEAR_SUCCESS
                confidence = llm_confidence
                
        elif llm_says_completed and llm_confidence > 0.6:
            # LLMä¸­ç­‰ç½®ä¿¡åº¦è®¤ä¸ºå®Œæˆï¼Œéœ€è¦è§„åˆ™éªŒè¯
            if (has_meaningful_output and 
                (tool_success_rate > 0.5 or has_answer_tags) and
                success_count > error_count):
                
                if self_correction_detected:
                    outcome_type = TaskOutcomeType.CORRECTED_SUCCESS
                else:
                    outcome_type = TaskOutcomeType.CLEAR_SUCCESS
                confidence = (llm_confidence + tool_success_rate) / 2
            else:
                outcome_type = TaskOutcomeType.PARTIAL_SUCCESS
                confidence = 0.6
                
        elif tool_success_rate > 0.7 and has_meaningful_output and success_count > 0:
            # LLMä¸ç¡®å®šï¼Œä½†è§„åˆ™æŒ‡æ ‡è‰¯å¥½
            outcome_type = TaskOutcomeType.PARTIAL_SUCCESS
            confidence = 0.7
            
        elif has_meaningful_output and error_count == 0:
            # æœ‰è¾“å‡ºä¸”æ— æ˜æ˜¾é”™è¯¯
            outcome_type = TaskOutcomeType.PARTIAL_SUCCESS  
            confidence = 0.5
            
        else:
            # å…¶ä»–æƒ…å†µè§†ä¸ºå¤±è´¥
            if error_count > 2 or tool_success_rate < 0.3:
                outcome_type = TaskOutcomeType.COMPLETE_FAILURE
            else:
                outcome_type = TaskOutcomeType.PROCESS_FAILURE
            confidence = 0.3
        
        # æ„å»ºè¯æ®
        evidence = []
        if llm_says_completed:
            evidence.append(OutcomeEvidence(
                evidence_type="llm_semantic",
                content=semantic_eval.get('success_evidence', ''),
                confidence=llm_confidence,
                source_step=-1,
                timestamp=""
            ))
        
        if tool_success_rate > 0.5:
            evidence.append(OutcomeEvidence(
                evidence_type="tool_execution",
                content=f"å·¥å…·æˆåŠŸç‡: {tool_success_rate:.2f}",
                confidence=tool_success_rate,
                source_step=-1,
                timestamp=""
            ))
        
        # æ„å»ºæœ€ç»ˆè¯„ä¼°
        evaluation = TaskEvaluation(
            outcome_type=outcome_type,
            confidence_score=confidence,
            primary_evidence=evidence,
            final_output=semantic_eval.get('success_evidence', ''),
            correction_detected=self_correction_detected,
            semantic_reasoning=semantic_eval.get('reasoning', '')
        )
        
        return evaluation

    def _fallback_evaluation(
        self, 
        final_output: str, 
        tool_results: List[Dict[str, Any]]
    ) -> TaskEvaluation:
        """é™çº§è¯„ä¼° - å½“æ™ºèƒ½è¯„ä¼°å¤±è´¥æ—¶ä½¿ç”¨"""
        
        has_output = len(final_output.strip()) > 20
        tool_success_rate = 0.0
        
        if tool_results:
            successful = sum(1 for r in tool_results if r.get('success', False))
            tool_success_rate = successful / len(tool_results)
        
        if has_output and tool_success_rate > 0.5:
            outcome_type = TaskOutcomeType.PARTIAL_SUCCESS
            confidence = 0.6
        else:
            outcome_type = TaskOutcomeType.PROCESS_FAILURE
            confidence = 0.4
        
        return TaskEvaluation(
            outcome_type=outcome_type,
            confidence_score=confidence,
            primary_evidence=[],
            final_output=final_output,
            correction_detected=False,
            semantic_reasoning="ä½¿ç”¨é™çº§è¯„ä¼°é€»è¾‘"
        )


# ä¾¿æ·æ¥å£å‡½æ•°
async def intelligent_task_evaluation(
    llm_client,
    task_input: str,
    trajectory: List[Dict[str, Any]],
    final_output: str,
    tool_results: List[Dict[str, Any]]
) -> Tuple[bool, float, str]:
    """
    ä¾¿æ·çš„æ™ºèƒ½ä»»åŠ¡è¯„ä¼°æ¥å£
    
    Returns:
        Tuple[is_success, confidence, reasoning]
    """
    try:
        evaluator = IntelligentStatusEvaluator(llm_client)
        
        evaluation = await evaluator.evaluate_task_completion(
            task_input, trajectory, final_output, tool_results
        )
        
        is_success = evaluation.outcome_type in [
            TaskOutcomeType.CLEAR_SUCCESS,
            TaskOutcomeType.CORRECTED_SUCCESS,
            TaskOutcomeType.PARTIAL_SUCCESS
        ]
        
        return is_success, evaluation.confidence_score, evaluation.semantic_reasoning
        
    except Exception as e:
        logger.error(f"âŒ æ™ºèƒ½ä»»åŠ¡è¯„ä¼°å¤±è´¥: {e}")
        return False, 0.3, f"è¯„ä¼°å¼‚å¸¸: {str(e)}"