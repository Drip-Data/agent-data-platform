#!/usr/bin/env python3
"""
æ™ºèƒ½Tokenç®¡ç†å™¨ - é›†æˆGemini 2.5çœŸå®tokenè®¡æ•°APIå’Œä¸Šä¸‹æ–‡ä¼˜åŒ–
Intelligent Token Manager - Integrates real Gemini 2.5 token counting API and context optimization
"""

import asyncio
import json
import logging
import time
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime

from core.context_cache_manager import ContextCacheManager, CacheStrategy
from core.llm_providers.gemini_provider import GeminiProvider

logger = logging.getLogger(__name__)

@dataclass
class TokenUsageRecord:
    """Tokenä½¿ç”¨è®°å½•"""
    timestamp: float
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    model: str
    provider: str
    task_id: str
    session_id: Optional[str]
    cached_tokens: int = 0
    cache_hits: int = 0
    estimated_cost: float = 0.0
    actual_api_call: bool = True  # æ˜¯å¦ä½¿ç”¨çœŸå®APIè®¡æ•°

@dataclass
class TokenOptimizationStats:
    """Tokenä¼˜åŒ–ç»Ÿè®¡"""
    total_requests: int = 0
    total_tokens_saved: int = 0
    total_cost_saved: float = 0.0
    cache_hit_ratio: float = 0.0
    avg_tokens_per_request: float = 0.0
    optimization_efficiency: float = 0.0  # ä¼˜åŒ–æ•ˆç‡ç™¾åˆ†æ¯”

class IntelligentTokenManager:
    """
    æ™ºèƒ½Tokenç®¡ç†å™¨
    
    åŠŸèƒ½ï¼š
    1. ä½¿ç”¨GeminiçœŸå®APIè¿›è¡Œç²¾ç¡®tokenè®¡æ•°
    2. æ™ºèƒ½ä¸Šä¸‹æ–‡ç¼“å­˜å’Œå¤ç”¨
    3. åŠ¨æ€tokenæ¶ˆè€—ä¼˜åŒ–
    4. è¯¦ç»†çš„æˆæœ¬åˆ†æå’Œå»ºè®®
    5. å®æ—¶tokenæ¶ˆè€—ç›‘æ§å’Œé¢„è­¦
    """
    
    def __init__(self, gemini_provider: GeminiProvider, redis_manager=None,
                 cache_strategy: CacheStrategy = CacheStrategy.BALANCED,
                 token_budget_limit: int = 1000000):  # é»˜è®¤100ä¸‡tokené¢„ç®—
        """
        åˆå§‹åŒ–æ™ºèƒ½Tokenç®¡ç†å™¨
        
        Args:
            gemini_provider: Geminiæä¾›å•†å®ä¾‹
            redis_manager: Redisç®¡ç†å™¨
            cache_strategy: ç¼“å­˜ç­–ç•¥
            token_budget_limit: Tokené¢„ç®—é™åˆ¶
        """
        self.gemini_provider = gemini_provider
        self.cache_manager = ContextCacheManager(
            redis_manager=redis_manager,
            cache_strategy=cache_strategy
        )
        self.token_budget_limit = token_budget_limit
        
        # Tokenä½¿ç”¨è®°å½•
        self.usage_records: List[TokenUsageRecord] = []
        self.optimization_stats = TokenOptimizationStats()
        
        # å®æ—¶ç»Ÿè®¡
        self.session_tokens: Dict[str, int] = {}  # session_id -> total_tokens
        self.daily_tokens: Dict[str, int] = {}    # date -> total_tokens
        self.current_budget_used = 0
        
        # Gemini 2.5ç³»åˆ—å®šä»·é…ç½®ï¼ˆç¾å…ƒæ¯100ä¸‡tokenï¼‰
        self.pricing_config = {
            "gemini-2.5-pro": {
                "input": 1.25,
                "output": 10.0,
                "cache": 0.3125,
                "storage_per_hour": 4.50
            },
            "gemini-2.5-flash": {
                "input": 0.30,
                "output": 2.50,
                "cache": 0.075,
                "storage_per_hour": 1.0
            },
            "gemini-2.5-flash-lite": {
                "input": 0.10,
                "output": 0.40,
                "cache": 0.025,
                "storage_per_hour": 1.0
            }
        }
        
        logger.info(f"IntelligentTokenManager initialized: budget={token_budget_limit:,}, strategy={cache_strategy.value}")
    
    async def count_tokens_accurately(self, text: str, model: str = "gemini-2.5-flash") -> int:
        """
        ä½¿ç”¨GeminiçœŸå®APIç²¾ç¡®è®¡ç®—tokenæ•°é‡
        
        Args:
            text: è¦è®¡ç®—çš„æ–‡æœ¬
            model: ä½¿ç”¨çš„æ¨¡å‹
            
        Returns:
            ç²¾ç¡®çš„tokenæ•°é‡
        """
        try:
            # ä½¿ç”¨Gemini Providerçš„çœŸå®APIè®¡æ•°
            token_count = await self.gemini_provider.count_tokens(text, model)
            
            logger.debug(f"ğŸ“Š ç²¾ç¡®tokenè®¡æ•°: {token_count} tokens, æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
            return token_count
            
        except Exception as e:
            logger.error(f"ç²¾ç¡®tokenè®¡æ•°å¤±è´¥: {e}")
            # å›é€€åˆ°ä¼°ç®—
            return self._fallback_token_estimation(text)
    
    def _fallback_token_estimation(self, text: str) -> int:
        """å›é€€tokenä¼°ç®—æ–¹æ³•"""
        if not text:
            return 0
        
        # ä½¿ç”¨Providerçš„å›é€€æ–¹æ³•
        return self.gemini_provider._accurate_token_estimation_fallback(text)
    
    async def optimize_messages_with_cache(self, messages: List[Dict[str, Any]], 
                                         model: str = "gemini-2.5-flash",
                                         session_id: Optional[str] = None) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–æ¶ˆæ¯åˆ—è¡¨ï¼Œå‡å°‘tokenæ¶ˆè€—
        
        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
            model: ä½¿ç”¨çš„æ¨¡å‹
            session_id: ä¼šè¯ID
            
        Returns:
            (ä¼˜åŒ–åçš„æ¶ˆæ¯, ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯)
        """
        try:
            optimization_info = {
                "original_messages": len(messages),
                "original_tokens": 0,
                "optimized_tokens": 0,
                "tokens_saved": 0,
                "cost_saved": 0.0,
                "cache_operations": []
            }
            
            optimized_messages = []
            
            for i, message in enumerate(messages):
                content = message.get('content', '')
                if not content:
                    optimized_messages.append(message)
                    continue
                
                # è®¡ç®—åŸå§‹tokenæ•°
                original_tokens = await self.count_tokens_accurately(content, model)
                optimization_info["original_tokens"] += original_tokens
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç¼“å­˜
                should_cache = await self.cache_manager.should_cache_content(
                    content, f"session:{session_id}" if session_id else ""
                )
                
                if should_cache:
                    # å°è¯•ç¼“å­˜å†…å®¹
                    cache_id = await self.cache_manager.cache_content(content, model)
                    if cache_id:
                        # åˆ›å»ºä¼˜åŒ–çš„æ¶ˆæ¯
                        optimized_message = message.copy()
                        optimized_message['content'] = f"[CACHED:{cache_id}]"
                        optimized_message['_original_tokens'] = original_tokens
                        optimized_message['_cache_id'] = cache_id
                        
                        optimized_messages.append(optimized_message)
                        
                        # ç¼“å­˜åçš„tokenè®¡ç®—ï¼ˆå¼•ç”¨tokenå¾ˆå°‘ï¼‰
                        cached_reference_tokens = await self.count_tokens_accurately(
                            f"[CACHED:{cache_id}]", model
                        )
                        optimization_info["optimized_tokens"] += cached_reference_tokens
                        
                        saved_tokens = original_tokens - cached_reference_tokens
                        optimization_info["tokens_saved"] += saved_tokens
                        
                        # è®¡ç®—æˆæœ¬èŠ‚çœ
                        cost_saved = self._calculate_cost_savings(saved_tokens, model)
                        optimization_info["cost_saved"] += cost_saved
                        
                        optimization_info["cache_operations"].append({
                            "message_index": i,
                            "action": "cached",
                            "cache_id": cache_id,
                            "original_tokens": original_tokens,
                            "cached_tokens": cached_reference_tokens,
                            "tokens_saved": saved_tokens,
                            "cost_saved": cost_saved
                        })
                        
                        logger.debug(f"âœ… æ¶ˆæ¯{i}å·²ç¼“å­˜ï¼ŒèŠ‚çœ{saved_tokens}ä¸ªtoken")
                        continue
                
                # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç°æœ‰ç¼“å­˜
                optimized_msg, used_cache_ids = self.cache_manager.optimize_messages_for_cache([message])
                if used_cache_ids:
                    cached_tokens = await self.count_tokens_accurately(
                        optimized_msg[0]['content'], model
                    )
                    optimization_info["optimized_tokens"] += cached_tokens
                    
                    saved_tokens = original_tokens - cached_tokens
                    optimization_info["tokens_saved"] += saved_tokens
                    optimization_info["cost_saved"] += self._calculate_cost_savings(saved_tokens, model)
                    
                    optimized_messages.extend(optimized_msg)
                    
                    optimization_info["cache_operations"].append({
                        "message_index": i,
                        "action": "cache_hit",
                        "cache_ids": used_cache_ids,
                        "original_tokens": original_tokens,
                        "cached_tokens": cached_tokens,
                        "tokens_saved": saved_tokens
                    })
                    
                    logger.debug(f"ğŸ¯ æ¶ˆæ¯{i}ä½¿ç”¨ç°æœ‰ç¼“å­˜ï¼ŒèŠ‚çœ{saved_tokens}ä¸ªtoken")
                else:
                    # æ²¡æœ‰ç¼“å­˜ä¼˜åŒ–ï¼Œä¿æŒåŸæ ·
                    optimized_messages.append(message)
                    optimization_info["optimized_tokens"] += original_tokens
            
            # è®¡ç®—ä¼˜åŒ–æ•ˆç‡
            if optimization_info["original_tokens"] > 0:
                optimization_info["optimization_ratio"] = (
                    optimization_info["tokens_saved"] / optimization_info["original_tokens"]
                )
            else:
                optimization_info["optimization_ratio"] = 0.0
            
            logger.info(f"ğŸš€ æ¶ˆæ¯ä¼˜åŒ–å®Œæˆ: åŸå§‹{optimization_info['original_tokens']:,}token â†’ "
                       f"ä¼˜åŒ–å{optimization_info['optimized_tokens']:,}token, "
                       f"èŠ‚çœ{optimization_info['tokens_saved']:,}token "
                       f"({optimization_info['optimization_ratio']:.1%})")
            
            return optimized_messages, optimization_info
            
        except Exception as e:
            logger.error(f"æ¶ˆæ¯ä¼˜åŒ–å¤±è´¥: {e}")
            return messages, {"error": str(e)}
    
    def _calculate_cost_savings(self, tokens_saved: int, model: str) -> float:
        """è®¡ç®—èŠ‚çœçš„æˆæœ¬"""
        try:
            pricing = self.pricing_config.get(model, self.pricing_config["gemini-2.5-flash"])
            input_cost_per_million = pricing["input"]
            
            cost_saved = (tokens_saved / 1_000_000) * input_cost_per_million
            return cost_saved
            
        except Exception as e:
            logger.warning(f"è®¡ç®—æˆæœ¬èŠ‚çœå¤±è´¥: {e}")
            return 0.0
    
    async def record_token_usage(self, prompt_tokens: int, completion_tokens: int,
                               model: str, provider: str = "gemini",
                               task_id: str = "", session_id: Optional[str] = None,
                               cached_tokens: int = 0, cache_hits: int = 0) -> str:
        """
        è®°å½•tokenä½¿ç”¨æƒ…å†µ
        
        Args:
            prompt_tokens: è¾“å…¥tokenæ•°
            completion_tokens: è¾“å‡ºtokenæ•°
            model: ä½¿ç”¨çš„æ¨¡å‹
            provider: æä¾›å•†
            task_id: ä»»åŠ¡ID
            session_id: ä¼šè¯ID
            cached_tokens: ç¼“å­˜tokenæ•°
            cache_hits: ç¼“å­˜å‘½ä¸­æ¬¡æ•°
            
        Returns:
            è®°å½•ID
        """
        try:
            total_tokens = prompt_tokens + completion_tokens
            estimated_cost = self._calculate_request_cost(
                prompt_tokens, completion_tokens, model, cached_tokens
            )
            
            # åˆ›å»ºä½¿ç”¨è®°å½•
            record = TokenUsageRecord(
                timestamp=time.time(),
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens,
                model=model,
                provider=provider,
                task_id=task_id,
                session_id=session_id,
                cached_tokens=cached_tokens,
                cache_hits=cache_hits,
                estimated_cost=estimated_cost,
                actual_api_call=True
            )
            
            # æ·»åŠ åˆ°è®°å½•åˆ—è¡¨
            self.usage_records.append(record)
            
            # æ›´æ–°ç»Ÿè®¡
            self.current_budget_used += total_tokens
            
            if session_id:
                self.session_tokens[session_id] = self.session_tokens.get(session_id, 0) + total_tokens
            
            date_key = datetime.now().strftime("%Y-%m-%d")
            self.daily_tokens[date_key] = self.daily_tokens.get(date_key, 0) + total_tokens
            
            # æ›´æ–°ä¼˜åŒ–ç»Ÿè®¡
            self.optimization_stats.total_requests += 1
            if cached_tokens > 0:
                self.optimization_stats.total_tokens_saved += cached_tokens
                self.optimization_stats.total_cost_saved += self._calculate_cost_savings(cached_tokens, model)
            
            if cache_hits > 0:
                hit_ratio = cache_hits / (cache_hits + 1)  # ç®€åŒ–è®¡ç®—
                self.optimization_stats.cache_hit_ratio = (
                    (self.optimization_stats.cache_hit_ratio * (self.optimization_stats.total_requests - 1) + hit_ratio) 
                    / self.optimization_stats.total_requests
                )
            
            # æ£€æŸ¥é¢„ç®—é™åˆ¶
            if self.current_budget_used > self.token_budget_limit * 0.9:  # 90%é¢„è­¦
                logger.warning(f"âš ï¸ Tokené¢„ç®—æ¥è¿‘é™åˆ¶: {self.current_budget_used:,}/{self.token_budget_limit:,} "
                             f"({self.current_budget_used/self.token_budget_limit:.1%})")
            
            record_id = f"token_record_{int(record.timestamp)}_{task_id[:8]}"
            
            logger.info(f"ğŸ“Š Tokenä½¿ç”¨å·²è®°å½•: {total_tokens:,}token, æˆæœ¬: ${estimated_cost:.6f}, "
                       f"ç¼“å­˜èŠ‚çœ: {cached_tokens:,}token")
            
            return record_id
            
        except Exception as e:
            logger.error(f"è®°å½•tokenä½¿ç”¨å¤±è´¥: {e}")
            return ""
    
    def _calculate_request_cost(self, prompt_tokens: int, completion_tokens: int, 
                              model: str, cached_tokens: int = 0) -> float:
        """è®¡ç®—è¯·æ±‚æˆæœ¬"""
        try:
            pricing = self.pricing_config.get(model, self.pricing_config["gemini-2.5-flash"])
            
            # è®¡ç®—å®é™…è¾“å…¥tokenæˆæœ¬ï¼ˆå‡å»ç¼“å­˜éƒ¨åˆ†ï¼‰
            actual_input_tokens = max(0, prompt_tokens - cached_tokens)
            input_cost = (actual_input_tokens / 1_000_000) * pricing["input"]
            
            # ç¼“å­˜æˆæœ¬
            cache_cost = (cached_tokens / 1_000_000) * pricing["cache"]
            
            # è¾“å‡ºæˆæœ¬
            output_cost = (completion_tokens / 1_000_000) * pricing["output"]
            
            total_cost = input_cost + cache_cost + output_cost
            return total_cost
            
        except Exception as e:
            logger.warning(f"è®¡ç®—è¯·æ±‚æˆæœ¬å¤±è´¥: {e}")
            return 0.0
    
    async def get_optimization_recommendations(self) -> Dict[str, Any]:
        """è·å–tokenä¼˜åŒ–å»ºè®®"""
        try:
            recommendations = {
                "current_status": {
                    "budget_used": self.current_budget_used,
                    "budget_limit": self.token_budget_limit,
                    "usage_percentage": self.current_budget_used / self.token_budget_limit * 100,
                    "total_requests": len(self.usage_records)
                },
                "optimization_stats": asdict(self.optimization_stats),
                "recommendations": []
            }
            
            # åˆ†æè¿‘æœŸä½¿ç”¨æ¨¡å¼
            recent_records = self.usage_records[-100:]  # æœ€è¿‘100æ¬¡è¯·æ±‚
            if recent_records:
                avg_tokens = sum(r.total_tokens for r in recent_records) / len(recent_records)
                avg_cost = sum(r.estimated_cost for r in recent_records) / len(recent_records)
                
                recommendations["recent_analysis"] = {
                    "avg_tokens_per_request": avg_tokens,
                    "avg_cost_per_request": avg_cost,
                    "cache_usage_rate": sum(1 for r in recent_records if r.cached_tokens > 0) / len(recent_records)
                }
                
                # ç”Ÿæˆå»ºè®®
                if avg_tokens > 10000:
                    recommendations["recommendations"].append({
                        "type": "high_token_usage",
                        "message": f"å¹³å‡è¯·æ±‚ä½¿ç”¨{avg_tokens:.0f}tokenï¼Œå»ºè®®å¯ç”¨æ›´ç§¯æçš„ç¼“å­˜ç­–ç•¥",
                        "action": "è€ƒè™‘ä½¿ç”¨AGGRESSIVEç¼“å­˜ç­–ç•¥"
                    })
                
                cache_rate = recommendations["recent_analysis"]["cache_usage_rate"]
                if cache_rate < 0.3:
                    recommendations["recommendations"].append({
                        "type": "low_cache_usage",
                        "message": f"ç¼“å­˜ä½¿ç”¨ç‡ä»…{cache_rate:.1%}ï¼Œå¯ä¼˜åŒ–ç©ºé—´è¾ƒå¤§",
                        "action": "æ£€æŸ¥å†…å®¹é‡å¤æ€§ï¼Œä¼˜åŒ–ç¼“å­˜ç­–ç•¥"
                    })
                
                if self.current_budget_used > self.token_budget_limit * 0.8:
                    recommendations["recommendations"].append({
                        "type": "budget_warning",
                        "message": f"å·²ä½¿ç”¨{self.current_budget_used/self.token_budget_limit:.1%}é¢„ç®—",
                        "action": "è€ƒè™‘ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹æˆ–å¢åŠ é¢„ç®—"
                    })
            
            # æ¨¡å‹æˆæœ¬åˆ†æ
            model_usage = {}
            for record in recent_records:
                if record.model not in model_usage:
                    model_usage[record.model] = {"count": 0, "total_cost": 0.0}
                model_usage[record.model]["count"] += 1
                model_usage[record.model]["total_cost"] += record.estimated_cost
            
            if model_usage:
                most_expensive = max(model_usage.keys(), 
                                   key=lambda k: model_usage[k]["total_cost"])
                
                recommendations["model_analysis"] = {
                    "most_used_model": max(model_usage.keys(), 
                                         key=lambda k: model_usage[k]["count"]),
                    "most_expensive_model": most_expensive,
                    "model_breakdown": model_usage
                }
                
                # å¦‚æœä½¿ç”¨æ˜‚è´µæ¨¡å‹ï¼Œå»ºè®®ä¼˜åŒ–
                if most_expensive.endswith("-pro"):
                    recommendations["recommendations"].append({
                        "type": "expensive_model",
                        "message": f"ä¸»è¦ä½¿ç”¨{most_expensive}ï¼Œæˆæœ¬è¾ƒé«˜",
                        "action": "è€ƒè™‘å¯¹ç®€å•ä»»åŠ¡ä½¿ç”¨flashæˆ–flash-liteæ¨¡å‹"
                    })
            
            return recommendations
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆä¼˜åŒ–å»ºè®®å¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def get_detailed_statistics(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†çš„tokenä½¿ç”¨ç»Ÿè®¡"""
        try:
            stats = {
                "summary": {
                    "total_requests": len(self.usage_records),
                    "total_tokens": sum(r.total_tokens for r in self.usage_records),
                    "total_cost": sum(r.estimated_cost for r in self.usage_records),
                    "total_cached_tokens": sum(r.cached_tokens for r in self.usage_records),
                    "current_budget_used": self.current_budget_used,
                    "budget_remaining": max(0, self.token_budget_limit - self.current_budget_used)
                },
                "optimization": asdict(self.optimization_stats),
                "daily_usage": self.daily_tokens,
                "session_usage": dict(list(self.session_tokens.items())[:10]),  # å‰10ä¸ªä¼šè¯
                "cache_stats": await self.cache_manager.get_cache_statistics()
            }
            
            # æ—¶é—´æ®µåˆ†æ
            if self.usage_records:
                recent_24h = [r for r in self.usage_records 
                             if time.time() - r.timestamp < 24 * 3600]
                recent_1h = [r for r in self.usage_records 
                            if time.time() - r.timestamp < 3600]
                
                stats["time_analysis"] = {
                    "last_24h_requests": len(recent_24h),
                    "last_24h_tokens": sum(r.total_tokens for r in recent_24h),
                    "last_1h_requests": len(recent_1h),
                    "last_1h_tokens": sum(r.total_tokens for r in recent_1h)
                }
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–è¯¦ç»†ç»Ÿè®¡å¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def reset_budget(self, new_limit: int):
        """é‡ç½®tokené¢„ç®—"""
        try:
            old_limit = self.token_budget_limit
            self.token_budget_limit = new_limit
            self.current_budget_used = 0
            
            logger.info(f"Tokené¢„ç®—å·²é‡ç½®: {old_limit:,} â†’ {new_limit:,}")
            
        except Exception as e:
            logger.error(f"é‡ç½®é¢„ç®—å¤±è´¥: {e}")
    
    async def cleanup_old_records(self, days_to_keep: int = 30):
        """æ¸…ç†æ—§çš„ä½¿ç”¨è®°å½•"""
        try:
            cutoff_time = time.time() - (days_to_keep * 24 * 3600)
            
            old_count = len(self.usage_records)
            self.usage_records = [r for r in self.usage_records if r.timestamp > cutoff_time]
            new_count = len(self.usage_records)
            
            cleaned = old_count - new_count
            if cleaned > 0:
                logger.info(f"æ¸…ç†äº†{cleaned}æ¡{days_to_keep}å¤©å‰çš„tokenä½¿ç”¨è®°å½•")
            
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§è®°å½•å¤±è´¥: {e}")