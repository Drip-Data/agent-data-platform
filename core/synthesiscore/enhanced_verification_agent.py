#!/usr/bin/env python3
"""
Enhanced Verification Agent - å¢å¼ºçš„å¤šç»´åº¦ä»»åŠ¡éªŒè¯å™¨
åŸºäºTaskCraftçš„è´¨é‡æ§åˆ¶ä½“ç³»ï¼Œå®ç°å¤šç»´åº¦ä»»åŠ¡è´¨é‡è¯„ä¼°
"""

import asyncio
import json
import logging
import re
from typing import Dict, List, Optional, Any, Union
from dataclasses import asdict

from core.llm_client import LLMClient
from .enhanced_interfaces import (
    AtomicTask, ExtendedTask, CompositeTask, TaskVerificationMetrics,
    TaskUnion, TaskType, TaskDifficulty
)

logger = logging.getLogger(__name__)


class EnhancedVerificationAgent:
    """å¢å¼ºéªŒè¯ä»£ç† - å¤šç»´åº¦ä»»åŠ¡è´¨é‡è¯„ä¼°"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
        
        # éªŒè¯æç¤ºè¯æ¨¡æ¿
        self.verification_prompts = {
            "executability": self._get_executability_prompt(),
            "difficulty": self._get_difficulty_prompt(),
            "answer_uniqueness": self._get_uniqueness_prompt(),
            "tool_requirements": self._get_tool_requirements_prompt(),
            "language_quality": self._get_language_quality_prompt(),
            "cognitive_complexity": self._get_cognitive_complexity_prompt(),
            "atomicity": self._get_atomicity_prompt()
        }
    
    async def verify_task(self, task: TaskUnion) -> TaskVerificationMetrics:
        """æ‰§è¡Œå¤šç»´åº¦ä»»åŠ¡éªŒè¯"""
        logger.info(f"ğŸ” å¼€å§‹å¤šç»´åº¦éªŒè¯ä»»åŠ¡: {task.task_id}")
        
        try:
            # åˆ›å»ºéªŒè¯æŒ‡æ ‡å¯¹è±¡
            metrics = TaskVerificationMetrics(task_id=task.task_id)
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ç»´åº¦çš„éªŒè¯
            verification_tasks = []
            
            # åŸºç¡€ç»´åº¦éªŒè¯ï¼ˆæ‰€æœ‰ä»»åŠ¡ç±»å‹ï¼‰
            basic_dimensions = [
                "executability", "difficulty", "answer_uniqueness", 
                "tool_requirements", "language_quality", "cognitive_complexity"
            ]
            
            for dimension in basic_dimensions:
                verification_tasks.append(
                    self._verify_dimension(task, dimension)
                )
            
            # åŸå­æ€§éªŒè¯ï¼ˆä»…é€‚ç”¨äºåŸå­ä»»åŠ¡ï¼‰
            if isinstance(task, AtomicTask):
                verification_tasks.append(
                    self._verify_dimension(task, "atomicity")
                )
            
            # æ‰§è¡Œå¹¶å‘éªŒè¯
            results = await asyncio.gather(*verification_tasks, return_exceptions=True)
            
            # å¤„ç†éªŒè¯ç»“æœ
            for i, result in enumerate(results):
                if isinstance(result, tuple):
                    dimension, score, feedback = result
                    metrics.verification_dimensions[dimension] = score
                    if feedback:
                        metrics.detailed_feedback.append(f"{dimension}: {feedback}")
                elif isinstance(result, Exception):
                    logger.error(f"âŒ ç»´åº¦éªŒè¯å¤±è´¥: {result}")
                    # å¯¹å¤±è´¥çš„ç»´åº¦ä½¿ç”¨é»˜è®¤åˆ†æ•°
                    dimension = basic_dimensions[i] if i < len(basic_dimensions) else "atomicity"
                    metrics.verification_dimensions[dimension] = 0.5
            
            # è®¡ç®—æ€»ä½“åˆ†æ•°
            overall_score = metrics.calculate_overall_score()
            
            logger.info(f"âœ… ä»»åŠ¡éªŒè¯å®Œæˆ: {task.task_id} (æ€»åˆ†: {overall_score:.3f})")
            return metrics
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡éªŒè¯å¤±è´¥ {task.task_id}: {e}")
            # è¿”å›å¤±è´¥çš„éªŒè¯ç»“æœ
            metrics = TaskVerificationMetrics(task_id=task.task_id)
            metrics.detailed_feedback.append(f"éªŒè¯è¿‡ç¨‹å¼‚å¸¸: {str(e)}")
            return metrics
    
    async def _verify_dimension(self, task: TaskUnion, dimension: str) -> tuple[str, float, str]:
        """éªŒè¯å•ä¸ªç»´åº¦"""
        try:
            prompt_template = self.verification_prompts[dimension]
            prompt = self._format_verification_prompt(task, prompt_template, dimension)
            
            response = await self.llm_client.generate_enhanced_reasoning(
                task_description=prompt,
                available_tools=[],
                tool_descriptions="",
                execution_context={"mode": f"verification_{dimension}"}
            )
            
            # è§£æéªŒè¯ç»“æœ
            score, feedback = self._parse_verification_response(response, dimension)
            return dimension, score, feedback
            
        except Exception as e:
            logger.error(f"âŒ ç»´åº¦ {dimension} éªŒè¯å¤±è´¥: {e}")
            return dimension, 0.5, f"éªŒè¯å¤±è´¥: {str(e)}"
    
    def _format_verification_prompt(self, task: TaskUnion, template: str, dimension: str) -> str:
        """æ ¼å¼åŒ–éªŒè¯æç¤ºè¯"""
        
        # æå–ä»»åŠ¡ä¿¡æ¯
        task_info = {
            "question": getattr(task, 'question', ''),
            "answer": getattr(task, 'golden_answer', '') or getattr(task, 'golden_answers', []),
            "tools": getattr(task, 'required_tools', []) or getattr(task, 'expected_tools', []),
            "task_type": task.task_type.value if hasattr(task, 'task_type') else 'unknown',
            "difficulty": task.difficulty_level.value if hasattr(task, 'difficulty_level') else 'unknown'
        }
        
        # é’ˆå¯¹ä¸åŒä»»åŠ¡ç±»å‹çš„ç‰¹æ®Šå¤„ç†
        if isinstance(task, ExtendedTask):
            task_info.update({
                "hop_level": task.hop_level,
                "source_task": task.source_atomic_task,
                "complexity_score": getattr(task, 'complexity_score', 0.0)
            })
        elif isinstance(task, CompositeTask):
            task_info.update({
                "source_tasks": task.source_atomic_tasks,
                "original_questions": task.original_questions,
                "merge_strategy": task.merge_strategy
            })
        
        return template.format(**task_info)
    
    def _parse_verification_response(self, response: Dict[str, Any], dimension: str) -> tuple[float, str]:
        """è§£æéªŒè¯å“åº”"""
        try:
            thinking = response.get('thinking', '')
            
            # å°è¯•è§£æJSONæ ¼å¼çš„å“åº”
            if thinking.strip().startswith('{'):
                result = json.loads(thinking)
                score = result.get('score', 0.5)
                feedback = result.get('feedback', result.get('reasoning', ''))
            else:
                # å°è¯•ä»æ–‡æœ¬ä¸­æå–åˆ†æ•°
                score_match = re.search(r'(?:score|åˆ†æ•°)[:ï¼š]\s*(\d+\.?\d*)', thinking, re.IGNORECASE)
                if score_match:
                    score = float(score_match.group(1))
                    # å¦‚æœåˆ†æ•° > 1ï¼Œå‡è®¾æ˜¯ç™¾åˆ†åˆ¶ï¼Œè½¬æ¢ä¸ºå°æ•°
                    if score > 1:
                        score = score / 100.0
                else:
                    # æ ¹æ®å…³é”®è¯åˆ¤æ–­åˆ†æ•°
                    score = self._extract_score_from_keywords(thinking)
                
                feedback = thinking.strip()
            
            # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
            score = max(0.0, min(1.0, score))
            
            return score, feedback
            
        except (json.JSONDecodeError, ValueError, Exception) as e:
            logger.error(f"âŒ è§£æ {dimension} éªŒè¯å“åº”å¤±è´¥: {e}")
            return 0.5, f"è§£æå¤±è´¥: {str(e)}"
    
    def _extract_score_from_keywords(self, text: str) -> float:
        """ä»å…³é”®è¯ä¸­æå–åˆ†æ•°"""
        text_lower = text.lower()
        
        # ä¼˜ç§€å…³é”®è¯
        excellent_keywords = ['excellent', 'outstanding', 'ä¼˜ç§€', 'å‡ºè‰²', 'éå¸¸å¥½', 'å®Œç¾']
        good_keywords = ['good', 'satisfactory', 'è‰¯å¥½', 'ä¸é”™', 'åˆé€‚', 'å¯ä»¥']
        average_keywords = ['average', 'acceptable', 'ä¸€èˆ¬', 'æ™®é€š', 'ä¸­ç­‰', 'è¿˜è¡Œ']
        poor_keywords = ['poor', 'unsatisfactory', 'å·®', 'ä¸å¥½', 'ä¸åˆé€‚', 'é—®é¢˜']
        
        if any(keyword in text_lower for keyword in excellent_keywords):
            return 0.9
        elif any(keyword in text_lower for keyword in good_keywords):
            return 0.75
        elif any(keyword in text_lower for keyword in average_keywords):
            return 0.6
        elif any(keyword in text_lower for keyword in poor_keywords):
            return 0.3
        else:
            return 0.5  # é»˜è®¤åˆ†æ•°
    
    # å„ç»´åº¦çš„éªŒè¯æç¤ºè¯æ¨¡æ¿
    def _get_executability_prompt(self) -> str:
        return """
è¯„ä¼°ä»¥ä¸‹ä»»åŠ¡çš„å¯æ‰§è¡Œæ€§ï¼š

ä»»åŠ¡ç±»å‹: {task_type}
é—®é¢˜: {question}
é¢„æœŸç­”æ¡ˆ: {answer}
æ‰€éœ€å·¥å…·: {tools}
éš¾åº¦çº§åˆ«: {difficulty}

è¯„ä¼°æ ‡å‡†:
1. ä»»åŠ¡æè¿°æ˜¯å¦æ¸…æ™°æ˜ç¡®ï¼Ÿ
2. æ‰€éœ€å·¥å…·æ˜¯å¦è¶³å¤Ÿå’Œåˆé€‚ï¼Ÿ
3. ä»»åŠ¡æ˜¯å¦æœ‰æ˜ç¡®çš„æ‰§è¡Œè·¯å¾„ï¼Ÿ
4. ç­”æ¡ˆæ˜¯å¦å¯éªŒè¯ï¼Ÿ
5. æ˜¯å¦å­˜åœ¨æŠ€æœ¯æˆ–é€»è¾‘éšœç¢ï¼Ÿ

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "score": 0.0-1.0,
    "feedback": "è¯¦ç»†è¯„ä¼°ç†ç”±",
    "executable": true/false,
    "potential_issues": ["é—®é¢˜1", "é—®é¢˜2"]
}}
"""
    
    def _get_difficulty_prompt(self) -> str:
        return """
è¯„ä¼°ä»¥ä¸‹ä»»åŠ¡çš„éš¾åº¦æ˜¯å¦é€‚ä¸­ï¼š

ä»»åŠ¡ç±»å‹: {task_type}
é—®é¢˜: {question}
é¢„æœŸç­”æ¡ˆ: {answer}
æ‰€éœ€å·¥å…·: {tools}
å½“å‰éš¾åº¦çº§åˆ«: {difficulty}

è¯„ä¼°æ ‡å‡†:
1. ä»»åŠ¡å¤æ‚åº¦æ˜¯å¦ä¸ç›®æ ‡ç”¨æˆ·åŒ¹é…ï¼Ÿ
2. æ­¥éª¤æ•°é‡æ˜¯å¦åˆç†ï¼Ÿ
3. è®¤çŸ¥è´Ÿè·æ˜¯å¦é€‚ä¸­ï¼Ÿ
4. å·¥å…·ä½¿ç”¨éš¾åº¦æ˜¯å¦åˆé€‚ï¼Ÿ
5. æ˜¯å¦è¿‡äºç®€å•æˆ–è¿‡äºå¤æ‚ï¼Ÿ

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "score": 0.0-1.0,
    "feedback": "éš¾åº¦è¯„ä¼°è¯´æ˜",
    "suggested_difficulty": "simple/medium/complex",
    "complexity_factors": ["å› ç´ 1", "å› ç´ 2"]
}}
"""
    
    def _get_uniqueness_prompt(self) -> str:
        return """
è¯„ä¼°ä»¥ä¸‹ä»»åŠ¡ç­”æ¡ˆçš„å”¯ä¸€æ€§ï¼š

é—®é¢˜: {question}
é¢„æœŸç­”æ¡ˆ: {answer}
ä»»åŠ¡ç±»å‹: {task_type}

è¯„ä¼°æ ‡å‡†:
1. ç­”æ¡ˆæ˜¯å¦å”¯ä¸€ä¸”æ˜ç¡®ï¼Ÿ
2. æ˜¯å¦å­˜åœ¨å¤šç§å¯èƒ½çš„æ­£ç¡®ç­”æ¡ˆï¼Ÿ
3. ç­”æ¡ˆçš„è¡¨è¿°æ˜¯å¦æ¸…æ™°ï¼Ÿ
4. æ˜¯å¦å®¹æ˜“äº§ç”Ÿæ­§ä¹‰ï¼Ÿ
5. éªŒè¯æ ‡å‡†æ˜¯å¦æ˜ç¡®ï¼Ÿ

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "score": 0.0-1.0,
    "feedback": "å”¯ä¸€æ€§è¯„ä¼°",
    "is_unique": true/false,
    "ambiguity_risk": "low/medium/high"
}}
"""
    
    def _get_tool_requirements_prompt(self) -> str:
        return """
è¯„ä¼°ä»¥ä¸‹ä»»åŠ¡çš„å·¥å…·éœ€æ±‚æ˜¯å¦å‡†ç¡®ï¼š

é—®é¢˜: {question}
é¢„æœŸç­”æ¡ˆ: {answer}
å½“å‰å·¥å…·åˆ—è¡¨: {tools}
ä»»åŠ¡ç±»å‹: {task_type}

è¯„ä¼°æ ‡å‡†:
1. åˆ—å‡ºçš„å·¥å…·æ˜¯å¦éƒ½æ˜¯å¿…éœ€çš„ï¼Ÿ
2. æ˜¯å¦é—æ¼äº†å¿…è¦çš„å·¥å…·ï¼Ÿ
3. å·¥å…·ç»„åˆæ˜¯å¦åˆç†ï¼Ÿ
4. å·¥å…·çš„ä½¿ç”¨é¡ºåºæ˜¯å¦æ¸…æ™°ï¼Ÿ
5. æ˜¯å¦å­˜åœ¨æ›´å¥½çš„å·¥å…·æ›¿ä»£æ–¹æ¡ˆï¼Ÿ

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "score": 0.0-1.0,
    "feedback": "å·¥å…·éœ€æ±‚è¯„ä¼°",
    "missing_tools": ["å·¥å…·1", "å·¥å…·2"],
    "unnecessary_tools": ["å·¥å…·3"],
    "optimal_tools": ["æ¨èå·¥å…·åˆ—è¡¨"]
}}
"""
    
    def _get_language_quality_prompt(self) -> str:
        return """
è¯„ä¼°ä»¥ä¸‹ä»»åŠ¡çš„è¯­è¨€è´¨é‡ï¼š

é—®é¢˜: {question}
é¢„æœŸç­”æ¡ˆ: {answer}
ä»»åŠ¡ç±»å‹: {task_type}

è¯„ä¼°æ ‡å‡†:
1. è¯­è¨€è¡¨è¾¾æ˜¯å¦æ¸…æ™°å‡†ç¡®ï¼Ÿ
2. è¯­æ³•æ˜¯å¦æ­£ç¡®ï¼Ÿ
3. ä¸“ä¸šæœ¯è¯­ä½¿ç”¨æ˜¯å¦æ°å½“ï¼Ÿ
4. æè¿°æ˜¯å¦ç®€æ´æ˜äº†ï¼Ÿ
5. æ˜¯å¦æ˜“äºç†è§£ï¼Ÿ

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "score": 0.0-1.0,
    "feedback": "è¯­è¨€è´¨é‡è¯„ä¼°",
    "clarity": "high/medium/low",
    "grammar_issues": ["é—®é¢˜1", "é—®é¢˜2"],
    "suggestions": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"]
}}
"""
    
    def _get_cognitive_complexity_prompt(self) -> str:
        return """
è¯„ä¼°ä»¥ä¸‹ä»»åŠ¡çš„è®¤çŸ¥å¤æ‚åº¦ï¼š

é—®é¢˜: {question}
é¢„æœŸç­”æ¡ˆ: {answer}
æ‰€éœ€å·¥å…·: {tools}
ä»»åŠ¡ç±»å‹: {task_type}

è¯„ä¼°æ ‡å‡†:
1. éœ€è¦å¤šå°‘æ­¥æ¨ç†ï¼Ÿ
2. æ˜¯å¦éœ€è¦å¤æ‚çš„é€»è¾‘æ€ç»´ï¼Ÿ
3. ä¿¡æ¯æ•´åˆçš„éš¾åº¦å¦‚ä½•ï¼Ÿ
4. æ˜¯å¦éœ€è¦é¢†åŸŸä¸“çŸ¥è¯†ï¼Ÿ
5. è®¤çŸ¥è´Ÿè·æ˜¯å¦åˆç†ï¼Ÿ

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "score": 0.0-1.0,
    "feedback": "è®¤çŸ¥å¤æ‚åº¦è¯„ä¼°",
    "reasoning_steps": æ•°å­—,
    "cognitive_load": "low/medium/high",
    "knowledge_domains": ["é¢†åŸŸ1", "é¢†åŸŸ2"]
}}
"""
    
    def _get_atomicity_prompt(self) -> str:
        return """
è¯„ä¼°ä»¥ä¸‹åŸå­ä»»åŠ¡çš„åŸå­æ€§ï¼š

é—®é¢˜: {question}
é¢„æœŸç­”æ¡ˆ: {answer}
æ‰€éœ€å·¥å…·: {tools}

è¯„ä¼°æ ‡å‡†:
1. ä»»åŠ¡æ˜¯å¦å¯ä»¥è¿›ä¸€æ­¥åˆ†è§£ï¼Ÿ
2. æ˜¯å¦åªå…³æ³¨ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼Ÿ
3. æ‰§è¡Œæ­¥éª¤æ˜¯å¦è¶³å¤Ÿç®€å•ï¼Ÿ
4. æ˜¯å¦é¿å…äº†å¤šä¸ªç‹¬ç«‹çš„å­ç›®æ ‡ï¼Ÿ
5. æ˜¯å¦ç¬¦åˆåŸå­ä»»åŠ¡çš„å®šä¹‰ï¼Ÿ

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "score": 0.0-1.0,
    "feedback": "åŸå­æ€§è¯„ä¼°",
    "is_atomic": true/false,
    "decomposition_suggestions": ["å¯èƒ½çš„åˆ†è§£æ–¹æ¡ˆ"],
    "atomic_level": "high/medium/low"
}}
"""


class BatchVerificationProcessor:
    """æ‰¹é‡éªŒè¯å¤„ç†å™¨"""
    
    def __init__(self, verification_agent: EnhancedVerificationAgent):
        self.verification_agent = verification_agent
    
    async def batch_verify_tasks(self, tasks: List[TaskUnion], 
                                max_concurrent: int = 5) -> List[TaskVerificationMetrics]:
        """æ‰¹é‡éªŒè¯ä»»åŠ¡"""
        logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡éªŒè¯ {len(tasks)} ä¸ªä»»åŠ¡")
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def verify_with_semaphore(task):
            async with semaphore:
                return await self.verification_agent.verify_task(task)
        
        try:
            results = await asyncio.gather(
                *[verify_with_semaphore(task) for task in tasks],
                return_exceptions=True
            )
            
            valid_results = []
            for i, result in enumerate(results):
                if isinstance(result, TaskVerificationMetrics):
                    valid_results.append(result)
                elif isinstance(result, Exception):
                    logger.error(f"âŒ ä»»åŠ¡ {tasks[i].task_id} éªŒè¯å¼‚å¸¸: {result}")
                    # åˆ›å»ºå¤±è´¥çš„éªŒè¯ç»“æœ
                    failed_metrics = TaskVerificationMetrics(task_id=tasks[i].task_id)
                    failed_metrics.detailed_feedback.append(f"éªŒè¯å¼‚å¸¸: {str(result)}")
                    valid_results.append(failed_metrics)
            
            # ç»Ÿè®¡éªŒè¯ç»“æœ
            passed_count = sum(1 for r in valid_results if r.verification_passed)
            logger.info(f"âœ… æ‰¹é‡éªŒè¯å®Œæˆ: {passed_count}/{len(valid_results)} é€šè¿‡éªŒè¯")
            
            return valid_results
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡éªŒè¯å¤±è´¥: {e}")
            return []
    
    def analyze_verification_results(self, results: List[TaskVerificationMetrics]) -> Dict[str, Any]:
        """åˆ†æéªŒè¯ç»“æœ"""
        if not results:
            return {"total_tasks": 0}
        
        # åŸºç¡€ç»Ÿè®¡
        total_tasks = len(results)
        passed_tasks = sum(1 for r in results if r.verification_passed)
        pass_rate = passed_tasks / total_tasks
        
        # å„ç»´åº¦å¹³å‡åˆ†æ•°
        dimension_scores = {}
        for result in results:
            for dimension, score in result.verification_dimensions.items():
                if dimension not in dimension_scores:
                    dimension_scores[dimension] = []
                dimension_scores[dimension].append(score)
        
        avg_dimension_scores = {
            dimension: sum(scores) / len(scores)
            for dimension, scores in dimension_scores.items()
        }
        
        # æ•´ä½“è´¨é‡åˆ†å¸ƒ
        score_distribution = {
            "excellent": sum(1 for r in results if r.overall_score >= 0.9),
            "good": sum(1 for r in results if 0.8 <= r.overall_score < 0.9),
            "fair": sum(1 for r in results if 0.6 <= r.overall_score < 0.8),
            "poor": sum(1 for r in results if r.overall_score < 0.6)
        }
        
        # å¸¸è§é—®é¢˜åˆ†æ
        common_issues = []
        for result in results:
            if not result.verification_passed:
                common_issues.extend(result.detailed_feedback)
        
        return {
            "total_tasks": total_tasks,
            "passed_tasks": passed_tasks,
            "pass_rate": pass_rate,
            "average_overall_score": sum(r.overall_score for r in results) / total_tasks,
            "dimension_scores": avg_dimension_scores,
            "score_distribution": score_distribution,
            "common_issues": common_issues[:10],  # å–å‰10ä¸ªå¸¸è§é—®é¢˜
            "recommendations": self._generate_recommendations(avg_dimension_scores, pass_rate)
        }
    
    def _generate_recommendations(self, dimension_scores: Dict[str, float], pass_rate: float) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if pass_rate < 0.7:
            recommendations.append("æ•´ä½“é€šè¿‡ç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥ä»»åŠ¡ç”Ÿæˆç­–ç•¥")
        
        for dimension, score in dimension_scores.items():
            if score < 0.6:
                recommendations.append(f"{dimension} ç»´åº¦å¾—åˆ†è¾ƒä½ ({score:.2f})ï¼Œéœ€è¦é‡ç‚¹æ”¹è¿›")
        
        if dimension_scores.get("executability", 0) < 0.7:
            recommendations.append("å¯æ‰§è¡Œæ€§ä¸è¶³ï¼Œå»ºè®®ä¼˜åŒ–å·¥å…·é€‰æ‹©å’Œä»»åŠ¡æè¿°")
        
        if dimension_scores.get("difficulty", 0) < 0.6:
            recommendations.append("ä»»åŠ¡éš¾åº¦ä¸åˆé€‚ï¼Œå»ºè®®è°ƒæ•´å¤æ‚åº¦æ§åˆ¶ç®—æ³•")
        
        return recommendations