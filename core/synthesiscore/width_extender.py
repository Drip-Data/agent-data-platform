#!/usr/bin/env python3
"""
Width Extender - å®½åº¦æ‰©å±•å™¨
åŸºäºTaskCraftç®—æ³•ï¼Œå®ç°åŸå­ä»»åŠ¡çš„å®½åº¦ä¼˜å…ˆæ‰©å±•ï¼ˆå¤åˆä»»åŠ¡ç”Ÿæˆï¼‰
"""

import asyncio
import json
import logging
import re
import time
from typing import Dict, List, Optional, Any, Tuple, Set
from dataclasses import asdict
from collections import defaultdict

from core.interfaces import TrajectoryResult
from core.llm_client import LLMClient
from core.toolscore.mcp_client import MCPToolClient
from .enhanced_interfaces import (
    AtomicTask, CompositeTask, TaskDifficulty, TaskType, 
    EnhancedSynthesisConfig, generate_task_id, calculate_complexity_score
)

logger = logging.getLogger(__name__)


class SemanticGrouper:
    """è¯­ä¹‰åˆ†ç»„å™¨ - å°†ç›¸ä¼¼çš„åŸå­ä»»åŠ¡åˆ†ç»„"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
        self.config = EnhancedSynthesisConfig()
    
    async def group_atomic_tasks(self, atomic_tasks: List[AtomicTask]) -> List[List[AtomicTask]]:
        """å°†åŸå­ä»»åŠ¡æŒ‰è¯­ä¹‰ç›¸ä¼¼æ€§åˆ†ç»„"""
        logger.info(f"ğŸ”„ å¼€å§‹å¯¹ {len(atomic_tasks)} ä¸ªåŸå­ä»»åŠ¡è¿›è¡Œè¯­ä¹‰åˆ†ç»„")
        
        if len(atomic_tasks) < self.config.WIDTH_EXTENSION_CONFIG['min_tasks_for_grouping']:
            logger.warning(f"âš ï¸ ä»»åŠ¡æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œåˆ†ç»„ ({len(atomic_tasks)} < {self.config.WIDTH_EXTENSION_CONFIG['min_tasks_for_grouping']})")
            return []
        
        try:
            # 1. è®¡ç®—ä»»åŠ¡é—´ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity_matrix = await self._calculate_similarity_matrix(atomic_tasks)
            
            # 2. åŸºäºç›¸ä¼¼åº¦è¿›è¡Œèšç±»
            task_groups = await self._cluster_tasks_by_similarity(atomic_tasks, similarity_matrix)
            
            # 3. è¿‡æ»¤å’ŒéªŒè¯åˆ†ç»„
            valid_groups = self._filter_valid_groups(task_groups)
            
            logger.info(f"âœ… è¯­ä¹‰åˆ†ç»„å®Œæˆï¼Œå¾—åˆ° {len(valid_groups)} ä¸ªæœ‰æ•ˆç»„")
            return valid_groups
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰åˆ†ç»„å¤±è´¥: {e}")
            return []
    
    async def _calculate_similarity_matrix(self, atomic_tasks: List[AtomicTask]) -> List[List[float]]:
        """è®¡ç®—ä»»åŠ¡é—´ç›¸ä¼¼åº¦çŸ©é˜µ"""
        n = len(atomic_tasks)
        similarity_matrix = [[0.0] * n for _ in range(n)]
        
        for i in range(n):
            for j in range(i + 1, n):
                try:
                    similarity = await self._calculate_task_similarity(atomic_tasks[i], atomic_tasks[j])
                    similarity_matrix[i][j] = similarity
                    similarity_matrix[j][i] = similarity
                except Exception as e:
                    logger.error(f"âŒ è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥ ({i}, {j}): {e}")
                    similarity_matrix[i][j] = 0.0
                    similarity_matrix[j][i] = 0.0
        
        return similarity_matrix
    
    async def _calculate_task_similarity(self, task1: AtomicTask, task2: AtomicTask) -> float:
        """è®¡ç®—ä¸¤ä¸ªä»»åŠ¡çš„ç›¸ä¼¼åº¦"""
        
        similarity_prompt = f"""
è¯„ä¼°ä»¥ä¸‹ä¸¤ä¸ªä»»åŠ¡çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼š

ä»»åŠ¡1:
é—®é¢˜: {task1.question}
ç­”æ¡ˆ: {task1.golden_answer}
å·¥å…·: {task1.required_tools}

ä»»åŠ¡2:
é—®é¢˜: {task2.question}
ç­”æ¡ˆ: {task2.golden_answer}
å·¥å…·: {task2.required_tools}

è¯„ä¼°ç»´åº¦:
1. é—®é¢˜é¢†åŸŸç›¸ä¼¼æ€§ï¼ˆå¦‚éƒ½å…³äºè‚¡ä»·ã€éƒ½å…³äºåœ°ç†ä¿¡æ¯ç­‰ï¼‰
2. ç­”æ¡ˆç±»å‹ç›¸ä¼¼æ€§ï¼ˆå¦‚éƒ½æ˜¯æ•°å€¼ã€éƒ½æ˜¯æ—¥æœŸç­‰ï¼‰
3. å·¥å…·ä½¿ç”¨ç›¸ä¼¼æ€§
4. çŸ¥è¯†èƒŒæ™¯ç›¸ä¼¼æ€§

è¯·è¿”å›0.0-1.0çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œ1.0è¡¨ç¤ºéå¸¸ç›¸ä¼¼ã€‚
åªè¿”å›æ•°å€¼ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚
"""
        
        try:
            response = await self.llm_client.generate_enhanced_reasoning(
                task_description=similarity_prompt,
                available_tools=[],
                tool_descriptions="",
                execution_context={"mode": "task_similarity_assessment"}
            )
            
            thinking = response.get('thinking', '0.0')
            
            # æå–æ•°å€¼
            score_match = re.search(r'(\d+\.?\d*)', thinking)
            if score_match:
                score = float(score_match.group(1))
                return min(score, 1.0) if score <= 1.0 else score / 10.0
            else:
                return 0.0
                
        except Exception as e:
            logger.error(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    async def _cluster_tasks_by_similarity(self, atomic_tasks: List[AtomicTask], 
                                         similarity_matrix: List[List[float]]) -> List[List[AtomicTask]]:
        """åŸºäºç›¸ä¼¼åº¦çŸ©é˜µè¿›è¡Œèšç±»"""
        n = len(atomic_tasks)
        threshold = self.config.WIDTH_EXTENSION_CONFIG['semantic_similarity_threshold']
        max_group_size = self.config.WIDTH_EXTENSION_CONFIG['max_tasks_per_group']
        
        # ä½¿ç”¨ç®€å•çš„åŸºäºé˜ˆå€¼çš„èšç±»ç®—æ³•
        groups = []
        used_indices = set()
        
        for i in range(n):
            if i in used_indices:
                continue
            
            # å¼€å§‹æ–°çš„èšç±»
            current_group = [i]
            used_indices.add(i)
            
            # å¯»æ‰¾ç›¸ä¼¼çš„ä»»åŠ¡
            for j in range(i + 1, n):
                if j in used_indices:
                    continue
                
                if len(current_group) >= max_group_size:
                    break
                
                # æ£€æŸ¥ä¸ç»„å†…æ‰€æœ‰ä»»åŠ¡çš„ç›¸ä¼¼åº¦
                avg_similarity = sum(similarity_matrix[k][j] for k in current_group) / len(current_group)
                
                if avg_similarity >= threshold:
                    current_group.append(j)
                    used_indices.add(j)
            
            # åªä¿ç•™åŒ…å«å¤šä¸ªä»»åŠ¡çš„ç»„
            if len(current_group) >= self.config.WIDTH_EXTENSION_CONFIG['min_tasks_for_grouping']:
                group_tasks = [atomic_tasks[idx] for idx in current_group]
                groups.append(group_tasks)
        
        return groups
    
    def _filter_valid_groups(self, task_groups: List[List[AtomicTask]]) -> List[List[AtomicTask]]:
        """è¿‡æ»¤å’ŒéªŒè¯åˆ†ç»„"""
        valid_groups = []
        
        for group in task_groups:
            if (self.config.WIDTH_EXTENSION_CONFIG['min_tasks_for_grouping'] <= 
                len(group) <= 
                self.config.WIDTH_EXTENSION_CONFIG['max_tasks_per_group']):
                
                # éªŒè¯ç»„å†…ä»»åŠ¡ä¸å®Œå…¨é‡å¤
                if self._check_group_diversity(group):
                    valid_groups.append(group)
        
        return valid_groups
    
    def _check_group_diversity(self, group: List[AtomicTask]) -> bool:
        """æ£€æŸ¥ç»„å†…ä»»åŠ¡çš„å¤šæ ·æ€§"""
        if len(group) < 2:
            return False
        
        # æ£€æŸ¥é—®é¢˜ä¸å®Œå…¨ç›¸åŒ
        questions = set(task.question.lower().strip() for task in group)
        if len(questions) < len(group):
            return False
        
        # æ£€æŸ¥ç­”æ¡ˆä¸å®Œå…¨ç›¸åŒ
        answers = set(task.golden_answer.lower().strip() for task in group)
        if len(answers) < 2:  # å…è®¸éƒ¨åˆ†ç­”æ¡ˆç›¸åŒï¼Œä½†ä¸èƒ½å…¨éƒ¨ç›¸åŒ
            return False
        
        return True


class TaskFuser:
    """ä»»åŠ¡èåˆå™¨ - å°†åˆ†ç»„çš„ä»»åŠ¡èåˆä¸ºå¤åˆä»»åŠ¡"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
        self.config = EnhancedSynthesisConfig()
    
    async def fuse_task_group(self, task_group: List[AtomicTask]) -> Optional[CompositeTask]:
        """èåˆä»»åŠ¡ç»„ä¸ºå¤åˆä»»åŠ¡"""
        logger.debug(f"ğŸ”— å¼€å§‹èåˆä»»åŠ¡ç»„ï¼ŒåŒ…å« {len(task_group)} ä¸ªä»»åŠ¡")
        
        try:
            # 1. åˆ†æä»»åŠ¡ç»„çš„å…±åŒä¸»é¢˜
            common_theme = await self._analyze_common_theme(task_group)
            
            # 2. ç”Ÿæˆå¤åˆé—®é¢˜
            composite_question = await self._generate_composite_question(task_group, common_theme)
            
            # 3. æ•´åˆç­”æ¡ˆ
            composite_answers = self._integrate_answers(task_group)
            
            # 4. ç¡®å®šå·¥å…·éœ€æ±‚å’Œå¤æ‚åº¦
            expected_tools = self._merge_tool_requirements(task_group)
            difficulty_level = self._determine_composite_difficulty(task_group)
            
            # 5. åˆ›å»ºå¤åˆä»»åŠ¡
            composite_task = CompositeTask(
                task_id=generate_task_id(TaskType.WIDTH_EXTENDED, f"group_{len(task_group)}"),
                question=composite_question,
                golden_answers=composite_answers,
                source_atomic_tasks=[task.task_id for task in task_group],
                original_questions=[task.question for task in task_group],
                content_identifier=self._generate_group_identifier(task_group),
                expected_tools=expected_tools,
                difficulty_level=difficulty_level,
                merge_strategy="width"
            )
            
            logger.info(f"âœ… ä»»åŠ¡ç»„èåˆå®Œæˆ: {composite_task.task_id}")
            return composite_task
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡ç»„èåˆå¤±è´¥: {e}")
            return None
    
    async def _analyze_common_theme(self, task_group: List[AtomicTask]) -> str:
        """åˆ†æä»»åŠ¡ç»„çš„å…±åŒä¸»é¢˜"""
        
        theme_prompt = f"""
åˆ†æä»¥ä¸‹åŸå­ä»»åŠ¡ç»„çš„å…±åŒä¸»é¢˜å’Œé¢†åŸŸï¼š

ä»»åŠ¡åˆ—è¡¨:
{chr(10).join(f"- {task.question} (ç­”æ¡ˆ: {task.golden_answer})" for task in task_group)}

è¯·è¯†åˆ«ï¼š
1. è¿™äº›ä»»åŠ¡çš„å…±åŒé¢†åŸŸæˆ–ä¸»é¢˜
2. å®ƒä»¬ä¹‹é—´çš„å†…åœ¨è”ç³»
3. å¯ä»¥å¦‚ä½•æ•´åˆä¸ºä¸€ä¸ªæ›´å¤§çš„é—®é¢˜

è¿”å›ä¸€ä¸ªç®€æ´çš„ä¸»é¢˜æè¿°ã€‚
"""
        
        try:
            response = await self.llm_client.generate_enhanced_reasoning(
                task_description=theme_prompt,
                available_tools=[],
                tool_descriptions="",
                execution_context={"mode": "theme_analysis"}
            )
            
            return response.get('thinking', '').strip() or "ç›¸å…³ä¿¡æ¯æŸ¥è¯¢"
            
        except Exception as e:
            logger.error(f"âŒ ä¸»é¢˜åˆ†æå¤±è´¥: {e}")
            return "ç›¸å…³ä¿¡æ¯æŸ¥è¯¢"
    
    async def _generate_composite_question(self, task_group: List[AtomicTask], common_theme: str) -> str:
        """ç”Ÿæˆå¤åˆé—®é¢˜"""
        
        question_prompt = f"""
åŸºäºä»¥ä¸‹åŸå­ä»»åŠ¡ç»„å’Œå…±åŒä¸»é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªç»¼åˆæ€§é—®é¢˜ã€‚

å…±åŒä¸»é¢˜: {common_theme}

åŸå­ä»»åŠ¡:
{chr(10).join(f"{i+1}. {task.question}" for i, task in enumerate(task_group))}

è¦æ±‚ï¼š
1. åŒ…å«æ‰€æœ‰åŸå­ä»»åŠ¡çš„ä¿¡æ¯éœ€æ±‚
2. æ¯”å•ä¸ªåŸå­ä»»åŠ¡æ›´å¤æ‚ä½†ä»å¯æ‰§è¡Œ
3. éœ€è¦å¤šæ­¥éª¤æ¨ç†å’Œå·¥å…·è°ƒç”¨
4. æœ‰æ˜ç¡®çš„æ‰§è¡Œè·¯å¾„

è¯·ç›´æ¥è¿”å›JSONæ ¼å¼ï¼š
{{
    "composite_question": "ç”Ÿæˆçš„å¤åˆé—®é¢˜å†…å®¹",
    "explanation": "ç®€è¦è¯´æ˜å¦‚ä½•æ•´åˆåŸå­ä»»åŠ¡"
}}
"""
        
        try:
            response = await self.llm_client.generate_enhanced_reasoning(
                task_description=question_prompt,
                available_tools=[],
                tool_descriptions="",
                execution_context={"mode": "composite_question_generation"}
            )
            
            # å°è¯•ä»JSONå“åº”ä¸­æå–å¤åˆé—®é¢˜
            thinking = response.get('thinking', '').strip()
            if thinking:
                try:
                    # å°è¯•è§£æJSON
                    import json
                    parsed = json.loads(thinking)
                    if 'composite_question' in parsed:
                        generated_question = parsed['composite_question'].strip()
                        if len(generated_question) > 20:
                            return generated_question
                except json.JSONDecodeError:
                    pass
            
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ä»thinkingä¸­æå–
            generated_question = self._extract_question_from_thinking(thinking)
            
            # å¦‚æœä»ç„¶å¤±è´¥ï¼Œä½¿ç”¨å›é€€ç­–ç•¥
            if not generated_question or len(generated_question) < 20:
                return self._fallback_composite_question(task_group, common_theme)
            
            return generated_question
            
        except Exception as e:
            logger.error(f"âŒ å¤åˆé—®é¢˜ç”Ÿæˆå¤±è´¥: {e}")
            return self._fallback_composite_question(task_group, common_theme)
    
    def _extract_question_from_thinking(self, thinking: str) -> str:
        """ä»thinkingä¸­æå–å¤åˆé—®é¢˜"""
        if not thinking:
            return ""
        
        # å°è¯•æ‰¾åˆ°ä»¥é—®å·ç»“å°¾çš„å¥å­
        sentences = thinking.split('ã€‚')
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence.endswith('ï¼Ÿ') or sentence.endswith('?'):
                # ç¡®ä¿å¥å­è¶³å¤Ÿé•¿ä¸”çœ‹èµ·æ¥åƒä¸€ä¸ªå®Œæ•´çš„é—®é¢˜
                if len(sentence) > 20 and not sentence.startswith('STEP'):
                    return sentence
        
        # å¦‚æœæ²¡æ‰¾åˆ°é—®å·ç»“å°¾çš„å¥å­ï¼Œå°è¯•æ‰¾åˆ°ä»¥è¯·æ±‚åŠ¨è¯å¼€å¤´çš„é•¿å¥å­
        lines = thinking.split('\n')
        for line in lines:
            line = line.strip()
            if (line.startswith('è¯·') or line.startswith('åŸºäº') or line.startswith('ç»“åˆ')) and len(line) > 30:
                # ç§»é™¤å¯èƒ½çš„ç¼–å·å‰ç¼€
                if '.' in line[:10]:
                    line = line.split('.', 1)[-1].strip()
                return line
        
        return ""
    
    def _fallback_composite_question(self, task_group: List[AtomicTask], common_theme: str) -> str:
        """å¤åˆé—®é¢˜ç”Ÿæˆå¤±è´¥æ—¶çš„å›é€€ç­–ç•¥"""
        if len(task_group) == 2:
            return f"è¯·åŒæ—¶å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š1) {task_group[0].question} 2) {task_group[1].question}"
        else:
            return f"è¯·æ”¶é›†å…³äº{common_theme}çš„ä»¥ä¸‹ä¿¡æ¯ï¼š" + "ï¼›".join(f"({i+1}) {task.question}" for i, task in enumerate(task_group))
    
    def _integrate_answers(self, task_group: List[AtomicTask]) -> List[str]:
        """æ•´åˆç­”æ¡ˆ"""
        return [task.golden_answer for task in task_group]
    
    def _merge_tool_requirements(self, task_group: List[AtomicTask]) -> List[str]:
        """åˆå¹¶å·¥å…·éœ€æ±‚"""
        all_tools = set()
        for task in task_group:
            all_tools.update(task.required_tools)
        return list(all_tools)
    
    def _determine_composite_difficulty(self, task_group: List[AtomicTask]) -> TaskDifficulty:
        """ç¡®å®šå¤åˆä»»åŠ¡éš¾åº¦"""
        # å¤åˆä»»åŠ¡çš„éš¾åº¦åŸºäºç»„å†…ä»»åŠ¡æ•°é‡å’Œå¤æ‚åº¦
        group_size = len(task_group)
        total_tools = len(self._merge_tool_requirements(task_group))
        
        if group_size <= 2 and total_tools <= 3:
            return TaskDifficulty.MEDIUM
        elif group_size <= 3 and total_tools <= 5:
            return TaskDifficulty.COMPLEX
        else:
            return TaskDifficulty.COMPLEX
    
    def _generate_group_identifier(self, task_group: List[AtomicTask]) -> str:
        """ç”Ÿæˆç»„æ ‡è¯†ç¬¦"""
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡çš„å†…å®¹æ ‡è¯†ç¬¦ä½œä¸ºåŸºç¡€
        base_identifier = task_group[0].content_identifier if task_group else "unknown"
        return f"composite_{base_identifier}_{len(task_group)}"


class DecompositionValidator:
    """åˆ†è§£éªŒè¯å™¨ - éªŒè¯å¤åˆä»»åŠ¡çš„åˆç†æ€§"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
        self.config = EnhancedSynthesisConfig()
    
    async def validate_composite_task(self, composite_task: CompositeTask) -> Dict[str, Any]:
        """éªŒè¯å¤åˆä»»åŠ¡çš„åˆç†æ€§"""
        logger.debug(f"ğŸ” éªŒè¯å¤åˆä»»åŠ¡: {composite_task.task_id}")
        
        try:
            # 1. åˆ†è§£éªŒè¯
            decomposition_result = await self._validate_decomposition(composite_task)
            
            # 2. å¤æ‚æ€§éªŒè¯
            complexity_result = await self._validate_complexity(composite_task)
            
            # 3. å¯æ‰§è¡Œæ€§è¯„ä¼°
            executability_result = await self._assess_executability(composite_task)
            
            # 4. ç»¼åˆè¯„åˆ†
            overall_score = self._calculate_overall_validation_score(
                decomposition_result, complexity_result, executability_result
            )
            
            validation_result = {
                "is_valid": overall_score >= 0.7,
                "overall_score": overall_score,
                "decomposition": decomposition_result,
                "complexity": complexity_result,
                "executability": executability_result,
                "recommendation": "accept" if overall_score >= 0.8 else "modify" if overall_score >= 0.6 else "reject"
            }
            
            logger.info(f"âœ… å¤åˆä»»åŠ¡éªŒè¯å®Œæˆ: {composite_task.task_id} (åˆ†æ•°: {overall_score:.3f})")
            return validation_result
            
        except Exception as e:
            logger.error(f"âŒ å¤åˆä»»åŠ¡éªŒè¯å¤±è´¥ {composite_task.task_id}: {e}")
            return {
                "is_valid": False,
                "overall_score": 0.0,
                "error": str(e),
                "recommendation": "reject"
            }
    
    async def _validate_decomposition(self, composite_task: CompositeTask) -> Dict[str, Any]:
        """éªŒè¯åˆ†è§£åˆç†æ€§"""
        
        decomposition_prompt = f"""
éªŒè¯ä»¥ä¸‹å¤åˆä»»åŠ¡æ˜¯å¦å¯ä»¥åˆç†åˆ†è§£ä¸ºå…¶ç»„æˆçš„åŸå­ä»»åŠ¡ï¼š

å¤åˆä»»åŠ¡é—®é¢˜: {composite_task.question}

åŸå­ä»»åŠ¡åˆ—è¡¨:
{chr(10).join(f"- {q}" for q in composite_task.original_questions)}

éªŒè¯æ ‡å‡†:
1. å¤åˆä»»åŠ¡æ˜¯å¦æ¶µç›–äº†æ‰€æœ‰åŸå­ä»»åŠ¡çš„ä¿¡æ¯éœ€æ±‚ï¼Ÿ
2. å„åŸå­ä»»åŠ¡ä¹‹é—´æ˜¯å¦å­˜åœ¨é€»è¾‘å…³è”ï¼Ÿ
3. å¤åˆä»»åŠ¡æ˜¯å¦æ¯”å•ä¸ªåŸå­ä»»åŠ¡æ›´æœ‰ä»·å€¼ï¼Ÿ
4. åˆ†è§£æ˜¯å¦è‡ªç„¶åˆç†ï¼Ÿ

è¯·è¿”å›JSONæ ¼å¼çš„éªŒè¯ç»“æœï¼š
{{
    "covers_all_atomics": true/false,
    "logical_connection": true/false,
    "added_value": true/false,
    "natural_decomposition": true/false,
    "score": 0.0-1.0,
    "reasoning": "éªŒè¯ç†ç”±"
}}
"""
        
        try:
            response = await self.llm_client.generate_enhanced_reasoning(
                task_description=decomposition_prompt,
                available_tools=[],
                tool_descriptions="",
                execution_context={"mode": "decomposition_validation"}
            )
            
            return self._parse_validation_response(response)
            
        except Exception as e:
            logger.error(f"âŒ åˆ†è§£éªŒè¯å¤±è´¥: {e}")
            return {"score": 0.0, "reasoning": f"éªŒè¯å¤±è´¥: {e}"}
    
    async def _validate_complexity(self, composite_task: CompositeTask) -> Dict[str, Any]:
        """éªŒè¯å¤æ‚æ€§åˆç†æ€§"""
        
        complexity_prompt = f"""
è¯„ä¼°ä»¥ä¸‹å¤åˆä»»åŠ¡çš„å¤æ‚æ€§æ˜¯å¦åˆç†ï¼š

å¤åˆä»»åŠ¡: {composite_task.question}
é¢„æœŸå·¥å…·: {composite_task.expected_tools}
åŸå­ä»»åŠ¡æ•°é‡: {len(composite_task.source_atomic_tasks)}

è¯„ä¼°æ ‡å‡†:
1. ä»»åŠ¡å¤æ‚åº¦æ˜¯å¦é€‚ä¸­ï¼ˆä¸è¿‡äºç®€å•ä¹Ÿä¸è¿‡äºå¤æ‚ï¼‰ï¼Ÿ
2. å·¥å…·éœ€æ±‚æ˜¯å¦åˆç†ï¼Ÿ
3. æ˜¯å¦éœ€è¦å¤šæ­¥éª¤æ¨ç†ï¼Ÿ
4. è®¤çŸ¥è´Ÿè·æ˜¯å¦åœ¨å¯æ¥å—èŒƒå›´å†…ï¼Ÿ

è¯·è¿”å›0.0-1.0çš„å¤æ‚æ€§è¯„åˆ†ã€‚
"""
        
        try:
            response = await self.llm_client.generate_enhanced_reasoning(
                task_description=complexity_prompt,
                available_tools=[],
                tool_descriptions="",
                execution_context={"mode": "complexity_validation"}
            )
            
            thinking = response.get('thinking', '0.5')
            score_match = re.search(r'(\d+\.?\d*)', thinking)
            if score_match:
                score = float(score_match.group(1))
                score = min(score, 1.0) if score <= 1.0 else score / 10.0
            else:
                score = 0.5
            
            return {"score": score, "reasoning": thinking}
            
        except Exception as e:
            logger.error(f"âŒ å¤æ‚æ€§éªŒè¯å¤±è´¥: {e}")
            return {"score": 0.5, "reasoning": f"éªŒè¯å¤±è´¥: {e}"}
    
    async def _assess_executability(self, composite_task: CompositeTask) -> Dict[str, Any]:
        """è¯„ä¼°å¯æ‰§è¡Œæ€§"""
        
        # åŸºäºå¯å‘å¼è§„åˆ™è¯„ä¼°å¯æ‰§è¡Œæ€§
        score = 1.0
        issues = []
        
        # æ£€æŸ¥é—®é¢˜é•¿åº¦
        if len(composite_task.question) > 500:
            score -= 0.2
            issues.append("é—®é¢˜è¿‡é•¿ï¼Œå¯èƒ½å½±å“ç†è§£")
        
        # æ£€æŸ¥å·¥å…·éœ€æ±‚
        if len(composite_task.expected_tools) > 5:
            score -= 0.2
            issues.append("å·¥å…·éœ€æ±‚è¿‡å¤š")
        elif len(composite_task.expected_tools) == 0:
            score -= 0.3
            issues.append("ç¼ºå°‘å·¥å…·éœ€æ±‚")
        
        # æ£€æŸ¥å­ä»»åŠ¡æ•°é‡
        if len(composite_task.source_atomic_tasks) > 4:
            score -= 0.2
            issues.append("å­ä»»åŠ¡è¿‡å¤š")
        
        # æ£€æŸ¥ç­”æ¡ˆæ•°é‡åŒ¹é…
        if len(composite_task.golden_answers) != len(composite_task.source_atomic_tasks):
            score -= 0.3
            issues.append("ç­”æ¡ˆæ•°é‡ä¸å­ä»»åŠ¡ä¸åŒ¹é…")
        
        return {
            "score": max(score, 0.0),
            "issues": issues,
            "reasoning": "; ".join(issues) if issues else "å¯æ‰§è¡Œæ€§è‰¯å¥½"
        }
    
    def _parse_validation_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æéªŒè¯å“åº”"""
        try:
            thinking = response.get('thinking', '{}')
            
            if thinking.strip().startswith('{'):
                result = json.loads(thinking)
            else:
                json_match = re.search(r'\{.*\}', thinking, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                else:
                    return {"score": 0.5, "reasoning": "è§£æå¤±è´¥"}
            
            return result
            
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"âŒ è§£æéªŒè¯å“åº”å¤±è´¥: {e}")
            return {"score": 0.5, "reasoning": f"è§£æå¤±è´¥: {e}"}
    
    def _calculate_overall_validation_score(self, decomposition_result: Dict[str, Any], 
                                          complexity_result: Dict[str, Any], 
                                          executability_result: Dict[str, Any]) -> float:
        """è®¡ç®—æ€»ä½“éªŒè¯åˆ†æ•°"""
        weights = {
            "decomposition": 0.4,
            "complexity": 0.3,
            "executability": 0.3
        }
        
        decomposition_score = decomposition_result.get('score', 0.0)
        complexity_score = complexity_result.get('score', 0.0)
        executability_score = executability_result.get('score', 0.0)
        
        overall_score = (
            decomposition_score * weights["decomposition"] +
            complexity_score * weights["complexity"] +
            executability_score * weights["executability"]
        )
        
        return overall_score


class WidthExtender:
    """å®½åº¦æ‰©å±•å™¨ - ç»Ÿä¸€çš„å®½åº¦æ‰©å±•æ¥å£"""
    
    def __init__(self, llm_client: LLMClient, mcp_client: Optional[MCPToolClient] = None):
        self.llm_client = llm_client
        self.mcp_client = mcp_client
        self.semantic_grouper = SemanticGrouper(llm_client)
        self.task_fuser = TaskFuser(llm_client)
        self.decomposition_validator = DecompositionValidator(llm_client)
        self.config = EnhancedSynthesisConfig()
    
    async def extend_atomic_tasks_width(self, atomic_tasks: List[AtomicTask], 
                                       adaptive_config: Optional[Any] = None) -> List[CompositeTask]:
        """ä¼˜åŒ–çš„å®½åº¦æ‰©å±•åŸå­ä»»åŠ¡ä¸ºå¤åˆä»»åŠ¡"""
        logger.info(f"ğŸš€ å¼€å§‹å®½åº¦æ‰©å±• {len(atomic_tasks)} ä¸ªåŸå­ä»»åŠ¡")
        
        if not atomic_tasks:
            return []
        
        try:
            # ä½¿ç”¨è‡ªé€‚åº”é…ç½®æ›´æ–°ç›¸ä¼¼åº¦é˜ˆå€¼
            if adaptive_config:
                original_threshold = self.config.WIDTH_EXTENSION_CONFIG['semantic_similarity_threshold']
                adaptive_threshold = adaptive_config.width_config["semantic_similarity_threshold"]
                self.config.WIDTH_EXTENSION_CONFIG['semantic_similarity_threshold'] = adaptive_threshold
                logger.debug(f"ğŸ”§ ä½¿ç”¨è‡ªé€‚åº”ç›¸ä¼¼åº¦é˜ˆå€¼: {adaptive_threshold} (åŸå€¼: {original_threshold})")
            
            # 1. è¯­ä¹‰åˆ†ç»„
            task_groups = await self.semantic_grouper.group_atomic_tasks(atomic_tasks)
            
            if not task_groups:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°å¯åˆ†ç»„çš„ä»»åŠ¡")
                return []
            
            logger.info(f"ğŸ“Š è¯­ä¹‰åˆ†ç»„å®Œæˆï¼Œå¾—åˆ° {len(task_groups)} ä¸ªä»»åŠ¡ç»„")
            
            # 2. æ‰¹é‡å¹¶è¡Œèåˆå„ç»„ä»»åŠ¡
            composite_tasks = await self._batch_fuse_task_groups(task_groups, adaptive_config)
            
            # 3. æ‰¹é‡éªŒè¯å¤åˆä»»åŠ¡
            validated_tasks = await self._batch_validate_composite_tasks(composite_tasks, adaptive_config)
            
            logger.info(f"âœ… å®½åº¦æ‰©å±•å®Œæˆï¼Œç”Ÿæˆ {len(validated_tasks)} ä¸ªæœ‰æ•ˆå¤åˆä»»åŠ¡")
            return validated_tasks
            
        except Exception as e:
            logger.error(f"âŒ å®½åº¦æ‰©å±•å¤±è´¥: {e}")
            return []
    
    async def _batch_fuse_task_groups(self, task_groups: List[List[AtomicTask]], 
                                     adaptive_config: Optional[Any] = None) -> List[CompositeTask]:
        """æ‰¹é‡èåˆä»»åŠ¡ç»„"""
        max_concurrent = adaptive_config.batch_config["max_concurrent_batches"] if adaptive_config else 3
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def fuse_with_semaphore(group: List[AtomicTask]):
            async with semaphore:
                try:
                    return await self.task_fuser.fuse_task_group(group)
                except Exception as e:
                    logger.error(f"âŒ ä»»åŠ¡ç»„èåˆå¤±è´¥: {e}")
                    return None
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰ç»„
        fusion_results = await asyncio.gather(
            *[fuse_with_semaphore(group) for group in task_groups],
            return_exceptions=True
        )
        
        # æ”¶é›†æœ‰æ•ˆç»“æœ
        composite_tasks = []
        for i, result in enumerate(fusion_results):
            if isinstance(result, CompositeTask):
                composite_tasks.append(result)
            elif isinstance(result, Exception):
                logger.error(f"âŒ ä»»åŠ¡ç»„ {i} èåˆå¼‚å¸¸: {result}")
            elif result is None:
                logger.warning(f"âš ï¸ ä»»åŠ¡ç»„ {i} èåˆè¿”å›ç©ºç»“æœ")
        
        logger.info(f"âœ… ä»»åŠ¡ç»„èåˆå®Œæˆï¼ŒæˆåŠŸèåˆ {len(composite_tasks)}/{len(task_groups)} ä¸ªç»„")
        return composite_tasks
    
    async def _batch_validate_composite_tasks(self, composite_tasks: List[CompositeTask], 
                                            adaptive_config: Optional[Any] = None) -> List[CompositeTask]:
        """æ‰¹é‡éªŒè¯å¤åˆä»»åŠ¡"""
        if not composite_tasks:
            return []
        
        max_concurrent = adaptive_config.batch_config["max_concurrent_batches"] if adaptive_config else 3
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def validate_with_semaphore(task: CompositeTask):
            async with semaphore:
                try:
                    validation_result = await self.decomposition_validator.validate_composite_task(task)
                    return task, validation_result
                except Exception as e:
                    logger.error(f"âŒ å¤åˆä»»åŠ¡éªŒè¯å¤±è´¥ {task.task_id}: {e}")
                    return task, {"is_valid": False, "error": str(e)}
        
        # å¹¶è¡ŒéªŒè¯æ‰€æœ‰ä»»åŠ¡
        validation_results = await asyncio.gather(
            *[validate_with_semaphore(task) for task in composite_tasks],
            return_exceptions=True
        )
        
        # ç­›é€‰æœ‰æ•ˆä»»åŠ¡
        validated_tasks = []
        for result in validation_results:
            if isinstance(result, tuple):
                task, validation = result
                if validation.get('is_valid', False):
                    validated_tasks.append(task)
                else:
                    logger.warning(f"âš ï¸ å¤åˆä»»åŠ¡éªŒè¯ä¸é€šè¿‡: {task.task_id}")
            elif isinstance(result, Exception):
                logger.error(f"âŒ å¤åˆä»»åŠ¡éªŒè¯å¼‚å¸¸: {result}")
        
        logger.info(f"âœ… å¤åˆä»»åŠ¡éªŒè¯å®Œæˆï¼Œ{len(validated_tasks)}/{len(composite_tasks)} ä¸ªä»»åŠ¡é€šè¿‡éªŒè¯")
        return validated_tasks
    
    async def get_width_extension_statistics(self, atomic_tasks: List[AtomicTask], 
                                           composite_tasks: List[CompositeTask]) -> Dict[str, Any]:
        """è·å–å®½åº¦æ‰©å±•ç»Ÿè®¡ä¿¡æ¯"""
        if not atomic_tasks:
            return {"total_atomic_tasks": 0, "total_composite_tasks": 0}
        
        # è®¡ç®—åˆ†ç»„æ•ˆç‡
        grouped_atomic_tasks = set()
        for composite_task in composite_tasks:
            grouped_atomic_tasks.update(composite_task.source_atomic_tasks)
        
        grouping_efficiency = len(grouped_atomic_tasks) / len(atomic_tasks)
        
        # è®¡ç®—å¹³å‡ç»„å¤§å°
        if composite_tasks:
            avg_group_size = sum(len(task.source_atomic_tasks) for task in composite_tasks) / len(composite_tasks)
        else:
            avg_group_size = 0.0
        
        # ç»Ÿè®¡å·¥å…·ä½¿ç”¨
        tool_usage_before = defaultdict(int)
        tool_usage_after = defaultdict(int)
        
        for task in atomic_tasks:
            for tool in task.required_tools:
                tool_usage_before[tool] += 1
        
        for task in composite_tasks:
            for tool in task.expected_tools:
                tool_usage_after[tool] += 1
        
        return {
            "total_atomic_tasks": len(atomic_tasks),
            "total_composite_tasks": len(composite_tasks),
            "grouping_efficiency": grouping_efficiency,
            "average_group_size": avg_group_size,
            "tool_usage_before": dict(tool_usage_before),
            "tool_usage_after": dict(tool_usage_after),
            "complexity_increase": self._calculate_average_complexity_increase(atomic_tasks, composite_tasks)
        }
    
    def _calculate_average_complexity_increase(self, atomic_tasks: List[AtomicTask], 
                                             composite_tasks: List[CompositeTask]) -> float:
        """è®¡ç®—å¹³å‡å¤æ‚åº¦å¢é•¿"""
        if not atomic_tasks or not composite_tasks:
            return 0.0
        
        # è®¡ç®—åŸå­ä»»åŠ¡çš„å¹³å‡å¤æ‚åº¦
        atomic_complexity = sum(calculate_complexity_score(task) for task in atomic_tasks) / len(atomic_tasks)
        
        # è®¡ç®—å¤åˆä»»åŠ¡çš„å¹³å‡å¤æ‚åº¦
        composite_complexity = sum(calculate_complexity_score(task) for task in composite_tasks) / len(composite_tasks)
        
        return composite_complexity - atomic_complexity