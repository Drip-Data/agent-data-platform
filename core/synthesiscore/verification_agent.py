#!/usr/bin/env python3
"""
Verification Agent - éªŒè¯ä»£ç†æ¡†æ¶
åŸºäºTaskCraftçš„éªŒè¯æœºåˆ¶ï¼Œå®ç°ä»»åŠ¡è´¨é‡çš„å¤šç»´åº¦éªŒè¯
"""

import asyncio
import json
import logging
import time
from typing import Dict, List, Optional, Any, Union

from core.interfaces import TrajectoryResult
from core.llm_client import LLMClient
from core.toolscore.mcp_client import MCPToolClient
from .enhanced_interfaces import (
    AtomicTask, ExtendedTask, CompositeTask, TaskUnion, 
    VerificationResult, TaskDifficulty, EnhancedSynthesisConfig
)

logger = logging.getLogger(__name__)


class TaskExecutor:
    """ä»»åŠ¡æ‰§è¡Œå™¨ - ç”¨äºéªŒè¯ä»»åŠ¡çš„å¯æ‰§è¡Œæ€§"""
    
    def __init__(self, llm_client: LLMClient, mcp_client: Optional[MCPToolClient] = None):
        self.llm_client = llm_client
        self.mcp_client = mcp_client
        self.config = EnhancedSynthesisConfig()
    
    async def execute_task_with_tools(self, question: str, expected_answer: Union[str, List[str]], 
                                     timeout: int = 60) -> Dict[str, Any]:
        """ä½¿ç”¨å·¥å…·æ‰§è¡Œä»»åŠ¡"""
        start_time = time.time()
        
        try:
            # è·å–å¯ç”¨å·¥å…·
            available_tools = await self._get_available_tools()
            
            # ä½¿ç”¨LLMè¿›è¡Œæ¨ç†å’Œå·¥å…·è°ƒç”¨
            reasoning_result = await self.llm_client.generate_enhanced_reasoning(
                task_description=question,
                available_tools=available_tools,
                tool_descriptions=await self._get_tool_descriptions(),
                execution_context={
                    "mode": "verification_execution",
                    "expected_answer": expected_answer,
                    "timeout": timeout
                }
            )
            
            # æ‰§è¡Œæ¨èçš„å·¥å…·è°ƒç”¨
            if reasoning_result.get('action') == 'tool_call':
                tool_result = await self._execute_tool_call(
                    reasoning_result.get('tool'),
                    reasoning_result.get('parameters', {})
                )
                
                execution_result = {
                    "success": True,
                    "agent_result": tool_result.get('result', ''),
                    "tools_used": [reasoning_result.get('tool')],
                    "execution_time": time.time() - start_time,
                    "trajectory": [
                        {
                            "thinking": reasoning_result.get('thinking', ''),
                            "action": reasoning_result.get('action'),
                            "tool": reasoning_result.get('tool'),
                            "parameters": reasoning_result.get('parameters'),
                            "result": tool_result.get('result', '')
                        }
                    ]
                }
            else:
                execution_result = {
                    "success": False,
                    "agent_result": reasoning_result.get('thinking', ''),
                    "tools_used": [],
                    "execution_time": time.time() - start_time,
                    "error": "No tool call generated"
                }
            
            # éªŒè¯ç­”æ¡ˆæ­£ç¡®æ€§
            answer_correct = await self._verify_answer_correctness(
                execution_result.get('agent_result', ''),
                expected_answer
            )
            
            execution_result['answer_correct'] = answer_correct
            execution_result['confidence'] = answer_correct * reasoning_result.get('confidence', 0.5)
            
            return execution_result
            
        except asyncio.TimeoutError:
            return {
                "success": False,
                "error": "Execution timeout",
                "execution_time": timeout,
                "tools_used": [],
                "confidence": 0.0
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "execution_time": time.time() - start_time,
                "tools_used": [],
                "confidence": 0.0
            }
    
    async def execute_task_without_tools(self, question: str, expected_answer: Union[str, List[str]]) -> Dict[str, Any]:
        """ä¸ä½¿ç”¨å·¥å…·æ‰§è¡Œä»»åŠ¡ï¼ˆçº¯LLMæ¨ç†ï¼‰"""
        start_time = time.time()
        
        try:
            # ç›´æ¥ä½¿ç”¨LLMå›ç­”é—®é¢˜
            reasoning_result = await self.llm_client.generate_reasoning(
                task_description=f"è¯·ç›´æ¥å›ç­”ä»¥ä¸‹é—®é¢˜ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•å·¥å…·: {question}",
                available_tools=[],  # ä¸æä¾›å·¥å…·
                execution_context={"mode": "pure_llm_reasoning"}
            )
            
            llm_answer = reasoning_result.get('thinking', '')
            
            # éªŒè¯ç­”æ¡ˆæ­£ç¡®æ€§
            answer_correct = await self._verify_answer_correctness(llm_answer, expected_answer)
            
            return {
                "success": answer_correct,
                "llm_result": llm_answer,
                "answer_correct": answer_correct,
                "execution_time": time.time() - start_time,
                "confidence": answer_correct * reasoning_result.get('confidence', 0.5)
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "llm_result": "",
                "execution_time": time.time() - start_time,
                "confidence": 0.0
            }
    
    async def _get_available_tools(self) -> List[str]:
        """è·å–å¯ç”¨å·¥å…·åˆ—è¡¨"""
        if not self.mcp_client:
            return []
        
        try:
            tools_info = await self.mcp_client.list_tools()
            return [tool.get('name', '') for tool in tools_info]
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–å·¥å…·åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    async def _get_tool_descriptions(self) -> str:
        """è·å–å·¥å…·æè¿°"""
        if not self.mcp_client:
            return "æ— å¯ç”¨å·¥å…·"
        
        try:
            tools_info = await self.mcp_client.list_tools()
            descriptions = []
            for tool in tools_info:
                name = tool.get('name', '')
                description = tool.get('description', '')
                descriptions.append(f"- {name}: {description}")
            return '\n'.join(descriptions)
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–å·¥å…·æè¿°å¤±è´¥: {e}")
            return "å·¥å…·æè¿°è·å–å¤±è´¥"
    
    async def _execute_tool_call(self, tool_name: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        if not self.mcp_client:
            return {"error": "MCPå®¢æˆ·ç«¯ä¸å¯ç”¨"}
        
        try:
            result = await self.mcp_client.call_tool(tool_name, parameters)
            return {"result": result, "success": True}
        except Exception as e:
            return {"error": str(e), "success": False}
    
    async def _verify_answer_correctness(self, actual_answer: str, expected_answer: Union[str, List[str]]) -> bool:
        """éªŒè¯ç­”æ¡ˆæ­£ç¡®æ€§"""
        if isinstance(expected_answer, list):
            # å¯¹äºå¤åˆä»»åŠ¡ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰æœŸæœ›ç­”æ¡ˆ
            for exp_ans in expected_answer:
                if not self._is_answer_similar(actual_answer, exp_ans):
                    return False
            return True
        else:
            return self._is_answer_similar(actual_answer, expected_answer)
    
    def _is_answer_similar(self, actual: str, expected: str) -> bool:
        """æ£€æŸ¥ç­”æ¡ˆç›¸ä¼¼æ€§"""
        actual_lower = actual.lower().strip()
        expected_lower = expected.lower().strip()
        
        # ç²¾ç¡®åŒ¹é…
        if actual_lower == expected_lower:
            return True
        
        # åŒ…å«åŒ¹é…
        if expected_lower in actual_lower or actual_lower in expected_lower:
            return True
        
        # æ•°å€¼åŒ¹é…
        import re
        actual_numbers = re.findall(r'\d+\.?\d*', actual)
        expected_numbers = re.findall(r'\d+\.?\d*', expected)
        
        if actual_numbers and expected_numbers:
            return any(abs(float(a) - float(e)) < 0.01 for a in actual_numbers for e in expected_numbers)
        
        return False


class AtomicityVerifier:
    """åŸå­æ€§éªŒè¯å™¨ - éªŒè¯ä»»åŠ¡çš„åŸå­æ€§"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
        self.config = EnhancedSynthesisConfig()
    
    async def verify_atomicity(self, task: AtomicTask) -> Dict[str, Any]:
        """éªŒè¯ä»»åŠ¡åŸå­æ€§"""
        
        # 1. ç»“æ„æ€§åŸå­æ€§æ£€æŸ¥
        structural_check = self._check_structural_atomicity(task.question)
        
        # 2. LLMè¾…åŠ©åŸå­æ€§æ£€æŸ¥
        llm_check = await self._llm_atomicity_check(task)
        
        # 3. ç»¼åˆè¯„åˆ†
        atomicity_score = (structural_check['score'] + llm_check['score']) / 2
        
        return {
            "atomicity_score": atomicity_score,
            "is_atomic": atomicity_score > self.config.ATOMIC_GENERATION_CONFIG['atomicity_verification_threshold'],
            "structural_check": structural_check,
            "llm_check": llm_check,
            "details": {
                "question_complexity": len(task.question.split()),
                "contains_conjunctions": self._contains_conjunctions(task.question),
                "has_multiple_questions": self._has_multiple_questions(task.question)
            }
        }
    
    def _check_structural_atomicity(self, question: str) -> Dict[str, Any]:
        """ç»“æ„æ€§åŸå­æ€§æ£€æŸ¥"""
        score = 1.0
        issues = []
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹¶åˆ—è¿è¯
        conjunctions = ['and', 'or', 'ä»¥åŠ', 'æˆ–è€…', 'åŒæ—¶', 'å¹¶ä¸”']
        if any(conj in question.lower() for conj in conjunctions):
            score -= 0.3
            issues.append("åŒ…å«å¹¶åˆ—è¿è¯")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªé—®å·
        question_marks = question.count('?') + question.count('ï¼Ÿ')
        if question_marks > 1:
            score -= 0.4
            issues.append("åŒ…å«å¤šä¸ªé—®å·")
        
        # æ£€æŸ¥å¥å­é•¿åº¦
        if len(question.split()) > 30:
            score -= 0.2
            issues.append("é—®é¢˜è¿‡é•¿")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªä¸»é¢˜
        if self._has_multiple_topics(question):
            score -= 0.3
            issues.append("åŒ…å«å¤šä¸ªä¸»é¢˜")
        
        return {
            "score": max(score, 0.0),
            "issues": issues
        }
    
    async def _llm_atomicity_check(self, task: AtomicTask) -> Dict[str, Any]:
        """LLMè¾…åŠ©åŸå­æ€§æ£€æŸ¥"""
        
        atomicity_prompt = f"""
        è¯·è¯„ä¼°ä»¥ä¸‹é—®é¢˜æ˜¯å¦æ˜¯ä¸€ä¸ªåŸå­æ€§ä»»åŠ¡ï¼ˆä¸å¯å†åˆ†çš„åŸºæœ¬ä»»åŠ¡ï¼‰ï¼š
        
        é—®é¢˜: {task.question}
        é¢„æœŸç­”æ¡ˆ: {task.golden_answer}
        
        è¯„ä¼°æ ‡å‡†:
        1. ä»»åŠ¡æ˜¯å¦å¯ä»¥æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹çš„å­ä»»åŠ¡ï¼Ÿ
        2. é—®é¢˜æ˜¯å¦åªè¯¢é—®ä¸€ä¸ªç‰¹å®šçš„ä¿¡æ¯ç‚¹ï¼Ÿ
        3. ç­”æ¡ˆæ˜¯å¦æ˜¯å•ä¸€ã€æ˜ç¡®çš„ï¼Ÿ
        
        è¯·è¿”å›JSONæ ¼å¼ï¼š
        {{
            "is_atomic": true/false,
            "confidence": 0.0-1.0,
            "reasoning": "è¯„ä¼°ç†ç”±",
            "suggested_splits": ["å­ä»»åŠ¡1", "å­ä»»åŠ¡2"] (å¦‚æœå¯æ‹†åˆ†)
        }}
        """
        
        try:
            response = await self.llm_client.generate_reasoning(
                task_description=atomicity_prompt,
                available_tools=[],
                execution_context={"mode": "atomicity_verification"}
            )
            
            # è§£æLLMå“åº”
            thinking = response.get('thinking', '{}')
            try:
                result = json.loads(thinking)
                return {
                    "score": result.get('confidence', 0.5) if result.get('is_atomic', False) else 1 - result.get('confidence', 0.5),
                    "is_atomic": result.get('is_atomic', False),
                    "reasoning": result.get('reasoning', ''),
                    "suggested_splits": result.get('suggested_splits', [])
                }
            except json.JSONDecodeError:
                return {"score": 0.5, "is_atomic": True, "reasoning": "LLMå“åº”è§£æå¤±è´¥"}
                
        except Exception as e:
            logger.error(f"âŒ LLMåŸå­æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return {"score": 0.5, "is_atomic": True, "reasoning": f"æ£€æŸ¥å¤±è´¥: {e}"}
    
    def _contains_conjunctions(self, question: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«è¿è¯"""
        conjunctions = ['and', 'or', 'ä»¥åŠ', 'æˆ–è€…', 'åŒæ—¶', 'å¹¶ä¸”', 'å¦å¤–', 'æ­¤å¤–']
        return any(conj in question.lower() for conj in conjunctions)
    
    def _has_multiple_questions(self, question: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªé—®é¢˜"""
        return (question.count('?') + question.count('ï¼Ÿ')) > 1
    
    def _has_multiple_topics(self, question: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªä¸»é¢˜"""
        # ç®€å•çš„å¯å‘å¼æ£€æŸ¥
        keywords_count = 0
        common_keywords = ['ä»€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å“ªä¸ª', 'å¤šå°‘', 'ä½•æ—¶', 'ä½•åœ°']
        
        for keyword in common_keywords:
            if keyword in question:
                keywords_count += 1
        
        return keywords_count > 1


class QualityAssessor:
    """è´¨é‡è¯„ä¼°å™¨ - è¯„ä¼°ä»»åŠ¡çš„å„ä¸ªè´¨é‡ç»´åº¦"""
    
    def __init__(self, llm_client: LLMClient, task_executor: TaskExecutor):
        self.llm_client = llm_client
        self.task_executor = task_executor
        self.config = EnhancedSynthesisConfig()
    
    async def assess_task_quality(self, task: TaskUnion) -> Dict[str, float]:
        """è¯„ä¼°ä»»åŠ¡è´¨é‡çš„å„ä¸ªç»´åº¦"""
        
        dimensions = {}
        
        # 1. å¯æ‰§è¡Œæ€§è¯„ä¼°
        dimensions['executability'] = await self._assess_executability(task)
        
        # 2. éš¾åº¦é€‚ä¸­æ€§è¯„ä¼°
        dimensions['difficulty'] = await self._assess_difficulty(task)
        
        # 3. ç­”æ¡ˆå”¯ä¸€æ€§è¯„ä¼°
        dimensions['answer_uniqueness'] = await self._assess_answer_uniqueness(task)
        
        # 4. å·¥å…·éœ€æ±‚å‡†ç¡®æ€§è¯„ä¼°
        dimensions['tool_requirements'] = await self._assess_tool_requirements(task)
        
        # 5. è¯­è¨€è´¨é‡è¯„ä¼°
        dimensions['language_quality'] = await self._assess_language_quality(task)
        
        # 6. è®¤çŸ¥å¤æ‚åº¦è¯„ä¼°
        dimensions['cognitive_complexity'] = await self._assess_cognitive_complexity(task)
        
        # 7. åŸå­æ€§è¯„ä¼°ï¼ˆä»…é€‚ç”¨äºåŸå­ä»»åŠ¡ï¼‰
        if isinstance(task, AtomicTask):
            atomicity_verifier = AtomicityVerifier(self.llm_client)
            atomicity_result = await atomicity_verifier.verify_atomicity(task)
            dimensions['atomicity'] = atomicity_result['atomicity_score']
        else:
            dimensions['atomicity'] = 1.0  # éåŸå­ä»»åŠ¡ä¸éœ€è¦åŸå­æ€§æ£€æŸ¥
        
        return dimensions
    
    async def _assess_executability(self, task: TaskUnion) -> float:
        """è¯„ä¼°å¯æ‰§è¡Œæ€§"""
        try:
            expected_answer = task.golden_answer if hasattr(task, 'golden_answer') else task.golden_answers
            
            execution_result = await self.task_executor.execute_task_with_tools(
                task.question, 
                expected_answer,
                timeout=self.config.VERIFICATION_CONFIG['execution_timeout_seconds']
            )
            
            if execution_result['success'] and execution_result.get('answer_correct', False):
                return 1.0
            elif execution_result['success']:
                return 0.7  # èƒ½æ‰§è¡Œä½†ç­”æ¡ˆä¸å®Œå…¨æ­£ç¡®
            else:
                return 0.3  # æ‰§è¡Œå¤±è´¥
                
        except Exception as e:
            logger.error(f"âŒ å¯æ‰§è¡Œæ€§è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    async def _assess_difficulty(self, task: TaskUnion) -> float:
        """è¯„ä¼°éš¾åº¦é€‚ä¸­æ€§"""
        # åŸºäºä»»åŠ¡ç±»å‹å’Œå¤æ‚åº¦çš„å¯å‘å¼è¯„ä¼°
        if isinstance(task, AtomicTask):
            base_score = 0.8  # åŸå­ä»»åŠ¡é€šå¸¸éš¾åº¦é€‚ä¸­
        elif isinstance(task, ExtendedTask):
            # æ·±åº¦æ‰©å±•ä»»åŠ¡çš„éš¾åº¦ä¸è·³è·ƒæ•°ç›¸å…³
            hop_score = min(task.hop_level / 3.0, 1.0)
            base_score = 0.5 + hop_score * 0.4
        elif isinstance(task, CompositeTask):
            # å¤åˆä»»åŠ¡çš„éš¾åº¦ä¸åŸå­ä»»åŠ¡æ•°é‡ç›¸å…³
            composite_score = min(len(task.source_atomic_tasks) / 3.0, 1.0)
            base_score = 0.6 + composite_score * 0.3
        else:
            base_score = 0.5
        
        # åŸºäºå·¥å…·æ•°é‡è°ƒæ•´
        tool_count = len(getattr(task, 'expected_tools', getattr(task, 'required_tools', [])))
        tool_adjustment = min(tool_count / 3.0, 0.2)
        
        return min(base_score + tool_adjustment, 1.0)
    
    async def _assess_answer_uniqueness(self, task: TaskUnion) -> float:
        """è¯„ä¼°ç­”æ¡ˆå”¯ä¸€æ€§"""
        
        uniqueness_prompt = f"""
        è¯·è¯„ä¼°ä»¥ä¸‹é—®é¢˜çš„ç­”æ¡ˆæ˜¯å¦å…·æœ‰å”¯ä¸€æ€§ï¼š
        
        é—®é¢˜: {task.question}
        
        è¯„ä¼°æ ‡å‡†:
        1. é—®é¢˜æ˜¯å¦æœ‰æ˜ç¡®ã€å”¯ä¸€çš„æ­£ç¡®ç­”æ¡ˆï¼Ÿ
        2. æ˜¯å¦å­˜åœ¨å¤šä¸ªåŒæ ·æ­£ç¡®çš„ç­”æ¡ˆï¼Ÿ
        3. ç­”æ¡ˆæ˜¯å¦å…·ä½“ã€å¯éªŒè¯ï¼Ÿ
        
        è¯·è¿”å›0.0-1.0çš„åˆ†æ•°ï¼Œ1.0è¡¨ç¤ºç­”æ¡ˆå®Œå…¨å”¯ä¸€ã€‚
        """
        
        try:
            response = await self.llm_client.generate_reasoning(
                task_description=uniqueness_prompt,
                available_tools=[],
                execution_context={"mode": "answer_uniqueness_assessment"}
            )
            
            # ä»å“åº”ä¸­æå–åˆ†æ•°
            thinking = response.get('thinking', '')
            import re
            score_match = re.search(r'(\d+\.?\d*)', thinking)
            if score_match:
                score = float(score_match.group(1))
                return min(score, 1.0) if score <= 1.0 else score / 10.0
            else:
                return 0.7  # é»˜è®¤åˆ†æ•°
                
        except Exception as e:
            logger.error(f"âŒ ç­”æ¡ˆå”¯ä¸€æ€§è¯„ä¼°å¤±è´¥: {e}")
            return 0.5
    
    async def _assess_tool_requirements(self, task: TaskUnion) -> float:
        """è¯„ä¼°å·¥å…·éœ€æ±‚å‡†ç¡®æ€§"""
        expected_tools = getattr(task, 'expected_tools', getattr(task, 'required_tools', []))
        
        if not expected_tools:
            return 0.5  # æ²¡æœ‰å·¥å…·éœ€æ±‚ä¿¡æ¯
        
        # æ£€æŸ¥å·¥å…·çš„å¯ç”¨æ€§å’Œç›¸å…³æ€§
        available_tools = await self.task_executor._get_available_tools()
        
        # è®¡ç®—å·¥å…·åŒ¹é…åº¦
        available_set = set(available_tools)
        expected_set = set(expected_tools)
        
        if not expected_set:
            return 1.0
        
        intersection = available_set & expected_set
        match_ratio = len(intersection) / len(expected_set)
        
        return match_ratio
    
    async def _assess_language_quality(self, task: TaskUnion) -> float:
        """è¯„ä¼°è¯­è¨€è´¨é‡"""
        question = task.question
        
        # åŸºç¡€è¯­è¨€è´¨é‡æ£€æŸ¥
        score = 1.0
        
        # æ£€æŸ¥è¯­æ³•å’Œæ‹¼å†™ï¼ˆç®€å•å¯å‘å¼ï¼‰
        if len(question) < 10:
            score -= 0.3  # é—®é¢˜è¿‡çŸ­
        
        if question.count('?') == 0 and question.count('ï¼Ÿ') == 0:
            score -= 0.2  # ç¼ºå°‘é—®å·
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ— æ„ä¹‰çš„é‡å¤
        words = question.split()
        if len(words) != len(set(words)) and len(set(words)) / len(words) < 0.7:
            score -= 0.3  # é‡å¤è¯æ±‡è¿‡å¤š
        
        # æ£€æŸ¥æ ‡ç‚¹ç¬¦å·ä½¿ç”¨
        if question.count(',') + question.count('ï¼Œ') == 0 and len(words) > 15:
            score -= 0.1  # é•¿å¥ç¼ºå°‘é€—å·
        
        return max(score, 0.0)
    
    async def _assess_cognitive_complexity(self, task: TaskUnion) -> float:
        """è¯„ä¼°è®¤çŸ¥å¤æ‚åº¦"""
        
        complexity_prompt = f"""
        è¯·è¯„ä¼°ä»¥ä¸‹ä»»åŠ¡çš„è®¤çŸ¥å¤æ‚åº¦ï¼š
        
        ä»»åŠ¡: {task.question}
        
        è¯„ä¼°ç»´åº¦:
        1. éœ€è¦å¤šå°‘æ­¥éª¤çš„æ€è€ƒï¼Ÿ
        2. æ˜¯å¦éœ€è¦ç»¼åˆå¤šä¸ªä¿¡æ¯æºï¼Ÿ
        3. æ˜¯å¦éœ€è¦é€»è¾‘æ¨ç†æˆ–åˆ†æï¼Ÿ
        4. æ˜¯å¦éœ€è¦ä¸“ä¸šçŸ¥è¯†ï¼Ÿ
        
        è¯·è¿”å›0.0-1.0çš„å¤æ‚åº¦åˆ†æ•°ã€‚
        """
        
        try:
            response = await self.llm_client.generate_reasoning(
                task_description=complexity_prompt,
                available_tools=[],
                execution_context={"mode": "cognitive_complexity_assessment"}
            )
            
            # ä»å“åº”ä¸­æå–åˆ†æ•°
            thinking = response.get('thinking', '')
            import re
            score_match = re.search(r'(\d+\.?\d*)', thinking)
            if score_match:
                score = float(score_match.group(1))
                return min(score, 1.0) if score <= 1.0 else score / 10.0
            else:
                return 0.6  # é»˜è®¤ä¸­ç­‰å¤æ‚åº¦
                
        except Exception as e:
            logger.error(f"âŒ è®¤çŸ¥å¤æ‚åº¦è¯„ä¼°å¤±è´¥: {e}")
            return 0.5


class EnhancedVerificationEngine:
    """å¢å¼ºéªŒè¯å¼•æ“ - ç»Ÿä¸€çš„ä»»åŠ¡éªŒè¯æ¥å£"""
    
    def __init__(self, llm_client: LLMClient, mcp_client: Optional[MCPToolClient] = None):
        self.llm_client = llm_client
        self.mcp_client = mcp_client
        self.task_executor = TaskExecutor(llm_client, mcp_client)
        self.quality_assessor = QualityAssessor(llm_client, self.task_executor)
        self.config = EnhancedSynthesisConfig()
    
    async def comprehensive_task_verification(self, task: TaskUnion) -> VerificationResult:
        """ç»¼åˆä»»åŠ¡éªŒè¯"""
        logger.info(f"ğŸ” å¼€å§‹éªŒè¯ä»»åŠ¡: {task.task_id}")
        
        try:
            # 1. è´¨é‡ç»´åº¦è¯„ä¼°
            quality_dimensions = await self.quality_assessor.assess_task_quality(task)
            
            # 2. è®¡ç®—ç»¼åˆåˆ†æ•°
            overall_score = self._calculate_overall_score(quality_dimensions)
            
            # 3. ç”Ÿæˆå»ºè®®
            recommendation = self._generate_recommendation(overall_score, quality_dimensions)
            
            # 4. ç”Ÿæˆæ”¹è¿›å»ºè®®
            improvements = self._suggest_improvements(quality_dimensions)
            
            # 5. åˆ›å»ºéªŒè¯ç»“æœ
            verification_result = VerificationResult(
                task_id=task.task_id,
                overall_score=overall_score,
                verification_dimensions=quality_dimensions,
                recommendation=recommendation,
                suggested_improvements=improvements,
                details={
                    "task_type": type(task).__name__,
                    "question_length": len(task.question),
                    "has_tools": bool(getattr(task, 'expected_tools', getattr(task, 'required_tools', []))),
                    "verification_timestamp": time.time()
                }
            )
            
            logger.info(f"âœ… ä»»åŠ¡éªŒè¯å®Œæˆ: {task.task_id} (åˆ†æ•°: {overall_score:.3f})")
            return verification_result
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡éªŒè¯å¤±è´¥ {task.task_id}: {e}")
            return VerificationResult(
                task_id=task.task_id,
                overall_score=0.0,
                recommendation="reject",
                suggested_improvements=[f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}"],
                details={"error": str(e)}
            )
    
    def _calculate_overall_score(self, dimensions: Dict[str, float]) -> float:
        """è®¡ç®—ç»¼åˆåˆ†æ•°"""
        weights = self.config.VERIFICATION_CONFIG['dimension_weight']
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for dimension, score in dimensions.items():
            if dimension in weights:
                weight = weights[dimension]
                weighted_sum += score * weight
                total_weight += weight
        
        return weighted_sum / total_weight if total_weight > 0 else 0.0
    
    def _generate_recommendation(self, overall_score: float, dimensions: Dict[str, float]) -> str:
        """ç”Ÿæˆå»ºè®®"""
        threshold = self.config.VERIFICATION_CONFIG['overall_quality_threshold']
        
        if overall_score >= threshold:
            return "accept"
        elif overall_score >= threshold * 0.7:
            return "modify"
        else:
            return "reject"
    
    def _suggest_improvements(self, dimensions: Dict[str, float]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        improvements = []
        
        for dimension, score in dimensions.items():
            if score < 0.6:  # ä½äº60%çš„ç»´åº¦éœ€è¦æ”¹è¿›
                if dimension == "executability":
                    improvements.append("æé«˜ä»»åŠ¡çš„å¯æ‰§è¡Œæ€§ï¼Œç¡®ä¿æœ‰æ˜ç¡®çš„æ‰§è¡Œè·¯å¾„")
                elif dimension == "difficulty":
                    improvements.append("è°ƒæ•´ä»»åŠ¡éš¾åº¦ï¼Œä½¿å…¶æ›´é€‚åˆç›®æ ‡ç”¨æˆ·")
                elif dimension == "answer_uniqueness":
                    improvements.append("ä½¿é—®é¢˜çš„ç­”æ¡ˆæ›´åŠ æ˜ç¡®å’Œå”¯ä¸€")
                elif dimension == "tool_requirements":
                    improvements.append("æ£€æŸ¥å’Œä¼˜åŒ–å·¥å…·éœ€æ±‚çš„å‡†ç¡®æ€§")
                elif dimension == "language_quality":
                    improvements.append("æ”¹è¿›é—®é¢˜çš„è¯­è¨€è¡¨è¾¾å’Œè¯­æ³•")
                elif dimension == "cognitive_complexity":
                    improvements.append("è°ƒæ•´è®¤çŸ¥å¤æ‚åº¦ï¼Œä½¿å…¶æ›´åˆç†")
                elif dimension == "atomicity":
                    improvements.append("ç¡®ä¿ä»»åŠ¡çš„åŸå­æ€§ï¼Œé¿å…åŒ…å«å¤šä¸ªå­ä»»åŠ¡")
        
        if not improvements:
            improvements.append("ä»»åŠ¡è´¨é‡è‰¯å¥½ï¼Œæ— éœ€ç‰¹åˆ«æ”¹è¿›")
        
        return improvements
    
    async def batch_verification(self, tasks: List[TaskUnion]) -> List[VerificationResult]:
        """æ‰¹é‡éªŒè¯ä»»åŠ¡"""
        logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡éªŒè¯ {len(tasks)} ä¸ªä»»åŠ¡")
        
        # å¹¶è¡ŒéªŒè¯ï¼ˆé™åˆ¶å¹¶å‘æ•°ä»¥é¿å…èµ„æºè€—å°½ï¼‰
        semaphore = asyncio.Semaphore(self.config.ATOMIC_GENERATION_CONFIG['parallel_workers'])
        
        async def verify_with_semaphore(task):
            async with semaphore:
                return await self.comprehensive_task_verification(task)
        
        results = await asyncio.gather(
            *[verify_with_semaphore(task) for task in tasks],
            return_exceptions=True
        )
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        verification_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"âŒ ä»»åŠ¡ {tasks[i].task_id} éªŒè¯å¼‚å¸¸: {result}")
                verification_results.append(VerificationResult(
                    task_id=tasks[i].task_id,
                    overall_score=0.0,
                    recommendation="reject",
                    suggested_improvements=[f"éªŒè¯å¼‚å¸¸: {str(result)}"],
                    details={"exception": str(result)}
                ))
            else:
                verification_results.append(result)
        
        # ç»Ÿè®¡ç»“æœ
        accepted = len([r for r in verification_results if r.recommendation == "accept"])
        modified = len([r for r in verification_results if r.recommendation == "modify"])
        rejected = len([r for r in verification_results if r.recommendation == "reject"])
        
        logger.info(f"âœ… æ‰¹é‡éªŒè¯å®Œæˆ: æ¥å— {accepted}, ä¿®æ”¹ {modified}, æ‹’ç» {rejected}")
        return verification_results