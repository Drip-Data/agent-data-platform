#!/usr/bin/env python3
"""
Synthesis ç»Ÿä¸€åˆæˆå¼•æ“
ä¸¥æ ¼æŒ‰ç…§ Synthesis ç®—æ³•å®ç°çš„å•ä¸€ã€æ¸…æ™°çš„å®ç°
"""

import asyncio
import logging
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple

from .interfaces import (
    AtomicTask, DepthExtendedTask, WidthExtendedTask, TaskUnion,
    TaskValidationResult, SynthesisResult, TaskType, TaskComplexity,
    SynthesisInput, SynthesisContent, SynthesisAnswer, SynthesisRelation
)
from .task_validator import TaskValidator
from .task_storage import TaskStorage
from .prompts import prompt_manager
from .trajectory_step_extractor import EnhancedTrajectoryBasedTaskGenerator
from .enhanced_task_extensions import EnhancedTaskExtensions
from .task_complexity_evaluator import TaskComplexityEvaluator, ComplexityScore

logger = logging.getLogger(__name__)


class SynthesisEngine:
    """
    Synthesis ç»Ÿä¸€åˆæˆå¼•æ“
    
    å®ç°å®Œæ•´çš„ Synthesis ç®—æ³•æµç¨‹ï¼š
    1. åŸå­ä»»åŠ¡ç”Ÿæˆï¼šiT â†’ C â†’ (a, R) â†’ q
    2. æ·±åº¦æ‰©å±•ï¼šè¶…é›†æœç´¢ + ä¸­é—´ä»»åŠ¡ + ä»»åŠ¡åˆå¹¶
    3. å®½åº¦æ‰©å±•ï¼šå¤šä»»åŠ¡åˆå¹¶
    4. æ™ºèƒ½éªŒè¯ï¼šå·¥å…·ä»»åŠ¡ vs æ¨ç†ä»»åŠ¡
    """
    
    def __init__(self, llm_client, mcp_client=None, storage_dir: str = "output", 
                 enable_strict_validation: bool = True):
        self.llm_client = llm_client
        self.mcp_client = mcp_client
        self.enable_strict_validation = enable_strict_validation
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        # self.corpus_ingestor = CorpusIngestor()
        # self.atomic_generator = AtomicTaskGenerator(llm_client, mcp_client)
        # self.depth_extender = DepthExtender(llm_client)
        # self.width_extender = WidthExtender(llm_client)
        self.validator = TaskValidator(llm_client, enable_strict_validation)
        self.storage = TaskStorage(storage_dir)
        
        # åˆå§‹åŒ–å¢å¼ºçš„åŸºäºè½¨è¿¹çš„ä»»åŠ¡ç”Ÿæˆå™¨
        self.trajectory_task_generator = EnhancedTrajectoryBasedTaskGenerator(llm_client, self.validator)
        
        # åˆå§‹åŒ–å¢å¼ºæ‰©å±•å™¨å’Œå¤æ‚åº¦è¯„ä¼°å™¨
        self.enhanced_extensions = EnhancedTaskExtensions(llm_client)
        self.complexity_evaluator = TaskComplexityEvaluator()
        
        # è¿è¡Œç»Ÿè®¡
        self.session_stats = {
            "sessions_completed": 0,
            "total_tasks_generated": 0,
            "valid_tasks_count": 0,
            "tool_required_count": 0,
            "reasoning_only_count": 0  # Deprecated: now only generating tool_required tasks
        }
        
        # LLMé‡è¯•é…ç½®
        self.max_retries = 3
        self.retry_delay = 2.0
        
        logger.info("âœ… SynthesisEngine åˆå§‹åŒ–å®Œæˆ")
    
    async def _call_llm_with_retry(self, prompt: str, operation_name: str) -> str:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„LLMè°ƒç”¨æ–¹æ³•
        
        Args:
            prompt: å‘é€ç»™LLMçš„æç¤º
            operation_name: æ“ä½œåç§°ï¼Œç”¨äºæ—¥å¿—
            
        Returns:
            LLMå“åº”å†…å®¹
            
        Raises:
            RuntimeError: é‡è¯•æ¬¡æ•°ç”¨å°½åä»ç„¶å¤±è´¥
        """
        last_error = None
        
        for attempt in range(1, self.max_retries + 1):
            try:
                logger.debug(f"ğŸ”„ {operation_name} - å°è¯• {attempt}/{self.max_retries}")
                # å°†å­—ç¬¦ä¸²promptè½¬æ¢ä¸ºæ¶ˆæ¯åˆ—è¡¨æ ¼å¼
                messages = [{"role": "user", "content": prompt}]
                response = await self.llm_client._call_api(messages)
                logger.debug(f"âœ… {operation_name} - ç¬¬{attempt}æ¬¡å°è¯•æˆåŠŸ")
                return response
                
            except Exception as e:
                last_error = e
                logger.warning(f"âš ï¸ {operation_name} - ç¬¬{attempt}æ¬¡å°è¯•å¤±è´¥: {e}")
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
                if attempt < self.max_retries:
                    logger.info(f"â° ç­‰å¾… {self.retry_delay}s åé‡è¯•...")
                    await asyncio.sleep(self.retry_delay)
                else:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼ŒæŠ¥é”™
                    logger.error(f"âŒ {operation_name} - æ‰€æœ‰ {self.max_retries} æ¬¡å°è¯•å‡å¤±è´¥")
                    break
        
        # æŠ›å‡ºè¿è¡Œæ—¶é”™è¯¯ï¼Œä¸å†ä½¿ç”¨ç®€å•å›é€€
        raise RuntimeError(f"{operation_name} å¤±è´¥ï¼šç»è¿‡ {self.max_retries} æ¬¡é‡è¯•ä»æ— æ³•ä¸LLMæ­£å¸¸é€šä¿¡ã€‚æœ€åé”™è¯¯: {last_error}")
    
    async def synthesize_from_trajectories(self, trajectories_data: List[Dict], 
                                         generate_depth_extensions: bool = True,
                                         generate_width_extensions: bool = True,
                                         max_atomic_tasks: int = 20,
                                         max_depth_extensions: int = 10,
                                         max_width_extensions: int = 5) -> SynthesisResult:
        """
        ä»è½¨è¿¹æ•°æ®åˆæˆä»»åŠ¡çš„ä¸»å…¥å£
        
        Args:
            trajectories_data: è½¨è¿¹æ•°æ®åˆ—è¡¨
            generate_depth_extensions: æ˜¯å¦ç”Ÿæˆæ·±åº¦æ‰©å±•
            generate_width_extensions: æ˜¯å¦ç”Ÿæˆå®½åº¦æ‰©å±•
            max_atomic_tasks: æœ€å¤§åŸå­ä»»åŠ¡æ•°é‡
            max_depth_extensions: æœ€å¤§æ·±åº¦æ‰©å±•æ•°é‡
            max_width_extensions: æœ€å¤§å®½åº¦æ‰©å±•æ•°é‡
        """
        session_id = f"synthesis_{uuid.uuid4().hex[:8]}"
        logger.info(f"ğŸš€ å¼€å§‹ Synthesis åˆæˆä¼šè¯: {session_id}")
        
        start_time = datetime.now()
        result = SynthesisResult(
            session_id=session_id,
            source_trajectories=[t.get("task_id", "unknown") for t in trajectories_data]
        )
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šç”ŸæˆåŸå­ä»»åŠ¡
            logger.info("ğŸ“‹ ç¬¬ä¸€æ­¥ï¼šç”ŸæˆåŸå­ä»»åŠ¡ (iT â†’ C â†’ (a, R) â†’ q)")
            atomic_tasks = await self._generate_atomic_tasks(trajectories_data, max_atomic_tasks)
            result.atomic_tasks = atomic_tasks
            
            # éªŒè¯åŸå­ä»»åŠ¡
            logger.info("ğŸ” éªŒè¯åŸå­ä»»åŠ¡")
            atomic_validations = await self.validator.batch_validate_tasks(atomic_tasks)
            result.validation_results.extend(atomic_validations)
            
            # è¿‡æ»¤æœ‰æ•ˆçš„åŸå­ä»»åŠ¡
            valid_atomic_tasks = [
                task for task, validation in zip(atomic_tasks, atomic_validations)
                if validation.is_valid
            ]
            logger.info(f"âœ… æœ‰æ•ˆåŸå­ä»»åŠ¡: {len(valid_atomic_tasks)}/{len(atomic_tasks)}")
            
            # ç¬¬äºŒæ­¥ï¼šå¢å¼ºæ·±åº¦æ‰©å±•ï¼ˆå¯é€‰ï¼‰
            if generate_depth_extensions and valid_atomic_tasks:
                logger.info("ğŸ“ˆ ç¬¬äºŒæ­¥ï¼šå¢å¼ºæ·±åº¦æ‰©å±• (LLMé©±åŠ¨çš„å¤šæ­¥éª¤æ¨ç†)")
                depth_tasks = await self.enhanced_extensions.generate_enhanced_depth_extensions(
                    valid_atomic_tasks, max_depth_extensions
                )
                
                # è¯„ä¼°å’Œè¿‡æ»¤æ·±åº¦æ‰©å±•ä»»åŠ¡
                filtered_depth_tasks = []
                for task in depth_tasks:
                    complexity_score = self.complexity_evaluator.evaluate_depth_extended_task(task)
                    logger.info(f"ğŸ“Š æ·±åº¦ä»»åŠ¡ {task.task_id} å¤æ‚åº¦: {complexity_score.complexity_level} (åˆ†æ•°: {complexity_score.total_score:.2f})")
                    
                    # ğŸ”§ ä¿®å¤ï¼šé™ä½å¤æ‚åº¦é—¨æ§›ï¼ŒåŒ…å«ç®€å•ä»»åŠ¡ä»¥æå‡ç»¼åˆä»»åŠ¡ç”Ÿæˆç‡
                    if complexity_score.complexity_level in ["simple", "moderate", "complex", "comprehensive"]:
                        filtered_depth_tasks.append(task)
                        logger.info(f"âœ… æ·±åº¦ä»»åŠ¡é€šè¿‡å¤æ‚åº¦æ£€æŸ¥: {task.task_id} (çº§åˆ«: {complexity_score.complexity_level})")
                    else:
                        logger.warning(f"âš ï¸ æ·±åº¦ä»»åŠ¡å¤æ‚åº¦è¿‡ä½: {task.task_id} - {complexity_score.quality_issues}")
                
                result.depth_extended_tasks = filtered_depth_tasks
                
                # éªŒè¯é€šè¿‡è¯„ä¼°çš„æ·±åº¦æ‰©å±•ä»»åŠ¡
                if filtered_depth_tasks:
                    depth_validations = await self.validator.batch_validate_tasks(filtered_depth_tasks)
                    result.validation_results.extend(depth_validations)
            
            # ç¬¬ä¸‰æ­¥ï¼šå¢å¼ºå®½åº¦æ‰©å±•ï¼ˆå¯é€‰ï¼‰
            if generate_width_extensions and len(valid_atomic_tasks) >= 2:
                logger.info("ğŸ“Š ç¬¬ä¸‰æ­¥ï¼šå¢å¼ºå®½åº¦æ‰©å±• (æ™ºèƒ½ååŒä»»åŠ¡)")
                width_tasks = await self.enhanced_extensions.generate_enhanced_width_extensions(
                    valid_atomic_tasks, max_width_extensions
                )
                
                # è¯„ä¼°å’Œè¿‡æ»¤å®½åº¦æ‰©å±•ä»»åŠ¡
                filtered_width_tasks = []
                for task in width_tasks:
                    complexity_score = self.complexity_evaluator.evaluate_width_extended_task(task)
                    logger.info(f"ğŸ“Š å®½åº¦ä»»åŠ¡ {task.task_id} å¤æ‚åº¦: {complexity_score.complexity_level} (åˆ†æ•°: {complexity_score.total_score:.2f})")
                    
                    # ğŸ”§ ä¿®å¤ï¼šé™ä½ååŒä»·å€¼é—¨æ§›ï¼ŒåŒ…å«ç®€å•ä»»åŠ¡ä»¥æå‡ç»¼åˆä»»åŠ¡ç”Ÿæˆç‡
                    if complexity_score.complexity_level in ["simple", "moderate", "complex", "comprehensive"]:
                        filtered_width_tasks.append(task)
                        logger.info(f"âœ… å®½åº¦ä»»åŠ¡é€šè¿‡å¤æ‚åº¦æ£€æŸ¥: {task.task_id} (çº§åˆ«: {complexity_score.complexity_level})")
                    else:
                        logger.warning(f"âš ï¸ å®½åº¦ä»»åŠ¡ååŒä»·å€¼è¿‡ä½: {task.task_id} - {complexity_score.quality_issues}")
                
                result.width_extended_tasks = filtered_width_tasks
                
                # éªŒè¯é€šè¿‡è¯„ä¼°çš„å®½åº¦æ‰©å±•ä»»åŠ¡
                if filtered_width_tasks:
                    width_validations = await self.validator.batch_validate_tasks(filtered_width_tasks)
                    result.validation_results.extend(width_validations)
            
            # ç¬¬å››æ­¥ï¼šå­˜å‚¨ç»“æœ
            logger.info("ğŸ’¾ ç¬¬å››æ­¥ï¼šå­˜å‚¨åˆæˆç»“æœ")
            await self._store_synthesis_results(result)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            self._calculate_result_statistics(result)
            
            # æ›´æ–°ä¼šè¯ç»Ÿè®¡
            self._update_session_stats(result)
            
            duration = (datetime.now() - start_time).total_seconds()
            logger.info(f"ğŸ‰ Synthesis åˆæˆå®Œæˆ: {session_id}, è€—æ—¶: {duration:.2f}s")
            logger.info(f"ğŸ“Š ç”Ÿæˆç»Ÿè®¡: åŸå­{len(result.atomic_tasks)}, æ·±åº¦{len(result.depth_extended_tasks)}, å®½åº¦{len(result.width_extended_tasks)}")
            logger.info(f"âœ… æœ‰æ•ˆä»»åŠ¡: {result.valid_tasks_count}/{result.total_tasks_generated}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Synthesis åˆæˆå¤±è´¥: {e}", exc_info=True)
            result.validation_results.append(TaskValidationResult(
                task_id="synthesis_error",
                is_valid=False,
                requires_tool=False,
                validation_score=0.0,
                tool_necessity_check=False,
                reasoning_sufficiency_check=False,
                atomicity_check=False,
                errors=[f"åˆæˆè¿‡ç¨‹å¼‚å¸¸: {str(e)}"]
            ))
            return result
    
    async def _generate_atomic_tasks(self, trajectories_data: List[Dict], max_tasks: int) -> List[AtomicTask]:
        """ç”ŸæˆåŸå­ä»»åŠ¡ - æ··åˆä½¿ç”¨åŸºäºè½¨è¿¹è¯æ®çš„ç”Ÿæˆå’ŒLLMç”Ÿæˆ"""
        logger.debug(f"ğŸ”¬ ä» {len(trajectories_data)} ä¸ªè½¨è¿¹ç”ŸæˆåŸå­ä»»åŠ¡")
        
        atomic_tasks = []
        
        try:
            # ğŸ†• æ­¥éª¤1ï¼šåŸºäºè½¨è¿¹è¯æ®ç”Ÿæˆä»»åŠ¡ï¼ˆå 60%ï¼‰
            evidence_task_count = max(1, int(max_tasks * 0.6))
            logger.info(f"ğŸ§¬ å¼€å§‹åŸºäºè½¨è¿¹è¯æ®ç”Ÿæˆ {evidence_task_count} ä¸ªä»»åŠ¡")
            
            evidence_tasks = await self.trajectory_task_generator.generate_evidence_based_tasks(
                trajectories_data, max_tasks=evidence_task_count
            )
            
            # å°†è¯æ®ä»»åŠ¡è½¬æ¢ä¸ºAtomicTaskå¯¹è±¡
            for i, task_data in enumerate(evidence_tasks):
                if len(atomic_tasks) >= max_tasks:
                    break
                    
                # åˆ›å»ºå¢å¼ºçš„Synthesisç»„ä»¶
                input_info = SynthesisInput(
                    input_id=f"evidence_input_{i}",
                    content=task_data.get("question", "æœªçŸ¥é—®é¢˜"),
                    metadata={
                        "difficulty": task_data.get("difficulty", "ä¸­ç­‰"),
                        "creativity_level": task_data.get("creativity_level", "3"),
                        "source_conclusion": "åŸºäºè½¨è¿¹è¯æ®ç”Ÿæˆ",
                        "task_pattern": task_data.get("relation_pattern", "trajectory_evidence")
                    }
                )
                
                answer = SynthesisAnswer(
                    answer_id=f"evidence_answer_{i}",
                    answer=task_data.get("expected_answer", "åŸºäºè½¨è¿¹è¯æ®çš„ç­”æ¡ˆ"),
                    confidence=float(task_data.get("creativity_level", "3")) / 5.0
                )
                
                relation = SynthesisRelation(
                    relation_id=f"evidence_relation_{i}",
                    relation_type=task_data.get("relation_pattern", "trajectory_evidence"),
                    description=task_data.get("creativity_explanation", "åŸºäºè½¨è¿¹è¯æ®çš„ä»»åŠ¡"),
                    parameters={
                        "reasoning_steps": task_data.get("reasoning_steps", []),
                        "entity_generalization": task_data.get("entity_generalization", ""),
                        "reverse_reasoning": task_data.get("reverse_reasoning", "")
                    }
                )
                
                # åˆ›å»ºåŸå­ä»»åŠ¡
                atomic_task = AtomicTask.create_atomic(
                    question=task_data.get("question", "æœªçŸ¥é—®é¢˜"),
                    input_info=input_info,
                    answer=answer,
                    relation=relation,
                    domain=task_data.get("domain", "general"),
                    requires_tool=task_data.get("required_tools", []) != [],
                    expected_tools=task_data.get("required_tools", [])
                )
                
                atomic_tasks.append(atomic_task)
                
            logger.info(f"âœ¨ åŸºäºè½¨è¿¹è¯æ®ç”Ÿæˆäº† {len(atomic_tasks)} ä¸ªä»»åŠ¡")
            
            # æ­¥éª¤2ï¼šåŸºäºç»“è®ºçš„ä¼ ç»ŸLLMç”Ÿæˆï¼ˆå 40%ï¼‰
            remaining_tasks = max_tasks - len(atomic_tasks)
            if remaining_tasks > 0:
                logger.info(f"ğŸ§  å¼€å§‹åŸºäºç»“è®ºçš„LLMç”Ÿæˆ {remaining_tasks} ä¸ªä»»åŠ¡")
                
                # ä»è½¨è¿¹æ•°æ®ä¸­æå–ç»“è®º
                conclusions = await self._extract_conclusions_from_trajectories(trajectories_data)
                logger.info(f"ğŸ“Š æå–åˆ° {len(conclusions)} ä¸ªç»“è®º")
                
                # åŸºäºç»“è®ºç”ŸæˆåŸå­ä»»åŠ¡
                for conclusion in conclusions[:remaining_tasks]:
                    try:
                        generated_tasks = await self._generate_tasks_from_conclusion(conclusion)
                        
                        for task_data in generated_tasks:
                            if len(atomic_tasks) >= max_tasks:
                                break
                                
                            # åˆ›å»ºå¢å¼ºçš„Synthesisç»„ä»¶
                            input_info = SynthesisInput(
                                input_id=f"llm_input_{len(atomic_tasks)}",
                                content=task_data.get("question", "æœªçŸ¥é—®é¢˜"),
                                metadata={
                                    "difficulty": task_data.get("difficulty", "ä¸­ç­‰"),
                                    "creativity_level": task_data.get("creativity_level", "3"),
                                    "source_conclusion": conclusion.get("content", ""),
                                    "task_pattern": task_data.get("relation_pattern", "general")
                                }
                            )
                            
                            answer = SynthesisAnswer(
                                answer_id=f"llm_answer_{len(atomic_tasks)}",
                                answer=task_data.get("expected_answer", "ç¤ºä¾‹ç­”æ¡ˆ"),
                                confidence=float(task_data.get("creativity_level", "3")) / 5.0
                            )
                            
                            # ä½¿ç”¨æ›´ä¸°å¯Œçš„å…³ç³»ä¿¡æ¯
                            relation_type = task_data.get("relation_pattern", "extract_info")
                            relation = SynthesisRelation(
                                relation_id=f"llm_relation_{len(atomic_tasks)}",
                                relation_type=relation_type,
                                description=task_data.get("creativity_explanation", "ä»è¾“å…¥ä¸­æå–ä¿¡æ¯"),
                                parameters={
                                    "reasoning_steps": task_data.get("reasoning_steps", []),
                                    "entity_generalization": task_data.get("entity_generalization", ""),
                                    "reverse_reasoning": task_data.get("reverse_reasoning", "")
                                }
                            )
                            
                            # åˆ›å»ºåŸå­ä»»åŠ¡
                            atomic_task = AtomicTask.create_atomic(
                                question=task_data.get("question", "æœªçŸ¥é—®é¢˜"),
                                input_info=input_info,
                                answer=answer,
                                relation=relation,
                                domain=task_data.get("domain", "general"),
                                requires_tool=task_data.get("required_tools", []) != [],
                                expected_tools=task_data.get("required_tools", [])
                            )
                            
                            atomic_tasks.append(atomic_task)
                            
                    except Exception as e:
                        logger.error(f"âŒ ä»ç»“è®ºç”ŸæˆåŸå­ä»»åŠ¡å¤±è´¥: {e}")
                        continue
                        
        except Exception as e:
            logger.error(f"âŒ åŸå­ä»»åŠ¡ç”Ÿæˆè¿‡ç¨‹å¤±è´¥: {e}")
            # å¤±è´¥æ—¶ç›´æ¥æŠ¥é”™ï¼Œä¸å†å›é€€
            raise RuntimeError(f"åŸå­ä»»åŠ¡ç”Ÿæˆå¤±è´¥: {e}")
        
        logger.info(f"ğŸ“‹ ç”ŸæˆåŸå­ä»»åŠ¡: {len(atomic_tasks)} ä¸ª (è¯æ®ä»»åŠ¡: {len([t for t in atomic_tasks if 'evidence' in t.input_info.input_id])}, LLMä»»åŠ¡: {len([t for t in atomic_tasks if 'llm' in t.input_info.input_id])})")
        return atomic_tasks
    
    async def _generate_depth_extensions(self, atomic_tasks: List[AtomicTask], max_extensions: int) -> List[DepthExtendedTask]:
        """ç”Ÿæˆæ·±åº¦æ‰©å±•ä»»åŠ¡ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        logger.debug(f"ğŸ“ˆ ä» {len(atomic_tasks)} ä¸ªåŸå­ä»»åŠ¡ç”Ÿæˆæ·±åº¦æ‰©å±•")
        
        depth_tasks = []
        
        # é€‰æ‹©åˆé€‚çš„åŸå­ä»»åŠ¡è¿›è¡Œæ·±åº¦æ‰©å±•
        suitable_tasks = [task for task in atomic_tasks if task.requires_tool]
        
        for task in suitable_tasks[:max_extensions]:
            try:
                # ç®€åŒ–çš„æ·±åº¦æ‰©å±•ï¼šåˆ›å»ºè¶…é›†è¾“å…¥
                superset_input = SynthesisInput(
                    input_id=f"superset_{task.task_id}",
                    content=f"æ‰©å±•çš„è¾“å…¥å†…å®¹ï¼ŒåŒ…å«ï¼š{task.input_info.content}"
                )
                
                superset_relation = SynthesisRelation(
                    relation_id=f"superset_relation_{task.task_id}",
                    relation_type="superset_extraction",
                    description="ä»æ›´å¤§èŒƒå›´æå–ä¿¡æ¯"
                )
                
                extended_task = DepthExtendedTask.create_depth_extended(
                    base_task=task,
                    superset_input=superset_input,
                    superset_relation=superset_relation,
                    intermediate_question=f"é¦–å…ˆå¤„ç†ï¼š{superset_input.content}",
                    combined_question=f"å…ˆå¤„ç†æ‰©å±•è¾“å…¥ï¼Œç„¶åè§£å†³ï¼š{task.question}",
                    combined_answer=f"é€šè¿‡ä¸¤æ­¥å¤„ç†å¾—åˆ°ï¼š{task.answer.answer}"
                )
                
                depth_tasks.append(extended_task)
                    
            except Exception as e:
                logger.error(f"âŒ æ·±åº¦æ‰©å±•å¤±è´¥ {task.task_id}: {e}")
                continue
        
        logger.info(f"ğŸ“ˆ ç”Ÿæˆæ·±åº¦æ‰©å±•ä»»åŠ¡: {len(depth_tasks)} ä¸ª")
        return depth_tasks
    
    async def _generate_width_extensions(self, atomic_tasks: List[AtomicTask], max_extensions: int) -> List[WidthExtendedTask]:
        """ç”Ÿæˆå®½åº¦æ‰©å±•ä»»åŠ¡ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        logger.debug(f"ğŸ“Š ä» {len(atomic_tasks)} ä¸ªåŸå­ä»»åŠ¡ç”Ÿæˆå®½åº¦æ‰©å±•")
        
        width_tasks = []
        
        # å°†åŸå­ä»»åŠ¡æŒ‰é¢†åŸŸåˆ†ç»„ï¼Œä¾¿äºåˆå¹¶
        domain_groups = {}
        for task in atomic_tasks:
            domain = task.domain
            if domain not in domain_groups:
                domain_groups[domain] = []
            domain_groups[domain].append(task)
        
        # ç”Ÿæˆå®½åº¦æ‰©å±•
        extensions_generated = 0
        for domain, tasks in domain_groups.items():
            if extensions_generated >= max_extensions:
                break
                
            if len(tasks) >= 2:
                try:
                    # æ¯ä¸ªé¢†åŸŸæœ€å¤šåˆå¹¶2-3ä¸ªä»»åŠ¡
                    for i in range(0, len(tasks), 2):
                        if extensions_generated >= max_extensions:
                            break
                            
                        component_tasks = tasks[i:i+2]
                        if len(component_tasks) >= 2:
                            # ç®€åŒ–çš„å®½åº¦æ‰©å±•ï¼šåˆå¹¶ä»»åŠ¡
                            merged_question = f"è¯·åŒæ—¶å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š1) {component_tasks[0].question} 2) {component_tasks[1].question}"
                            merged_answer = f"1) {component_tasks[0].answer.answer} 2) {component_tasks[1].answer.answer}"
                            
                            extended_task = WidthExtendedTask.create_width_extended(
                                component_tasks=component_tasks,
                                merged_question=merged_question,
                                merged_answer=merged_answer,
                                merge_strategy="parallel"
                            )
                            
                            width_tasks.append(extended_task)
                            extensions_generated += 1
                                
                except Exception as e:
                    logger.error(f"âŒ å®½åº¦æ‰©å±•å¤±è´¥ {domain}: {e}")
                    continue
        
        logger.info(f"ğŸ“Š ç”Ÿæˆå®½åº¦æ‰©å±•ä»»åŠ¡: {len(width_tasks)} ä¸ª")
        return width_tasks
    
    async def _extract_conclusions_from_trajectories(self, trajectories_data: List[Dict]) -> List[Dict]:
        """ä»è½¨è¿¹æ•°æ®ä¸­æå–æ·±åº¦ç»“è®ºå’Œç»“æ„åŒ–å…³ç³» - ä½¿ç”¨å…³ç³»é©±åŠ¨çš„æ¨¡æ¿"""
        try:
            # å‡†å¤‡è½¨è¿¹æ•°æ®æ‘˜è¦ï¼ˆä¿®å¤å­—æ®µæ˜ å°„ï¼‰
            trajectory_summary = []
            for trajectory in trajectories_data[:5]:  # é™åˆ¶å¤„ç†æ•°é‡
                # ä¿®å¤å­—æ®µæ˜ å°„ï¼šä½¿ç”¨å®é™…å­˜åœ¨çš„å­—æ®µ
                raw_response = trajectory.get("raw_response", "")
                
                # è§£ææ­¥éª¤å’Œå·¥å…·ä¿¡æ¯
                parsed_steps = self._parse_steps_from_response(raw_response)
                tools_used = self._extract_tools_from_response(raw_response)
                reasoning_blocks = self._extract_reasoning_from_response(raw_response)
                
                summary = {
                    "task_id": trajectory.get("task_id", "unknown"),
                    "question": trajectory.get("task_description", "æœªçŸ¥é—®é¢˜"),  # ä¿®å¤ï¼štask_description
                    "steps": parsed_steps[:5],  # ä»raw_responseè§£æçš„çœŸå®æ­¥éª¤
                    "final_answer": trajectory.get("final_result", "æ— ç­”æ¡ˆ"),  # ä¿®å¤ï¼šfinal_result
                    "success": trajectory.get("success", False),
                    "tools_used": tools_used,  # ä»raw_responseè§£æçš„å·¥å…·
                    "reasoning_process": reasoning_blocks,  # ä»raw_responseè§£æçš„æ¨ç†è¿‡ç¨‹
                    "domain": self._infer_domain_from_content(trajectory.get("task_description", "")),
                    "duration": trajectory.get("duration", 0),
                    "raw_content": raw_response[:500]  # ä¿ç•™éƒ¨åˆ†åŸå§‹å†…å®¹ç”¨äºåˆ†æ
                }
                trajectory_summary.append(summary)
            
            # ä½¿ç”¨å¢å¼ºçš„æ·±åº¦ç»“è®ºæå–æ¨¡æ¿ï¼ˆå…³ç³»é©±åŠ¨ï¼‰
            prompt = prompt_manager.render_template(
                "extract_conclusions",
                trajectory_data=str(trajectory_summary),
                max_conclusions=3
            )
            
            # è°ƒç”¨LLMè¿›è¡Œæ·±åº¦åˆ†æï¼ˆä½¿ç”¨é‡è¯•æœºåˆ¶ï¼‰
            response = await self._call_llm_with_retry(prompt, "æ·±åº¦ç»“è®ºæå–")
            
            # è§£æå“åº”ï¼ˆåŒ…æ‹¬ç»“æ„åŒ–å…³ç³»ï¼‰
            try:
                import json
                # ç¡®ä¿responseæ˜¯å­—ç¬¦ä¸²
                if isinstance(response, dict):
                    result = response
                else:
                    result = json.loads(response)
                conclusions = result.get("conclusions", [])
                
                # éªŒè¯ç»“è®ºè´¨é‡ï¼ˆç¡®ä¿åŒ…å«å…³ç³»ä¿¡æ¯ï¼‰
                valid_conclusions = []
                for conclusion in conclusions:
                    if (
                        conclusion.get("content") and 
                        conclusion.get("entities") and 
                        conclusion.get("relation") and
                        conclusion.get("relation_type")
                    ):
                        valid_conclusions.append(conclusion)
                    else:
                        logger.warning(f"âš ï¸ ç»“è®ºç¼ºå°‘å…³é”®å…³ç³»ä¿¡æ¯ï¼Œå·²è¿‡æ»¤: {conclusion.get('content', 'Unknown')}")
                
                logger.info(f"âœ… æˆåŠŸæå– {len(valid_conclusions)} ä¸ªåŒ…å«ç»“æ„åŒ–å…³ç³»çš„ç»“è®º")
                return valid_conclusions if valid_conclusions else self._get_default_conclusions(trajectories_data)
                
            except json.JSONDecodeError:
                logger.warning("âš ï¸ LLMå“åº”æ ¼å¼ä¸æ­£ç¡®ï¼Œå°è¯•ä¿®å¤åå†è§£æ")
                # å°è¯•ä¿®å¤å’Œé‡æ–°è§£æ
                fixed_response = self._attempt_json_repair(response)
                if fixed_response:
                    try:
                        result = json.loads(fixed_response)
                        conclusions = result.get("conclusions", [])
                        logger.info(f"âœ… JSONä¿®å¤æˆåŠŸï¼Œæå–åˆ° {len(conclusions)} ä¸ªç»“è®º")
                        return conclusions if conclusions else self._get_default_conclusions(trajectories_data)
                    except json.JSONDecodeError:
                        logger.warning("âš ï¸ JSONä¿®å¤å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç»“è®º")
                
                return self._get_default_conclusions(trajectories_data)
                
        except Exception as e:
            logger.error(f"âŒ æ·±åº¦ç»“è®ºæå–å¤±è´¥: {e}")
            return self._get_default_conclusions(trajectories_data)
    
    async def _generate_tasks_from_conclusion(self, conclusion: Dict) -> List[Dict]:
        """åŸºäºç»“è®ºå’Œç»“æ„åŒ–å…³ç³»ç”Ÿæˆåˆ›é€ æ€§åŸå­ä»»åŠ¡ - ä½¿ç”¨å…³ç³»é©±åŠ¨çš„åå‘æ¨ç†"""
        try:
            # éªŒè¯ç»“è®ºæ˜¯å¦åŒ…å«å¿…è¦çš„å…³ç³»ä¿¡æ¯
            if not all(k in conclusion for k in ["content", "entities", "relation", "relation_type"]):
                logger.warning(f"âš ï¸ ç»“è®ºç¼ºå°‘å…³é”®å…³ç³»ä¿¡æ¯ï¼Œä½¿ç”¨ç®€åŒ–ç”Ÿæˆ: {conclusion.get('content', 'Unknown')}")
                return await self._fallback_task_generation(conclusion)
            
            # ä½¿ç”¨å…³ç³»é©±åŠ¨çš„åŸå­ä»»åŠ¡ç”Ÿæˆæ¨¡æ¿
            prompt = prompt_manager.render_template(
                "generate_atomic_tasks",
                conclusion=str(conclusion),
                max_tasks=2
            )
            
            # è°ƒç”¨LLMè¿›è¡Œåˆ›é€ æ€§ä»»åŠ¡ç”Ÿæˆï¼ˆä½¿ç”¨é‡è¯•æœºåˆ¶ï¼‰
            response = await self._call_llm_with_retry(prompt, "å…³ç³»é©±åŠ¨ä»»åŠ¡ç”Ÿæˆ")
            
            # è§£æå“åº”å¹¶éªŒè¯åˆ›é€ æ€§
            try:
                import json
                # ç¡®ä¿responseæ˜¯å­—ç¬¦ä¸²
                if isinstance(response, dict):
                    result = response
                else:
                    result = json.loads(response)
                tasks = result.get("atomic_tasks", [])
                
                # éªŒè¯ä»»åŠ¡çš„åˆ›é€ æ€§å’Œå…³ç³»é©±åŠ¨ç‰¹å¾
                creative_tasks = []
                for task in tasks:
                    creativity_level = task.get("creativity_level", "1")
                    relation_pattern = task.get("relation_pattern", "")
                    entity_generalization = task.get("entity_generalization", "")
                    
                    # è¦æ±‚åˆ›é€ æ€§ç­‰çº§â‰¥3ä¸”åŒ…å«å…³ç³»ä¿¡æ¯
                    if (
                        int(creativity_level) >= 3 and 
                        relation_pattern and 
                        entity_generalization and
                        task.get("question") and 
                        task.get("reverse_reasoning")
                    ):
                        creative_tasks.append(task)
                        logger.debug(f"âœ¨ åˆ›é€ æ€§ä»»åŠ¡: {task.get('question', 'Unknown')[:50]}... (åˆ›é€ æ€§: {creativity_level}æ˜Ÿ)")
                    else:
                        logger.debug(f"âŒ è¿‡æ»¤ä½åˆ›é€ æ€§ä»»åŠ¡: {task.get('question', 'Unknown')[:30]}...")
                
                if creative_tasks:
                    logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(creative_tasks)} ä¸ªé«˜åˆ›é€ æ€§ä»»åŠ¡ (å…³ç³»: {conclusion.get('relation_type', 'Unknown')})")
                    return creative_tasks
                else:
                    logger.warning("âš ï¸ æœªç”Ÿæˆç¬¦åˆè¦æ±‚çš„åˆ›é€ æ€§ä»»åŠ¡ï¼Œä½¿ç”¨ç®€åŒ–ç”Ÿæˆ")
                    return await self._fallback_task_generation(conclusion)
                    
            except json.JSONDecodeError:
                logger.warning("âš ï¸ åˆ›é€ æ€§ä»»åŠ¡ç”Ÿæˆå“åº”æ ¼å¼ä¸æ­£ç¡®ï¼Œå°è¯•ä¿®å¤åå†è§£æ")
                # å°è¯•ä¿®å¤å’Œé‡æ–°è§£æ
                fixed_response = self._attempt_json_repair(response)
                if fixed_response:
                    try:
                        result = json.loads(fixed_response)
                        atomic_tasks = result.get("atomic_tasks", [])
                        logger.info(f"âœ… JSONä¿®å¤æˆåŠŸï¼Œæå–åˆ° {len(atomic_tasks)} ä¸ªä»»åŠ¡")
                        if atomic_tasks:
                            return atomic_tasks
                    except json.JSONDecodeError:
                        logger.warning("âš ï¸ JSONä¿®å¤å¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ³•")
                
                return await self._fallback_task_generation(conclusion)
                
        except Exception as e:
            logger.error(f"âŒ å…³ç³»é©±åŠ¨çš„ä»»åŠ¡ç”Ÿæˆå¤±è´¥: {e}")
            return await self._fallback_task_generation(conclusion)
    
    async def _fallback_atomic_generation(self, trajectories_data: List[Dict], max_tasks: int) -> List[AtomicTask]:
        """å›é€€çš„åŸå­ä»»åŠ¡ç”Ÿæˆæ–¹æ³•"""
        logger.info("ğŸ”„ ä½¿ç”¨å›é€€æ–¹æ³•ç”ŸæˆåŸå­ä»»åŠ¡")
        
        atomic_tasks = []
        
        for trajectory_data in trajectories_data[:max_tasks]:
            try:
                # ç®€åŒ–çš„åŸå­ä»»åŠ¡ç”Ÿæˆ
                task_id = trajectory_data.get('task_id', f'trajectory_{len(atomic_tasks)}')
                description = trajectory_data.get('question', 'æœªçŸ¥ä»»åŠ¡')
                
                # åˆ›å»ºç®€å•çš„Synthesisç»„ä»¶
                input_info = SynthesisInput(
                    input_id=f"input_{task_id}",
                    content=description
                )
                
                answer = SynthesisAnswer(
                    answer_id=f"answer_{task_id}",
                    answer="ç¤ºä¾‹ç­”æ¡ˆ"
                )
                
                relation = SynthesisRelation(
                    relation_id=f"relation_{task_id}",
                    relation_type="extract_info",
                    description="ä»è¾“å…¥ä¸­æå–ä¿¡æ¯"
                )
                
                atomic_task = AtomicTask.create_atomic(
                    question=f"è¯·è§£å†³ä»¥ä¸‹ä»»åŠ¡ï¼š{description}",
                    input_info=input_info,
                    answer=answer,
                    relation=relation,
                    domain="general",
                    requires_tool=True  # é»˜è®¤éœ€è¦å·¥å…·
                )
                
                atomic_tasks.append(atomic_task)
                
                if len(atomic_tasks) >= max_tasks:
                    break
                    
            except Exception as e:
                logger.error(f"âŒ å›é€€ç”Ÿæˆå¤±è´¥ {trajectory_data.get('task_id', 'unknown')}: {e}")
                continue
        
        return atomic_tasks
    
    def _attempt_json_repair(self, response: str) -> str:
        """
        å°è¯•ä¿®å¤æŸåçš„JSONå“åº”ï¼Œå¢å¼ºç³»ç»Ÿçš„å®¹é”™èƒ½åŠ›
        """
        import re
        import json
        
        # æ–¹æ³•1: æå–JSONå—
        try:
            # æŸ¥æ‰¾JSONå—æ ‡è®°
            json_markers = ['```json', '```', '{', '[']
            
            for marker in json_markers:
                if marker in response:
                    # æ‰¾åˆ°JSONå†…å®¹
                    if marker == '```json':
                        # ä»```jsonåˆ°```ä¹‹é—´çš„å†…å®¹
                        pattern = r'```json\s*(.*?)\s*```'
                        match = re.search(pattern, response, re.DOTALL)
                        if match:
                            json_content = match.group(1).strip()
                            # éªŒè¯JSONæœ‰æ•ˆæ€§
                            json.loads(json_content)
                            logger.info("ğŸ”§ é€šè¿‡```jsonæ ‡è®°ä¿®å¤JSON")
                            return json_content
                    elif marker == '```':
                        # ä»```åˆ°```ä¹‹é—´çš„å†…å®¹
                        pattern = r'```\s*(.*?)\s*```'
                        match = re.search(pattern, response, re.DOTALL)
                        if match:
                            json_content = match.group(1).strip()
                            if json_content.startswith('{') or json_content.startswith('['):
                                json.loads(json_content)
                                logger.info("ğŸ”§ é€šè¿‡```æ ‡è®°ä¿®å¤JSON")
                                return json_content
                    elif marker in ['{', '[']:
                        # æå–ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                        start_pos = response.find(marker)
                        if start_pos != -1:
                            json_content = self._extract_complete_json(response, start_pos)
                            if json_content:
                                json.loads(json_content)
                                logger.info("ğŸ”§ é€šè¿‡JSONå¯¹è±¡æå–ä¿®å¤")
                                return json_content
                    break
                    
        except (json.JSONDecodeError, IndexError):
            pass
        
        # æ–¹æ³•2: æ™ºèƒ½æ¸…ç†å’Œä¿®å¤
        try:
            # ç§»é™¤éJSONå†…å®¹
            cleaned_response = response.strip()
            
            # ç§»é™¤å¸¸è§çš„å‰ç¼€å’Œåç¼€
            prefixes_to_remove = [
                'ä»¥ä¸‹æ˜¯JSONæ ¼å¼çš„å›å¤ï¼š',
                'å›å¤æ ¼å¼å¦‚ä¸‹ï¼š',
                'è¿™æ˜¯JSONæ ¼å¼çš„å›å¤ï¼š',
                'Here is the JSON response:',
                'JSON response:',
                'Response:',
                '```json',
                '```'
            ]
            
            suffixes_to_remove = [
                '```',
                'ä»¥ä¸Šæ˜¯å®Œæ•´çš„JSONå›å¤',
                'è¿™æ˜¯å®Œæ•´çš„JSONæ ¼å¼å›å¤',
                'This is the complete JSON response'
            ]
            
            for prefix in prefixes_to_remove:
                if cleaned_response.startswith(prefix):
                    cleaned_response = cleaned_response[len(prefix):].strip()
                    break
                    
            for suffix in suffixes_to_remove:
                if cleaned_response.endswith(suffix):
                    cleaned_response = cleaned_response[:-len(suffix)].strip()
                    break
            
            # å°è¯•è§£ææ¸…ç†åçš„å†…å®¹
            if cleaned_response.startswith('{') or cleaned_response.startswith('['):
                json.loads(cleaned_response)
                logger.info("ğŸ”§ é€šè¿‡æ¸…ç†å‰ç¼€åç¼€ä¿®å¤JSON")
                return cleaned_response
                
        except json.JSONDecodeError:
            pass
        
        # æ–¹æ³•3: åŸºäºæ¨¡å¼çš„JSONé‡å»º
        try:
            # æŸ¥æ‰¾å…³é”®JSONç»“æ„
            conclusions_pattern = r'"conclusions":\s*\[(.*?)\]'
            tasks_pattern = r'"atomic_tasks":\s*\[(.*?)\]'
            
            conclusions_match = re.search(conclusions_pattern, response, re.DOTALL)
            tasks_match = re.search(tasks_pattern, response, re.DOTALL)
            
            if conclusions_match:
                # é‡å»ºconclusions JSON
                conclusions_content = conclusions_match.group(1).strip()
                rebuilt_json = f'{{"conclusions": [{conclusions_content}]}}'
                json.loads(rebuilt_json)  # éªŒè¯
                logger.info("ğŸ”§ é€šè¿‡conclusionsæ¨¡å¼é‡å»ºJSON")
                return rebuilt_json
                
            if tasks_match:
                # é‡å»ºatomic_tasks JSON
                tasks_content = tasks_match.group(1).strip()
                rebuilt_json = f'{{"atomic_tasks": [{tasks_content}]}}'
                json.loads(rebuilt_json)  # éªŒè¯
                logger.info("ğŸ”§ é€šè¿‡atomic_tasksæ¨¡å¼é‡å»ºJSON")
                return rebuilt_json
                
        except json.JSONDecodeError:
            pass
        
        # æ–¹æ³•4: å°è¯•ä¿®å¤å¸¸è§çš„JSONé”™è¯¯
        try:
            # ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é”™è¯¯
            fixed_response = response
            
            # ä¿®å¤å•å¼•å·ä¸ºåŒå¼•å·
            fixed_response = re.sub(r"'([^']*)':", r'"\1":', fixed_response)
            fixed_response = re.sub(r":\s*'([^']*)'", r': "\1"', fixed_response)
            
            # ä¿®å¤ç¼ºå°‘å¼•å·çš„é”®
            fixed_response = re.sub(r'([a-zA-Z_]\w*)\s*:', r'"\1":', fixed_response)
            
            # ä¿®å¤å°¾éšé€—å·
            fixed_response = re.sub(r',\s*([}\]])', r'\1', fixed_response)
            
            # å°è¯•è§£æä¿®å¤åçš„å†…å®¹
            if fixed_response.strip().startswith('{') or fixed_response.strip().startswith('['):
                json.loads(fixed_response)
                logger.info("ğŸ”§ é€šè¿‡æ ¼å¼ä¿®å¤JSON")
                return fixed_response
                
        except json.JSONDecodeError:
            pass
        
        return ""
    
    def _extract_complete_json(self, text: str, start_pos: int) -> str:
        """
        ä»æŒ‡å®šä½ç½®æå–å®Œæ•´çš„JSONå¯¹è±¡
        """
        import json
        
        try:
            bracket_count = 0
            brace_count = 0
            in_string = False
            escape_next = False
            
            start_char = text[start_pos]
            target_char = '}' if start_char == '{' else ']'
            
            for i, char in enumerate(text[start_pos:], start_pos):
                if escape_next:
                    escape_next = False
                    continue
                    
                if char == '\\' and in_string:
                    escape_next = True
                    continue
                    
                if char == '"' and not escape_next:
                    in_string = not in_string
                    continue
                    
                if in_string:
                    continue
                    
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                elif char == '[':
                    bracket_count += 1
                elif char == ']':
                    bracket_count -= 1
                
                # æ‰¾åˆ°åŒ¹é…çš„é—­åˆæ ‡è®°
                if start_char == '{' and brace_count == 0 and i > start_pos:
                    return text[start_pos:i+1]
                elif start_char == '[' and bracket_count == 0 and i > start_pos:
                    return text[start_pos:i+1]
                    
        except Exception:
            pass
        
        return ""
    
    def _get_default_conclusions(self, trajectories_data: List[Dict]) -> List[Dict]:
        """è·å–å¢å¼ºçš„é»˜è®¤ç»“è®ºï¼ˆæ·±åº¦æŒ–æ˜è½¨è¿¹ä¿¡æ¯ï¼‰"""
        default_conclusions = []
        
        for i, trajectory in enumerate(trajectories_data[:5]):  # å¤„ç†æ›´å¤šè½¨è¿¹
            task_id = trajectory.get('task_id', f'trajectory_{i}')
            question = trajectory.get('question', 'æœªçŸ¥ä»»åŠ¡')
            tools_used = trajectory.get("tools_used", [])
            success = trajectory.get("success", True)
            final_answer = trajectory.get("final_answer", "")
            execution_steps = trajectory.get("execution_steps", [])
            
            # æ ¹æ®è½¨è¿¹å†…å®¹ç”Ÿæˆä¸åŒç±»å‹çš„ç»“è®º
            if tools_used:
                # å·¥å…·ä½¿ç”¨ç±»å‹
                primary_tool = tools_used[0] if tools_used else "unknown"
                conclusion = {
                    "content": f"ä½¿ç”¨{primary_tool}å·¥å…·æˆåŠŸå®Œæˆ{question}ä»»åŠ¡ï¼Œå±•ç°äº†å·¥å…·é›†æˆèƒ½åŠ›",
                    "entities": [primary_tool, question, "å·¥å…·é›†æˆ", "ä»»åŠ¡å®Œæˆ"],
                    "relation": f"{primary_tool}-å·¥å…·æ‰§è¡Œ-{question}-æˆåŠŸå®Œæˆ",
                    "relation_type": "tool_integration",
                    "scenario": f"{primary_tool}å·¥å…·åº”ç”¨åœºæ™¯",
                    "difficulty": "ä¸­ç­‰" if len(tools_used) == 1 else "å›°éš¾",
                    "required_tools": tools_used,
                    "generalization_potential": f"å¯æ‰©å±•åˆ°æ‰€æœ‰{primary_tool}ç›¸å…³ä»»åŠ¡",
                    "confidence": 0.8 if success else 0.5,
                    "domain_knowledge": self._extract_domain_from_question(question),
                    "task_pattern": "tool_execution_pattern"
                }
            else:
                # çº¯æ¨ç†ç±»å‹
                conclusion = {
                    "content": f"é€šè¿‡é€»è¾‘æ¨ç†è§£å†³{question}ï¼Œå±•ç°äº†åˆ†æèƒ½åŠ›",
                    "entities": ["é€»è¾‘æ¨ç†", question, "åˆ†æèƒ½åŠ›", "è§£å†³æ–¹æ¡ˆ"],
                    "relation": "æ¨ç†åˆ†æ-é—®é¢˜æ±‚è§£-ç»“è®ºç”Ÿæˆ",
                    "relation_type": "logical_reasoning",
                    "scenario": "å¤æ‚æ¨ç†åœºæ™¯",
                    "difficulty": "ä¸­ç­‰",
                    "required_tools": ["code_execution"],
                    "generalization_potential": "å¯åº”ç”¨äºåŒç±»æ¨ç†é—®é¢˜",
                    "confidence": 0.7,
                    "domain_knowledge": self._extract_domain_from_question(question),
                    "task_pattern": "reasoning_pattern"
                }
            
            # æ·»åŠ æ‰§è¡Œæ­¥éª¤ä¿¡æ¯
            if execution_steps:
                conclusion["execution_complexity"] = len(execution_steps)
                conclusion["step_pattern"] = [step.get("action", "unknown") for step in execution_steps[:3]]
            
            default_conclusions.append(conclusion)
        
        logger.info(f"ğŸ” ç”Ÿæˆå¢å¼ºé»˜è®¤ç»“è®ºï¼ŒåŒ…å« {len(default_conclusions)} ä¸ªå¤šæ ·åŒ–ç»“è®º")
        return default_conclusions
    
    def _extract_domain_from_question(self, question: str) -> str:
        """ä»é—®é¢˜ä¸­æå–é¢†åŸŸä¿¡æ¯"""
        question_lower = question.lower()
        
        if any(keyword in question_lower for keyword in ['è®¡ç®—', 'æ•°å­¦', 'å…¬å¼', 'é¢ç§¯']):
            return "æ•°å­¦è®¡ç®—"
        elif any(keyword in question_lower for keyword in ['json', 'æ•°æ®', 'è§£æ', 'æ ¼å¼']):
            return "æ•°æ®å¤„ç†"
        elif any(keyword in question_lower for keyword in ['ä»£ç ', 'ç¼–ç¨‹', 'è„šæœ¬']):
            return "ç¼–ç¨‹å¼€å‘"
        elif any(keyword in question_lower for keyword in ['åˆ†æ', 'ç ”ç©¶', 'æŠ¥å‘Š']):
            return "åˆ†æç ”ç©¶"
        else:
            return "é€šç”¨ä»»åŠ¡"
    
    async def _fallback_task_generation(self, conclusion: Dict) -> List[Dict]:
        """åˆ›é€ æ€§å›é€€ä»»åŠ¡ç”Ÿæˆæ–¹æ³•"""
        try:
            content = conclusion.get("content", "æœªçŸ¥ç»“è®º")
            relation_type = conclusion.get("relation_type", "general")
            domain = conclusion.get("domain_knowledge", "é€šç”¨ä»»åŠ¡")
            required_tools = conclusion.get("required_tools", [])
            task_pattern = conclusion.get("task_pattern", "general_pattern")
            
            creative_tasks = []
            
            # æ ¹æ®å…³ç³»ç±»å‹ç”Ÿæˆä¸åŒçš„åˆ›é€ æ€§ä»»åŠ¡
            if relation_type == "tool_integration":
                # å·¥å…·é›†æˆç±»ä»»åŠ¡
                primary_tool = required_tools[0] if required_tools else "code_execution"
                creative_tasks.extend([
                    {
                        "question": f"è®¾è®¡ä¸€ä¸ª{primary_tool}å·¥å…·çš„é«˜çº§åº”ç”¨åœºæ™¯ï¼Œè¦æ±‚æ¯”åŸºç¡€ç”¨æ³•æ›´å¤æ‚",
                        "expected_answer": f"åŸºäº{primary_tool}çš„åˆ›æ–°åº”ç”¨æ–¹æ¡ˆ",
                        "task_type": "tool_required",
                        "domain": domain,
                        "difficulty": "å›°éš¾",
                        "required_tools": required_tools,
                        "reasoning_steps": ["åˆ†æå·¥å…·èƒ½åŠ›", "è®¾è®¡åº”ç”¨åœºæ™¯", "ä¼˜åŒ–å®ç°æ–¹æ¡ˆ"],
                        "relation_pattern": f"{primary_tool}_advanced_application",
                        "entity_generalization": "é«˜çº§å·¥å…·åº”ç”¨æ¨¡å¼",
                        "creativity_level": "4",
                        "creativity_explanation": f"ä»åŸºç¡€{primary_tool}ä½¿ç”¨æ‰©å±•åˆ°åˆ›æ–°åº”ç”¨è®¾è®¡",
                        "reverse_reasoning": f"åå‘æ¨ç†ï¼šä»{primary_tool}èƒ½åŠ›è¾¹ç•Œæ¢ç´¢åˆ›æ–°ç”¨æ³•"
                    },
                    {
                        "question": f"å¦‚æœ{primary_tool}å·¥å…·å¤±æ•ˆï¼Œè®¾è®¡3ç§æ›¿ä»£è§£å†³æ–¹æ¡ˆ",
                        "expected_answer": "å¤šå…ƒåŒ–çš„é—®é¢˜è§£å†³ç­–ç•¥",
                        "task_type": "tool_required",
                        "domain": domain,
                        "difficulty": "å›°éš¾",
                        "required_tools": ["code_execution"],
                        "reasoning_steps": ["åˆ†æå·¥å…·ä¾èµ–", "æ¢ç´¢æ›¿ä»£æ–¹æ¡ˆ", "è¯„ä¼°å¯è¡Œæ€§"],
                        "relation_pattern": "tool_failure_contingency",
                        "entity_generalization": "å®¹é”™æ€§è®¾è®¡æ€ç»´",
                        "creativity_level": "5",
                        "creativity_explanation": "ä»å·¥å…·ä¾èµ–è½¬å‘å¤šè·¯å¾„é—®é¢˜è§£å†³",
                        "reverse_reasoning": "é€†å‘æ€ç»´ï¼šä»å¤±è´¥åœºæ™¯æ¨å¯¼æˆåŠŸç­–ç•¥"
                    }
                ])
            
            elif relation_type == "logical_reasoning":
                # é€»è¾‘æ¨ç†ç±»ä»»åŠ¡
                creative_tasks.extend([
                    {
                        "question": f"åŸºäº{domain}é¢†åŸŸçŸ¥è¯†ï¼Œæ„å»ºä¸€ä¸ªéœ€è¦å¤šæ­¥æ¨ç†çš„å¤æ‚é—®é¢˜",
                        "expected_answer": "ç»“æ„åŒ–çš„æ¨ç†é—®é¢˜å’Œè§£å†³è·¯å¾„",
                        "task_type": "tool_required",
                        "domain": domain,
                        "difficulty": "å›°éš¾",
                        "required_tools": ["code_execution"],
                        "reasoning_steps": ["é—®é¢˜æ„é€ ", "æ¨ç†é“¾è®¾è®¡", "éªŒè¯é€»è¾‘"],
                        "relation_pattern": "multi_step_reasoning_construction",
                        "entity_generalization": "å¤æ‚æ¨ç†é—®é¢˜è®¾è®¡",
                        "creativity_level": "5",
                        "creativity_explanation": "ä»ç®€å•æ¨ç†å‡çº§åˆ°å¤æ‚æ¨ç†é—®é¢˜æ„é€ ",
                        "reverse_reasoning": "ä»è§£ç­”åæ¨æ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜è®¾è®¡"
                    },
                    {
                        "question": f"è®¾è®¡ä¸€ä¸ª{domain}é¢†åŸŸçš„æ€ç»´é™·é˜±é¢˜ï¼Œå¹¶æä¾›ç ´è§£æ€è·¯",
                        "expected_answer": "å…·æœ‰å¯å‘æ€§çš„é™·é˜±é¢˜ç›®å’Œè§£é¢˜æ–¹æ³•",
                        "task_type": "tool_required",
                        "domain": domain,
                        "difficulty": "å›°éš¾",
                        "required_tools": ["code_execution"],
                        "reasoning_steps": ["è¯†åˆ«è®¤çŸ¥åè¯¯", "è®¾è®¡é™·é˜±æœºåˆ¶", "æ„å»ºè§£é¢˜è·¯å¾„"],
                        "relation_pattern": "cognitive_trap_design",
                        "entity_generalization": "è®¤çŸ¥æŒ‘æˆ˜é¢˜è®¾è®¡",
                        "creativity_level": "5",
                        "creativity_explanation": "ä»ç›´æ¥æ¨ç†è½¬å‘è®¤çŸ¥åè¯¯çš„è¯†åˆ«å’Œåˆ©ç”¨",
                        "reverse_reasoning": "åå‘å·¥ç¨‹ï¼šä»å¸¸è§é”™è¯¯æ„é€ æŒ‘æˆ˜é¢˜"
                    }
                ])
            
            else:
                # é€šç”¨åˆ›é€ æ€§ä»»åŠ¡
                creative_tasks.extend([
                    {
                        "question": f"å°†{domain}é¢†åŸŸçš„æ¦‚å¿µè·¨ç•Œåº”ç”¨åˆ°å¦ä¸€ä¸ªå®Œå…¨ä¸åŒçš„é¢†åŸŸ",
                        "expected_answer": "åˆ›æ–°çš„è·¨é¢†åŸŸåº”ç”¨æ–¹æ¡ˆ",
                        "task_type": "tool_required",
                        "domain": "è·¨é¢†åŸŸåˆ›æ–°",
                        "difficulty": "å›°éš¾",
                        "required_tools": ["code_execution"],
                        "reasoning_steps": ["æ¦‚å¿µæŠ½è±¡", "é¢†åŸŸæ˜ å°„", "åˆ›æ–°æ•´åˆ"],
                        "relation_pattern": "cross_domain_innovation",
                        "entity_generalization": "è·¨ç•Œæ€ç»´æ¨¡å¼",
                        "creativity_level": "5",
                        "creativity_explanation": "ä»å•é¢†åŸŸçŸ¥è¯†æ‰©å±•åˆ°è·¨é¢†åŸŸåˆ›æ–°æ€ç»´",
                        "reverse_reasoning": "ç±»æ¯”æ¨ç†ï¼šä»ç›¸ä¼¼ç»“æ„å‘ç°åˆ›æ–°æœºä¼š"
                    }
                ])
            
            # é™åˆ¶ä»»åŠ¡æ•°é‡å¹¶ç¡®ä¿è´¨é‡
            selected_tasks = creative_tasks[:2]  # é€‰æ‹©å‰2ä¸ªé«˜è´¨é‡ä»»åŠ¡
            
            logger.info(f"ğŸ¨ ç”Ÿæˆåˆ›é€ æ€§å›é€€ä»»åŠ¡: {len(selected_tasks)} ä¸ªé«˜åˆ›é€ æ€§ä»»åŠ¡")
            return selected_tasks
            
        except Exception as e:
            logger.error(f"âŒ åˆ›é€ æ€§å›é€€ä»»åŠ¡ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    async def _store_synthesis_results(self, result: SynthesisResult) -> None:
        """å­˜å‚¨åˆæˆç»“æœ"""
        logger.debug(f"ğŸ’¾ å­˜å‚¨åˆæˆä¼šè¯ç»“æœ: {result.session_id}")
        
        try:
            # å­˜å‚¨åŸå­ä»»åŠ¡
            for i, task in enumerate(result.atomic_tasks):
                validation = result.validation_results[i] if i < len(result.validation_results) else None
                await self.storage.store_atomic_task(task, validation)
            
            # å­˜å‚¨æ·±åº¦æ‰©å±•ä»»åŠ¡
            depth_start_idx = len(result.atomic_tasks)
            for i, task in enumerate(result.depth_extended_tasks):
                validation_idx = depth_start_idx + i
                validation = result.validation_results[validation_idx] if validation_idx < len(result.validation_results) else None
                await self.storage.store_depth_extended_task(task, validation)
            
            # å­˜å‚¨å®½åº¦æ‰©å±•ä»»åŠ¡
            width_start_idx = depth_start_idx + len(result.depth_extended_tasks)
            for i, task in enumerate(result.width_extended_tasks):
                validation_idx = width_start_idx + i
                validation = result.validation_results[validation_idx] if validation_idx < len(result.validation_results) else None
                await self.storage.store_width_extended_task(task, validation)
            
            # å­˜å‚¨éªŒè¯ç»“æœ
            for validation in result.validation_results:
                self.storage.store_validation_result(validation)
            
            # å­˜å‚¨ä¼šè¯ä¿¡æ¯
            await self.storage.store_synthesis_session(result)
            
            logger.debug("âœ… åˆæˆç»“æœå­˜å‚¨å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨åˆæˆç»“æœå¤±è´¥: {e}")
    
    def _parse_steps_from_response(self, raw_response: str) -> List[str]:
        """ä»åŸå§‹å“åº”ä¸­è§£ææ‰§è¡Œæ­¥éª¤"""
        import re
        
        steps = []
        
        # 1. æå–å·¥å…·è°ƒç”¨æ­¥éª¤
        tool_patterns = [
            r'<(browser_use|microsandbox|deepsearch|memory_staging)>([^<]+)</\1>',
            r'<(browser_search_google|browser_extract_content|microsandbox_execute|microsandbox_install_package)>([^<]+)</\1>'
        ]
        
        for pattern in tool_patterns:
            matches = re.findall(pattern, raw_response, re.DOTALL)
            for tool_name, content in matches:
                # æå–å·¥å…·è°ƒç”¨çš„æ ¸å¿ƒæ“ä½œ
                step_desc = self._extract_step_description(tool_name, content.strip()[:100])
                if step_desc:
                    steps.append(step_desc)
        
        # 2. æå–æ€è€ƒå’Œæ¨ç†æ­¥éª¤
        think_blocks = re.findall(r'<think>(.*?)</think>', raw_response, re.DOTALL)
        for think in think_blocks:
            reasoning_steps = self._extract_reasoning_steps(think.strip())
            steps.extend(reasoning_steps)
        
        # 3. æå–answeræ ‡ç­¾ä¸­çš„æ€»ç»“æ­¥éª¤
        answer_blocks = re.findall(r'<answer>(.*?)</answer>', raw_response, re.DOTALL)
        for answer in answer_blocks:
            summary_steps = self._extract_summary_steps(answer.strip())
            steps.extend(summary_steps)
        
        return list(dict.fromkeys(steps))  # å»é‡ä½†ä¿æŒé¡ºåº
    
    def _extract_tools_from_response(self, raw_response: str) -> List[str]:
        """ä»åŸå§‹å“åº”ä¸­æå–ä½¿ç”¨çš„å·¥å…·"""
        import re
        
        tools = []
        
        # æå–æ‰€æœ‰å·¥å…·è°ƒç”¨
        tool_patterns = [
            r'<(browser_use|microsandbox|deepsearch|memory_staging)',
            r'<(browser_search_google|browser_extract_content|microsandbox_execute|microsandbox_install_package)'
        ]
        
        for pattern in tool_patterns:
            matches = re.findall(pattern, raw_response)
            tools.extend(matches)
        
        return list(set(tools))  # å»é‡
    
    def _extract_reasoning_from_response(self, raw_response: str) -> str:
        """ä»åŸå§‹å“åº”ä¸­æå–æ¨ç†è¿‡ç¨‹"""
        import re
        
        reasoning_blocks = []
        
        # æå–thinkå—
        think_matches = re.findall(r'<think>(.*?)</think>', raw_response, re.DOTALL)
        for think in think_matches:
            clean_think = think.strip()[:200]  # é™åˆ¶é•¿åº¦
            if clean_think:
                reasoning_blocks.append(clean_think)
        
        return " | ".join(reasoning_blocks)
    
    def _extract_step_description(self, tool_name: str, content: str) -> str:
        """æ ¹æ®å·¥å…·åç§°å’Œå†…å®¹æå–æ­¥éª¤æè¿°"""
        tool_mappings = {
            "browser_search_google": f"æœç´¢ä¿¡æ¯: {content[:50]}",
            "browser_extract_content": f"æå–å†…å®¹: {content[:50]}",
            "microsandbox_execute": f"æ‰§è¡Œä»£ç : {content[:50]}",
            "microsandbox_install_package": f"å®‰è£…åŒ…: {content}",
            "deepsearch": f"æ·±åº¦æœç´¢: {content[:50]}",
            "memory_staging": f"å†…å­˜æ“ä½œ: {content[:50]}"
        }
        
        return tool_mappings.get(tool_name, f"{tool_name}: {content[:50]}")
    
    def _extract_reasoning_steps(self, think_content: str) -> List[str]:
        """ä»æ€è€ƒå†…å®¹ä¸­æå–æ¨ç†æ­¥éª¤"""
        steps = []
        
        # æŸ¥æ‰¾æ˜ç¡®çš„æ­¥éª¤æŒ‡ç¤ºè¯
        step_indicators = ["é¦–å…ˆ", "ç„¶å", "æ¥ä¸‹æ¥", "æœ€å", "æ­¥éª¤", "ç¬¬ä¸€", "ç¬¬äºŒ", "ç¬¬ä¸‰"]
        
        lines = think_content.split('\n')
        for line in lines:
            line = line.strip()
            if len(line) > 10:  # è¿‡æ»¤è¿‡çŸ­çš„è¡Œ
                for indicator in step_indicators:
                    if indicator in line:
                        steps.append(f"æ¨ç†æ­¥éª¤: {line[:80]}")
                        break
        
        return steps[:3]  # é™åˆ¶æ¨ç†æ­¥éª¤æ•°é‡
    
    def _extract_summary_steps(self, answer_content: str) -> List[str]:
        """ä»ç­”æ¡ˆå†…å®¹ä¸­æå–æ€»ç»“æ­¥éª¤"""
        steps = []
        
        # æå–boxedå†…å®¹
        import re
        boxed_matches = re.findall(r'\\boxed\{([^}]+)\}', answer_content, re.DOTALL)
        for boxed in boxed_matches:
            if len(boxed.strip()) > 20:  # åªå–æœ‰æ„ä¹‰çš„å†…å®¹
                steps.append(f"æ€»ç»“ç»“æœ: {boxed.strip()[:80]}")
        
        return steps
    
    def _infer_domain_from_content(self, content: str) -> str:
        """ä»å†…å®¹æ¨æ–­é¢†åŸŸ"""
        domain_keywords = {
            "è‚¡ç¥¨|è‚¡ä»·|é‡‘è|æŠ•èµ„": "é‡‘è",
            "é‡å­|ç‰©ç†|ç§‘å­¦": "ç§‘å­¦ç ”ç©¶",
            "ä»£ç |ç¼–ç¨‹|Python|ç®—æ³•": "ç¼–ç¨‹",
            "æœç´¢|ç ”ç©¶|è®ºæ–‡": "ç ”ç©¶åˆ†æ",
            "å¤§å­¦|å­¦æ ¡|æ•™è‚²": "æ•™è‚²",
            "è›‹ç™½è´¨|ç”Ÿç‰©|åŒ»å­¦": "ç”Ÿç‰©åŒ»å­¦"
        }
        
        import re
        for pattern, domain in domain_keywords.items():
            if re.search(pattern, content):
                return domain
        
        return "é€šç”¨"
    
    def _calculate_result_statistics(self, result: SynthesisResult) -> None:
        """è®¡ç®—ç»“æœç»Ÿè®¡ä¿¡æ¯"""
        all_tasks = result.atomic_tasks + result.depth_extended_tasks + result.width_extended_tasks
        
        result.total_tasks_generated = len(all_tasks)
        result.valid_tasks_count = sum(1 for v in result.validation_results if v.is_valid)
        result.tool_required_count = sum(1 for v in result.validation_results if v.requires_tool)
        result.reasoning_only_count = result.valid_tasks_count - result.tool_required_count
    
    def _update_session_stats(self, result: SynthesisResult) -> None:
        """æ›´æ–°ä¼šè¯ç»Ÿè®¡"""
        self.session_stats["sessions_completed"] += 1
        self.session_stats["total_tasks_generated"] += result.total_tasks_generated
        self.session_stats["valid_tasks_count"] += result.valid_tasks_count
        self.session_stats["tool_required_count"] += result.tool_required_count
        self.session_stats["reasoning_only_count"] += result.reasoning_only_count
    
    async def get_storage_statistics(self) -> Dict[str, Any]:
        """è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        storage_stats = await self.storage.get_statistics()
        
        return {
            "session_statistics": self.session_stats,
            "storage_statistics": storage_stats,
            "storage_info": self.storage.get_storage_info()
        }
    
    async def validate_existing_tasks(self, task_type: Optional[TaskType] = None) -> List[TaskValidationResult]:
        """éªŒè¯å·²å­˜å‚¨çš„ä»»åŠ¡"""
        logger.info("ğŸ” å¼€å§‹éªŒè¯å·²å­˜å‚¨çš„ä»»åŠ¡")
        
        # åŠ è½½ä»»åŠ¡
        task_data_list = await self.storage.load_tasks_by_type(task_type or TaskType.TOOL_REQUIRED)
        
        # è½¬æ¢ä¸ºä»»åŠ¡å¯¹è±¡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä»…ç”¨äºéªŒè¯ï¼‰
        tasks = []
        for task_data in task_data_list:
            # è¿™é‡Œéœ€è¦æ ¹æ®æ•°æ®ç»“æ„é‡æ„ä»»åŠ¡å¯¹è±¡
            # ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦å®Œæ•´çš„ååºåˆ—åŒ–é€»è¾‘
            pass
        
        # æ‰§è¡ŒéªŒè¯
        if tasks:
            validations = await self.validator.batch_validate_tasks(tasks)
            
            # å­˜å‚¨éªŒè¯ç»“æœ
            for validation in validations:
                self.storage.store_validation_result(validation)
            
            return validations
        
        return []
    
    def get_component_info(self) -> Dict[str, Any]:
        """è·å–ç»„ä»¶ä¿¡æ¯"""
        return {
            "engine": "SynthesisEngine",
            "version": "1.0.0",
            "components": {
                "validator": self.validator.__class__.__name__,
                "storage": self.storage.__class__.__name__
            },
            "configuration": {
                "enable_strict_validation": self.enable_strict_validation,
                "storage_directory": str(self.storage.storage_dir)
            },
            "capabilities": {
                "atomic_generation": True,
                "depth_extension": True,
                "width_extension": True,
                "intelligent_validation": True,
                "tool_vs_reasoning_classification": True,
                "simplified_storage": True,
                "taskcraft_algorithm": True,
                "backward_search": True,
                "theme_aware_fusion": True,
                "relation_driven_reasoning": True
            }
        }
    
    # ========== TaskCraft ç®—æ³•æ–°å¢æ–¹æ³• ==========
    
    async def _perform_backward_search(self, known_fact: str) -> Optional[Dict]:
        """æ‰§è¡Œåå‘æœç´¢ï¼Œæ‰¾åˆ°å·²çŸ¥äº‹å®çš„èƒŒæ™¯çŸ¥è¯†"""
        try:
            prompt = prompt_manager.render_template(
                "backward_search",
                known_fact=known_fact
            )
            
            response = await self._call_llm_with_retry(prompt, "åå‘æœç´¢ç®—æ³•")
            
            import json
            # ç¡®ä¿responseæ˜¯å­—ç¬¦ä¸²
            if isinstance(response, dict):
                result = response
            else:
                result = json.loads(response)
            return result.get("backward_search_result")
            
        except Exception as e:
            logger.error(f"âŒ åå‘æœç´¢å¤±è´¥: {e}")
            return None
    
    async def _perform_task_fusion(self, background_task: str, core_task: str, logical_relation: str) -> Optional[Dict]:
        """æ‰§è¡Œä»»åŠ¡èåˆï¼Œå°†èƒŒæ™¯ä»»åŠ¡å’Œæ ¸å¿ƒä»»åŠ¡èåˆæˆè¿è´¯çš„å¤æ‚é—®é¢˜"""
        try:
            prompt = prompt_manager.render_template(
                "task_fusion",
                background_task=background_task,
                core_task=core_task,
                logical_relation=logical_relation
            )
            
            response = await self._call_llm_with_retry(prompt, "ä»»åŠ¡èåˆç®—æ³•")
            
            import json
            # ç¡®ä¿responseæ˜¯å­—ç¬¦ä¸²
            if isinstance(response, dict):
                result = response
            else:
                result = json.loads(response)
            return result.get("task_fusion_result")
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡èåˆå¤±è´¥: {e}")
            return None
    
    async def _semantic_cluster_tasks(self, atomic_tasks: List[AtomicTask]) -> Dict[str, List[AtomicTask]]:
        """å¯¹åŸå­ä»»åŠ¡è¿›è¡Œè¯­ä¹‰èšç±»ï¼Œè¿”å›ä¸»é¢˜ç»„"""
        try:
            # ç®€åŒ–çš„ä¸»é¢˜èšç±»ï¼šæŒ‰é¢†åŸŸå’Œé—®é¢˜å…³é”®è¯åˆ†ç»„
            theme_groups = {}
            
            for task in atomic_tasks:
                # åŸºäºé¢†åŸŸå’Œé—®é¢˜å…³é”®è¯ç”Ÿæˆä¸»é¢˜
                domain = task.domain
                question_keywords = self._extract_keywords(task.question)
                theme = f"{domain}_{question_keywords}"
                
                if theme not in theme_groups:
                    theme_groups[theme] = []
                theme_groups[theme].append(task)
            
            # è¿‡æ»¤å‡ºåªæœ‰ä¸€ä¸ªä»»åŠ¡çš„ç»„
            filtered_groups = {k: v for k, v in theme_groups.items() if len(v) >= 2}
            
            logger.debug(f"ğŸ“‹ è¯­ä¹‰èšç±»ç»“æœ: {len(filtered_groups)} ä¸ªä¸»é¢˜ç»„")
            return filtered_groups
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰èšç±»å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•é¢†åŸŸåˆ†ç»„
            domain_groups = {}
            for task in atomic_tasks:
                domain = task.domain
                if domain not in domain_groups:
                    domain_groups[domain] = []
                domain_groups[domain].append(task)
            return {k: v for k, v in domain_groups.items() if len(v) >= 2}
    
    def _extract_keywords(self, question: str) -> str:
        """ä»é—®é¢˜ä¸­æå–å…³é”®è¯ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        try:
            # ç®€åŒ–çš„å…³é”®è¯æå–ï¼šæ‰¾åè¯å’ŒåŠ¨è¯
            keywords = []
            question_lower = question.lower()
            
            # å¸¸è§çš„æŠ€æœ¯å…³é”®è¯
            tech_keywords = [
                'python', 'pandas', 'numpy', 'transformer', 'gpt', 'ai', 'model', 
                'è‚¡ä»·', 'å…¬å¸', 'åˆ†æ', 'æ•°æ®', 'ä»£ç ', 'ç®—æ³•'
            ]
            
            for keyword in tech_keywords:
                if keyword in question_lower:
                    keywords.append(keyword)
            
            return '_'.join(keywords[:2]) if keywords else 'general'
            
        except Exception:
            return 'general'
    
    async def _analyze_theme_relationships(self, tasks: List[AtomicTask]) -> str:
        """åˆ†æä»»åŠ¡ç»„çš„ä¸»é¢˜å…³ç³»"""
        try:
            # ç”Ÿæˆä¸»é¢˜åˆ†æ
            analysis = {
                "common_domain": tasks[0].domain,
                "task_count": len(tasks),
                "questions": [task.question for task in tasks],
                "complexity_levels": [task.complexity.value for task in tasks],
                "requires_tools": [task.requires_tool for task in tasks]
            }
            
            return str(analysis)
            
        except Exception as e:
            logger.error(f"âŒ ä¸»é¢˜å…³ç³»åˆ†æå¤±è´¥: {e}")
            return "ç®€åŒ–ä¸»é¢˜åˆ†æ"
    
    async def _perform_theme_aware_fusion(self, tasks: List[AtomicTask], theme_analysis: str) -> Optional[Dict]:
        """æ‰§è¡Œä¸»é¢˜æ„ŸçŸ¥åˆå¹¶"""
        try:
            # å‡†å¤‡è¾“å…¥æ•°æ®
            related_tasks_data = []
            for task in tasks:
                task_data = {
                    "question": task.question,
                    "domain": task.domain,
                    "complexity": task.complexity.value,
                    "requires_tool": task.requires_tool
                }
                related_tasks_data.append(task_data)
            
            prompt = prompt_manager.render_template(
                "theme_aware_fusion",
                related_tasks=str(related_tasks_data),
                theme_analysis=theme_analysis
            )
            
            response = await self._call_llm_with_retry(prompt, "ä¸»é¢˜æ„ŸçŸ¥èåˆ")
            
            import json
            # ç¡®ä¿responseæ˜¯å­—ç¬¦ä¸²
            if isinstance(response, dict):
                result = response
            else:
                result = json.loads(response)
            return result.get("theme_fusion_result")
            
        except Exception as e:
            logger.error(f"âŒ ä¸»é¢˜æ„ŸçŸ¥åˆå¹¶å¤±è´¥: {e}")
            return None
    
    async def _fallback_width_extension(self, atomic_tasks: List[AtomicTask], max_tasks: int) -> List[WidthExtendedTask]:
        """ç®€åŒ–çš„å®½åº¦æ‰©å±•å›é€€æ–¹æ³•"""
        try:
            width_tasks = []
            
            # ç®€å•çš„é¢†åŸŸåˆ†ç»„åˆå¹¶
            domain_groups = {}
            for task in atomic_tasks:
                domain = task.domain
                if domain not in domain_groups:
                    domain_groups[domain] = []
                domain_groups[domain].append(task)
            
            tasks_generated = 0
            for domain, tasks in domain_groups.items():
                if tasks_generated >= max_tasks or len(tasks) < 2:
                    continue
                    
                # ç®€å•åˆå¹¶å‰ä¸¤ä¸ªä»»åŠ¡
                component_tasks = tasks[:2]
                merged_question = f"è¯·åŒæ—¶å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š1) {component_tasks[0].question} 2) {component_tasks[1].question}"
                merged_answer = f"1) {component_tasks[0].answer.answer} 2) {component_tasks[1].answer.answer}"
                
                extended_task = WidthExtendedTask.create_width_extended(
                    component_tasks=component_tasks,
                    merged_question=merged_question,
                    merged_answer=merged_answer,
                    merge_strategy="simple_parallel"
                )
                
                width_tasks.append(extended_task)
                tasks_generated += 1
            
            logger.debug(f"ğŸ”„ ç®€åŒ–å›é€€ç”Ÿæˆ {len(width_tasks)} ä¸ªå®½åº¦æ‰©å±•ä»»åŠ¡")
            return width_tasks
            
        except Exception as e:
            logger.error(f"âŒ ç®€åŒ–å®½åº¦æ‰©å±•å›é€€å¤±è´¥: {e}")
            return []