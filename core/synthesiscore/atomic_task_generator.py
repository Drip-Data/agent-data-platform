#!/usr/bin/env python3
"""
Atomic Task Generator - åŸå­ä»»åŠ¡ç”Ÿæˆå™¨
åŸºäºTaskCraftç®—æ³•ï¼Œå®ç°ä»è¯­æ–™åˆ°åŸå­ä»»åŠ¡çš„è‡ªåŠ¨ç”Ÿæˆ
"""

import asyncio
import json
import logging
import re
import time
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import asdict

from core.interfaces import TrajectoryResult
from core.llm_client import LLMClient
from core.toolscore.mcp_client import MCPToolClient
from .enhanced_interfaces import (
    CorpusContent, AtomicTask, TaskConclusion, TaskDifficulty, 
    TaskType, EnhancedSynthesisConfig, generate_task_id
)
from .tool_validator import ToolValidator

logger = logging.getLogger(__name__)


def clean_json_string(json_str: str) -> str:
    """æ¸…ç†JSONå­—ç¬¦ä¸²ï¼Œä¿®å¤å¸¸è§æ ¼å¼é”™è¯¯"""
    import re
    
    # 1. ç§»é™¤é‡å¤çš„content_identifieré”®
    pattern = r'"content_identifier"\s*:\s*"[^"]*"'
    matches = list(re.finditer(pattern, json_str))
    
    if len(matches) > 1:
        # å¦‚æœæœ‰å¤šä¸ªåŒ¹é…ï¼Œä¿ç•™ç¬¬ä¸€ä¸ªï¼Œåˆ é™¤å…¶ä»–çš„
        offset = 0
        for i in range(1, len(matches)):
            match = matches[i]
            start = match.start() - offset
            end = match.end() - offset
            
            # æ£€æŸ¥å‰é¢æ˜¯å¦æœ‰é€—å·ï¼Œå¦‚æœæœ‰ï¼Œä¸€èµ·åˆ é™¤
            check_start = max(0, start - 10)
            prefix = json_str[check_start:start]
            if ',' in prefix:
                comma_pos = prefix.rfind(',')
                start = check_start + comma_pos
            
            json_str = json_str[:start] + json_str[end:]
            offset += (end - start)
    
    # 2. ä¿®å¤ç¼ºå¤±é€—å·çš„é—®é¢˜
    # æŸ¥æ‰¾ },\s*{} æˆ– },\s*\n\s*{} è¿™æ ·çš„æ¨¡å¼ï¼Œå¹¶åœ¨}åæ·»åŠ é€—å·
    json_str = re.sub(r'(\})\s*(\{)', r'\1,\2', json_str)
    
    # 3. ä¿®å¤ "key": "value"\n "key2" è¿™æ ·ç¼ºå¤±é€—å·çš„æ¨¡å¼
    json_str = re.sub(r'("\w+":\s*"[^"]*")\s*\n\s*(")', r'\1,\n            \2', json_str)
    json_str = re.sub(r'("\w+":\s*[0-9.]+)\s*\n\s*(")', r'\1,\n            \2', json_str)
    
    # 4. ä¿®å¤å¯¹è±¡æœ«å°¾å¤šä½™é€—å·çš„é—®é¢˜
    json_str = re.sub(r',\s*\}', '}', json_str)
    json_str = re.sub(r',\s*\]', ']', json_str)
    
    return json_str


class ConclusionExtractor:
    """ç»“è®ºæå–å™¨ - ä»è¯­æ–™ä¸­æå–åŸå­ç»“è®º"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
        self.config = EnhancedSynthesisConfig()
    
    async def extract_conclusions_from_corpus(self, corpus_content: CorpusContent) -> List[TaskConclusion]:
        """ä»è¯­æ–™ä¸­æå–ç»“è®º"""
        logger.debug(f"ğŸ” å¼€å§‹ä»è¯­æ–™ä¸­æå–ç»“è®º: {corpus_content.corpus_id}")
        
        try:
            # æ„å»ºç»“è®ºæå–æç¤ºè¯
            extraction_prompt = self._build_conclusion_extraction_prompt(corpus_content)
            
            # ä½¿ç”¨LLMè¿›è¡Œå†…å®¹ç”Ÿæˆ - é€šè¿‡summaryæ–¹æ³•è·å–åŸå§‹å“åº”
            raw_response = await self.llm_client.generate_task_summary(
                task_description=extraction_prompt,
                steps=[],
                final_outputs=[]
            )
            
            # è§£æLLMå“åº”
            conclusions = self._parse_raw_conclusion_response(raw_response, corpus_content)
            
            logger.info(f"âœ… ä»è¯­æ–™ {corpus_content.corpus_id} ä¸­æå–äº† {len(conclusions)} ä¸ªç»“è®º")
            return conclusions
            
        except Exception as e:
            logger.error(f"âŒ ç»“è®ºæå–å¤±è´¥ {corpus_content.corpus_id}: {e}")
            return []
    
    def _build_conclusion_extraction_prompt(self, corpus_content: CorpusContent) -> str:
        """æ„å»ºç»“è®ºæå–æç¤ºè¯"""
        
        content_preview = corpus_content.text_content[:1000] + "..." if len(corpus_content.text_content) > 1000 else corpus_content.text_content
        
        return f"""
è¯·ä»ä»¥ä¸‹å†…å®¹ä¸­æå–å¯ä»¥å½¢æˆåŸå­ä»»åŠ¡çš„å…³é”®ç»“è®ºã€‚æ¯ä¸ªç»“è®ºå¿…é¡»æ»¡è¶³ï¼š
1. åŒ…å«å…·ä½“ã€å¯éªŒè¯çš„äº‹å®ï¼ˆæ•°å€¼ã€æ—¶é—´ã€åç§°ç­‰ï¼‰
2. å…·æœ‰æ˜ç¡®çš„å…³ç³»æè¿°
3. å¯ä»¥è½¬æ¢ä¸ºä¸€ä¸ªä¸å¯å†åˆ†çš„é—®é¢˜

å†…å®¹ç±»å‹: {corpus_content.content_type.value}
å†…å®¹æ¥æº: {corpus_content.source}
å†…å®¹ç‰‡æ®µ:
{content_preview}

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“è®ºåˆ—è¡¨ï¼Œæ¯ä¸ªç»“è®ºåŒ…å«ï¼š
- conclusion: ç»“è®ºå†…å®¹
- relationship: å…³ç³»æè¿° (ä¾‹å¦‚ï¼š"Xå±äºY", "Xçš„å€¼æ˜¯Y", "Xå‘ç”Ÿåœ¨Yæ—¶é—´")
- content_identifier: å†…å®¹æ ‡è¯†ç¬¦
- confidence: æå–ç½®ä¿¡åº¦ (0.0-1.0)

ç¤ºä¾‹æ ¼å¼:
{{
    "conclusions": [
        {{
            "conclusion": "è‹¹æœå…¬å¸çš„è‚¡ä»·åœ¨2023å¹´12æœˆ15æ—¥æ”¶ç›˜ä»·ä¸º198.11ç¾å…ƒ",
            "relationship": "è‚¡ä»·-å…¬å¸-æ—¶é—´-æ•°å€¼",
            "content_identifier": "stock_price_apple_20231215",
            "confidence": 0.95
        }}
    ]
}}

è¦æ±‚ï¼š
- æœ€å¤šæå–{self.config.ATOMIC_GENERATION_CONFIG['max_conclusions_per_corpus']}ä¸ªç»“è®º
- åªæå–å…·æœ‰é«˜ç½®ä¿¡åº¦(>0.7)çš„ç»“è®º
- é¿å…é‡å¤æˆ–ç›¸ä¼¼çš„ç»“è®º
"""
    
    def _parse_raw_conclusion_response(self, raw_response: str, corpus_content: CorpusContent) -> List[TaskConclusion]:
        """è§£æåŸå§‹ç»“è®ºæå–å“åº”"""
        try:
            logger.debug(f"ğŸ” å¼€å§‹è§£æå“åº”: {raw_response[:200]}...")
            
            # å°è¯•ç›´æ¥è§£æJSONå“åº”
            response_clean = raw_response.strip()
            
            # é¦–å…ˆå°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”ä¸ºJSON
            conclusion_data = None
            try:
                conclusion_data = json.loads(response_clean)
                if 'conclusions' in conclusion_data:
                    logger.debug("âœ… ç›´æ¥è§£æJSONæˆåŠŸ")
                else:
                    conclusion_data = None
            except json.JSONDecodeError as e:
                logger.debug(f"âŒ ç›´æ¥JSONè§£æå¤±è´¥: {e}")
                pass
            
            # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•ä»ä»£ç å—ä¸­æå–
            if not conclusion_data:
                # å°è¯•ä»markdownä»£ç å—æå–
                pattern = r'```json\s*(\{.*?\})\s*```'
                match = re.search(pattern, response_clean, re.DOTALL)
                if match:
                    json_content = match.group(1)
                    logger.debug(f"ğŸ“‹ æå–åˆ°JSONå†…å®¹: {json_content[:200]}...")
                    
                    # æ¸…ç†JSONå­—ç¬¦ä¸²
                    json_content = clean_json_string(json_content)
                    logger.debug(f"ğŸ§¹ æ¸…ç†åçš„JSONå†…å®¹: {json_content[:200]}...")
                    
                    try:
                        conclusion_data = json.loads(json_content)
                        if 'conclusions' in conclusion_data:
                            logger.debug(f"âœ… ä½¿ç”¨markdownæ¨¡å¼æå–JSONæˆåŠŸ")
                        else:
                            logger.warning(f"âš ï¸ JSONä¸­æ²¡æœ‰conclusionså­—æ®µ")
                    except json.JSONDecodeError as e:
                        logger.error(f"âŒ Markdown JSONè§£æå¤±è´¥: {e}")
                        logger.error(f"å¤±è´¥çš„JSONå†…å®¹: {json_content}")
                
                # å¦‚æœmarkdownå¤±è´¥ï¼Œå°è¯•æ™®é€šä»£ç å—
                if not conclusion_data:
                    pattern = r'```\s*(\{.*?\})\s*```'
                    match = re.search(pattern, response_clean, re.DOTALL)
                    if match:
                        json_content = clean_json_string(match.group(1))
                        try:
                            conclusion_data = json.loads(json_content)
                            if 'conclusions' in conclusion_data:
                                logger.debug(f"âœ… ä½¿ç”¨æ™®é€šä»£ç å—æå–JSONæˆåŠŸ")
                        except json.JSONDecodeError as e:
                            logger.error(f"âŒ æ™®é€šä»£ç å—JSONè§£æå¤±è´¥: {e}")
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«conclusionsçš„ä»»ä½•JSONç»“æ„
            if not conclusion_data:
                # æŸ¥æ‰¾åŒ…å«"conclusions"å…³é”®å­—çš„ä½ç½®ï¼Œç„¶åå‘å‰å’Œå‘åå¯»æ‰¾å®Œæ•´çš„JSON
                conclusions_pos = response_clean.find('"conclusions"')
                if conclusions_pos > 0:
                    # å‘å‰æ‰¾åˆ°æœ€è¿‘çš„ {
                    start_pos = response_clean.rfind('{', 0, conclusions_pos)
                    if start_pos >= 0:
                        # ä»è¿™ä¸ªä½ç½®å¼€å§‹å°è¯•è§£æJSON
                        for end_pos in range(len(response_clean), start_pos, -1):
                            try:
                                json_candidate = response_clean[start_pos:end_pos]
                                conclusion_data = json.loads(json_candidate)
                                if 'conclusions' in conclusion_data:
                                    logger.debug("âœ… é€šè¿‡ä½ç½®æœç´¢æ‰¾åˆ°JSON")
                                    break
                            except json.JSONDecodeError:
                                continue
            
            if not conclusion_data or 'conclusions' not in conclusion_data:
                logger.warning(f"âš ï¸ æ— æ³•è§£æç»“è®ºå“åº”: {corpus_content.corpus_id}")
                logger.debug(f"å®Œæ•´å“åº”å†…å®¹: {raw_response}")
                return []
            
            conclusions = []
            for item in conclusion_data.get('conclusions', []):
                if item.get('confidence', 0.0) >= self.config.ATOMIC_GENERATION_CONFIG['conclusion_extraction_confidence']:
                    conclusion = TaskConclusion(
                        conclusion=item['conclusion'],
                        relationship=item['relationship'],
                        content_identifier=item['content_identifier'],
                        extraction_confidence=item['confidence'],
                        verifiability=self._assess_verifiability(item['conclusion'])
                    )
                    conclusions.append(conclusion)
            
            logger.debug(f"âœ… æˆåŠŸè§£æ {len(conclusions)} ä¸ªç»“è®º")
            return conclusions
            
        except (json.JSONDecodeError, KeyError, Exception) as e:
            logger.error(f"âŒ è§£æç»“è®ºå“åº”å¤±è´¥: {e}")
            logger.debug(f"å¤±è´¥çš„å“åº”å†…å®¹: {raw_response}")
            return []
    
    def _parse_conclusion_response(self, response: Dict[str, Any], corpus_content: CorpusContent) -> List[TaskConclusion]:
        """è§£æç»“è®ºæå–å“åº”ï¼ˆå‘åå…¼å®¹ï¼‰"""
        try:
            thinking = response.get('thinking', '{}')
            
            # å°è¯•è§£æJSONå“åº”
            if thinking.strip().startswith('{'):
                conclusion_data = json.loads(thinking)
            else:
                # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–
                json_match = re.search(r'\{.*\}', thinking, re.DOTALL)
                if json_match:
                    conclusion_data = json.loads(json_match.group())
                else:
                    logger.warning(f"âš ï¸ æ— æ³•è§£æç»“è®ºå“åº”: {corpus_content.corpus_id}")
                    return []
            
            conclusions = []
            for item in conclusion_data.get('conclusions', []):
                if item.get('confidence', 0.0) >= self.config.ATOMIC_GENERATION_CONFIG['conclusion_extraction_confidence']:
                    conclusion = TaskConclusion(
                        conclusion=item['conclusion'],
                        relationship=item['relationship'],
                        content_identifier=item['content_identifier'],
                        extraction_confidence=item['confidence'],
                        verifiability=self._assess_verifiability(item['conclusion'])
                    )
                    conclusions.append(conclusion)
            
            return conclusions
            
        except (json.JSONDecodeError, KeyError, Exception) as e:
            logger.error(f"âŒ è§£æç»“è®ºå“åº”å¤±è´¥: {e}")
            return []
    
    def _assess_verifiability(self, conclusion: str) -> bool:
        """è¯„ä¼°ç»“è®ºçš„å¯éªŒè¯æ€§"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“çš„æ•°å€¼ã€æ—¶é—´ã€åç§°ç­‰å¯éªŒè¯å…ƒç´ 
        verifiable_patterns = [
            r'\d+\.?\d*',              # æ•°å€¼
            r'\d{4}[-/]\d{1,2}[-/]\d{1,2}',  # æ—¥æœŸ
            r'\d{1,2}:\d{2}',          # æ—¶é—´
            r'[A-Z][a-z]+ [A-Z][a-z]+',  # ä¸“æœ‰åè¯
            r'https?://\S+',           # URL
            r'\$\d+',                  # è´§å¸
            r'\d+%',                   # ç™¾åˆ†æ¯”
        ]
        
        verification_count = sum(1 for pattern in verifiable_patterns if re.search(pattern, conclusion))
        return verification_count >= 2


class QuestionGenerator:
    """é—®é¢˜ç”Ÿæˆå™¨ - å°†ç»“è®ºè½¬æ¢ä¸ºé—®é¢˜"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
        self.config = EnhancedSynthesisConfig()
    
    async def generate_questions_from_conclusions(self, conclusions: List[TaskConclusion]) -> List[Dict[str, Any]]:
        """ä»ç»“è®ºç”Ÿæˆå€™é€‰é—®é¢˜"""
        logger.debug(f"ğŸ”„ å¼€å§‹ä» {len(conclusions)} ä¸ªç»“è®ºç”Ÿæˆé—®é¢˜")
        
        candidate_questions = []
        
        for conclusion in conclusions:
            try:
                questions = await self._generate_questions_for_conclusion(conclusion)
                candidate_questions.extend(questions)
                
            except Exception as e:
                logger.error(f"âŒ ä»ç»“è®ºç”Ÿæˆé—®é¢˜å¤±è´¥: {e}")
                continue
        
        logger.info(f"âœ… ç”Ÿæˆäº† {len(candidate_questions)} ä¸ªå€™é€‰é—®é¢˜")
        return candidate_questions
    
    async def _generate_questions_for_conclusion(self, conclusion: TaskConclusion) -> List[Dict[str, Any]]:
        """ä¸ºå•ä¸ªç»“è®ºç”Ÿæˆé—®é¢˜"""
        
        question_prompt = f"""
åŸºäºä»¥ä¸‹ç»“è®ºï¼Œç”Ÿæˆéœ€è¦çœŸå®å·¥å…·è°ƒç”¨çš„åŸå­ä»»åŠ¡é—®é¢˜ï¼š

ç»“è®º: {conclusion.conclusion}
å…³ç³»: {conclusion.relationship}
å†…å®¹æ ‡è¯†ç¬¦: {conclusion.content_identifier}

âš ï¸ å…³é”®è¦æ±‚ - TaskCraftåŸåˆ™ï¼š
1. é—®é¢˜å¿…é¡»æ˜¯åŸå­æ€§çš„ï¼ˆä¸å¯å†åˆ†çš„å•ä¸€é—®é¢˜ï¼‰
2. é—®é¢˜å¿…é¡»éœ€è¦å¤šæ­¥éª¤å·¥å…·è°ƒç”¨ï¼ˆæœç´¢+åˆ†æ+éªŒè¯ç­‰ï¼‰
3. ä¸èƒ½æ˜¯ç®€å•çš„äº‹å®æŸ¥è¯¢ï¼Œå¿…é¡»æ¶‰åŠæ¨ç†å’Œè®¡ç®—
4. å·¥å…·éœ€æ±‚åº”è¯¥æ˜¯ç°å®å­˜åœ¨çš„ï¼ˆå¦‚web_search, python_executor, deepsearch, browser_navigatorç­‰ï¼‰
5. ä»»åŠ¡åº”è¯¥æµ‹è¯•Agentçš„å·¥å…·ç»„åˆä½¿ç”¨èƒ½åŠ›

âŒ é¿å…ç”Ÿæˆï¼š
- ç®€å•äº‹å®æŸ¥è¯¢ï¼š"Xçš„åç§°æ˜¯ä»€ä¹ˆï¼Ÿ"
- è¿‡äºå…·ä½“çš„å·¥å…·ï¼š["get_specific_database_name"]
- ç›´æ¥ç­”æ¡ˆé—®é¢˜ï¼šç­”æ¡ˆç›´æ¥åœ¨å†…å®¹ä¸­

âœ… åº”è¯¥ç”Ÿæˆç±»ä¼¼ï¼š
- "åˆ†æå¹¶æ¯”è¾ƒä¸åŒå‘é‡æ•°æ®åº“çš„æ€§èƒ½ç‰¹ç‚¹ï¼Œæ¨èæœ€é€‚åˆé•¿æœŸè®°å¿†å­˜å‚¨çš„è§£å†³æ–¹æ¡ˆ"
- "ç¼–å†™ä»£ç éªŒè¯ç‰¹å®šæŠ€æœ¯æ–¹æ¡ˆçš„å¯è¡Œæ€§ï¼Œå¹¶ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"
- "æœç´¢ç›¸å…³æŠ€æœ¯æ–‡æ¡£ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶æ„å»ºçŸ¥è¯†ç»“æ„å›¾"

è¯·ç”Ÿæˆ1-2ä¸ªç¬¦åˆè¦æ±‚çš„é—®é¢˜ï¼Œä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "questions": [
        {{
            "question": "éœ€è¦å¤šå·¥å…·åä½œçš„å¤æ‚é—®é¢˜",
            "answer": "é¢„æœŸçš„åˆ†æç»“æœæˆ–æ¨èæ–¹æ¡ˆ",
            "required_tools": ["web_search", "python_executor", "deepsearch"],
            "reasoning": "ä¸ºä»€ä¹ˆè¿™ä¸ªé—®é¢˜éœ€è¦å·¥å…·è°ƒç”¨ä¸”å…·æœ‰æŒ‘æˆ˜æ€§",
            "complexity_score": 0.8
        }}
    ]
}}
"""
        
        try:
            # ä½¿ç”¨LLMè¿›è¡Œå†…å®¹ç”Ÿæˆ - é€šè¿‡summaryæ–¹æ³•è·å–åŸå§‹å“åº”
            raw_response = await self.llm_client.generate_task_summary(
                task_description=question_prompt,
                steps=[],
                final_outputs=[]
            )
            
            return self._parse_raw_question_response(raw_response, conclusion)
            
        except Exception as e:
            logger.error(f"âŒ é—®é¢˜ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def _parse_raw_question_response(self, raw_response: str, conclusion: TaskConclusion) -> List[Dict[str, Any]]:
        """è§£æåŸå§‹é—®é¢˜ç”Ÿæˆå“åº”"""
        try:
            # å°è¯•ç›´æ¥è§£æJSONå“åº”
            response_clean = raw_response.strip()
            
            # å°è¯•å¤šç§JSONæå–æ–¹å¼
            json_patterns = [
                r'```json\s*(\{.*?\})\s*```',  # markdownä»£ç å—
                r'```\s*(\{.*?\})\s*```',      # æ™®é€šä»£ç å—  
                r'(\{.*?"questions".*?\})',    # åŒ…å«questionsçš„JSON
                r'(\{.*?\})',                  # ä»»ä½•JSONå¯¹è±¡
            ]
            
            question_data = None
            for pattern in json_patterns:
                match = re.search(pattern, response_clean, re.DOTALL)
                if match:
                    try:
                        question_data = json.loads(match.group(1))
                        if 'questions' in question_data:
                            break
                    except json.JSONDecodeError:
                        continue
            
            if not question_data or 'questions' not in question_data:
                return []
            
            questions = []
            for item in question_data.get('questions', []):
                # éªŒè¯ä»»åŠ¡è´¨é‡
                if self._validate_task_quality(item):
                    question_info = {
                        "question": item['question'],
                        "answer": item['answer'],
                        "required_tools": item.get('required_tools', []),
                        "reasoning": item.get('reasoning', ''),
                        "complexity_score": item.get('complexity_score', 0.5),
                        "source_conclusion": conclusion,
                        "content_identifier": conclusion.content_identifier
                    }
                    questions.append(question_info)
                else:
                    logger.debug(f"âš ï¸ ä»»åŠ¡è´¨é‡ä¸ç¬¦åˆè¦æ±‚ï¼Œè·³è¿‡: {item.get('question', '')}")
            
            return questions
            
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"âŒ è§£æé—®é¢˜å“åº”å¤±è´¥: {e}")
            return []
    
    def _parse_question_response(self, response: Dict[str, Any], conclusion: TaskConclusion) -> List[Dict[str, Any]]:
        """è§£æé—®é¢˜ç”Ÿæˆå“åº”"""
        try:
            thinking = response.get('thinking', '{}')
            
            if thinking.strip().startswith('{'):
                question_data = json.loads(thinking)
            else:
                json_match = re.search(r'\{.*\}', thinking, re.DOTALL)
                if json_match:
                    question_data = json.loads(json_match.group())
                else:
                    return []
            
            questions = []
            for item in question_data.get('questions', []):
                question_info = {
                    "question": item['question'],
                    "answer": item['answer'],
                    "required_tools": item.get('required_tools', []),
                    "reasoning": item.get('reasoning', ''),
                    "source_conclusion": conclusion,
                    "content_identifier": conclusion.content_identifier
                }
                questions.append(question_info)
            
            return questions
            
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"âŒ è§£æé—®é¢˜å“åº”å¤±è´¥: {e}")
            return []
    
    def _validate_task_quality(self, task_item: Dict[str, Any]) -> bool:
        """éªŒè¯ä»»åŠ¡è´¨é‡æ˜¯å¦ç¬¦åˆTaskCraftåŸåˆ™"""
        try:
            question = task_item.get('question', '')
            required_tools = task_item.get('required_tools', [])
            complexity_score = task_item.get('complexity_score', 0.0)
            
            # æ£€æŸ¥1: é¿å…ç®€å•äº‹å®æŸ¥è¯¢
            simple_patterns = [
                r'.*çš„åç§°æ˜¯ä»€ä¹ˆ',
                r'ä»€ä¹ˆæ˜¯.*',
                r'.*å«ä»€ä¹ˆ',
                r'.*æ˜¯ä»€ä¹ˆ.*',
                r'.*æ ‡è¯†ç¬¦æ˜¯.*'
            ]
            
            for pattern in simple_patterns:
                if re.search(pattern, question, re.IGNORECASE):
                    logger.debug(f"âŒ æ‹’ç»ç®€å•äº‹å®æŸ¥è¯¢: {question}")
                    return False
            
            # æ£€æŸ¥2: å¿…é¡»æœ‰ç°å®çš„å·¥å…·éœ€æ±‚
            realistic_tools = {
                'web_search', 'python_executor', 'deepsearch', 'browser_navigator', 
                'browser_navigator', 'file_reader', 'data_processor',
                'search_engine', 'code_executor', 'document_analyzer'
            }
            
            if not any(tool in realistic_tools for tool in required_tools):
                logger.debug(f"âŒ å·¥å…·éœ€æ±‚ä¸ç°å®: {required_tools}")
                return False
            
            # æ£€æŸ¥3: å¤æ‚åº¦è¦æ±‚
            if complexity_score < 0.6:
                logger.debug(f"âŒ å¤æ‚åº¦ä¸è¶³: {complexity_score}")
                return False
            
            # æ£€æŸ¥4: éœ€è¦å¤šä¸ªå·¥å…·åä½œ
            if len(required_tools) < 2:
                logger.debug(f"âŒ å·¥å…·æ•°é‡ä¸è¶³: {len(required_tools)}")
                return False
            
            # æ£€æŸ¥5: é—®é¢˜é•¿åº¦åˆç†
            if len(question) < 30:
                logger.debug(f"âŒ é—®é¢˜è¿‡çŸ­: {len(question)}")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡è´¨é‡éªŒè¯å¤±è´¥: {e}")
            return False


class AtomicityVerifier:
    """åŸå­æ€§éªŒè¯å™¨ - éªŒè¯ä»»åŠ¡çš„åŸå­æ€§"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
        self.config = EnhancedSynthesisConfig()
    
    async def verify_atomic_questions(self, candidate_questions: List[Dict[str, Any]]) -> List[AtomicTask]:
        """éªŒè¯å€™é€‰é—®é¢˜çš„åŸå­æ€§"""
        logger.debug(f"ğŸ” å¼€å§‹éªŒè¯ {len(candidate_questions)} ä¸ªå€™é€‰é—®é¢˜çš„åŸå­æ€§")
        
        atomic_tasks = []
        
        # ä½¿ç”¨å¹¶å‘å¤„ç†æé«˜æ•ˆç‡
        semaphore = asyncio.Semaphore(self.config.ATOMIC_GENERATION_CONFIG['parallel_workers'])
        
        async def verify_single_question(question_info):
            async with semaphore:
                return await self._verify_single_question_atomicity(question_info)
        
        verification_results = await asyncio.gather(
            *[verify_single_question(q) for q in candidate_questions],
            return_exceptions=True
        )
        
        for i, result in enumerate(verification_results):
            if isinstance(result, AtomicTask):
                atomic_tasks.append(result)
            elif isinstance(result, Exception):
                logger.error(f"âŒ é—®é¢˜éªŒè¯å¼‚å¸¸: {candidate_questions[i].get('question', 'Unknown')}: {result}")
        
        logger.info(f"âœ… éªŒè¯å®Œæˆï¼Œå¾—åˆ° {len(atomic_tasks)} ä¸ªåŸå­ä»»åŠ¡")
        return atomic_tasks
    
    async def _verify_single_question_atomicity(self, question_info: Dict[str, Any]) -> Optional[AtomicTask]:
        """éªŒè¯å•ä¸ªé—®é¢˜çš„åŸå­æ€§"""
        
        atomicity_prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹é—®é¢˜æ˜¯å¦é€‚åˆä½œä¸ºAI Agentä»»åŠ¡ï¼š

é—®é¢˜: {question_info['question']}
é¢„æœŸç­”æ¡ˆ: {question_info['answer']}
æ‰€éœ€å·¥å…·: {question_info.get('required_tools', [])}

è¯„ä¼°æ ‡å‡†ï¼ˆç›¸å¯¹å®½æ¾ï¼‰:
1. æ ¸å¿ƒç„¦ç‚¹ï¼šé—®é¢˜æœ‰æ˜ç¡®çš„æ ¸å¿ƒç›®æ ‡
2. å·¥å…·éœ€æ±‚ï¼šéœ€è¦ä½¿ç”¨å¤šä¸ªå·¥å…·åä½œå®Œæˆ
3. å¯æ‰§è¡Œæ€§ï¼šAgentèƒ½å¤Ÿé€šè¿‡å·¥å…·è°ƒç”¨å®Œæˆæ­¤ä»»åŠ¡
4. ç»“æœå¯¼å‘ï¼šæœ‰æ˜ç¡®çš„è¾“å‡ºå½¢å¼æˆ–ç›®æ ‡

æ³¨æ„ï¼šåˆ†æç±»ã€æ¯”è¾ƒç±»ã€è®¾è®¡ç±»ä»»åŠ¡éƒ½æ˜¯å¯æ¥å—çš„ï¼Œåªè¦å®ƒä»¬éœ€è¦å·¥å…·åä½œä¸”æœ‰æ˜ç¡®ç›®æ ‡ã€‚

è¯·è¿”å›JSONæ ¼å¼çš„è¯„ä¼°ç»“æœï¼š
{{
    "is_atomic": true/false,
    "atomicity_score": 0.0-1.0,
    "meets_criteria": {{
        "indivisible": true/false,
        "unique_answer": true/false, 
        "tool_dependent": true/false,
        "verifiable": true/false
    }},
    "reasoning": "è¯„ä¼°ç†ç”±",
    "suggested_improvements": ["æ”¹è¿›å»ºè®®"]
}}
"""
        
        try:
            # ä½¿ç”¨LLMè¿›è¡Œå†…å®¹ç”Ÿæˆ - é€šè¿‡summaryæ–¹æ³•è·å–åŸå§‹å“åº”
            raw_response = await self.llm_client.generate_task_summary(
                task_description=atomicity_prompt,
                steps=[],
                final_outputs=[]
            )
            
            verification_result = self._parse_raw_atomicity_response(raw_response)
            
            # åªä¾èµ–åˆ†æ•°ï¼Œä¸å¼ºåˆ¶è¦æ±‚is_atomicä¸ºtrueï¼ˆå› ä¸ºLLMå¯¹å¤æ‚ä»»åŠ¡è¿‡äºä¿å®ˆï¼‰
            if verification_result.get('atomicity_score', 0.0) >= self.config.ATOMIC_GENERATION_CONFIG['atomicity_verification_threshold']:
                
                # éªŒè¯å¹¶ä¿®æ­£å·¥å…·åˆ—è¡¨
                suggested_tools = question_info.get('required_tools', [])
                validated_tools = await self.tool_validator.filter_available_tools(suggested_tools)
                
                # åˆ›å»ºåŸå­ä»»åŠ¡
                atomic_task = AtomicTask(
                    task_id=generate_task_id(TaskType.ATOMIC, question_info['content_identifier']),
                    question=question_info['question'],
                    golden_answer=question_info['answer'],
                    content_identifier=question_info['content_identifier'],
                    source_corpus=question_info.get('source_conclusion').content_identifier if question_info.get('source_conclusion') else '',
                    verification_score=verification_result.get('atomicity_score', 0.0),
                    required_tools=validated_tools,
                    difficulty_level=self._determine_difficulty_level(question_info),
                    atomicity_verified=True
                )
                
                return atomic_task
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ åŸå­æ€§éªŒè¯å¤±è´¥: {e}")
            return None
    
    def _parse_atomicity_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æåŸå­æ€§éªŒè¯å“åº”"""
        try:
            thinking = response.get('thinking', '{}')
            
            if thinking.strip().startswith('{'):
                return json.loads(thinking)
            else:
                json_match = re.search(r'\{.*\}', thinking, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
                else:
                    return {"is_atomic": False, "atomicity_score": 0.0}
                    
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"âŒ è§£æåŸå­æ€§éªŒè¯å“åº”å¤±è´¥: {e}")
            return {"is_atomic": False, "atomicity_score": 0.0}
    
    def _parse_raw_atomicity_response(self, raw_response: str) -> Dict[str, Any]:
        """è§£æåŸå§‹åŸå­æ€§éªŒè¯å“åº”"""
        try:
            logger.debug(f"ğŸ” è§£æåŸå­æ€§éªŒè¯å“åº”")
            
            # å°è¯•ç›´æ¥è§£æJSONå“åº”
            response_clean = raw_response.strip()
            
            # é¦–å…ˆå°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”ä¸ºJSON
            atomicity_data = None
            try:
                atomicity_data = json.loads(response_clean)
                if 'is_atomic' in atomicity_data:
                    pass
                    return atomicity_data
            except json.JSONDecodeError:
                pass
            
            # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•ä»ä»£ç å—ä¸­æå–
            if not atomicity_data:
                # å°è¯•ä»markdownä»£ç å—æå–
                pattern = r'```json\s*(\{.*?\})\s*```'
                match = re.search(pattern, response_clean, re.DOTALL)
                if match:
                    json_content = clean_json_string(match.group(1))
                    try:
                        atomicity_data = json.loads(json_content)
                        if 'is_atomic' in atomicity_data:
                            return atomicity_data
                    except json.JSONDecodeError as e:
                        logger.error(f"âŒ MarkdownåŸå­æ€§JSONè§£æå¤±è´¥: {e}")
                
                # å¦‚æœmarkdownå¤±è´¥ï¼Œå°è¯•æ™®é€šä»£ç å—
                if not atomicity_data:
                    pattern = r'```\s*(\{.*?\})\s*```'
                    match = re.search(pattern, response_clean, re.DOTALL)
                    if match:
                        json_content = clean_json_string(match.group(1))
                        try:
                            atomicity_data = json.loads(json_content)
                            if 'is_atomic' in atomicity_data:
                                return atomicity_data
                        except json.JSONDecodeError as e:
                            logger.error(f"âŒ æ™®é€šä»£ç å—åŸå­æ€§JSONè§£æå¤±è´¥: {e}")
            
            # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›é»˜è®¤å€¼
            logger.warning(f"âš ï¸ æ— æ³•è§£æåŸå­æ€§éªŒè¯å“åº”ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            return {"is_atomic": False, "atomicity_score": 0.0}
            
        except Exception as e:
            logger.error(f"âŒ è§£æåŸå­æ€§éªŒè¯å“åº”å¤±è´¥: {e}")
            return {"is_atomic": False, "atomicity_score": 0.0}
    
    def _determine_difficulty_level(self, question_info: Dict[str, Any]) -> TaskDifficulty:
        """ç¡®å®šä»»åŠ¡éš¾åº¦çº§åˆ«"""
        required_tools = question_info.get('required_tools', [])
        question_complexity = len(question_info['question'].split())
        
        if len(required_tools) <= 1 and question_complexity <= 15:
            return TaskDifficulty.SIMPLE
        elif len(required_tools) <= 2 and question_complexity <= 25:
            return TaskDifficulty.MEDIUM
        else:
            return TaskDifficulty.COMPLEX


class AtomicTaskGenerator:
    """åŸå­ä»»åŠ¡ç”Ÿæˆå™¨ - ç»Ÿä¸€çš„åŸå­ä»»åŠ¡ç”Ÿæˆæ¥å£"""
    
    def __init__(self, llm_client: LLMClient, mcp_client: Optional[MCPToolClient] = None):
        self.llm_client = llm_client
        self.mcp_client = mcp_client
        self.conclusion_extractor = ConclusionExtractor(llm_client)
        self.question_generator = QuestionGenerator(llm_client)
        self.atomicity_verifier = AtomicityVerifier(llm_client)
        self.tool_validator = ToolValidator(mcp_client)
        self.config = EnhancedSynthesisConfig()
    
    async def generate_atomic_tasks_from_corpus(self, corpus_contents: List[CorpusContent]) -> List[AtomicTask]:
        """ä»è¯­æ–™æ‰¹é‡ç”ŸæˆåŸå­ä»»åŠ¡"""
        logger.info(f"ğŸš€ å¼€å§‹ä» {len(corpus_contents)} ä¸ªè¯­æ–™ç”ŸæˆåŸå­ä»»åŠ¡")
        
        all_atomic_tasks = []
        
        for corpus in corpus_contents:
            try:
                tasks = await self._generate_atomic_tasks_for_single_corpus(corpus)
                all_atomic_tasks.extend(tasks)
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†è¯­æ–™å¤±è´¥ {corpus.corpus_id}: {e}")
                continue
        
        logger.info(f"âœ… åŸå­ä»»åŠ¡ç”Ÿæˆå®Œæˆï¼Œæ€»è®¡ç”Ÿæˆ {len(all_atomic_tasks)} ä¸ªåŸå­ä»»åŠ¡")
        return all_atomic_tasks
    
    async def _generate_atomic_tasks_for_single_corpus(self, corpus_content: CorpusContent) -> List[AtomicTask]:
        """ä¸ºå•ä¸ªè¯­æ–™ç”ŸæˆåŸå­ä»»åŠ¡"""
        logger.debug(f"ğŸ”„ å¤„ç†è¯­æ–™: {corpus_content.corpus_id}")
        
        start_time = time.time()
        
        try:
            # 1. ç»“è®ºæå–
            conclusions = await self.conclusion_extractor.extract_conclusions_from_corpus(corpus_content)
            if not conclusions:
                logger.warning(f"âš ï¸ è¯­æ–™ {corpus_content.corpus_id} æœªæå–åˆ°ç»“è®º")
                return []
            
            # 2. é—®é¢˜ç”Ÿæˆ
            candidate_questions = await self.question_generator.generate_questions_from_conclusions(conclusions)
            if not candidate_questions:
                logger.warning(f"âš ï¸ è¯­æ–™ {corpus_content.corpus_id} æœªç”Ÿæˆå€™é€‰é—®é¢˜")
                return []
            
            # 3. åŸå­æ€§éªŒè¯
            atomic_tasks = await self.atomicity_verifier.verify_atomic_questions(candidate_questions)
            
            processing_time = time.time() - start_time
            logger.info(f"âœ… è¯­æ–™ {corpus_content.corpus_id} å¤„ç†å®Œæˆ: {len(atomic_tasks)} ä¸ªåŸå­ä»»åŠ¡ (ç”¨æ—¶ {processing_time:.2f}s)")
            
            return atomic_tasks
            
        except Exception as e:
            logger.error(f"âŒ è¯­æ–™å¤„ç†å¤±è´¥ {corpus_content.corpus_id}: {e}")
            return []
    
    async def generate_atomic_tasks_from_trajectories(self, trajectories: List[TrajectoryResult]) -> List[AtomicTask]:
        """ä»è½¨è¿¹ç›´æ¥ç”ŸæˆåŸå­ä»»åŠ¡ï¼ˆé›†æˆè¯­æ–™æå–ï¼‰"""
        logger.info(f"ğŸ”„ å¼€å§‹ä» {len(trajectories)} ä¸ªè½¨è¿¹ç”ŸæˆåŸå­ä»»åŠ¡")
        
        # é¦–å…ˆéœ€è¦ä»è½¨è¿¹ä¸­æå–è¯­æ–™
        from .corpus_ingestor import CorpusIngestor
        corpus_ingestor = CorpusIngestor(self.mcp_client)
        
        # æå–è¯­æ–™
        corpus_contents = await corpus_ingestor.ingest_from_trajectories(trajectories)
        
        # ç”ŸæˆåŸå­ä»»åŠ¡
        atomic_tasks = await self.generate_atomic_tasks_from_corpus(corpus_contents)
        
        logger.info(f"âœ… ä»è½¨è¿¹ç”ŸæˆåŸå­ä»»åŠ¡å®Œæˆ: {len(atomic_tasks)} ä¸ªä»»åŠ¡")
        return atomic_tasks
    
    async def validate_and_execute_atomic_task(self, atomic_task: AtomicTask) -> Dict[str, Any]:
        """éªŒè¯å’Œæ‰§è¡ŒåŸå­ä»»åŠ¡ï¼ˆç”¨äºè´¨é‡æ£€æŸ¥ï¼‰"""
        if not self.mcp_client:
            logger.warning(f"âš ï¸ MCPå®¢æˆ·ç«¯æœªé…ç½®ï¼Œæ— æ³•æ‰§è¡Œä»»åŠ¡éªŒè¯: {atomic_task.task_id}")
            return {"success": False, "error": "MCPå®¢æˆ·ç«¯æœªé…ç½®"}
        
        try:
            # ä½¿ç”¨éªŒè¯å¼•æ“æ‰§è¡Œä»»åŠ¡
            from .verification_agent import TaskExecutor
            task_executor = TaskExecutor(self.llm_client, self.mcp_client)
            
            execution_result = await task_executor.execute_task_with_tools(
                atomic_task.question,
                atomic_task.golden_answer,
                timeout=self.config.VERIFICATION_CONFIG['execution_timeout_seconds']
            )
            
            # æ›´æ–°ä»»åŠ¡çš„å¯æ‰§è¡Œæ€§éªŒè¯çŠ¶æ€
            if execution_result.get('success', False) and execution_result.get('answer_correct', False):
                atomic_task.executability_verified = True
            
            return execution_result
            
        except Exception as e:
            logger.error(f"âŒ åŸå­ä»»åŠ¡æ‰§è¡ŒéªŒè¯å¤±è´¥ {atomic_task.task_id}: {e}")
            return {"success": False, "error": str(e)}
    
    async def get_generation_statistics(self, atomic_tasks: List[AtomicTask]) -> Dict[str, Any]:
        """è·å–ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        if not atomic_tasks:
            return {"total_tasks": 0}
        
        # æŒ‰éš¾åº¦çº§åˆ«ç»Ÿè®¡
        difficulty_stats = {
            TaskDifficulty.SIMPLE.value: 0,
            TaskDifficulty.MEDIUM.value: 0,
            TaskDifficulty.COMPLEX.value: 0
        }
        
        # æŒ‰å·¥å…·ä½¿ç”¨ç»Ÿè®¡
        tool_usage = {}
        verification_scores = []
        
        for task in atomic_tasks:
            difficulty_stats[task.difficulty_level.value] += 1
            verification_scores.append(task.verification_score)
            
            for tool in task.required_tools:
                tool_usage[tool] = tool_usage.get(tool, 0) + 1
        
        return {
            "total_tasks": len(atomic_tasks),
            "difficulty_distribution": difficulty_stats,
            "tool_usage": tool_usage,
            "average_verification_score": sum(verification_scores) / len(verification_scores) if verification_scores else 0.0,
            "atomicity_verified_count": sum(1 for task in atomic_tasks if task.atomicity_verified),
            "executability_verified_count": sum(1 for task in atomic_tasks if task.executability_verified),
            "unique_content_identifiers": len(set(task.content_identifier for task in atomic_tasks)),
            "unique_source_corpus": len(set(task.source_corpus for task in atomic_tasks if task.source_corpus))
        }