#!/usr/bin/env python3
"""
Atomic Task Generator - åŸå­ä»»åŠ¡ç”Ÿæˆå™¨
åŸºäºTaskCraftç®—æ³•ï¼Œå®ç°ä»è¯­æ–™åˆ°åŸå­ä»»åŠ¡çš„è‡ªåŠ¨ç”Ÿæˆ
"""

import asyncio
import json
import logging
import re
import time
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import asdict

from core.interfaces import TrajectoryResult
from core.llm_client import LLMClient
from core.toolscore.mcp_client import MCPToolClient
from .enhanced_interfaces import (
    CorpusContent, AtomicTask, TaskConclusion, TaskDifficulty, 
    TaskType, EnhancedSynthesisConfig, generate_task_id
)

logger = logging.getLogger(__name__)


class ConclusionExtractor:
    """ç»“è®ºæå–å™¨ - ä»è¯­æ–™ä¸­æå–åŸå­ç»“è®º"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
        self.config = EnhancedSynthesisConfig()
    
    async def extract_conclusions_from_corpus(self, corpus_content: CorpusContent) -> List[TaskConclusion]:
        """ä»è¯­æ–™ä¸­æå–ç»“è®º"""
        logger.debug(f"ğŸ” å¼€å§‹ä»è¯­æ–™ä¸­æå–ç»“è®º: {corpus_content.corpus_id}")
        
        try:
            # æ„å»ºç»“è®ºæå–æç¤ºè¯
            extraction_prompt = self._build_conclusion_extraction_prompt(corpus_content)
            
            # è°ƒç”¨LLMè¿›è¡Œç»“è®ºæå–
            response = await self.llm_client.generate_reasoning(
                task_description=extraction_prompt,
                available_tools=[]
            )
            
            # è§£æLLMå“åº”
            conclusions = self._parse_conclusion_response(response, corpus_content)
            
            logger.info(f"âœ… ä»è¯­æ–™ {corpus_content.corpus_id} ä¸­æå–äº† {len(conclusions)} ä¸ªç»“è®º")
            return conclusions
            
        except Exception as e:
            logger.error(f"âŒ ç»“è®ºæå–å¤±è´¥ {corpus_content.corpus_id}: {e}")
            return []
    
    def _build_conclusion_extraction_prompt(self, corpus_content: CorpusContent) -> str:
        """æ„å»ºç»“è®ºæå–æç¤ºè¯"""
        
        content_preview = corpus_content.text_content[:1000] + "..." if len(corpus_content.text_content) > 1000 else corpus_content.text_content
        
        return f"""
è¯·ä»ä»¥ä¸‹å†…å®¹ä¸­æå–å¯ä»¥å½¢æˆåŸå­ä»»åŠ¡çš„å…³é”®ç»“è®ºã€‚æ¯ä¸ªç»“è®ºå¿…é¡»æ»¡è¶³ï¼š
1. åŒ…å«å…·ä½“ã€å¯éªŒè¯çš„äº‹å®ï¼ˆæ•°å€¼ã€æ—¶é—´ã€åç§°ç­‰ï¼‰
2. å…·æœ‰æ˜ç¡®çš„å…³ç³»æè¿°
3. å¯ä»¥è½¬æ¢ä¸ºä¸€ä¸ªä¸å¯å†åˆ†çš„é—®é¢˜

å†…å®¹ç±»å‹: {corpus_content.content_type.value}
å†…å®¹æ¥æº: {corpus_content.source}
å†…å®¹ç‰‡æ®µ:
{content_preview}

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“è®ºåˆ—è¡¨ï¼Œæ¯ä¸ªç»“è®ºåŒ…å«ï¼š
- conclusion: ç»“è®ºå†…å®¹
- relationship: å…³ç³»æè¿° (ä¾‹å¦‚ï¼š"Xå±äºY", "Xçš„å€¼æ˜¯Y", "Xå‘ç”Ÿåœ¨Yæ—¶é—´")
- content_identifier: å†…å®¹æ ‡è¯†ç¬¦
- confidence: æå–ç½®ä¿¡åº¦ (0.0-1.0)

ç¤ºä¾‹æ ¼å¼:
{{
    "conclusions": [
        {{
            "conclusion": "è‹¹æœå…¬å¸çš„è‚¡ä»·åœ¨2023å¹´12æœˆ15æ—¥æ”¶ç›˜ä»·ä¸º198.11ç¾å…ƒ",
            "relationship": "è‚¡ä»·-å…¬å¸-æ—¶é—´-æ•°å€¼",
            "content_identifier": "stock_price_apple_20231215",
            "confidence": 0.95
        }}
    ]
}}

è¦æ±‚ï¼š
- æœ€å¤šæå–{self.config.ATOMIC_GENERATION_CONFIG['max_conclusions_per_corpus']}ä¸ªç»“è®º
- åªæå–å…·æœ‰é«˜ç½®ä¿¡åº¦(>0.7)çš„ç»“è®º
- é¿å…é‡å¤æˆ–ç›¸ä¼¼çš„ç»“è®º
"""
    
    def _parse_conclusion_response(self, response: Dict[str, Any], corpus_content: CorpusContent) -> List[TaskConclusion]:
        """è§£æç»“è®ºæå–å“åº”"""
        try:
            thinking = response.get('thinking', '{}')
            
            # å°è¯•è§£æJSONå“åº”
            if thinking.strip().startswith('{'):
                conclusion_data = json.loads(thinking)
            else:
                # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–
                json_match = re.search(r'\{.*\}', thinking, re.DOTALL)
                if json_match:
                    conclusion_data = json.loads(json_match.group())
                else:
                    logger.warning(f"âš ï¸ æ— æ³•è§£æç»“è®ºå“åº”: {corpus_content.corpus_id}")
                    return []
            
            conclusions = []
            for item in conclusion_data.get('conclusions', []):
                if item.get('confidence', 0.0) >= self.config.ATOMIC_GENERATION_CONFIG['conclusion_extraction_confidence']:
                    conclusion = TaskConclusion(
                        conclusion=item['conclusion'],
                        relationship=item['relationship'],
                        content_identifier=item['content_identifier'],
                        extraction_confidence=item['confidence'],
                        verifiability=self._assess_verifiability(item['conclusion'])
                    )
                    conclusions.append(conclusion)
            
            return conclusions
            
        except (json.JSONDecodeError, KeyError, Exception) as e:
            logger.error(f"âŒ è§£æç»“è®ºå“åº”å¤±è´¥: {e}")
            return []
    
    def _assess_verifiability(self, conclusion: str) -> bool:
        """è¯„ä¼°ç»“è®ºçš„å¯éªŒè¯æ€§"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“çš„æ•°å€¼ã€æ—¶é—´ã€åç§°ç­‰å¯éªŒè¯å…ƒç´ 
        verifiable_patterns = [
            r'\d+\.?\d*',              # æ•°å€¼
            r'\d{4}[-/]\d{1,2}[-/]\d{1,2}',  # æ—¥æœŸ
            r'\d{1,2}:\d{2}',          # æ—¶é—´
            r'[A-Z][a-z]+ [A-Z][a-z]+',  # ä¸“æœ‰åè¯
            r'https?://\S+',           # URL
            r'\$\d+',                  # è´§å¸
            r'\d+%',                   # ç™¾åˆ†æ¯”
        ]
        
        verification_count = sum(1 for pattern in verifiable_patterns if re.search(pattern, conclusion))
        return verification_count >= 2


class QuestionGenerator:
    """é—®é¢˜ç”Ÿæˆå™¨ - å°†ç»“è®ºè½¬æ¢ä¸ºé—®é¢˜"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
        self.config = EnhancedSynthesisConfig()
    
    async def generate_questions_from_conclusions(self, conclusions: List[TaskConclusion]) -> List[Dict[str, Any]]:
        """ä»ç»“è®ºç”Ÿæˆå€™é€‰é—®é¢˜"""
        logger.debug(f"ğŸ”„ å¼€å§‹ä» {len(conclusions)} ä¸ªç»“è®ºç”Ÿæˆé—®é¢˜")
        
        candidate_questions = []
        
        for conclusion in conclusions:
            try:
                questions = await self._generate_questions_for_conclusion(conclusion)
                candidate_questions.extend(questions)
                
            except Exception as e:
                logger.error(f"âŒ ä»ç»“è®ºç”Ÿæˆé—®é¢˜å¤±è´¥: {e}")
                continue
        
        logger.info(f"âœ… ç”Ÿæˆäº† {len(candidate_questions)} ä¸ªå€™é€‰é—®é¢˜")
        return candidate_questions
    
    async def _generate_questions_for_conclusion(self, conclusion: TaskConclusion) -> List[Dict[str, Any]]:
        """ä¸ºå•ä¸ªç»“è®ºç”Ÿæˆé—®é¢˜"""
        
        question_prompt = f"""
åŸºäºä»¥ä¸‹ç»“è®ºï¼Œç”Ÿæˆç›¸åº”çš„åŸå­ä»»åŠ¡é—®é¢˜ï¼š

ç»“è®º: {conclusion.conclusion}
å…³ç³»: {conclusion.relationship}
å†…å®¹æ ‡è¯†ç¬¦: {conclusion.content_identifier}

è¦æ±‚ï¼š
1. é—®é¢˜å¿…é¡»æ˜¯åŸå­æ€§çš„ï¼ˆä¸å¯å†åˆ†çš„å•ä¸€é—®é¢˜ï¼‰
2. ç­”æ¡ˆåº”è¯¥æ˜¯ç»“è®ºä¸­çš„å…·ä½“äº‹å®
3. é—®é¢˜åº”è¯¥éœ€è¦å·¥å…·è°ƒç”¨æ‰èƒ½å›ç­”ï¼ˆè€Œä¸æ˜¯çº¯LLMæ¨ç†ï¼‰
4. é¿å…æ˜¯éé¢˜ï¼Œä¼˜å…ˆé€‰æ‹©å…·ä½“æ•°å€¼ã€åç§°ã€æ—¶é—´ç­‰

è¯·ç”Ÿæˆ2-3ä¸ªä¸åŒè§’åº¦çš„é—®é¢˜ï¼Œä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "questions": [
        {{
            "question": "é—®é¢˜å†…å®¹",
            "answer": "é¢„æœŸç­”æ¡ˆ",
            "required_tools": ["å·¥å…·1", "å·¥å…·2"],
            "reasoning": "ä¸ºä»€ä¹ˆè¿™ä¸ªé—®é¢˜æ˜¯åŸå­æ€§çš„"
        }}
    ]
}}
"""
        
        try:
            response = await self.llm_client.generate_reasoning(
                task_description=question_prompt,
                available_tools=[]
            )
            
            return self._parse_question_response(response, conclusion)
            
        except Exception as e:
            logger.error(f"âŒ é—®é¢˜ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def _parse_question_response(self, response: Dict[str, Any], conclusion: TaskConclusion) -> List[Dict[str, Any]]:
        """è§£æé—®é¢˜ç”Ÿæˆå“åº”"""
        try:
            thinking = response.get('thinking', '{}')
            
            if thinking.strip().startswith('{'):
                question_data = json.loads(thinking)
            else:
                json_match = re.search(r'\{.*\}', thinking, re.DOTALL)
                if json_match:
                    question_data = json.loads(json_match.group())
                else:
                    return []
            
            questions = []
            for item in question_data.get('questions', []):
                question_info = {
                    "question": item['question'],
                    "answer": item['answer'],
                    "required_tools": item.get('required_tools', []),
                    "reasoning": item.get('reasoning', ''),
                    "source_conclusion": conclusion,
                    "content_identifier": conclusion.content_identifier
                }
                questions.append(question_info)
            
            return questions
            
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"âŒ è§£æé—®é¢˜å“åº”å¤±è´¥: {e}")
            return []


class AtomicityVerifier:
    """åŸå­æ€§éªŒè¯å™¨ - éªŒè¯ä»»åŠ¡çš„åŸå­æ€§"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
        self.config = EnhancedSynthesisConfig()
    
    async def verify_atomic_questions(self, candidate_questions: List[Dict[str, Any]]) -> List[AtomicTask]:
        """éªŒè¯å€™é€‰é—®é¢˜çš„åŸå­æ€§"""
        logger.debug(f"ğŸ” å¼€å§‹éªŒè¯ {len(candidate_questions)} ä¸ªå€™é€‰é—®é¢˜çš„åŸå­æ€§")
        
        atomic_tasks = []
        
        # ä½¿ç”¨å¹¶å‘å¤„ç†æé«˜æ•ˆç‡
        semaphore = asyncio.Semaphore(self.config.ATOMIC_GENERATION_CONFIG['parallel_workers'])
        
        async def verify_single_question(question_info):
            async with semaphore:
                return await self._verify_single_question_atomicity(question_info)
        
        verification_results = await asyncio.gather(
            *[verify_single_question(q) for q in candidate_questions],
            return_exceptions=True
        )
        
        for i, result in enumerate(verification_results):
            if isinstance(result, AtomicTask):
                atomic_tasks.append(result)
            elif isinstance(result, Exception):
                logger.error(f"âŒ é—®é¢˜éªŒè¯å¼‚å¸¸: {candidate_questions[i].get('question', 'Unknown')}: {result}")
        
        logger.info(f"âœ… éªŒè¯å®Œæˆï¼Œå¾—åˆ° {len(atomic_tasks)} ä¸ªåŸå­ä»»åŠ¡")
        return atomic_tasks
    
    async def _verify_single_question_atomicity(self, question_info: Dict[str, Any]) -> Optional[AtomicTask]:
        """éªŒè¯å•ä¸ªé—®é¢˜çš„åŸå­æ€§"""
        
        atomicity_prompt = f"""
è¯·ä¸¥æ ¼è¯„ä¼°ä»¥ä¸‹é—®é¢˜æ˜¯å¦ç¬¦åˆåŸå­ä»»åŠ¡çš„æ ‡å‡†ï¼š

é—®é¢˜: {question_info['question']}
é¢„æœŸç­”æ¡ˆ: {question_info['answer']}
æ‰€éœ€å·¥å…·: {question_info.get('required_tools', [])}

åŸå­ä»»åŠ¡æ ‡å‡†:
1. ä¸å¯å†åˆ†ï¼šé—®é¢˜ä¸èƒ½è¢«æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹çš„å­é—®é¢˜
2. ç­”æ¡ˆå”¯ä¸€ï¼šæœ‰æ˜ç¡®ã€å”¯ä¸€çš„æ­£ç¡®ç­”æ¡ˆ
3. å·¥å…·ä¾èµ–ï¼šéœ€è¦ä½¿ç”¨å·¥å…·æ‰èƒ½å›ç­”ï¼Œçº¯LLMæ— æ³•è§£ç­”
4. å¯éªŒè¯æ€§ï¼šç­”æ¡ˆåŒ…å«å…·ä½“çš„æ•°å€¼ã€æ—¶é—´ã€åç§°ç­‰å¯éªŒè¯ä¿¡æ¯

è¯·è¿”å›JSONæ ¼å¼çš„è¯„ä¼°ç»“æœï¼š
{{
    "is_atomic": true/false,
    "atomicity_score": 0.0-1.0,
    "meets_criteria": {{
        "indivisible": true/false,
        "unique_answer": true/false, 
        "tool_dependent": true/false,
        "verifiable": true/false
    }},
    "reasoning": "è¯¦ç»†è¯„ä¼°ç†ç”±",
    "suggested_improvements": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"]
}}
"""
        
        try:
            response = await self.llm_client.generate_reasoning(
                task_description=atomicity_prompt,
                available_tools=[]
            )
            
            verification_result = self._parse_atomicity_response(response)
            
            if (verification_result.get('is_atomic', False) and 
                verification_result.get('atomicity_score', 0.0) >= self.config.ATOMIC_GENERATION_CONFIG['atomicity_verification_threshold']):
                
                # åˆ›å»ºåŸå­ä»»åŠ¡
                atomic_task = AtomicTask(
                    task_id=generate_task_id(TaskType.ATOMIC, question_info['content_identifier']),
                    question=question_info['question'],
                    golden_answer=question_info['answer'],
                    content_identifier=question_info['content_identifier'],
                    source_corpus=question_info.get('source_conclusion').content_identifier if question_info.get('source_conclusion') else '',
                    verification_score=verification_result.get('atomicity_score', 0.0),
                    required_tools=question_info.get('required_tools', []),
                    difficulty_level=self._determine_difficulty_level(question_info),
                    atomicity_verified=True
                )
                
                return atomic_task
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ åŸå­æ€§éªŒè¯å¤±è´¥: {e}")
            return None
    
    def _parse_atomicity_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æåŸå­æ€§éªŒè¯å“åº”"""
        try:
            thinking = response.get('thinking', '{}')
            
            if thinking.strip().startswith('{'):
                return json.loads(thinking)
            else:
                json_match = re.search(r'\{.*\}', thinking, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
                else:
                    return {"is_atomic": False, "atomicity_score": 0.0}
                    
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"âŒ è§£æåŸå­æ€§éªŒè¯å“åº”å¤±è´¥: {e}")
            return {"is_atomic": False, "atomicity_score": 0.0}
    
    def _determine_difficulty_level(self, question_info: Dict[str, Any]) -> TaskDifficulty:
        """ç¡®å®šä»»åŠ¡éš¾åº¦çº§åˆ«"""
        required_tools = question_info.get('required_tools', [])
        question_complexity = len(question_info['question'].split())
        
        if len(required_tools) <= 1 and question_complexity <= 15:
            return TaskDifficulty.SIMPLE
        elif len(required_tools) <= 2 and question_complexity <= 25:
            return TaskDifficulty.MEDIUM
        else:
            return TaskDifficulty.COMPLEX


class AtomicTaskGenerator:
    """åŸå­ä»»åŠ¡ç”Ÿæˆå™¨ - ç»Ÿä¸€çš„åŸå­ä»»åŠ¡ç”Ÿæˆæ¥å£"""
    
    def __init__(self, llm_client: LLMClient, mcp_client: Optional[MCPToolClient] = None):
        self.llm_client = llm_client
        self.mcp_client = mcp_client
        self.conclusion_extractor = ConclusionExtractor(llm_client)
        self.question_generator = QuestionGenerator(llm_client)
        self.atomicity_verifier = AtomicityVerifier(llm_client)
        self.config = EnhancedSynthesisConfig()
    
    async def generate_atomic_tasks_from_corpus(self, corpus_contents: List[CorpusContent]) -> List[AtomicTask]:
        """ä»è¯­æ–™æ‰¹é‡ç”ŸæˆåŸå­ä»»åŠ¡"""
        logger.info(f"ğŸš€ å¼€å§‹ä» {len(corpus_contents)} ä¸ªè¯­æ–™ç”ŸæˆåŸå­ä»»åŠ¡")
        
        all_atomic_tasks = []
        
        for corpus in corpus_contents:
            try:
                tasks = await self._generate_atomic_tasks_for_single_corpus(corpus)
                all_atomic_tasks.extend(tasks)
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†è¯­æ–™å¤±è´¥ {corpus.corpus_id}: {e}")
                continue
        
        logger.info(f"âœ… åŸå­ä»»åŠ¡ç”Ÿæˆå®Œæˆï¼Œæ€»è®¡ç”Ÿæˆ {len(all_atomic_tasks)} ä¸ªåŸå­ä»»åŠ¡")
        return all_atomic_tasks
    
    async def _generate_atomic_tasks_for_single_corpus(self, corpus_content: CorpusContent) -> List[AtomicTask]:
        """ä¸ºå•ä¸ªè¯­æ–™ç”ŸæˆåŸå­ä»»åŠ¡"""
        logger.debug(f"ğŸ”„ å¤„ç†è¯­æ–™: {corpus_content.corpus_id}")
        
        start_time = time.time()
        
        try:
            # 1. ç»“è®ºæå–
            conclusions = await self.conclusion_extractor.extract_conclusions_from_corpus(corpus_content)
            if not conclusions:
                logger.warning(f"âš ï¸ è¯­æ–™ {corpus_content.corpus_id} æœªæå–åˆ°ç»“è®º")
                return []
            
            # 2. é—®é¢˜ç”Ÿæˆ
            candidate_questions = await self.question_generator.generate_questions_from_conclusions(conclusions)
            if not candidate_questions:
                logger.warning(f"âš ï¸ è¯­æ–™ {corpus_content.corpus_id} æœªç”Ÿæˆå€™é€‰é—®é¢˜")
                return []
            
            # 3. åŸå­æ€§éªŒè¯
            atomic_tasks = await self.atomicity_verifier.verify_atomic_questions(candidate_questions)
            
            processing_time = time.time() - start_time
            logger.info(f"âœ… è¯­æ–™ {corpus_content.corpus_id} å¤„ç†å®Œæˆ: {len(atomic_tasks)} ä¸ªåŸå­ä»»åŠ¡ (ç”¨æ—¶ {processing_time:.2f}s)")
            
            return atomic_tasks
            
        except Exception as e:
            logger.error(f"âŒ è¯­æ–™å¤„ç†å¤±è´¥ {corpus_content.corpus_id}: {e}")
            return []
    
    async def generate_atomic_tasks_from_trajectories(self, trajectories: List[TrajectoryResult]) -> List[AtomicTask]:
        """ä»è½¨è¿¹ç›´æ¥ç”ŸæˆåŸå­ä»»åŠ¡ï¼ˆé›†æˆè¯­æ–™æå–ï¼‰"""
        logger.info(f"ğŸ”„ å¼€å§‹ä» {len(trajectories)} ä¸ªè½¨è¿¹ç”ŸæˆåŸå­ä»»åŠ¡")
        
        # é¦–å…ˆéœ€è¦ä»è½¨è¿¹ä¸­æå–è¯­æ–™
        from .corpus_ingestor import CorpusIngestor
        corpus_ingestor = CorpusIngestor(self.mcp_client)
        
        # æå–è¯­æ–™
        corpus_contents = await corpus_ingestor.ingest_from_trajectories(trajectories)
        
        # ç”ŸæˆåŸå­ä»»åŠ¡
        atomic_tasks = await self.generate_atomic_tasks_from_corpus(corpus_contents)
        
        logger.info(f"âœ… ä»è½¨è¿¹ç”ŸæˆåŸå­ä»»åŠ¡å®Œæˆ: {len(atomic_tasks)} ä¸ªä»»åŠ¡")
        return atomic_tasks
    
    async def validate_and_execute_atomic_task(self, atomic_task: AtomicTask) -> Dict[str, Any]:
        """éªŒè¯å’Œæ‰§è¡ŒåŸå­ä»»åŠ¡ï¼ˆç”¨äºè´¨é‡æ£€æŸ¥ï¼‰"""
        if not self.mcp_client:
            logger.warning(f"âš ï¸ MCPå®¢æˆ·ç«¯æœªé…ç½®ï¼Œæ— æ³•æ‰§è¡Œä»»åŠ¡éªŒè¯: {atomic_task.task_id}")
            return {"success": False, "error": "MCPå®¢æˆ·ç«¯æœªé…ç½®"}
        
        try:
            # ä½¿ç”¨éªŒè¯å¼•æ“æ‰§è¡Œä»»åŠ¡
            from .verification_agent import TaskExecutor
            task_executor = TaskExecutor(self.llm_client, self.mcp_client)
            
            execution_result = await task_executor.execute_task_with_tools(
                atomic_task.question,
                atomic_task.golden_answer,
                timeout=self.config.VERIFICATION_CONFIG['execution_timeout_seconds']
            )
            
            # æ›´æ–°ä»»åŠ¡çš„å¯æ‰§è¡Œæ€§éªŒè¯çŠ¶æ€
            if execution_result.get('success', False) and execution_result.get('answer_correct', False):
                atomic_task.executability_verified = True
            
            return execution_result
            
        except Exception as e:
            logger.error(f"âŒ åŸå­ä»»åŠ¡æ‰§è¡ŒéªŒè¯å¤±è´¥ {atomic_task.task_id}: {e}")
            return {"success": False, "error": str(e)}
    
    async def get_generation_statistics(self, atomic_tasks: List[AtomicTask]) -> Dict[str, Any]:
        """è·å–ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        if not atomic_tasks:
            return {"total_tasks": 0}
        
        # æŒ‰éš¾åº¦çº§åˆ«ç»Ÿè®¡
        difficulty_stats = {
            TaskDifficulty.SIMPLE.value: 0,
            TaskDifficulty.MEDIUM.value: 0,
            TaskDifficulty.COMPLEX.value: 0
        }
        
        # æŒ‰å·¥å…·ä½¿ç”¨ç»Ÿè®¡
        tool_usage = {}
        verification_scores = []
        
        for task in atomic_tasks:
            difficulty_stats[task.difficulty_level.value] += 1
            verification_scores.append(task.verification_score)
            
            for tool in task.required_tools:
                tool_usage[tool] = tool_usage.get(tool, 0) + 1
        
        return {
            "total_tasks": len(atomic_tasks),
            "difficulty_distribution": difficulty_stats,
            "tool_usage": tool_usage,
            "average_verification_score": sum(verification_scores) / len(verification_scores) if verification_scores else 0.0,
            "atomicity_verified_count": sum(1 for task in atomic_tasks if task.atomicity_verified),
            "executability_verified_count": sum(1 for task in atomic_tasks if task.executability_verified),
            "unique_content_identifiers": len(set(task.content_identifier for task in atomic_tasks)),
            "unique_source_corpus": len(set(task.source_corpus for task in atomic_tasks if task.source_corpus))
        }