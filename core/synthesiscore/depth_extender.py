#!/usr/bin/env python3
"""
Depth Extender - æ·±åº¦æ‰©å±•å™¨
åŸºäºTaskCraftç®—æ³•ï¼Œå®ç°åŸå­ä»»åŠ¡çš„æ·±åº¦ä¼˜å…ˆæ‰©å±•
"""

import asyncio
import json
import logging
import re
import time
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import asdict

from core.interfaces import TrajectoryResult
from core.llm_client import LLMClient
from core.toolscore.mcp_client import MCPToolClient
from .enhanced_interfaces import (
    AtomicTask, ExtendedTask, SupersetInfo, TaskDifficulty, 
    TaskType, EnhancedSynthesisConfig, generate_task_id
)

logger = logging.getLogger(__name__)


class SupersetSearcher:
    """è¶…é›†æœç´¢å™¨ - å¯»æ‰¾åŸå­ä»»åŠ¡ç­”æ¡ˆçš„è¶…é›†"""
    
    def __init__(self, llm_client: LLMClient, mcp_client: Optional[MCPToolClient] = None):
        self.llm_client = llm_client
        self.mcp_client = mcp_client
        self.config = EnhancedSynthesisConfig()
    
    async def backward_search_superset(self, atomic_task: AtomicTask) -> List[SupersetInfo]:
        """åå‘æœç´¢è¶…é›†ä¿¡æ¯"""
        logger.debug(f"ğŸ” å¼€å§‹ä¸ºåŸå­ä»»åŠ¡æœç´¢è¶…é›†: {atomic_task.task_id}")
        
        try:
            # 1. ç”Ÿæˆæœç´¢æŸ¥è¯¢
            search_queries = await self._generate_search_queries(atomic_task)
            
            # 2. æ‰§è¡Œæœç´¢å¹¶æ”¶é›†å€™é€‰è¶…é›†
            candidate_supersets = []
            for query in search_queries:
                supersets = await self._search_and_extract_supersets(query, atomic_task)
                candidate_supersets.extend(supersets)
            
            # 3. éªŒè¯å’Œæ’åºè¶…é›†
            validated_supersets = await self._validate_and_rank_supersets(candidate_supersets, atomic_task)
            
            logger.info(f"âœ… ä¸ºä»»åŠ¡ {atomic_task.task_id} æ‰¾åˆ° {len(validated_supersets)} ä¸ªæœ‰æ•ˆè¶…é›†")
            return validated_supersets
            
        except Exception as e:
            logger.error(f"âŒ è¶…é›†æœç´¢å¤±è´¥ {atomic_task.task_id}: {e}")
            return []
    
    async def _generate_search_queries(self, atomic_task: AtomicTask) -> List[str]:
        """ç”Ÿæˆæœç´¢æŸ¥è¯¢"""
        
        query_prompt = f"""
åŸºäºä»¥ä¸‹åŸå­ä»»åŠ¡ï¼Œç”Ÿæˆæœç´¢æŸ¥è¯¢æ¥å¯»æ‰¾åŒ…å«ç­”æ¡ˆçš„æ›´å¤§èŒƒå›´ä¿¡æ¯ï¼ˆè¶…é›†ï¼‰ï¼š

åŸå­ä»»åŠ¡é—®é¢˜: {atomic_task.question}
åŸå­ä»»åŠ¡ç­”æ¡ˆ: {atomic_task.golden_answer}

ç¤ºä¾‹ï¼š
- å¦‚æœç­”æ¡ˆæ˜¯"æŸé¦–æ­Œ"ï¼Œè¶…é›†å¯èƒ½æ˜¯"ä¸“è¾‘"ã€"æ­Œæ‰‹çš„æ‰€æœ‰ä½œå“"
- å¦‚æœç­”æ¡ˆæ˜¯"æŸä¸ªåŸå¸‚"ï¼Œè¶…é›†å¯èƒ½æ˜¯"å›½å®¶"ã€"åœ°åŒº"
- å¦‚æœç­”æ¡ˆæ˜¯"æŸä¸ªæ•°å€¼"ï¼Œè¶…é›†å¯èƒ½æ˜¯"å®Œæ•´ç»Ÿè®¡è¡¨"ã€"å¹´åº¦æŠ¥å‘Š"

è¯·ç”Ÿæˆ3-5ä¸ªæœç´¢æŸ¥è¯¢ï¼Œç”¨äºå¯»æ‰¾åŒ…å«è¯¥ç­”æ¡ˆçš„æ›´å¤§ä¿¡æ¯é›†åˆï¼š

è¿”å›JSONæ ¼å¼ï¼š
{{
    "search_queries": [
        "æŸ¥è¯¢1: å¯»æ‰¾åŒ…å«ç­”æ¡ˆçš„æ›´å¤§ç±»åˆ«",
        "æŸ¥è¯¢2: å¯»æ‰¾ç­”æ¡ˆæ‰€å±çš„é›†åˆ",
        "æŸ¥è¯¢3: å¯»æ‰¾ç›¸å…³çš„ä¸Šçº§æ¦‚å¿µ"
    ]
}}
"""
        
        try:
            response = await self.llm_client.generate_reasoning(
                task_description=query_prompt,
                available_tools=[],
                execution_context={"mode": "superset_query_generation"}
            )
            
            return self._parse_search_queries(response)
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæœç´¢æŸ¥è¯¢å¤±è´¥: {e}")
            return [f"{atomic_task.golden_answer} ç›¸å…³ä¿¡æ¯"]
    
    def _parse_search_queries(self, response: Dict[str, Any]) -> List[str]:
        """è§£ææœç´¢æŸ¥è¯¢å“åº”"""
        try:
            thinking = response.get('thinking', '{}')
            
            if thinking.strip().startswith('{'):
                query_data = json.loads(thinking)
            else:
                json_match = re.search(r'\{.*\}', thinking, re.DOTALL)
                if json_match:
                    query_data = json.loads(json_match.group())
                else:
                    return []
            
            return query_data.get('search_queries', [])
            
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"âŒ è§£ææœç´¢æŸ¥è¯¢å¤±è´¥: {e}")
            return []
    
    async def _search_and_extract_supersets(self, query: str, atomic_task: AtomicTask) -> List[SupersetInfo]:
        """æœç´¢å¹¶æå–è¶…é›†ä¿¡æ¯"""
        if not self.mcp_client:
            logger.warning("âš ï¸ MCPå®¢æˆ·ç«¯æœªé…ç½®ï¼Œæ— æ³•æ‰§è¡Œæœç´¢")
            return []
        
        try:
            # æ‰§è¡Œæœç´¢
            search_result = await self.mcp_client.call_tool("deepsearch", {
                "query": query,
                "max_results": self.config.DEPTH_EXTENSION_CONFIG['max_search_results_per_query']
            })
            
            if not search_result or 'results' not in search_result:
                return []
            
            # ä»æœç´¢ç»“æœä¸­æå–è¶…é›†ä¿¡æ¯
            supersets = []
            for result in search_result['results']:
                superset_info = await self._extract_superset_from_result(result, atomic_task, query)
                if superset_info:
                    supersets.append(superset_info)
            
            return supersets
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢æ‰§è¡Œå¤±è´¥ '{query}': {e}")
            return []
    
    async def _extract_superset_from_result(self, search_result: Dict[str, Any], 
                                          atomic_task: AtomicTask, query: str) -> Optional[SupersetInfo]:
        """ä»æœç´¢ç»“æœä¸­æå–è¶…é›†ä¿¡æ¯"""
        
        extraction_prompt = f"""
åˆ†æä»¥ä¸‹æœç´¢ç»“æœï¼Œåˆ¤æ–­æ˜¯å¦åŒ…å«åŸå­ä»»åŠ¡ç­”æ¡ˆçš„è¶…é›†ä¿¡æ¯ï¼š

åŸå­ä»»åŠ¡ç­”æ¡ˆ: {atomic_task.golden_answer}
æœç´¢æŸ¥è¯¢: {query}

æœç´¢ç»“æœ:
æ ‡é¢˜: {search_result.get('title', '')}
æ‘˜è¦: {search_result.get('snippet', '')}
URL: {search_result.get('url', '')}

è¯·åˆ¤æ–­ï¼š
1. è¿™ä¸ªæœç´¢ç»“æœæ˜¯å¦åŒ…å«åŸå­ä»»åŠ¡ç­”æ¡ˆï¼Ÿ
2. æ˜¯å¦å­˜åœ¨åŒ…å«è¯¥ç­”æ¡ˆçš„æ›´å¤§ä¿¡æ¯é›†åˆï¼Ÿ
3. è¯¥é›†åˆä¸ç­”æ¡ˆçš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ

è¿”å›JSONæ ¼å¼ï¼š
{{
    "contains_answer": true/false,
    "superset_info": {{
        "identifier": "è¶…é›†æ ‡è¯†ç¬¦ï¼ˆå¦‚ä¸“è¾‘åã€å›½å®¶åç­‰ï¼‰",
        "relation": "ä¸åŸç­”æ¡ˆçš„å…³ç³»æè¿°",
        "confidence": 0.0-1.0,
        "reasoning": "åˆ¤æ–­ç†ç”±"
    }}
}}
"""
        
        try:
            response = await self.llm_client.generate_reasoning(
                task_description=extraction_prompt,
                available_tools=[],
                execution_context={"mode": "superset_extraction"}
            )
            
            result_data = self._parse_superset_extraction(response)
            
            if (result_data.get('contains_answer', False) and 
                result_data.get('superset_info', {}).get('confidence', 0.0) > 0.6):
                
                superset_data = result_data['superset_info']
                return SupersetInfo(
                    identifier=superset_data['identifier'],
                    relation=superset_data['relation'],
                    search_query=query,
                    confidence=superset_data['confidence'],
                    source_urls=[search_result.get('url', '')],
                    validation_passed=False  # éœ€è¦åç»­éªŒè¯
                )
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ è¶…é›†æå–å¤±è´¥: {e}")
            return None
    
    def _parse_superset_extraction(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æè¶…é›†æå–å“åº”"""
        try:
            thinking = response.get('thinking', '{}')
            
            if thinking.strip().startswith('{'):
                return json.loads(thinking)
            else:
                json_match = re.search(r'\{.*\}', thinking, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
                else:
                    return {"contains_answer": False}
                    
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"âŒ è§£æè¶…é›†æå–å“åº”å¤±è´¥: {e}")
            return {"contains_answer": False}
    
    async def _validate_and_rank_supersets(self, candidate_supersets: List[SupersetInfo], 
                                         atomic_task: AtomicTask) -> List[SupersetInfo]:
        """éªŒè¯å’Œæ’åºè¶…é›†"""
        if not candidate_supersets:
            return []
        
        validated_supersets = []
        
        for superset in candidate_supersets:
            try:
                # éªŒè¯è¶…é›†çš„æœ‰æ•ˆæ€§
                is_valid = await self._validate_superset_relationship(superset, atomic_task)
                
                if is_valid:
                    superset.validation_passed = True
                    validated_supersets.append(superset)
                
            except Exception as e:
                logger.error(f"âŒ è¶…é›†éªŒè¯å¤±è´¥: {e}")
                continue
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        validated_supersets.sort(key=lambda x: x.confidence, reverse=True)
        
        # è¿”å›æœ€å¤š3ä¸ªé«˜è´¨é‡è¶…é›†
        return validated_supersets[:3]
    
    async def _validate_superset_relationship(self, superset: SupersetInfo, atomic_task: AtomicTask) -> bool:
        """éªŒè¯è¶…é›†å…³ç³»çš„æœ‰æ•ˆæ€§"""
        
        validation_prompt = f"""
éªŒè¯ä»¥ä¸‹è¶…é›†å…³ç³»æ˜¯å¦æœ‰æ•ˆï¼š

åŸå­ä»»åŠ¡ç­”æ¡ˆ: {atomic_task.golden_answer}
è¶…é›†æ ‡è¯†ç¬¦: {superset.identifier}
å…³ç³»æè¿°: {superset.relation}

éªŒè¯æ ‡å‡†:
1. è¶…é›†ç¡®å®åŒ…å«åŸå­ä»»åŠ¡ç­”æ¡ˆ
2. å­˜åœ¨å”¯ä¸€çš„ä»è¶…é›†åˆ°ç­”æ¡ˆçš„æ˜ å°„å…³ç³»
3. è¶…é›†æ¯”åŸç­”æ¡ˆåŒ…å«æ›´å¤šä¿¡æ¯
4. å¯ä»¥åŸºäºè¶…é›†ç”Ÿæˆæœ‰æ„ä¹‰çš„æ‰©å±•é—®é¢˜

è¯·è¿”å›éªŒè¯ç»“æœ (true/false) å’Œç†ç”±ã€‚
"""
        
        try:
            response = await self.llm_client.generate_reasoning(
                task_description=validation_prompt,
                available_tools=[],
                execution_context={"mode": "superset_validation"}
            )
            
            thinking = response.get('thinking', '').lower()
            return 'true' in thinking and 'valid' in thinking
            
        except Exception as e:
            logger.error(f"âŒ è¶…é›†å…³ç³»éªŒè¯å¤±è´¥: {e}")
            return False


class IntermediateTaskGenerator:
    """ä¸­é—´ä»»åŠ¡ç”Ÿæˆå™¨ - åŸºäºè¶…é›†ç”Ÿæˆä¸­é—´æ‰©å±•ä»»åŠ¡"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
        self.config = EnhancedSynthesisConfig()
    
    async def generate_intermediate_task(self, superset_info: SupersetInfo, 
                                       source_task: AtomicTask) -> Optional[Dict[str, Any]]:
        """ç”Ÿæˆä¸­é—´ä»»åŠ¡"""
        logger.debug(f"ğŸ”„ ç”Ÿæˆä¸­é—´ä»»åŠ¡: {superset_info.identifier}")
        
        try:
            intermediate_prompt = f"""
åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸€ä¸ªä¸­é—´æ‰©å±•ä»»åŠ¡ï¼š

åŸå­ä»»åŠ¡:
- é—®é¢˜: {source_task.question}
- ç­”æ¡ˆ: {source_task.golden_answer}

è¶…é›†ä¿¡æ¯:
- æ ‡è¯†ç¬¦: {superset_info.identifier}
- å…³ç³»: {superset_info.relation}
- ç½®ä¿¡åº¦: {superset_info.confidence}

è¦æ±‚ç”Ÿæˆä¸€ä¸ªä¸­é—´ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡ï¼š
1. æ¯”åŸå­ä»»åŠ¡æ›´å¤æ‚ï¼ˆéœ€è¦æ›´å¤šæ­¥éª¤ï¼‰
2. ç­”æ¡ˆåŒ…å«æˆ–æŒ‡å‘åŸå­ä»»åŠ¡çš„ç­”æ¡ˆ
3. éœ€è¦ä½¿ç”¨æœç´¢ç­‰å·¥å…·è·å–è¶…é›†ä¿¡æ¯
4. å…·æœ‰æ˜ç¡®çš„æ‰§è¡Œè·¯å¾„

è¿”å›JSONæ ¼å¼ï¼š
{{
    "intermediate_question": "æ‰©å±•åçš„é—®é¢˜",
    "intermediate_answer": "æ‰©å±•åçš„ç­”æ¡ˆ",
    "execution_steps": [
        "æ­¥éª¤1: å…·ä½“çš„æ‰§è¡Œæ­¥éª¤",
        "æ­¥éª¤2: ...",
        "æ­¥éª¤3: ..."
    ],
    "required_tools": ["tool1", "tool2"],
    "complexity_increase": "å¤æ‚åº¦æå‡è¯´æ˜"
}}
"""
            
            response = await self.llm_client.generate_reasoning(
                task_description=intermediate_prompt,
                available_tools=[],
                execution_context={"mode": "intermediate_task_generation"}
            )
            
            return self._parse_intermediate_task_response(response)
            
        except Exception as e:
            logger.error(f"âŒ ä¸­é—´ä»»åŠ¡ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _parse_intermediate_task_response(self, response: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """è§£æä¸­é—´ä»»åŠ¡ç”Ÿæˆå“åº”"""
        try:
            thinking = response.get('thinking', '{}')
            
            if thinking.strip().startswith('{'):
                task_data = json.loads(thinking)
            else:
                json_match = re.search(r'\{.*\}', thinking, re.DOTALL)
                if json_match:
                    task_data = json.loads(json_match.group())
                else:
                    return None
            
            # éªŒè¯å¿…è¦å­—æ®µ
            required_fields = ['intermediate_question', 'intermediate_answer', 'execution_steps', 'required_tools']
            if all(field in task_data for field in required_fields):
                return task_data
            else:
                return None
                
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"âŒ è§£æä¸­é—´ä»»åŠ¡å“åº”å¤±è´¥: {e}")
            return None


class TaskMerger:
    """ä»»åŠ¡åˆå¹¶å™¨ - å°†åŸå­ä»»åŠ¡ä¸ä¸­é—´ä»»åŠ¡åˆå¹¶"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
        self.config = EnhancedSynthesisConfig()
    
    async def merge_tasks(self, source_task: AtomicTask, 
                         intermediate_tasks: List[Dict[str, Any]], 
                         superset_chain: List[SupersetInfo]) -> Optional[ExtendedTask]:
        """åˆå¹¶ä»»åŠ¡ç”Ÿæˆæ‰©å±•ä»»åŠ¡"""
        logger.debug(f"ğŸ”— åˆå¹¶ä»»åŠ¡: {source_task.task_id}")
        
        if not intermediate_tasks:
            return None
        
        try:
            # æ„å»ºæœ€ç»ˆçš„æ‰©å±•é—®é¢˜
            final_question = await self._build_final_question(source_task, intermediate_tasks, superset_chain)
            
            # æ„å»ºæœ€ç»ˆç­”æ¡ˆ
            final_answer = await self._build_final_answer(source_task, intermediate_tasks)
            
            # ç¡®å®šå¤æ‚åº¦å’Œå·¥å…·éœ€æ±‚
            complexity_score = self._calculate_complexity_score(intermediate_tasks)
            expected_tools = self._extract_required_tools(intermediate_tasks)
            difficulty_level = self._determine_difficulty_level(len(superset_chain), len(expected_tools))
            
            # åˆ›å»ºæ‰©å±•ä»»åŠ¡
            extended_task = ExtendedTask(
                task_id=generate_task_id(TaskType.DEPTH_EXTENDED, source_task.content_identifier),
                question=final_question,
                golden_answer=final_answer,
                hop_level=len(superset_chain),
                source_atomic_task=source_task.task_id,
                intermediate_steps=superset_chain,
                expected_tools=expected_tools,
                difficulty_level=difficulty_level,
                complexity_score=complexity_score
            )
            
            logger.info(f"âœ… ä»»åŠ¡åˆå¹¶å®Œæˆ: {extended_task.task_id} (è·³è·ƒçº§åˆ«: {len(superset_chain)})")
            return extended_task
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡åˆå¹¶å¤±è´¥: {e}")
            return None
    
    async def _build_final_question(self, source_task: AtomicTask, 
                                   intermediate_tasks: List[Dict[str, Any]], 
                                   superset_chain: List[SupersetInfo]) -> str:
        """æ„å»ºæœ€ç»ˆæ‰©å±•é—®é¢˜"""
        
        build_prompt = f"""
åŸºäºä»¥ä¸‹ä¿¡æ¯æ„å»ºä¸€ä¸ªå®Œæ•´çš„æ‰©å±•é—®é¢˜ï¼š

åŸå­ä»»åŠ¡é—®é¢˜: {source_task.question}

ä¸­é—´ä»»åŠ¡åºåˆ—:
{chr(10).join(f"- {task['intermediate_question']}" for task in intermediate_tasks)}

è¶…é›†é“¾æ¡:
{chr(10).join(f"- {step.identifier} ({step.relation})" for step in superset_chain)}

è¦æ±‚æ„å»ºä¸€ä¸ªé—®é¢˜ï¼Œè¯¥é—®é¢˜ï¼š
1. åŒ…å«å®Œæ•´çš„æ¨ç†é“¾æ¡
2. æ¯”åŸå­ä»»åŠ¡æ›´å¤æ‚ä½†ä»å¯æ‰§è¡Œ
3. ç­”æ¡ˆæœ€ç»ˆæŒ‡å‘åŸå­ä»»åŠ¡çš„ç­”æ¡ˆ
4. è¡¨è¿°æ¸…æ™°ã€é€»è¾‘è¿è´¯

è¿”å›æ‰©å±•åçš„é—®é¢˜æ–‡æœ¬ã€‚
"""
        
        try:
            response = await self.llm_client.generate_reasoning(
                task_description=build_prompt,
                available_tools=[],
                execution_context={"mode": "final_question_building"}
            )
            
            return response.get('thinking', '').strip() or source_task.question
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºæœ€ç»ˆé—®é¢˜å¤±è´¥: {e}")
            return source_task.question
    
    async def _build_final_answer(self, source_task: AtomicTask, 
                                 intermediate_tasks: List[Dict[str, Any]]) -> str:
        """æ„å»ºæœ€ç»ˆç­”æ¡ˆ"""
        
        # ç®€å•ç­–ç•¥ï¼šä½¿ç”¨æœ€åä¸€ä¸ªä¸­é—´ä»»åŠ¡çš„ç­”æ¡ˆï¼Œå¦‚æœå­˜åœ¨çš„è¯
        if intermediate_tasks:
            return intermediate_tasks[-1].get('intermediate_answer', source_task.golden_answer)
        else:
            return source_task.golden_answer
    
    def _calculate_complexity_score(self, intermediate_tasks: List[Dict[str, Any]]) -> float:
        """è®¡ç®—å¤æ‚åº¦åˆ†æ•°"""
        base_score = 1.0  # åŸå­ä»»åŠ¡åŸºç¡€åˆ†æ•°
        
        # æ¯ä¸ªä¸­é—´ä»»åŠ¡å¢åŠ å¤æ‚åº¦
        for task in intermediate_tasks:
            steps_count = len(task.get('execution_steps', []))
            tools_count = len(task.get('required_tools', []))
            base_score += 0.3 + (steps_count * 0.1) + (tools_count * 0.2)
        
        return min(base_score / 5.0, 1.0)  # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
    
    def _extract_required_tools(self, intermediate_tasks: List[Dict[str, Any]]) -> List[str]:
        """æå–æ‰€éœ€å·¥å…·"""
        all_tools = set()
        
        for task in intermediate_tasks:
            tools = task.get('required_tools', [])
            all_tools.update(tools)
        
        return list(all_tools)
    
    def _determine_difficulty_level(self, hop_count: int, tool_count: int) -> TaskDifficulty:
        """ç¡®å®šéš¾åº¦çº§åˆ«"""
        if hop_count == 1 and tool_count <= 2:
            return TaskDifficulty.MEDIUM
        elif hop_count <= 2 and tool_count <= 3:
            return TaskDifficulty.MEDIUM
        else:
            return TaskDifficulty.COMPLEX


class DepthExtender:
    """æ·±åº¦æ‰©å±•å™¨ - ç»Ÿä¸€çš„æ·±åº¦æ‰©å±•æ¥å£"""
    
    def __init__(self, llm_client: LLMClient, mcp_client: Optional[MCPToolClient] = None):
        self.llm_client = llm_client
        self.mcp_client = mcp_client
        self.superset_searcher = SupersetSearcher(llm_client, mcp_client)
        self.intermediate_generator = IntermediateTaskGenerator(llm_client)
        self.task_merger = TaskMerger(llm_client)
        self.config = EnhancedSynthesisConfig()
    
    async def extend_atomic_task(self, atomic_task: AtomicTask) -> List[ExtendedTask]:
        """æ‰©å±•å•ä¸ªåŸå­ä»»åŠ¡"""
        logger.info(f"ğŸš€ å¼€å§‹æ·±åº¦æ‰©å±•åŸå­ä»»åŠ¡: {atomic_task.task_id}")
        
        try:
            extended_tasks = []
            current_task = atomic_task
            superset_chain = []
            
            max_hops = self.config.DEPTH_EXTENSION_CONFIG['max_hops']
            
            for hop in range(1, max_hops + 1):
                logger.debug(f"ğŸ”„ æ‰§è¡Œç¬¬ {hop} è·³æ‰©å±•")
                
                # 1. æœç´¢è¶…é›†
                supersets = await self.superset_searcher.backward_search_superset(current_task)
                
                if not supersets:
                    logger.info(f"âš ï¸ ç¬¬ {hop} è·³æœªæ‰¾åˆ°æœ‰æ•ˆè¶…é›†ï¼Œåœæ­¢æ‰©å±•")
                    break
                
                # é€‰æ‹©æœ€ä½³è¶…é›†
                best_superset = supersets[0]
                superset_chain.append(best_superset)
                
                # 2. ç”Ÿæˆä¸­é—´ä»»åŠ¡
                intermediate_task = await self.intermediate_generator.generate_intermediate_task(
                    best_superset, atomic_task
                )
                
                if not intermediate_task:
                    logger.warning(f"âš ï¸ ç¬¬ {hop} è·³ä¸­é—´ä»»åŠ¡ç”Ÿæˆå¤±è´¥")
                    break
                
                # 3. éªŒè¯ä¸­é—´ä»»åŠ¡è´¨é‡
                if not await self._validate_intermediate_task_quality(intermediate_task, atomic_task):
                    logger.warning(f"âš ï¸ ç¬¬ {hop} è·³ä¸­é—´ä»»åŠ¡è´¨é‡ä¸ç¬¦åˆè¦æ±‚")
                    break
                
                # 4. åˆå¹¶ç”Ÿæˆæ‰©å±•ä»»åŠ¡
                extended_task = await self.task_merger.merge_tasks(
                    atomic_task, [intermediate_task], superset_chain.copy()
                )
                
                if extended_task:
                    extended_tasks.append(extended_task)
                    logger.info(f"âœ… ç¬¬ {hop} è·³æ‰©å±•ä»»åŠ¡ç”ŸæˆæˆåŠŸ: {extended_task.task_id}")
                else:
                    logger.warning(f"âš ï¸ ç¬¬ {hop} è·³ä»»åŠ¡åˆå¹¶å¤±è´¥")
                    break
            
            logger.info(f"âœ… åŸå­ä»»åŠ¡ {atomic_task.task_id} æ·±åº¦æ‰©å±•å®Œæˆï¼Œç”Ÿæˆ {len(extended_tasks)} ä¸ªæ‰©å±•ä»»åŠ¡")
            return extended_tasks
            
        except Exception as e:
            logger.error(f"âŒ æ·±åº¦æ‰©å±•å¤±è´¥ {atomic_task.task_id}: {e}")
            return []
    
    async def batch_extend_atomic_tasks(self, atomic_tasks: List[AtomicTask]) -> List[ExtendedTask]:
        """æ‰¹é‡æ‰©å±•åŸå­ä»»åŠ¡"""
        logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡æ·±åº¦æ‰©å±• {len(atomic_tasks)} ä¸ªåŸå­ä»»åŠ¡")
        
        all_extended_tasks = []
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(self.config.ATOMIC_GENERATION_CONFIG['parallel_workers'])
        
        async def extend_single_task(task):
            async with semaphore:
                return await self.extend_atomic_task(task)
        
        results = await asyncio.gather(
            *[extend_single_task(task) for task in atomic_tasks],
            return_exceptions=True
        )
        
        for i, result in enumerate(results):
            if isinstance(result, list):
                all_extended_tasks.extend(result)
            elif isinstance(result, Exception):
                logger.error(f"âŒ ä»»åŠ¡æ‰©å±•å¼‚å¸¸ {atomic_tasks[i].task_id}: {result}")
        
        logger.info(f"âœ… æ‰¹é‡æ·±åº¦æ‰©å±•å®Œæˆï¼Œæ€»è®¡ç”Ÿæˆ {len(all_extended_tasks)} ä¸ªæ‰©å±•ä»»åŠ¡")
        return all_extended_tasks
    
    async def _validate_intermediate_task_quality(self, intermediate_task: Dict[str, Any], 
                                                source_task: AtomicTask) -> bool:
        """éªŒè¯ä¸­é—´ä»»åŠ¡è´¨é‡"""
        try:
            # åŸºæœ¬è´¨é‡æ£€æŸ¥
            question = intermediate_task.get('intermediate_question', '')
            answer = intermediate_task.get('intermediate_answer', '')
            steps = intermediate_task.get('execution_steps', [])
            tools = intermediate_task.get('required_tools', [])
            
            # æ£€æŸ¥åŸºæœ¬å®Œæ•´æ€§
            if not question or not answer or len(steps) < 2:
                return False
            
            # æ£€æŸ¥å¤æ‚åº¦æå‡
            if len(question.split()) <= len(source_task.question.split()) + 5:
                return False
            
            # æ£€æŸ¥å·¥å…·éœ€æ±‚
            if not tools:
                return False
            
            # æ£€æŸ¥ç­”æ¡ˆå…³è”æ€§
            if source_task.golden_answer.lower() not in answer.lower():
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¸­é—´ä»»åŠ¡è´¨é‡éªŒè¯å¤±è´¥: {e}")
            return False
    
    async def get_extension_statistics(self, extended_tasks: List[ExtendedTask]) -> Dict[str, Any]:
        """è·å–æ‰©å±•ç»Ÿè®¡ä¿¡æ¯"""
        if not extended_tasks:
            return {"total_extended_tasks": 0}
        
        # æŒ‰è·³è·ƒçº§åˆ«ç»Ÿè®¡
        hop_distribution = {}
        complexity_scores = []
        source_tasks = set()
        
        for task in extended_tasks:
            hop_level = task.hop_level
            hop_distribution[hop_level] = hop_distribution.get(hop_level, 0) + 1
            complexity_scores.append(task.complexity_score)
            source_tasks.add(task.source_atomic_task)
        
        return {
            "total_extended_tasks": len(extended_tasks),
            "hop_distribution": hop_distribution,
            "average_complexity_score": sum(complexity_scores) / len(complexity_scores),
            "max_hop_level": max(task.hop_level for task in extended_tasks),
            "unique_source_tasks": len(source_tasks),
            "extension_efficiency": len(extended_tasks) / len(source_tasks) if source_tasks else 0.0
        }