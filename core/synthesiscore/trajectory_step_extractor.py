#!/usr/bin/env python3
"""
è½¨è¿¹æ­¥éª¤æå–å™¨å’Œå˜ä½“ç”Ÿæˆå™¨
å®ç°åŸºäºçœŸå®è½¨è¿¹çš„åŸå­æ­¥æ‹†åˆ†å’Œå˜ä½“æ‰©æ•£
"""

import re
import logging
import json
from typing import List, Dict, Any, Optional, Set, Tuple
from dataclasses import dataclass
from datetime import datetime

logger = logging.getLogger(__name__)


@dataclass
class AtomicStep:
    """åŸå­æ­¥éª¤æ•°æ®ç»“æ„"""
    step_id: str
    step_type: str  # tool_call, reasoning, summary
    tool_name: Optional[str]
    operation: str  # å…·ä½“æ“ä½œæè¿°
    content: str    # æ“ä½œå†…å®¹
    domain: str     # é¢†åŸŸ
    complexity: str # å¤æ‚åº¦
    requires_tools: List[str]
    original_trajectory_id: str
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "step_id": self.step_id,
            "step_type": self.step_type,
            "tool_name": self.tool_name,
            "operation": self.operation,
            "content": self.content,
            "domain": self.domain,
            "complexity": self.complexity,
            "requires_tools": self.requires_tools,
            "original_trajectory_id": self.original_trajectory_id
        }


@dataclass
class StepVariant:
    """æ­¥éª¤å˜ä½“æ•°æ®ç»“æ„"""
    variant_id: str
    base_step: AtomicStep
    variant_operation: str
    variant_content: str
    variant_domain: str
    substitution_mapping: Dict[str, str]  # å®ä½“æ›¿æ¢æ˜ å°„
    creativity_level: int
    generated_at: datetime
    
    def to_task_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºä»»åŠ¡æ ¼å¼"""
        return {
            "question": self.variant_operation,
            "expected_answer": f"åŸºäº{self.variant_domain}é¢†åŸŸçš„{self.variant_content}",
            "task_type": "tool_required",
            "domain": self.variant_domain,
            "difficulty": self.base_step.complexity,
            "required_tools": self.base_step.requires_tools,
            "reasoning_steps": [f"æ‰§è¡Œ{self.variant_operation}", f"è·å–{self.variant_content}"],
            "relation_pattern": f"step_variant_from_{self.base_step.step_type}",
            "entity_generalization": str(self.substitution_mapping),
            "creativity_level": str(self.creativity_level),
            "creativity_explanation": f"åŸºäºçœŸå®è½¨è¿¹æ­¥éª¤çš„å˜ä½“ç”Ÿæˆï¼ŒåŸå§‹æ­¥éª¤ï¼š{self.base_step.operation}",
            "reverse_reasoning": f"ä»è½¨è¿¹æ­¥éª¤'{self.base_step.operation}'è¡ç”Ÿå‡ºå˜ä½“'{self.variant_operation}'"
        }


class TrajectoryStepExtractor:
    """è½¨è¿¹æ­¥éª¤æå–å™¨"""
    
    def __init__(self):
        self.tool_patterns = [
            r'<(browser_search_google)>([^<]+)</\1>',
            r'<(browser_extract_content)>([^<]+)</\1>',
            r'<(browser_use_execute_task)>([^<]+)</\1>',
            r'<(browser_navigate)>([^<]+)</\1>',
            r'<(browser_click_element)>([^<]+)</\1>',
            r'<(browser_input_text)>([^<]+)</\1>',
            r'<(microsandbox_execute)>([^<]+)</\1>',
            r'<(microsandbox_install_package)>([^<]+)</\1>',
            r'<(deepsearch)><research>([^<]+)</research></\1>',
            r'<(deepsearch)><quick_research>([^<]+)</quick_research></\1>',
            r'<(deepsearch)><comprehensive_research>([^<]+)</comprehensive_research></\1>',
            r'<(search_file_content)>([^<]+)</\1>',
            r'<(list_code_definitions)>([^<]+)</\1>',
            r'<(search_and_install_tools)>([^<]+)</\1>',
            r'<(memory_staging)>([^<]+)</\1>'
        ]
        
        self.operation_templates = {
            "browser_search_google": "æœç´¢{content}",
            "browser_extract_content": "æå–{content}",
            "browser_use_execute_task": "æ‰§è¡Œæµè§ˆå™¨ä»»åŠ¡{content}",
            "browser_navigate": "å¯¼èˆªåˆ°{content}",
            "browser_click_element": "ç‚¹å‡»{content}",
            "browser_input_text": "è¾“å…¥{content}",
            "microsandbox_execute": "æ‰§è¡Œ{content}",
            "microsandbox_install_package": "å®‰è£…åŒ…{content}",
            "deepsearch": "æ·±åº¦ç ”ç©¶{content}",
            "search_file_content": "æœç´¢æ–‡ä»¶{content}",
            "list_code_definitions": "åˆ—å‡ºä»£ç å®šä¹‰{content}",
            "search_and_install_tools": "æœç´¢å·¥å…·{content}",
            "memory_staging": "å†…å­˜æ“ä½œ{content}"
        }
    
    def extract_atomic_steps(self, trajectory: Dict) -> List[AtomicStep]:
        """ä»è½¨è¿¹ä¸­æå–åŸå­æ­¥éª¤"""
        raw_response = trajectory.get("raw_response", "")
        task_id = trajectory.get("task_id", "unknown")
        domain = self._infer_domain(trajectory.get("task_description", ""))
        
        steps = []
        step_counter = 0
        
        # 1. æå–å·¥å…·è°ƒç”¨æ­¥éª¤
        for pattern in self.tool_patterns:
            matches = re.findall(pattern, raw_response, re.DOTALL)
            for tool_name, content in matches:
                step_counter += 1
                
                # æ¸…ç†å†…å®¹
                clean_content = self._clean_content(content)
                if not clean_content:
                    continue
                
                # ç”Ÿæˆæ“ä½œæè¿°
                operation = self._generate_operation_description(tool_name, clean_content)
                
                step = AtomicStep(
                    step_id=f"{task_id}_step_{step_counter}",
                    step_type="tool_call",
                    tool_name=tool_name,
                    operation=operation,
                    content=clean_content,
                    domain=domain,
                    complexity=self._assess_complexity(clean_content),
                    requires_tools=[tool_name],
                    original_trajectory_id=task_id
                )
                steps.append(step)
        
        # 2. æå–æ¨ç†æ­¥éª¤
        reasoning_steps = self._extract_reasoning_steps(raw_response, task_id, domain)
        steps.extend(reasoning_steps)
        
        # 3. æå–æ€»ç»“æ­¥éª¤
        summary_steps = self._extract_summary_steps(raw_response, task_id, domain)
        steps.extend(summary_steps)
        
        logger.info(f"âœ… ä»è½¨è¿¹ {task_id} æå–åˆ° {len(steps)} ä¸ªåŸå­æ­¥éª¤")
        return steps
    
    def _clean_content(self, content: str) -> str:
        """æ¸…ç†å’Œæ ‡å‡†åŒ–å†…å®¹"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å’Œç‰¹æ®Šå­—ç¬¦
        clean = re.sub(r'\s+', ' ', content.strip())
        
        # ç§»é™¤è¿‡é•¿çš„å†…å®¹
        if len(clean) > 200:
            clean = clean[:200] + "..."
        
        # ç§»é™¤ç©ºå†…å®¹
        if len(clean) < 5:
            return ""
        
        return clean
    
    def _generate_operation_description(self, tool_name: str, content: str) -> str:
        """ç”Ÿæˆæ“ä½œæè¿°"""
        template = self.operation_templates.get(tool_name, f"ä½¿ç”¨{tool_name}å¤„ç†{{content}}")
        
        # æå–å…³é”®è¯ç”¨äºæ›¿æ¢
        key_content = self._extract_key_content(content)
        
        return template.format(content=key_content)
    
    def _extract_key_content(self, content: str) -> str:
        """ä»å†…å®¹ä¸­æå–å…³é”®è¯"""
        # ç§»é™¤ä»£ç å’ŒæŠ€æœ¯ç»†èŠ‚ï¼Œæå–æ ¸å¿ƒæ¦‚å¿µ
        if "import" in content or "def " in content:
            return "ä»£ç åˆ†æ"
        elif "http" in content:
            return "ç½‘ç»œèµ„æº"
        elif any(keyword in content for keyword in ["è‚¡ç¥¨", "ä»·æ ¼", "é‡‘è"]):
            return "é‡‘èæ•°æ®"
        elif any(keyword in content for keyword in ["é‡å­", "æœºå™¨å­¦ä¹ ", "AI"]):
            return "AIæŠ€æœ¯"
        else:
            # æå–å‰å‡ ä¸ªæœ‰æ„ä¹‰çš„è¯
            words = content.split()[:3]
            return " ".join(words)
    
    def _extract_reasoning_steps(self, raw_response: str, task_id: str, domain: str) -> List[AtomicStep]:
        """æå–æ¨ç†æ­¥éª¤"""
        steps = []
        
        # æå–thinkå—
        think_blocks = re.findall(r'<think>(.*?)</think>', raw_response, re.DOTALL)
        
        for i, think in enumerate(think_blocks):
            clean_think = self._clean_content(think)
            if clean_think:
                step = AtomicStep(
                    step_id=f"{task_id}_reasoning_{i+1}",
                    step_type="reasoning",
                    tool_name=None,
                    operation=f"åˆ†ææ¨ç†{clean_think[:30]}",
                    content=clean_think,
                    domain=domain,
                    complexity="ä¸­ç­‰",
                    requires_tools=[],
                    original_trajectory_id=task_id
                )
                steps.append(step)
        
        return steps
    
    def _extract_summary_steps(self, raw_response: str, task_id: str, domain: str) -> List[AtomicStep]:
        """æå–æ€»ç»“æ­¥éª¤"""
        steps = []
        
        # æå–answerå—
        answer_blocks = re.findall(r'<answer>(.*?)</answer>', raw_response, re.DOTALL)
        
        for i, answer in enumerate(answer_blocks):
            clean_answer = self._clean_content(answer)
            if clean_answer:
                step = AtomicStep(
                    step_id=f"{task_id}_summary_{i+1}",
                    step_type="summary",
                    tool_name=None,
                    operation=f"æ€»ç»“{domain}ç»“æœ",
                    content=clean_answer,
                    domain=domain,
                    complexity="ç®€å•",
                    requires_tools=[],
                    original_trajectory_id=task_id
                )
                steps.append(step)
        
        return steps
    
    def _infer_domain(self, content: str) -> str:
        """æ¨æ–­é¢†åŸŸ"""
        domain_keywords = {
            "è‚¡ç¥¨|è‚¡ä»·|é‡‘è|æŠ•èµ„": "é‡‘è",
            "é‡å­|ç‰©ç†|ç§‘å­¦": "ç§‘å­¦ç ”ç©¶",
            "ä»£ç |ç¼–ç¨‹|Python|ç®—æ³•": "ç¼–ç¨‹",
            "æœç´¢|ç ”ç©¶|è®ºæ–‡": "ç ”ç©¶åˆ†æ",
            "å¤§å­¦|å­¦æ ¡|æ•™è‚²": "æ•™è‚²",
            "è›‹ç™½è´¨|ç”Ÿç‰©|åŒ»å­¦": "ç”Ÿç‰©åŒ»å­¦"
        }
        
        for pattern, domain in domain_keywords.items():
            if re.search(pattern, content):
                return domain
        
        return "é€šç”¨"
    
    def _assess_complexity(self, content: str) -> str:
        """è¯„ä¼°å¤æ‚åº¦"""
        if len(content) > 100 or any(keyword in content for keyword in ["è®¡ç®—", "åˆ†æ", "å¯¹æ¯”"]):
            return "å›°éš¾"
        elif len(content) > 50:
            return "ä¸­ç­‰"
        else:
            return "ç®€å•"


class LLMDrivenVariantGenerator:
    """LLMé©±åŠ¨çš„å˜ä½“ç”Ÿæˆå™¨ - ä½¿ç”¨è¯­ä¹‰è”æƒ³ç”Ÿæˆæœ‰æ„ä¹‰çš„å˜ä½“"""
    
    def __init__(self, llm_client):
        self.llm_client = llm_client
        
        # è¯­ä¹‰å˜ä½“ç”Ÿæˆçš„Promptæ¨¡æ¿
        self.semantic_variant_prompt = """ä½ æ˜¯ä¸€ä¸ªåˆ›æ„ä»»åŠ¡è®¾è®¡ä¸“å®¶ã€‚åŸºäºç»™å®šçš„åŸå­æ­¥éª¤ï¼Œç”Ÿæˆæœ‰æ„ä¹‰çš„å˜ä½“ä»»åŠ¡ã€‚

åŸå­æ­¥éª¤ä¿¡æ¯ï¼š
- æ“ä½œï¼š{operation}
- å†…å®¹ï¼š{content}  
- é¢†åŸŸï¼š{domain}
- å·¥å…·ï¼š{tool_name}

ç³»ç»Ÿå¯ç”¨å·¥å…·å…¨æ™¯ï¼š
**ğŸ” ç ”ç©¶å·¥å…·**
- deepsearch: æ·±åº¦ç ”ç©¶ã€å¿«é€Ÿç ”ç©¶ã€å…¨é¢ç ”ç©¶
- search_file_content: æœç´¢æ–‡ä»¶å†…å®¹
- list_code_definitions: åˆ—å‡ºä»£ç å®šä¹‰

**ğŸŒ æµè§ˆå™¨å·¥å…·**  
- browser_search_google: Googleæœç´¢
- browser_extract_content: æå–é¡µé¢å†…å®¹
- browser_use_execute_task: å¤æ‚AIæµè§ˆå™¨ä»»åŠ¡
- browser_navigate: å¯¼èˆªåˆ°URL
- browser_click_element: ç‚¹å‡»é¡µé¢å…ƒç´ 
- browser_input_text: è¾“å…¥æ–‡æœ¬

**ğŸ’» ä»£ç æ‰§è¡Œå·¥å…·**
- microsandbox_execute: æ‰§è¡ŒPythonä»£ç 
- microsandbox_install_package: å®‰è£…PythonåŒ…

**ğŸ”§ å·¥å…·ç®¡ç†**
- search_and_install_tools: æœç´¢å¹¶å®‰è£…æ–°å·¥å…·

å˜ä½“ç”Ÿæˆç­–ç•¥ï¼š
1. **æ¦‚å¿µæ‰©å±•**ï¼šå°†æ ¸å¿ƒæ¦‚å¿µæ‰©å±•åˆ°ç›¸å…³é¢†åŸŸï¼Œæ™ºèƒ½åˆ†é…åˆé€‚å·¥å…·
2. **å¤æ‚åº¦é€’å¢**ï¼šåœ¨åŸæœ‰åŸºç¡€ä¸Šå¢åŠ åˆ†æã€æ¯”è¾ƒã€è¯„ä¼°ç­‰è¦æ±‚
3. **åº”ç”¨åœºæ™¯å˜åŒ–**ï¼šæ”¹å˜åº”ç”¨ä¸Šä¸‹æ–‡ä½†ä¿æŒæ ¸å¿ƒæŠ€èƒ½
4. **è·¨åŸŸè”æƒ³**ï¼šåŸºäºç›¸åŒæŠ€èƒ½è¦æ±‚è”æƒ³åˆ°å…¶ä»–é¢†åŸŸ
5. **å·¥å…·å¤šæ ·åŒ–**ï¼šæ ¹æ®ä»»åŠ¡ç‰¹æ€§é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ç»„åˆ

ç”Ÿæˆ{max_variants}ä¸ªå˜ä½“ï¼Œæ¯ä¸ªå˜ä½“åº”è¯¥ï¼š
- ä¿æŒä¸åŸå§‹ä»»åŠ¡çš„è¯­ä¹‰ç›¸å…³æ€§
- å…·æœ‰ç‹¬ç«‹çš„ä»»åŠ¡ä»·å€¼  
- é€‚åº¦æå‡å¤æ‚åº¦
- å…·å¤‡æ¸…æ™°çš„æ‰§è¡Œè·¯å¾„
- æ™ºèƒ½é€‰æ‹©åˆé€‚çš„å·¥å…·ç»„åˆ

å·¥å…·é€‰æ‹©æŒ‡å—ä¸å¤šæ ·åŒ–è¦æ±‚ï¼š
- **ä¿¡æ¯ç ”ç©¶ç±»**: ä¼˜å…ˆä½¿ç”¨ deepsearch ç³»åˆ—å·¥å…·
- **ç½‘é¡µäº¤äº’ç±»**: ä¼˜å…ˆä½¿ç”¨ browser_use_execute_task, browser_navigate
- **å†…å®¹æå–ç±»**: ä½¿ç”¨ browser_extract_content
- **æœç´¢ç±»**: é¿å…è¿‡åº¦ä½¿ç”¨ browser_search_googleï¼Œä¼˜å…ˆé€‰æ‹© deepsearch
- **ä»£ç åˆ†æç±»**: ä½¿ç”¨ microsandbox_execute, search_file_content
- **æ•°æ®å¤„ç†ç±»**: ä½¿ç”¨ microsandbox_execute
- **æ–‡æ¡£æŸ¥æ‰¾ç±»**: ä½¿ç”¨ list_code_definitions, search_file_content

âš ï¸ å·¥å…·å¤šæ ·åŒ–è¦æ±‚ï¼š
- å¿…é¡»é¿å…æ‰€æœ‰å˜ä½“éƒ½ä½¿ç”¨ç›¸åŒå·¥å…· (å¦‚éƒ½ç”¨browser_search_google)
- æ¯ä¸ªå˜ä½“åº”ä½¿ç”¨ä¸åŒçš„å·¥å…·ç»„åˆ
- ä¼˜å…ˆé€‰æ‹©æ›´æ™ºèƒ½ã€æ›´å¤æ‚çš„å·¥å…· (å¦‚browser_use_execute_taskè€Œébrowser_search_google)
- å±•ç¤ºç³»ç»Ÿå®Œæ•´çš„å·¥å…·ç”Ÿæ€ï¼Œè€Œä¸æ˜¯å±€é™äºç®€å•å·¥å…·

ç¤ºä¾‹å˜ä½“æ€è·¯ï¼š
- å¦‚æœåŸå§‹æ˜¯"æœç´¢å¤§å­¦ä¿¡æ¯"ï¼Œå˜ä½“å¯ä»¥æ˜¯ï¼š
  * "ä½¿ç”¨æ·±åº¦ç ”ç©¶å·¥å…·åˆ†æå…¨çƒé¡¶å°–å¤§å­¦çš„æ’åè¶‹åŠ¿" (deepsearch)
  * "é€šè¿‡æµè§ˆå™¨è‡ªåŠ¨åŒ–æ”¶é›†å¤šæ‰€å¤§å­¦çš„å®˜ç½‘æ•°æ®" (browser_use_execute_task)
- å¦‚æœåŸå§‹æ˜¯"æ‰§è¡ŒPythonä»£ç "ï¼Œå˜ä½“å¯ä»¥æ˜¯ï¼š
  * "åˆ†æPythoné¡¹ç›®ä¸­çš„å‡½æ•°å®šä¹‰å’Œä¾èµ–å…³ç³»" (list_code_definitions)
  * "è‡ªåŠ¨åŒ–å®‰è£…å’Œæµ‹è¯•PythonåŒ…çš„å…¼å®¹æ€§" (microsandbox_install_package)

è¿”å›JSONæ ¼å¼ï¼š
{{
    "variants": [
        {{
            "question": "å…·ä½“çš„å˜ä½“ä»»åŠ¡æè¿°",
            "expected_answer": "é¢„æœŸç­”æ¡ˆç±»å‹æè¿°", 
            "reasoning_steps": ["æ­¥éª¤1", "æ­¥éª¤2"],
            "semantic_relation": "ä¸åŸå§‹ä»»åŠ¡çš„è¯­ä¹‰å…³ç³»è¯´æ˜",
            "complexity_level": "ç®€å•|ä¸­ç­‰|å›°éš¾",
            "required_tools": ["åŸºäºä»»åŠ¡ç‰¹æ€§æ™ºèƒ½é€‰æ‹©çš„å·¥å…·"],
            "creativity_explanation": "å˜ä½“ç”Ÿæˆçš„åˆ›æ„æ€è·¯å’Œå·¥å…·é€‰æ‹©ç†ç”±",
            "domain": "ä»»åŠ¡é¢†åŸŸ"
        }}
    ]
}}"""
    
    async def generate_step_variants(self, step: AtomicStep, max_variants: int = 3) -> List[Dict[str, Any]]:
        """ä¸ºåŸå­æ­¥éª¤ç”ŸæˆLLMé©±åŠ¨çš„è¯­ä¹‰å˜ä½“"""
        try:
            prompt = self.semantic_variant_prompt.format(
                operation=step.operation,
                content=step.content,
                domain=step.domain,
                tool_name=step.tool_name or "é€šç”¨å·¥å…·",
                max_variants=max_variants
            )
            
            # è°ƒç”¨LLMç”Ÿæˆå˜ä½“ - ä½¿ç”¨ç³»ç»Ÿæ ‡å‡†çš„_call_apiæ–¹æ³•
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_client._call_api(messages, timeout=60)
            
            # è§£æLLMå“åº”
            variants = self._parse_variant_response(response, step, max_variants)
            
            logger.info(f"âœ¨ ä¸ºæ­¥éª¤ {step.step_id} ç”Ÿæˆäº† {len(variants)} ä¸ªLLMé©±åŠ¨çš„å˜ä½“")
            return variants
            
        except Exception as e:
            logger.error(f"âŒ LLMå˜ä½“ç”Ÿæˆå¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•å˜ä½“
            return self._generate_fallback_variants(step, max_variants)
    
    def _parse_variant_response(self, response: str, step: AtomicStep, max_variants: int) -> List[Dict[str, Any]]:
        """è§£æLLMå“åº”å¹¶è½¬æ¢ä¸ºå˜ä½“æ ¼å¼"""
        try:
            # æå–JSONéƒ¨åˆ†
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                parsed_data = json.loads(json_str)
                
                variants = []
                for i, variant_data in enumerate(parsed_data.get("variants", [])[:max_variants]):
                    variant = {
                        "question": variant_data.get("question", f"å˜ä½“ä»»åŠ¡{i+1}"),
                        "expected_answer": variant_data.get("expected_answer", "å˜ä½“ä»»åŠ¡çš„é¢„æœŸç­”æ¡ˆ"),
                        "task_type": "tool_required",
                        "domain": variant_data.get("domain", step.domain),
                        "difficulty": variant_data.get("complexity_level", "ä¸­ç­‰"),
                        "required_tools": variant_data.get("required_tools", step.requires_tools),
                        "reasoning_steps": variant_data.get("reasoning_steps", [f"æ‰§è¡Œ{variant_data.get('question', 'å˜ä½“ä»»åŠ¡')}"]),
                        "relation_pattern": "llm_semantic_variant",
                        "creativity_level": str(4 + i),  # LLMå˜ä½“åˆ›é€ æ€§è¾ƒé«˜
                        "creativity_explanation": variant_data.get("creativity_explanation", "LLMç”Ÿæˆçš„è¯­ä¹‰å˜ä½“"),
                        "reverse_reasoning": f"åŸºäºåŸå§‹æ­¥éª¤'{step.operation}'çš„è¯­ä¹‰è”æƒ³",
                        "entity_generalization": variant_data.get("semantic_relation", "è¯­ä¹‰æ‰©å±•"),
                        "semantic_relation": variant_data.get("semantic_relation", "ä¸åŸå§‹ä»»åŠ¡è¯­ä¹‰ç›¸å…³")
                    }
                    variants.append(variant)
                
                return variants
            else:
                raise ValueError("å“åº”ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
                
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"âš ï¸ LLMå˜ä½“å“åº”è§£æå¤±è´¥: {e}")
            return self._generate_fallback_variants(step, max_variants)
    
    def _generate_fallback_variants(self, step: AtomicStep, max_variants: int) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå›é€€å˜ä½“ï¼ˆæ™ºèƒ½çš„åŸºäºè§„åˆ™çš„å˜ä½“ï¼‰"""
        variants = []
        
        # åŸºäºåŸå§‹å·¥å…·æ™ºèƒ½æ¨èæ–°å·¥å…·çš„æ˜ å°„ - å¢å¼ºå·¥å…·å¤šæ ·åŒ–
        tool_diversification_map = {
            "browser_search_google": ["deepsearch", "browser_use_execute_task", "browser_extract_content"],
            "microsandbox_execute": ["search_file_content", "list_code_definitions", "microsandbox_install_package"],
            "deepsearch": ["browser_use_execute_task", "search_file_content", "browser_navigate"],
            "browser_extract_content": ["browser_use_execute_task", "deepsearch", "browser_navigate"],
            "browser_use_execute_task": ["deepsearch", "browser_extract_content", "search_file_content"],
            "browser_navigate": ["browser_use_execute_task", "browser_extract_content", "deepsearch"],
            "search_file_content": ["list_code_definitions", "microsandbox_execute", "deepsearch"],
            "list_code_definitions": ["search_file_content", "microsandbox_execute", "microsandbox_install_package"],
            "microsandbox_install_package": ["microsandbox_execute", "search_and_install_tools", "list_code_definitions"],
            "memory_staging": ["microsandbox_execute", "search_file_content", "list_code_definitions"],
            "search_and_install_tools": ["microsandbox_install_package", "list_code_definitions", "search_file_content"]
        }
        
        # åŸºäºé¢†åŸŸçš„æ™ºèƒ½å˜ä½“æ¨¡å¼ - å¢å¼ºå·¥å…·å¤šæ ·åŒ–
        domain_patterns = {
            "æ•™è‚²": [
                ("æ·±åº¦ç ”ç©¶{content}çš„æ•™è‚²ä»·å€¼å’Œå½±å“", ["deepsearch"]),
                ("é€šè¿‡æµè§ˆå™¨è‡ªåŠ¨åŒ–æ”¶é›†{content}çš„å®˜æ–¹ä¿¡æ¯", ["browser_use_execute_task"]),
                ("åˆ†æ{content}ç›¸å…³çš„å­¦æœ¯ä»£ç å’Œæ–‡æ¡£", ["list_code_definitions"]),
                ("æ™ºèƒ½å¯¼èˆªå¹¶æå–{content}çš„è¯¦ç»†å†…å®¹", ["browser_navigate", "browser_extract_content"])
            ],
            "é‡‘è": [
                ("ä½¿ç”¨ä»£ç åˆ†æ{content}çš„å¸‚åœºæ•°æ®è¶‹åŠ¿", ["microsandbox_execute"]),
                ("æ·±åº¦ç ”ç©¶{content}çš„é‡‘èæ”¿ç­–å½±å“", ["deepsearch"]),
                ("è‡ªåŠ¨åŒ–è·å–{content}çš„å®æ—¶é‡‘èä¿¡æ¯", ["browser_use_execute_task"]),
                ("æœç´¢å¹¶å®‰è£…{content}ç›¸å…³çš„é‡‘èåˆ†æå·¥å…·", ["search_and_install_tools"])
            ],
            "ç§‘å­¦ç ”ç©¶": [
                ("æ‰§è¡Œ{content}ç›¸å…³çš„æ•°æ®åˆ†æä»£ç ", ["microsandbox_execute"]),
                ("æ·±åº¦ç ”ç©¶{content}é¢†åŸŸçš„æœ€æ–°æ–‡çŒ®", ["deepsearch"]),
                ("åˆ†æ{content}é¡¹ç›®çš„ä»£ç ç»“æ„å’Œä¾èµ–", ["search_file_content"]),
                ("å®‰è£…å¹¶æµ‹è¯•{content}ç ”ç©¶æ‰€éœ€çš„å·¥å…·åŒ…", ["microsandbox_install_package"])
            ],
            "ç¼–ç¨‹": [
                ("æ™ºèƒ½åˆ†æ{content}çš„ä»£ç å®šä¹‰å’Œç»“æ„", ["list_code_definitions"]),
                ("æ·±åº¦ç ”ç©¶{content}çš„æœ€ä½³å®è·µå’Œæ¡ˆä¾‹", ["deepsearch"]),
                ("è‡ªåŠ¨åŒ–å®‰è£…{content}å¼€å‘ç¯å¢ƒå’Œä¾èµ–", ["microsandbox_install_package"]),
                ("æœç´¢{content}ç›¸å…³çš„å¼€å‘å·¥å…·å’Œåº“", ["search_and_install_tools"])
            ]
        }
        
        # è·å–å½“å‰é¢†åŸŸçš„å˜ä½“æ¨¡å¼ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é€šç”¨æ¨¡å¼ - å¢å¼ºå·¥å…·å¤šæ ·åŒ–
        patterns = domain_patterns.get(step.domain, [
            ("æ·±åº¦ç ”ç©¶{content}çš„æ ¸å¿ƒç‰¹å¾å’Œè¶‹åŠ¿", ["deepsearch"]),
            ("é€šè¿‡æµè§ˆå™¨è‡ªåŠ¨åŒ–æ”¶é›†{content}çš„è¯¦ç»†ä¿¡æ¯", ["browser_use_execute_task"]),
            ("ä½¿ç”¨ä»£ç å·¥å…·åˆ†æ{content}çš„æ•°æ®æ¨¡å¼", ["microsandbox_execute"]),
            ("æœç´¢{content}ç›¸å…³çš„æ–‡ä»¶å†…å®¹å’Œç»“æ„", ["search_file_content"]),
            ("åˆ†æ{content}çš„ä»£ç å®šä¹‰å’Œæ¶æ„", ["list_code_definitions"]),
            ("è‡ªåŠ¨åŒ–å®‰è£…{content}ç›¸å…³çš„å·¥å…·å’Œä¾èµ–", ["search_and_install_tools"])
        ])
        
        # å¦‚æœæœ‰åŸå§‹å·¥å…·ï¼Œå°è¯•å¤šæ ·åŒ–å·¥å…·é€‰æ‹©
        if step.tool_name and step.tool_name in tool_diversification_map:
            suggested_tools = tool_diversification_map[step.tool_name]
            # å°†å»ºè®®çš„å·¥å…·èå…¥åˆ°å˜ä½“ä¸­
            for i, (pattern_template, default_tools) in enumerate(patterns[:max_variants]):
                if i < len(suggested_tools):
                    selected_tools = [suggested_tools[i]]
                else:
                    selected_tools = default_tools
                    
                pattern = pattern_template.format(content=step.content)
                variant = {
                    "question": pattern,
                    "expected_answer": f"å…³äº{step.content}çš„{['æ·±åº¦åˆ†æ', 'è‡ªåŠ¨åŒ–å¤„ç†', 'æ•°æ®åˆ†æ'][i % 3]}ç»“æœ",
                    "task_type": "tool_required",
                    "domain": step.domain,
                    "difficulty": "ä¸­ç­‰",
                    "required_tools": selected_tools,
                    "reasoning_steps": [f"ä½¿ç”¨{selected_tools[0]}æ‰§è¡Œ{pattern}"],
                    "relation_pattern": "intelligent_fallback_variant",
                    "creativity_level": str(3 + i),
                    "creativity_explanation": f"æ™ºèƒ½å›é€€å˜ä½“ï¼šä»{step.tool_name}æ‰©å±•åˆ°{selected_tools[0]}ï¼Œ{pattern}",
                    "reverse_reasoning": f"åŸºäºåŸå§‹æ­¥éª¤'{step.operation}'çš„å·¥å…·å¤šæ ·åŒ–æ‰©å±•",
                    "entity_generalization": f"å·¥å…·å¤šæ ·åŒ–ï¼š{step.tool_name} -> {selected_tools[0]}"
                }
                variants.append(variant)
        else:
            # æ²¡æœ‰åŸå§‹å·¥å…·æ—¶ï¼Œä½¿ç”¨æ ‡å‡†å˜ä½“
            for i, (pattern_template, default_tools) in enumerate(patterns[:max_variants]):
                pattern = pattern_template.format(content=step.content)
                variant = {
                    "question": pattern,
                    "expected_answer": f"å…³äº{step.content}çš„{['ç ”ç©¶', 'åˆ†æ', 'å¤„ç†'][i]}ç»“æœ",
                    "task_type": "tool_required",
                    "domain": step.domain,
                    "difficulty": "ä¸­ç­‰",
                    "required_tools": default_tools,
                    "reasoning_steps": [f"ä½¿ç”¨{default_tools[0]}æ‰§è¡Œ{pattern}"],
                    "relation_pattern": "domain_fallback_variant",
                    "creativity_level": str(2 + i),
                    "creativity_explanation": f"é¢†åŸŸå›é€€å˜ä½“ï¼š{pattern}",
                    "reverse_reasoning": f"åŸºäºåŸå§‹æ­¥éª¤'{step.operation}'çš„é¢†åŸŸç‰¹å®šæ‰©å±•",
                    "entity_generalization": f"é¢†åŸŸç‰¹å®šå˜ä½“ç”Ÿæˆï¼š{step.domain}"
                }
                variants.append(variant)
        
        return variants


# ä¿ç•™StepVariantæ•°æ®ç±»ä»¥ç»´æŠ¤å…¼å®¹æ€§ï¼Œä½†æ ‡è®°ä¸ºå¼ƒç”¨
@dataclass 
class StepVariant:
    """æ­¥éª¤å˜ä½“æ•°æ®ç»“æ„ï¼ˆå·²å¼ƒç”¨ï¼Œä¿ç•™ä»¥ç»´æŠ¤å…¼å®¹æ€§ï¼‰"""
    variant_id: str
    base_step: AtomicStep
    variant_operation: str
    variant_content: str
    variant_domain: str
    substitution_mapping: Dict[str, str]
    creativity_level: int
    generated_at: datetime
    
    def to_task_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºä»»åŠ¡æ ¼å¼"""
        return {
            "question": self.variant_operation,
            "expected_answer": f"åŸºäº{self.variant_domain}é¢†åŸŸçš„{self.variant_content}",
            "task_type": "tool_required",
            "domain": self.variant_domain,
            "difficulty": self.base_step.complexity,
            "required_tools": self.base_step.requires_tools,
            "reasoning_steps": [f"æ‰§è¡Œ{self.variant_operation}", f"è·å–{self.variant_content}"],
            "relation_pattern": f"step_variant_from_{self.base_step.step_type}",
            "entity_generalization": str(self.substitution_mapping),
            "creativity_level": str(self.creativity_level),
            "creativity_explanation": f"åŸºäºçœŸå®è½¨è¿¹æ­¥éª¤çš„å˜ä½“ç”Ÿæˆï¼ŒåŸå§‹æ­¥éª¤ï¼š{self.base_step.operation}",
            "reverse_reasoning": f"ä»è½¨è¿¹æ­¥éª¤'{self.base_step.operation}'è¡ç”Ÿå‡ºå˜ä½“'{self.variant_operation}'"
        }


class EnhancedTrajectoryBasedTaskGenerator:
    """å¢å¼ºçš„åŸºäºè½¨è¿¹çš„ä»»åŠ¡ç”Ÿæˆå™¨"""
    
    def __init__(self, llm_client, validator):
        self.step_extractor = TrajectoryStepExtractor()
        self.step_validator = AtomicStepValidator(llm_client, validator)
        self.variant_generator = LLMDrivenVariantGenerator(llm_client)
        self.generated_tasks_cache: Set[str] = set()  # é˜²é‡å¤
    
    async def generate_evidence_based_tasks(self, trajectories: List[Dict], max_tasks: int = 10) -> List[Dict]:
        """ç”ŸæˆåŸºäºè¯æ®çš„ä»»åŠ¡ - æ··åˆç›´æ¥è½¬æ¢å’ŒLLMå˜ä½“"""
        all_tasks = []
        direct_tasks = []  # ç›´æ¥ä»åŸå­æ­¥è½¬æ¢çš„ä»»åŠ¡
        variant_tasks = []  # LLMå˜ä½“ä»»åŠ¡
        
        logger.info(f"ğŸ”„ å¼€å§‹åŸºäº {len(trajectories)} ä¸ªè½¨è¿¹ç”Ÿæˆæœ‰ä¾æ®çš„ä»»åŠ¡")
        
        for trajectory in trajectories:
            if len(all_tasks) >= max_tasks:
                break
            
            try:
                # æ­¥éª¤1ï¼šæå–åŸå­æ­¥
                atomic_steps = self.step_extractor.extract_atomic_steps(trajectory)
                logger.debug(f"ğŸ“‹ ä»è½¨è¿¹ {trajectory.get('task_id', 'unknown')} æå–äº† {len(atomic_steps)} ä¸ªåŸå­æ­¥")
                
                for step in atomic_steps:
                    if len(all_tasks) >= max_tasks:
                        break
                    
                    # æ­¥éª¤2ï¼šéªŒè¯åŸå­æ€§å¹¶ä¿®æ”¹ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    is_atomic, direct_task = await self.step_validator.validate_and_fix_atomicity(step)
                    
                    # é˜²é‡å¤æ£€æŸ¥
                    task_signature = f"{direct_task['question']}_{direct_task['domain']}"
                    if task_signature not in self.generated_tasks_cache:
                        self.generated_tasks_cache.add(task_signature)
                        direct_tasks.append(direct_task)
                        
                        logger.debug(f"âœ… ç”Ÿæˆç›´æ¥ä»»åŠ¡: {direct_task['question'][:50]}... (åŸå­æ€§: {'é€šè¿‡' if is_atomic else 'LLMä¿®å¤'})")
                    
                    # æ­¥éª¤3ï¼šç”ŸæˆLLMé©±åŠ¨çš„å˜ä½“ï¼ˆå¦‚æœè¿˜æœ‰ç©ºé—´ï¼‰
                    if len(all_tasks) + len(direct_tasks) < max_tasks:
                        try:
                            variants = await self.variant_generator.generate_step_variants(step, max_variants=2)
                            
                            for variant in variants:
                                if len(all_tasks) + len(direct_tasks) + len(variant_tasks) >= max_tasks:
                                    break
                                
                                # é˜²é‡å¤æ£€æŸ¥
                                variant_signature = f"{variant['question']}_{variant['domain']}"
                                if variant_signature not in self.generated_tasks_cache:
                                    self.generated_tasks_cache.add(variant_signature)
                                    variant_tasks.append(variant)
                                    
                                    logger.debug(f"âœ¨ ç”ŸæˆLLMå˜ä½“: {variant['question'][:50]}...")
                                    
                        except Exception as e:
                            logger.warning(f"âš ï¸ ä¸ºæ­¥éª¤ {step.step_id} ç”Ÿæˆå˜ä½“å¤±è´¥: {e}")
                            continue
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†è½¨è¿¹ {trajectory.get('task_id', 'unknown')} æ—¶å‡ºé”™: {e}")
                continue
        
        # æ­¥éª¤4ï¼šåˆå¹¶ä»»åŠ¡ï¼ˆç»´æŒæ¯”ä¾‹ï¼‰
        target_direct = min(len(direct_tasks), max_tasks // 2)  # ç›´æ¥ä»»åŠ¡å 50%
        target_variant = min(len(variant_tasks), max_tasks - target_direct)  # å˜ä½“ä»»åŠ¡å å‰©ä½™
        
        all_tasks.extend(direct_tasks[:target_direct])
        all_tasks.extend(variant_tasks[:target_variant])
        
        logger.info(f"ğŸ‰ åŸºäºè½¨è¿¹è¯æ®ç”Ÿæˆäº† {len(all_tasks)} ä¸ªä»»åŠ¡ (ç›´æ¥ä»»åŠ¡: {target_direct}, LLMå˜ä½“: {target_variant})")
        return all_tasks


# ä¿ç•™åŸæ¥çš„TrajectoryBasedTaskGeneratorä»¥ç»´æŠ¤å‘ä¸‹å…¼å®¹æ€§
class TrajectoryBasedTaskGenerator:
    """åŸºäºè½¨è¿¹çš„ä»»åŠ¡ç”Ÿæˆå™¨ï¼ˆå…¼å®¹æ€§ä¿ç•™ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self):
        self.step_extractor = TrajectoryStepExtractor()
        self.generated_tasks_cache: Set[str] = set()  # é˜²é‡å¤
        logger.warning("âš ï¸ ä½¿ç”¨äº†å…¼å®¹æ€§ç‰ˆæœ¬çš„TrajectoryBasedTaskGeneratorï¼Œå»ºè®®å‡çº§åˆ°EnhancedTrajectoryBasedTaskGenerator")
    
    def generate_evidence_based_tasks(self, trajectories: List[Dict], max_tasks: int = 10) -> List[Dict]:
        """ç”ŸæˆåŸºäºè¯æ®çš„ä»»åŠ¡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        all_tasks = []
        tasks_generated = 0
        
        logger.info(f"ğŸ”„ å¼€å§‹åŸºäº {len(trajectories)} ä¸ªè½¨è¿¹ç”Ÿæˆæœ‰ä¾æ®çš„ä»»åŠ¡ï¼ˆå…¼å®¹æ€§æ¨¡å¼ï¼‰")
        
        for trajectory in trajectories:
            if tasks_generated >= max_tasks:
                break
            
            try:
                # 1. æå–åŸå­æ­¥éª¤
                atomic_steps = self.step_extractor.extract_atomic_steps(trajectory)
                
                # 2. ç›´æ¥è½¬æ¢ä¸ºä»»åŠ¡ï¼ˆæ— LLMéªŒè¯ï¼‰
                for step in atomic_steps:
                    if tasks_generated >= max_tasks:
                        break
                    
                    # ç®€å•è½¬æ¢
                    task_dict = {
                        "question": step.operation,
                        "expected_answer": f"åŸºäº{step.domain}é¢†åŸŸçš„{step.content}",
                        "task_type": "tool_required",
                        "domain": step.domain,
                        "difficulty": step.complexity,
                        "required_tools": step.requires_tools,
                        "reasoning_steps": [f"æ‰§è¡Œ{step.operation}"],
                        "relation_pattern": "simple_step_conversion",
                        "creativity_level": "1",
                        "creativity_explanation": f"ç®€å•è½¬æ¢ï¼š{step.operation}",
                        "reverse_reasoning": f"è½¨è¿¹æ­¥éª¤ï¼š{step.step_id}",
                        "entity_generalization": f"æ­¥éª¤ç±»å‹ï¼š{step.step_type}"
                    }
                    
                    # é˜²é‡å¤æ£€æŸ¥
                    task_signature = f"{task_dict['question']}_{task_dict['domain']}"
                    if task_signature not in self.generated_tasks_cache:
                        self.generated_tasks_cache.add(task_signature)
                        all_tasks.append(task_dict)
                        tasks_generated += 1
                        
                        logger.debug(f"âœ¨ ç”Ÿæˆç®€å•ä»»åŠ¡: {task_dict['question'][:50]}...")
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†è½¨è¿¹ {trajectory.get('task_id', 'unknown')} æ—¶å‡ºé”™: {e}")
                continue
        
        logger.info(f"ğŸ‰ åŸºäºè½¨è¿¹è¯æ®ç”Ÿæˆäº† {len(all_tasks)} ä¸ªä»»åŠ¡ï¼ˆå…¼å®¹æ€§æ¨¡å¼ï¼‰")
        return all_tasks


class AtomicStepValidator:
    """åŸå­æ­¥éªŒè¯å™¨ - ä½¿ç”¨LLMéªŒè¯å’Œä¿®æ”¹åŸå­æ€§"""
    
    def __init__(self, llm_client, validator):
        self.llm_client = llm_client
        self.validator = validator
        
        # åŸå­æ€§ä¿®å¤çš„Promptæ¨¡æ¿
        self.atomicity_fix_prompt = """ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡åŸå­æ€§ä¸“å®¶ã€‚ä»¥ä¸‹åŸå­æ­¥éª¤æœªé€šè¿‡åŸå­æ€§æ£€æŸ¥ï¼Œè¯·å°†å…¶ä¿®æ”¹ä¸ºç¬¦åˆåŸå­æ€§è¦æ±‚çš„ä»»åŠ¡ã€‚

åŸå§‹æ­¥éª¤ï¼š{operation}
å†…å®¹ï¼š{content}
åŸå­æ€§é—®é¢˜ï¼š{validation_issues}

åŸå­æ€§è¦æ±‚ï¼š
1. ä»»åŠ¡åªèƒ½åŒ…å«ä¸€ä¸ªæ˜ç¡®çš„åŠ¨ä½œ
2. ä¸èƒ½åˆ†è§£ä¸ºå¤šä¸ªç‹¬ç«‹çš„å­ä»»åŠ¡
3. å¯ä»¥ä¸€æ¬¡æ€§å®Œæˆ
4. æè¿°ä¸­ä¸åŒ…å«"ç„¶å"ã€"æ¥ç€"ã€"åŒæ—¶"ç­‰è¿æ¥è¯

è¯·å°†åŸå§‹æ­¥éª¤ä¿®æ”¹ä¸ºç¬¦åˆåŸå­æ€§çš„ä»»åŠ¡ï¼š

è¿”å›JSONæ ¼å¼ï¼š
{{
    "fixed_question": "ä¿®æ”¹åçš„åŸå­ä»»åŠ¡æè¿°",
    "expected_answer": "é¢„æœŸç­”æ¡ˆ",
    "reasoning_steps": ["æ‰§è¡Œæ­¥éª¤"],
    "required_tools": ["{tool_name}"],
    "fix_explanation": "ä¿®æ”¹è¯´æ˜",
    "domain": "{domain}",
    "difficulty": "{complexity}"
}}"""
    
    async def validate_and_fix_atomicity(self, step: AtomicStep) -> Tuple[bool, Dict[str, Any]]:
        """éªŒè¯åŸå­æ­¥çš„åŸå­æ€§ï¼Œä¸ç¬¦åˆåˆ™LLMä¿®æ”¹"""
        try:
            # ä½¿ç”¨TaskValidatoræ£€æŸ¥åŸå­æ€§
            atomicity_check = await self.validator._check_atomicity(step.operation)
            
            if atomicity_check:
                # ç›´æ¥è½¬æ¢ä¸ºåŸå­ä»»åŠ¡
                logger.debug(f"âœ… åŸå­æ­¥ {step.step_id} é€šè¿‡åŸå­æ€§æ£€æŸ¥")
                return True, self._convert_step_to_task(step)
            else:
                # LLMä¿®æ”¹ä¸ºç¬¦åˆåŸå­æ€§çš„ä»»åŠ¡
                logger.info(f"ğŸ”§ åŸå­æ­¥ {step.step_id} éœ€è¦åŸå­æ€§ä¿®æ”¹")
                fixed_task = await self._llm_fix_atomicity(step, "æœªé€šè¿‡åŸå­æ€§æ£€æŸ¥")
                return False, fixed_task
                
        except Exception as e:
            logger.error(f"âŒ éªŒè¯åŸå­æ­¥ {step.step_id} æ—¶å‡ºé”™: {e}")
            # å‡ºé”™æ—¶ç›´æ¥è½¬æ¢
            return True, self._convert_step_to_task(step)
    
    def _convert_step_to_task(self, step: AtomicStep) -> Dict[str, Any]:
        """å°†åŸå­æ­¥ç›´æ¥è½¬æ¢ä¸ºä»»åŠ¡æ ¼å¼"""
        return {
            "question": step.operation,
            "expected_answer": f"åŸºäº{step.domain}é¢†åŸŸçš„{step.content}",
            "task_type": "tool_required",
            "domain": step.domain,
            "difficulty": step.complexity,
            "required_tools": step.requires_tools,
            "reasoning_steps": [f"æ‰§è¡Œ{step.operation}"],
            "relation_pattern": "direct_atomic_step",
            "creativity_level": "1",  # ç›´æ¥è½¬æ¢çš„åˆ›é€ æ€§æœ€ä½
            "creativity_explanation": f"ç›´æ¥ä»è½¨è¿¹æ­¥éª¤è½¬æ¢ï¼š{step.operation}",
            "reverse_reasoning": f"åŸå§‹è½¨è¿¹æ­¥éª¤ï¼š{step.step_id}",
            "entity_generalization": f"åŸå­æ­¥éª¤ç±»å‹ï¼š{step.step_type}ï¼Œå·¥å…·ï¼š{step.tool_name}"
        }
    
    async def _llm_fix_atomicity(self, step: AtomicStep, validation_issues: str) -> Dict[str, Any]:
        """ä½¿ç”¨LLMä¿®å¤åŸå­æ€§é—®é¢˜"""
        try:
            prompt = self.atomicity_fix_prompt.format(
                operation=step.operation,
                content=step.content,
                validation_issues=validation_issues,
                tool_name=step.tool_name or "é€šç”¨å·¥å…·",
                domain=step.domain,
                complexity=step.complexity
            )
            
            # è°ƒç”¨LLM - ä½¿ç”¨ç³»ç»Ÿæ ‡å‡†çš„_call_apiæ–¹æ³•
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_client._call_api(messages, timeout=60)
            
            # è§£æLLMå“åº”
            try:
                # æå–JSONéƒ¨åˆ†
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                if json_start >= 0 and json_end > json_start:
                    json_str = response[json_start:json_end]
                    fixed_data = json.loads(json_str)
                    
                    # è¡¥å……å¿…è¦å­—æ®µ
                    result = {
                        "question": fixed_data.get("fixed_question", step.operation),
                        "expected_answer": fixed_data.get("expected_answer", f"ä¿®å¤åçš„{step.content}"),
                        "task_type": "tool_required",
                        "domain": fixed_data.get("domain", step.domain),
                        "difficulty": fixed_data.get("difficulty", step.complexity),
                        "required_tools": fixed_data.get("required_tools", step.requires_tools),
                        "reasoning_steps": fixed_data.get("reasoning_steps", [f"æ‰§è¡Œ{step.operation}"]),
                        "relation_pattern": "llm_fixed_atomic_step",
                        "creativity_level": "2",  # LLMä¿®å¤çš„åˆ›é€ æ€§ç¨é«˜
                        "creativity_explanation": f"LLMä¿®å¤åŸå­æ€§ï¼š{fixed_data.get('fix_explanation', 'ä¿®å¤åŸå­æ€§é—®é¢˜')}",
                        "reverse_reasoning": f"åŸå§‹æ­¥éª¤ï¼š{step.operation} -> ä¿®å¤åï¼š{fixed_data.get('fixed_question', step.operation)}",
                        "entity_generalization": f"åŸå­æ€§ä¿®å¤ï¼š{validation_issues}"
                    }
                    
                    logger.info(f"ğŸ”§ LLMæˆåŠŸä¿®å¤åŸå­æ­¥ {step.step_id}")
                    return result
                    
                else:
                    raise ValueError("LLMå“åº”ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„JSON")
                    
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"âš ï¸ LLMå“åº”è§£æå¤±è´¥: {e}, ä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                return self._create_fallback_fixed_task(step, validation_issues)
                
        except Exception as e:
            logger.error(f"âŒ LLMä¿®å¤åŸå­æ€§å¤±è´¥: {e}")
            return self._create_fallback_fixed_task(step, validation_issues)
    
    def _create_fallback_fixed_task(self, step: AtomicStep, validation_issues: str) -> Dict[str, Any]:
        """åˆ›å»ºå›é€€çš„ä¿®å¤ä»»åŠ¡"""
        # ç®€å•çš„åŸå­æ€§ä¿®å¤ï¼šæå–ç¬¬ä¸€ä¸ªåŠ¨ä½œè¯
        operation_words = step.operation.split()
        if operation_words:
            first_action = operation_words[0]
            simplified_operation = f"{first_action}ç›¸å…³ä¿¡æ¯"
        else:
            simplified_operation = step.operation
            
        return {
            "question": simplified_operation,
            "expected_answer": f"åŸºäº{step.domain}çš„{step.content}",
            "task_type": "tool_required",
            "domain": step.domain,
            "difficulty": step.complexity,
            "required_tools": step.requires_tools,
            "reasoning_steps": [f"æ‰§è¡Œ{simplified_operation}"],
            "relation_pattern": "fallback_fixed_atomic_step",
            "creativity_level": "1",
            "creativity_explanation": f"å›é€€ä¿®å¤åŸå­æ€§ï¼š{validation_issues}",
            "reverse_reasoning": f"ç®€åŒ–åŸå§‹æ­¥éª¤ï¼š{step.operation}",
            "entity_generalization": "å›é€€åŸå­æ€§ä¿®å¤"
        }