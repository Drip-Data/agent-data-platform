#!/usr/bin/env python3
"""
Trajectory Monitor - è½¨è¿¹ç›‘æ§å™¨
è‡ªåŠ¨ç›‘æ§è½¨è¿¹æ–‡ä»¶å˜åŒ–ï¼Œä½¿ç”¨SynthesisCore v2.0ç”Ÿæˆseed_task
"""

import asyncio
import json
import logging
import os
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

from core.interfaces import TrajectoryResult, TaskSpec, TaskType, ExecutionStep, ActionType
from core.llm_client import LLMClient
from core.toolscore.mcp_client import MCPToolClient
from .enhanced_synthesis_engine import SynthesisCoreV2
from .enhanced_interfaces import AtomicTask, ExtendedTask, CompositeTask

logger = logging.getLogger(__name__)


class TrajectoryFileHandler(FileSystemEventHandler):
    """è½¨è¿¹æ–‡ä»¶äº‹ä»¶å¤„ç†å™¨"""
    
    def __init__(self, trajectory_monitor):
        self.trajectory_monitor = trajectory_monitor
        self.last_processed = {}
        
    def on_modified(self, event):
        """æ–‡ä»¶ä¿®æ”¹äº‹ä»¶"""
        if event.is_directory:
            return
            
        if event.src_path.endswith('trajectories_collection.json'):
            # é¿å…é¢‘ç¹è§¦å‘ï¼Œè®¾ç½®æœ€å°é—´éš”
            current_time = time.time()
            last_time = self.last_processed.get(event.src_path, 0)
            
            if current_time - last_time > 2.0:  # 2ç§’é—´éš”
                self.last_processed[event.src_path] = current_time
                logger.info(f"ğŸ“ æ£€æµ‹åˆ°è½¨è¿¹æ–‡ä»¶å˜åŒ–: {event.src_path}")
                
                # å¼‚æ­¥å¤„ç†
                asyncio.create_task(
                    self.trajectory_monitor.process_trajectory_changes(event.src_path)
                )


class TrajectoryMonitor:
    """è½¨è¿¹ç›‘æ§å™¨ - é›†æˆSynthesisCore v2.0"""
    
    def __init__(self, llm_client: LLMClient, mcp_client: Optional[MCPToolClient] = None,
                 trajectories_dir: str = None, seed_tasks_file: str = None):
        self.llm_client = llm_client
        self.mcp_client = mcp_client
        
        # è·¯å¾„é…ç½®
        self.trajectories_dir = trajectories_dir or "/Users/zhaoxiang/Documents/Datapresso/agent-data-platform/output/trajectories"
        self.seed_tasks_file = seed_tasks_file or "/Users/zhaoxiang/Documents/Datapresso/agent-data-platform/output/seed_tasks.jsonl"
        self.trajectories_collection_file = os.path.join(self.trajectories_dir, "trajectories_collection.json")
        self.processed_trajectories_file = os.path.join(self.trajectories_dir, "processed_trajectories.json")
        
        # SynthesisCore v2.0
        self.synthesis_core = SynthesisCoreV2(llm_client, mcp_client)
        
        # æ–‡ä»¶ç›‘æ§
        self.observer = Observer()
        self.file_handler = TrajectoryFileHandler(self)
        
        # å·²å¤„ç†è½¨è¿¹è®°å½•
        self.processed_trajectories = self._load_processed_trajectories()
        
        logger.info(f"ğŸ”§ TrajectoryMonitoråˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“‚ ç›‘æ§ç›®å½•: {self.trajectories_dir}")
        logger.info(f"ğŸ“ ç§å­æ–‡ä»¶: {self.seed_tasks_file}")
    
    async def initialize(self):
        """åˆå§‹åŒ–ç›‘æ§å™¨"""
        try:
            # åˆå§‹åŒ–SynthesisCore v2.0
            await self.synthesis_core.initialize()
            
            # ç¡®ä¿ç›®å½•å’Œæ–‡ä»¶å­˜åœ¨
            os.makedirs(os.path.dirname(self.seed_tasks_file), exist_ok=True)
            os.makedirs(self.trajectories_dir, exist_ok=True)
            
            # å¤„ç†ç°æœ‰è½¨è¿¹
            await self.process_existing_trajectories()
            
            logger.info("âœ… TrajectoryMonitoråˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ TrajectoryMonitoråˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def start_monitoring(self):
        """å¼€å§‹ç›‘æ§è½¨è¿¹æ–‡ä»¶"""
        try:
            # è®¾ç½®æ–‡ä»¶ç›‘æ§
            self.observer.schedule(
                self.file_handler,
                path=self.trajectories_dir,
                recursive=False
            )
            
            # å¯åŠ¨ç›‘æ§
            self.observer.start()
            logger.info(f"ğŸ‘ï¸ å¼€å§‹ç›‘æ§è½¨è¿¹æ–‡ä»¶å˜åŒ–: {self.trajectories_dir}")
            
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨æ–‡ä»¶ç›‘æ§å¤±è´¥: {e}")
            raise
    
    async def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        try:
            if self.observer.is_alive():
                self.observer.stop()
                self.observer.join()
                logger.info("ğŸ›‘ è½¨è¿¹æ–‡ä»¶ç›‘æ§å·²åœæ­¢")
            
            await self.synthesis_core.close()
            logger.info("ğŸ”’ SynthesisCoreå·²å…³é—­")
            
        except Exception as e:
            logger.error(f"âŒ åœæ­¢ç›‘æ§å¤±è´¥: {e}")
    
    async def process_existing_trajectories(self):
        """å¤„ç†ç°æœ‰è½¨è¿¹"""
        logger.info("ğŸ”„ æ£€æŸ¥å¹¶å¤„ç†ç°æœ‰è½¨è¿¹...")
        
        if os.path.exists(self.trajectories_collection_file):
            await self.process_trajectory_changes(self.trajectories_collection_file)
        else:
            logger.info("ğŸ“ æ²¡æœ‰ç°æœ‰è½¨è¿¹æ–‡ä»¶")
    
    async def process_trajectory_changes(self, file_path: str):
        """å¤„ç†è½¨è¿¹æ–‡ä»¶å˜åŒ–"""
        logger.info(f"ğŸ”„ å¤„ç†è½¨è¿¹æ–‡ä»¶: {file_path}")
        
        try:
            # è¯»å–è½¨è¿¹æ–‡ä»¶
            trajectories = self._load_trajectories_from_file(file_path)
            
            if not trajectories:
                logger.warning("âš ï¸ è½¨è¿¹æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ•ˆ")
                return
            
            # ç­›é€‰æœªå¤„ç†çš„è½¨è¿¹
            new_trajectories = self._filter_new_trajectories(trajectories)
            
            if not new_trajectories:
                logger.info("âœ… æ²¡æœ‰æ–°çš„è½¨è¿¹éœ€è¦å¤„ç†")
                return
            
            logger.info(f"ğŸ†• å‘ç° {len(new_trajectories)} ä¸ªæ–°è½¨è¿¹ï¼Œå¼€å§‹å¤„ç†...")
            
            # è°ƒè¯•ï¼šæ˜¾ç¤ºæ‰€æœ‰æ–°è½¨è¿¹çš„ä¿¡æ¯
            logger.info("ğŸ” æ‰€æœ‰æ–°è½¨è¿¹è¯¦æƒ…:")
            for i, trajectory in enumerate(new_trajectories):
                logger.info(f"  {i+1}. task_id={trajectory.task_id}, runtime_id={trajectory.runtime_id}, success={trajectory.success}")
            
            # è¿‡æ»¤å‡ºå€¼å¾—å¤„ç†çš„è½¨è¿¹
            valid_trajectories = []
            for trajectory in new_trajectories:
                if self._should_process_trajectory(trajectory):
                    valid_trajectories.append(trajectory)
                    logger.info(f"âœ… è½¨è¿¹é€šè¿‡è¿‡æ»¤: {trajectory.task_id} (runtime={trajectory.runtime_id}, success={trajectory.success})")
                else:
                    logger.debug(f"â­ï¸ è·³è¿‡è½¨è¿¹: {trajectory.task_id}")
            
            logger.info(f"ğŸ“Š è½¨è¿¹è¿‡æ»¤ç»“æœ: {len(valid_trajectories)}/{len(new_trajectories)} ä¸ªè½¨è¿¹é€šè¿‡è¿‡æ»¤")
            
            if not valid_trajectories:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆè½¨è¿¹å¯å¤„ç†")
                return {"success": False, "message": "No valid trajectories to process"}
            
            # ä½¿ç”¨SynthesisCore v2.0å¤„ç†æœ‰æ•ˆè½¨è¿¹
            result = await self.synthesis_core.synthesize_tasks(
                trajectories=valid_trajectories,
                mode="full",  # ç”Ÿæˆæ‰€æœ‰ç±»å‹çš„ä»»åŠ¡
                verify_quality=True
            )
            
            if result["success"]:
                # è½¬æ¢ä¸ºç§å­ä»»åŠ¡å¹¶ä¿å­˜
                await self._convert_and_save_seed_tasks(result)
                
                # åªæ ‡è®°æˆåŠŸå¤„ç†çš„æœ‰æ•ˆè½¨è¿¹ä¸ºå·²å¤„ç†
                self._update_processed_trajectories([t.task_id for t in valid_trajectories])
                
                logger.info(f"âœ… è½¨è¿¹å¤„ç†å®Œæˆï¼Œç”Ÿæˆç§å­ä»»åŠ¡: åŸå­ {len(result['atomic_tasks'])}, æ‰©å±• {len(result['extended_tasks'])}, å¤åˆ {len(result['composite_tasks'])}")
                logger.info(f"âœ… æ ‡è®° {len(valid_trajectories)} ä¸ªæœ‰æ•ˆè½¨è¿¹ä¸ºå·²å¤„ç†")
            else:
                logger.error(f"âŒ SynthesisCoreå¤„ç†å¤±è´¥: {result.get('error', 'Unknown error')}")
                logger.warning(f"âš ï¸ ä¸æ ‡è®°è½¨è¿¹ä¸ºå·²å¤„ç†ï¼Œä»¥ä¾¿ä¸‹æ¬¡é‡è¯•")
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†è½¨è¿¹å˜åŒ–å¤±è´¥: {e}")
    
    def _load_trajectories_from_file(self, file_path: str) -> List[TrajectoryResult]:
        """ä»æ–‡ä»¶åŠ è½½è½¨è¿¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            trajectories = []
            trajectory_list = data.get('trajectories', []) if isinstance(data, dict) else data
            
            for traj_data in trajectory_list:
                try:
                    trajectory = self._convert_to_trajectory_result(traj_data)
                    if trajectory:
                        trajectories.append(trajectory)
                except Exception as e:
                    logger.error(f"âŒ è½¬æ¢è½¨è¿¹æ•°æ®å¤±è´¥: {e}")
                    continue
            
            logger.info(f"ğŸ“‹ ä»æ–‡ä»¶åŠ è½½ {len(trajectories)} ä¸ªè½¨è¿¹")
            return trajectories
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½è½¨è¿¹æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return []
    
    def _convert_to_trajectory_result(self, traj_data: Dict) -> Optional[TrajectoryResult]:
        """è½¬æ¢è½¨è¿¹æ•°æ®æ ¼å¼"""
        try:
            # è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥æ•°æ®ç»“æ„
            logger.debug(f"ğŸ” TM Converting trajectory: {traj_data.get('task_id', 'unknown')}")
            
            # åŸºç¡€ä¿¡æ¯
            task_id = traj_data.get('task_id', f"traj_{int(time.time())}")
            task_description = traj_data.get('task_description', traj_data.get('description', ''))
            runtime_id = traj_data.get('runtime_id', 'unknown')
            
            # æ‰§è¡Œç»“æœ
            success = traj_data.get('success', False)
            final_result = traj_data.get('final_result', traj_data.get('result', ''))
            total_duration = traj_data.get('total_duration', 0.0)
            
            # æ­¥éª¤ä¿¡æ¯
            steps = []
            steps_data = traj_data.get('steps', traj_data.get('execution_steps', []))
            
            for i, step_data in enumerate(steps_data):
                step = self._convert_step_data(step_data, i)
                if step:
                    steps.append(step)
            
            # åˆ›å»ºè½¨è¿¹ç»“æœ 
            # æ³¨æ„: TrajectoryResultä½¿ç”¨created_atå­—æ®µï¼Œä¸æ˜¯completed_at
            completed_at_str = traj_data.get('completed_at', datetime.now().isoformat())
            try:
                # å°è¯•è§£æISOæ ¼å¼æ—¶é—´æˆ³
                if isinstance(completed_at_str, str):
                    completed_at_time = datetime.fromisoformat(completed_at_str.replace('Z', '+00:00')).timestamp()
                else:
                    completed_at_time = float(completed_at_str)
            except:
                completed_at_time = time.time()
            
            trajectory = TrajectoryResult(
                task_name=task_id,  # task_nameå­—æ®µ
                task_id=task_id,
                task_description=task_description,
                runtime_id=runtime_id,
                success=success,
                final_result=final_result,
                steps=steps,
                total_duration=total_duration,
                created_at=completed_at_time  # ä½¿ç”¨created_atè€Œä¸æ˜¯completed_at
            )
            
            logger.debug(f"ğŸ” TM Created: {trajectory.task_id}")
            return trajectory
            
        except Exception as e:
            logger.error(f"âŒ è½¨è¿¹æ•°æ®è½¬æ¢å¤±è´¥: {e}")
            return None
    
    def _should_process_trajectory(self, trajectory: TrajectoryResult) -> bool:
        """åˆ¤æ–­è½¨è¿¹æ˜¯å¦å€¼å¾—å¤„ç†"""
        # 1. æˆåŠŸçš„è½¨è¿¹æ€»æ˜¯å¤„ç†
        if trajectory.success:
            return True
        
        # 2. reasoning runtimeçš„è½¨è¿¹ï¼Œå³ä½¿å¤±è´¥ä¹Ÿå¯èƒ½æœ‰ä»·å€¼ï¼ˆä¸è¦æ±‚æœ‰æ‰§è¡Œæ­¥éª¤ï¼‰
        runtime_id = trajectory.runtime_id.lower()
        if 'reasoning' in runtime_id:
            logger.info(f"ğŸ§  Found reasoning trajectory: {trajectory.task_id}")
            return True
        
        # 3. æœ‰æ‰§è¡Œæ­¥éª¤çš„è½¨è¿¹
        if len(trajectory.steps) > 0:
            # æœ‰å¤šä¸ªæ­¥éª¤çš„å¤æ‚ä»»åŠ¡ï¼Œå³ä½¿å¤±è´¥ä¹Ÿå¯èƒ½æœ‰ä»·å€¼
            if len(trajectory.steps) >= 2:
                return True
        
        # 4. ä»»åŠ¡æè¿°åŒ…å«ç‰¹å®šå…³é”®è¯
        task_desc = trajectory.task_description.lower()
        valuable_keywords = ['reasoning', 'æ¨ç†', 'åˆ†æ', 'analysis', 'compare', 'å¯¹æ¯”', 'ç ”ç©¶']
        if any(keyword in task_desc for keyword in valuable_keywords):
            logger.info(f"ğŸ” Found valuable keywords in task description: {trajectory.task_id}")
            return True
        
        # 5. æœ‰æœ€ç»ˆç»“æœçš„è½¨è¿¹ï¼Œå³ä½¿å¤±è´¥ä¹Ÿå¯èƒ½æœ‰ä»·å€¼
        if trajectory.final_result and len(trajectory.final_result.strip()) > 50:
            logger.info(f"ğŸ“ Found trajectory with substantial final result: {trajectory.task_id}")
            return True
        
        return False
    
    def _convert_step_data(self, step_data: Dict, step_index: int) -> Optional[ExecutionStep]:
        """è½¬æ¢æ­¥éª¤æ•°æ®"""
        try:
            
            step_id = step_data.get('step_id', f"step_{step_index}")
            thinking = step_data.get('thinking', '')
            action_type = ActionType.TOOL_CALL  # é»˜è®¤ä¸ºå·¥å…·è°ƒç”¨
            action_params = step_data.get('action_params', step_data.get('tool_call', {}))
            observation = step_data.get('observation', step_data.get('result', ''))
            success = step_data.get('success', True)
            duration = step_data.get('duration', 0.0)
            
            step = ExecutionStep(
                step_id=step_id,
                thinking=thinking,
                action_type=action_type,
                action_params=action_params,
                observation=observation,
                success=success,
                duration=duration
            )
            
            return step
            
        except Exception as e:
            logger.error(f"âŒ æ­¥éª¤æ•°æ®è½¬æ¢å¤±è´¥: {e}")
            return None
    
    def _filter_new_trajectories(self, trajectories: List[TrajectoryResult]) -> List[TrajectoryResult]:
        """ç­›é€‰æœªå¤„ç†çš„è½¨è¿¹"""
        new_trajectories = []
        
        for trajectory in trajectories:
            if trajectory.task_id not in self.processed_trajectories:
                new_trajectories.append(trajectory)
        
        return new_trajectories
    
    async def _convert_and_save_seed_tasks(self, synthesis_result: Dict):
        """è½¬æ¢ä»»åŠ¡ç»“æœä¸ºç§å­ä»»åŠ¡å¹¶ä¿å­˜"""
        try:
            seed_tasks = []
            
            # å¤„ç†åŸå­ä»»åŠ¡
            for atomic_task in synthesis_result.get('atomic_tasks', []):
                seed_task = self._convert_atomic_task_to_seed(atomic_task)
                if seed_task:
                    seed_tasks.append(seed_task)
            
            # å¤„ç†æ·±åº¦æ‰©å±•ä»»åŠ¡
            for extended_task in synthesis_result.get('extended_tasks', []):
                seed_task = self._convert_extended_task_to_seed(extended_task)
                if seed_task:
                    seed_tasks.append(seed_task)
            
            # å¤„ç†å®½åº¦æ‰©å±•ä»»åŠ¡
            for composite_task in synthesis_result.get('composite_tasks', []):
                seed_task = self._convert_composite_task_to_seed(composite_task)
                if seed_task:
                    seed_tasks.append(seed_task)
            
            # ä¿å­˜åˆ°seed_tasks.jsonl
            if seed_tasks:
                await self._append_seed_tasks_to_file(seed_tasks)
                logger.info(f"ğŸ’¾ ä¿å­˜ {len(seed_tasks)} ä¸ªç§å­ä»»åŠ¡åˆ° {self.seed_tasks_file}")
            
        except Exception as e:
            logger.error(f"âŒ è½¬æ¢å’Œä¿å­˜ç§å­ä»»åŠ¡å¤±è´¥: {e}")
    
    def _convert_atomic_task_to_seed(self, atomic_task: AtomicTask) -> Dict:
        """è½¬æ¢åŸå­ä»»åŠ¡ä¸ºç§å­ä»»åŠ¡"""
        return {
            "task_id": f"seed_atomic_{atomic_task.task_id}",
            "task_type": self._map_task_type(atomic_task.required_tools),
            "description": atomic_task.question,
            "expected_tools": atomic_task.required_tools,
            "max_steps": self._estimate_max_steps(atomic_task.difficulty_level.value),
            "success_criteria": {
                "contains": [atomic_task.golden_answer],
                "accuracy_threshold": 0.8
            },
            "metadata": {
                "source": "atomic_task",
                "original_task_id": atomic_task.task_id,
                "difficulty": atomic_task.difficulty_level.value,
                "verification_score": atomic_task.verification_score,
                "created_at": datetime.now().isoformat()
            }
        }
    
    def _convert_extended_task_to_seed(self, extended_task: ExtendedTask) -> Dict:
        """è½¬æ¢æ‰©å±•ä»»åŠ¡ä¸ºç§å­ä»»åŠ¡"""
        return {
            "task_id": f"seed_extended_{extended_task.task_id}",
            "task_type": self._map_task_type(extended_task.expected_tools),
            "description": extended_task.question,
            "expected_tools": extended_task.expected_tools,
            "max_steps": self._estimate_max_steps(extended_task.difficulty_level.value) + extended_task.hop_level,
            "success_criteria": {
                "contains": [extended_task.golden_answer],
                "accuracy_threshold": 0.7
            },
            "metadata": {
                "source": "extended_task",
                "original_task_id": extended_task.task_id,
                "hop_level": extended_task.hop_level,
                "source_atomic_task": extended_task.source_atomic_task,
                "difficulty": extended_task.difficulty_level.value,
                "complexity_score": extended_task.complexity_score,
                "created_at": datetime.now().isoformat()
            }
        }
    
    def _convert_composite_task_to_seed(self, composite_task: CompositeTask) -> Dict:
        """è½¬æ¢å¤åˆä»»åŠ¡ä¸ºç§å­ä»»åŠ¡"""
        return {
            "task_id": f"seed_composite_{composite_task.task_id}",
            "task_type": self._map_task_type(composite_task.expected_tools),
            "description": composite_task.question,
            "expected_tools": composite_task.expected_tools,
            "max_steps": self._estimate_max_steps(composite_task.difficulty_level.value) + len(composite_task.source_atomic_tasks),
            "success_criteria": {
                "contains": composite_task.golden_answers,
                "accuracy_threshold": 0.6
            },
            "metadata": {
                "source": "composite_task",
                "original_task_id": composite_task.task_id,
                "source_atomic_tasks": composite_task.source_atomic_tasks,
                "original_questions": composite_task.original_questions,
                "difficulty": composite_task.difficulty_level.value,
                "merge_strategy": composite_task.merge_strategy,
                "created_at": datetime.now().isoformat()
            }
        }
    
    def _map_task_type(self, tools: List[str]) -> str:
        """æ ¹æ®å·¥å…·æ¨æ–­ä»»åŠ¡ç±»å‹"""
        if not tools:
            return "reasoning"
        
        tool_lower = [tool.lower() for tool in tools]
        
        if any("browser" in tool or "web" in tool for tool in tool_lower):
            return "web"
        elif any("python" in tool or "code" in tool for tool in tool_lower):
            return "code"
        elif any("search" in tool for tool in tool_lower):
            return "research"
        else:
            return "reasoning"
    
    def _estimate_max_steps(self, difficulty: str) -> int:
        """ä¼°ç®—æœ€å¤§æ­¥æ•°"""
        mapping = {
            "simple": 3,
            "medium": 6,
            "complex": 10
        }
        return mapping.get(difficulty, 5)
    
    async def _append_seed_tasks_to_file(self, seed_tasks: List[Dict]):
        """è¿½åŠ ç§å­ä»»åŠ¡åˆ°JSONLæ–‡ä»¶"""
        try:
            with open(self.seed_tasks_file, 'a', encoding='utf-8') as f:
                for task in seed_tasks:
                    f.write(json.dumps(task, ensure_ascii=False) + '\n')
            
            logger.info(f"ğŸ“ è¿½åŠ  {len(seed_tasks)} ä¸ªç§å­ä»»åŠ¡åˆ°æ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"âŒ å†™å…¥ç§å­ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {e}")
    
    def _load_processed_trajectories(self) -> set:
        """åŠ è½½å·²å¤„ç†è½¨è¿¹è®°å½•"""
        try:
            if os.path.exists(self.processed_trajectories_file):
                with open(self.processed_trajectories_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return set(data.get('processed', []))
            return set()
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å·²å¤„ç†è½¨è¿¹è®°å½•å¤±è´¥: {e}")
            return set()
    
    def _update_processed_trajectories(self, trajectory_ids: List[str]):
        """æ›´æ–°å·²å¤„ç†è½¨è¿¹è®°å½•"""
        try:
            # æ›´æ–°å†…å­˜è®°å½•
            self.processed_trajectories.update(trajectory_ids)
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            data = {
                "processed": list(self.processed_trajectories),
                "last_updated": datetime.now().isoformat(),
                "total_count": len(self.processed_trajectories)
            }
            
            with open(self.processed_trajectories_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“Š æ›´æ–°å·²å¤„ç†è½¨è¿¹è®°å½•: +{len(trajectory_ids)}, æ€»è®¡: {len(self.processed_trajectories)}")
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°å·²å¤„ç†è½¨è¿¹è®°å½•å¤±è´¥: {e}")
    
    async def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è¯»å–ç§å­ä»»åŠ¡æ–‡ä»¶ç»Ÿè®¡
            seed_count = 0
            if os.path.exists(self.seed_tasks_file):
                with open(self.seed_tasks_file, 'r', encoding='utf-8') as f:
                    seed_count = sum(1 for line in f if line.strip())
            
            # è·å–SynthesisCoreæŒ‡æ ‡
            synthesis_metrics = await self.synthesis_core.get_metrics("global")
            
            return {
                "processed_trajectories": len(self.processed_trajectories),
                "total_seed_tasks": seed_count,
                "synthesis_metrics": synthesis_metrics,
                "files": {
                    "trajectories_file": self.trajectories_collection_file,
                    "seed_tasks_file": self.seed_tasks_file,
                    "processed_record": self.processed_trajectories_file
                },
                "monitoring_status": self.observer.is_alive() if hasattr(self, 'observer') else False
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}