#!/usr/bin/env python3
"""
ç®€åŒ–è½¨è¿¹ç›‘æ§å™¨ - è‡ªåŠ¨ç›‘æ§è½¨è¿¹æ–‡ä»¶å˜åŒ–å¹¶ç”Ÿæˆç§å­ä»»åŠ¡
ç»•è¿‡å¤æ‚çš„Redisé…ç½®ï¼Œä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½
"""

import asyncio
import json
import logging
import os
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

logger = logging.getLogger(__name__)


class SimpleTrajectoryFileHandler(FileSystemEventHandler):
    """ç®€åŒ–çš„è½¨è¿¹æ–‡ä»¶äº‹ä»¶å¤„ç†å™¨"""
    
    def __init__(self, monitor):
        self.monitor = monitor
        self.last_processed = {}
        
    def on_modified(self, event):
        """æ–‡ä»¶ä¿®æ”¹äº‹ä»¶ - ç›‘æ§å®é™…çš„è½¨è¿¹æ–‡ä»¶æ ¼å¼"""
        if event.is_directory:
            return
            
        # ç›‘æ§å®é™…çš„è½¨è¿¹æ–‡ä»¶ï¼štrajectories_YYYY-MM-DD.jsonl
        if (event.src_path.endswith('.jsonl') and 
            'trajectories_' in os.path.basename(event.src_path) and
            os.path.basename(event.src_path).startswith('trajectories_')):
            
            # é¿å…é¢‘ç¹è§¦å‘ï¼Œè®¾ç½®æœ€å°é—´éš”
            current_time = time.time()
            last_time = self.last_processed.get(event.src_path, 0)
            
            if current_time - last_time > 5.0:  # 5ç§’é—´éš”ï¼Œé¿å…å¤„ç†è¿‡äºé¢‘ç¹
                self.last_processed[event.src_path] = current_time
                logger.info(f"ğŸ“ æ£€æµ‹åˆ°è½¨è¿¹æ–‡ä»¶å˜åŒ–: {event.src_path}")
                logger.info(f"ğŸš€ å¯åŠ¨TaskCraftä»»åŠ¡åˆæˆæµç¨‹...")
                
                # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    loop.run_until_complete(
                        self.monitor.process_trajectory_changes(event.src_path)
                    )
                finally:
                    loop.close()


class SimpleTrajectoryMonitor:
    """ç®€åŒ–è½¨è¿¹ç›‘æ§å™¨ - ä¸“æ³¨äºæ–‡ä»¶ç›‘æ§å’Œç§å­ä»»åŠ¡ç”Ÿæˆ"""
    
    def __init__(self, trajectories_dir: str = None, seed_tasks_file: str = None):
        # è·¯å¾„é…ç½® - ä½¿ç”¨åŠ¨æ€è·¯å¾„æ›¿ä»£ç¡¬ç¼–ç 
        from core.utils.path_utils import get_output_dir
        
        self.trajectories_dir = trajectories_dir or str(get_output_dir("trajectories"))
        self.seed_tasks_file = seed_tasks_file or str(get_output_dir() / "seed_tasks.jsonl")
        self.trajectories_collection_file = os.path.join(self.trajectories_dir, "trajectories_collection.json")
        self.processed_trajectories_file = os.path.join(self.trajectories_dir, "..", "processed_trajectories.json")
        
        # æ–‡ä»¶ç›‘æ§
        self.observer = Observer()
        self.file_handler = SimpleTrajectoryFileHandler(self)
        
        # å·²å¤„ç†è½¨è¿¹è®°å½•
        self.processed_trajectories = self._load_processed_trajectories()
        
        logger.info(f"ğŸ”§ SimpleTrajectoryMonitoråˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“‚ ç›‘æ§ç›®å½•: {self.trajectories_dir}")
        logger.info(f"ğŸ“ ç§å­æ–‡ä»¶: {self.seed_tasks_file}")
    
    async def initialize(self):
        """åˆå§‹åŒ–ç›‘æ§å™¨"""
        try:
            # ç¡®ä¿ç›®å½•å’Œæ–‡ä»¶å­˜åœ¨
            os.makedirs(os.path.dirname(self.seed_tasks_file), exist_ok=True)
            os.makedirs(self.trajectories_dir, exist_ok=True)
            
            # å¤„ç†ç°æœ‰è½¨è¿¹
            await self.process_existing_trajectories()
            
            logger.info("âœ… SimpleTrajectoryMonitoråˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ SimpleTrajectoryMonitoråˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def start_monitoring(self):
        """å¼€å§‹ç›‘æ§è½¨è¿¹æ–‡ä»¶"""
        try:
            # è®¾ç½®æ–‡ä»¶ç›‘æ§ - é€’å½’ç›‘æ§åŒ…æ‹¬groupedå­ç›®å½•
            self.observer.schedule(
                self.file_handler,
                path=self.trajectories_dir,
                recursive=True  # å¯ç”¨é€’å½’ç›‘æ§ï¼Œç›‘æ§grouped/YYYY-MM-DD/å­ç›®å½•
            )
            
            # å¯åŠ¨ç›‘æ§
            self.observer.start()
            logger.info(f"ğŸ‘ï¸ å¼€å§‹ç›‘æ§è½¨è¿¹æ–‡ä»¶å˜åŒ–: {self.trajectories_dir}")
            
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨æ–‡ä»¶ç›‘æ§å¤±è´¥: {e}")
            raise
    
    async def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        try:
            if self.observer.is_alive():
                self.observer.stop()
                self.observer.join()
                logger.info("ğŸ›‘ è½¨è¿¹æ–‡ä»¶ç›‘æ§å·²åœæ­¢")
            
        except Exception as e:
            logger.error(f"âŒ åœæ­¢ç›‘æ§å¤±è´¥: {e}")
    
    async def process_existing_trajectories(self):
        """å¤„ç†ç°æœ‰è½¨è¿¹ - æ‰«ægroupedç›®å½•ä¸‹çš„æ‰€æœ‰.jsonlæ–‡ä»¶"""
        logger.info("ğŸ”„ æ£€æŸ¥å¹¶å¤„ç†ç°æœ‰è½¨è¿¹...")
        
        # æ‰«ægroupedç›®å½•ä¸‹çš„æ‰€æœ‰è½¨è¿¹æ–‡ä»¶
        grouped_dir = os.path.join(self.trajectories_dir, "grouped")
        if os.path.exists(grouped_dir):
            for date_dir in os.listdir(grouped_dir):
                date_path = os.path.join(grouped_dir, date_dir)
                if os.path.isdir(date_path):
                    for file_name in os.listdir(date_path):
                        if (file_name.startswith('trajectories_') and 
                            file_name.endswith('.jsonl')):
                            file_path = os.path.join(date_path, file_name)
                            logger.info(f"ğŸ“ å‘ç°ç°æœ‰è½¨è¿¹æ–‡ä»¶: {file_path}")
                            await self.process_trajectory_changes(file_path)
        else:
            logger.info("ğŸ“ æ²¡æœ‰ç°æœ‰è½¨è¿¹æ–‡ä»¶")
    
    async def process_trajectory_changes(self, file_path: str):
        """å¤„ç†è½¨è¿¹æ–‡ä»¶å˜åŒ– - ä½¿ç”¨SynthesisEngineè¿›è¡ŒLLMé©±åŠ¨çš„ä»»åŠ¡ç”Ÿæˆ"""
        logger.info(f"ğŸ”„ å¤„ç†è½¨è¿¹æ–‡ä»¶: {file_path}")
        
        try:
            # ä½¿ç”¨SynthesisEngineè¿›è¡ŒçœŸæ­£çš„TaskCraftç®—æ³•å¤„ç†
            from .synthesis_engine import SynthesisEngine
            from core.llm_client import LLMClient
            
            # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
            llm_client = await self._initialize_llm_client()
            if not llm_client:
                logger.error("âŒ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½ä»»åŠ¡ç”Ÿæˆ")
                return
            
            # åˆ›å»º SynthesisEngineï¼Œä½¿ç”¨ä¸“é—¨çš„SynthesisTaskç›®å½•
            engine = SynthesisEngine(
                llm_client=llm_client,
                storage_dir="output/SynthesisTask"
            )
            
            # åŠ è½½è½¨è¿¹æ•°æ®
            trajectories_data = await self._load_trajectories_data(file_path)
            if not trajectories_data:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è½¨è¿¹æ•°æ®")
                return
            
            # æ‰§è¡ŒTaskCraftç®—æ³•è¿›è¡Œä»»åŠ¡åˆæˆ
            logger.info(f"ğŸ¤– å¼€å§‹æ‰§è¡ŒTaskCraftç®—æ³•ï¼Œå¤„ç† {len(trajectories_data)} ä¸ªè½¨è¿¹")
            result = await engine.synthesize_from_trajectories(
                trajectories_data=trajectories_data,
                generate_depth_extensions=True,
                generate_width_extensions=True,
                max_atomic_tasks=10
            )
            
            if result and result.total_tasks_generated > 0:
                logger.info(f"âœ… TaskCraftä»»åŠ¡åˆæˆå®Œæˆ:")
                logger.info(f"  åŸå­ä»»åŠ¡: {len(result.atomic_tasks)} ä¸ª")
                logger.info(f"  æ·±åº¦æ‰©å±•: {len(result.depth_extended_tasks)} ä¸ª")
                logger.info(f"  å®½åº¦æ‰©å±•: {len(result.width_extended_tasks)} ä¸ª")
                logger.info(f"  æœ‰æ•ˆä»»åŠ¡: {result.valid_tasks_count}/{result.total_tasks_generated}")
                
                # æ›´æ–°å·²å¤„ç†è®°å½•
                await self._update_processed_trajectories(result.source_trajectories)
                
                # å¯¼å‡ºä¸ºä¼ ç»Ÿçš„seed_tasks.jsonlæ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                await self._export_to_seed_tasks(result)
            else:
                logger.warning("âš ï¸ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„ä»»åŠ¡")
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†è½¨è¿¹å˜åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    async def _initialize_llm_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            from core.llm_client import LLMClient
            from core.unified_tool_manager import UnifiedToolManager
            import yaml
            import os
            
            # è¯»å–LLMé…ç½®
            config_path = os.path.join(os.path.dirname(__file__), "..", "..", "config", "llm_config.yaml")
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config_data = yaml.safe_load(f)
                    
                    # ä½¿ç”¨ç»Ÿä¸€çš„LLMé…ç½®æ ¼å¼
                    default_provider = config_data.get('default_provider', 'gemini')
                    provider_config = config_data.get('llm_providers', {}).get(default_provider, {})
                    
                    llm_config = {
                        'provider': default_provider,
                        'model': provider_config.get('model', 'gemini-2.5-flash-lite-preview-06-17'),
                        'api_key': provider_config.get('api_key', ''),
                        'temperature': provider_config.get('temperature', 0.2),
                        'max_tokens': provider_config.get('max_tokens', 8192)
                    }
                    
                    if default_provider == 'gemini' and provider_config.get('api_base'):
                        llm_config['api_base'] = provider_config['api_base']
            else:
                logger.warning("âš ï¸ LLMé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                # ä½¿ç”¨é»˜è®¤é…ç½®
                llm_config = {
                    'provider': 'gemini',
                    'model': 'gemini-2.5-flash-lite-preview-06-17',
                    'temperature': 0.2
                }
            
            # åˆ›å»ºtool_managerå®ä¾‹
            tool_manager = UnifiedToolManager()
            
            # åˆ›å»ºLLMå®¢æˆ·ç«¯
            client = LLMClient(config=llm_config, tool_manager=tool_manager)
            logger.info("âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            return client
                
        except Exception as e:
            logger.error(f"âŒ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return None
    
    async def _load_trajectories_data(self, file_path: str) -> List[Dict]:
        """åŠ è½½è½¨è¿¹æ•°æ® - æ”¯æŒJSONLæ ¼å¼"""
        try:
            trajectories = []
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯JSONLæ–‡ä»¶
            if file_path.endswith('.jsonl'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            try:
                                trajectory = json.loads(line)
                                trajectories.append(trajectory)
                            except json.JSONDecodeError as e:
                                logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆçš„JSONLè¡Œ: {e}")
                                continue
            else:
                # å¤„ç†JSONæ ¼å¼
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if isinstance(data, dict) and 'trajectories' in data:
                    trajectories = data['trajectories']
                elif isinstance(data, list):
                    trajectories = data
                else:
                    logger.error("âŒ è½¨è¿¹æ–‡ä»¶æ ¼å¼æ— æ•ˆ")
                    return []
            
            # è¿‡æ»¤å‡ºæœªå¤„ç†çš„è½¨è¿¹
            new_trajectories = []
            for traj in trajectories:
                traj_id = traj.get('task_id', f"traj_{hash(str(traj))}")
                if traj_id not in self.processed_trajectories:
                    new_trajectories.append(traj)
            
            logger.info(f"ğŸ“Š åŠ è½½è½¨è¿¹æ•°æ®: æ€»è®¡{len(trajectories)}ä¸ªï¼Œæ–°å¢{len(new_trajectories)}ä¸ª")
            return new_trajectories
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½è½¨è¿¹æ•°æ®å¤±è´¥: {e}")
            return []
    
    async def _update_processed_trajectories(self, trajectory_ids: List[str]):
        """æ›´æ–°å·²å¤„ç†è½¨è¿¹è®°å½•"""
        try:
            # æ›´æ–°å†…å­˜è®°å½•
            self.processed_trajectories.update(trajectory_ids)
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            data = {
                "processed": list(self.processed_trajectories),
                "last_updated": datetime.now().isoformat(),
                "total_count": len(self.processed_trajectories)
            }
            
            with open(self.processed_trajectories_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
            logger.debug(f"ğŸ“ æ›´æ–°å·²å¤„ç†è½¨è¿¹è®°å½•: {len(trajectory_ids)} ä¸ªæ–°å¢")
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°å·²å¤„ç†è½¨è¿¹è®°å½•å¤±è´¥: {e}")
    
    async def _export_to_seed_tasks(self, synthesis_result):
        """å¯¼å‡ºä»»åŠ¡ä¸ºä¼ ç»Ÿçš„seed_tasks.jsonlæ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰"""
        try:
            from datetime import datetime
            
            # åˆå¹¶æ‰€æœ‰ä»»åŠ¡
            all_tasks = []
            
            # å¯¼å‡ºåŸå­ä»»åŠ¡
            for task in synthesis_result.atomic_tasks:
                seed_task = {
                    "task_id": task.task_id,
                    "question": task.question,
                    "expected_answer": task.answer.answer,
                    "task_type": task.task_type.value,
                    "domain": task.domain,
                    "requires_tool": task.requires_tool,
                    "expected_tools": task.expected_tools,
                    "complexity": "atomic",
                    "source": "synthesis_engine",
                    "created_at": task.created_at.isoformat()
                }
                all_tasks.append(seed_task)
            
            # å¯¼å‡ºæ·±åº¦æ‰©å±•ä»»åŠ¡
            for task in synthesis_result.depth_extended_tasks:
                seed_task = {
                    "task_id": task.task_id,
                    "question": task.combined_question,
                    "expected_answer": task.combined_answer,
                    "task_type": task.base_task.task_type.value,
                    "domain": task.base_task.domain,
                    "requires_tool": True,
                    "expected_tools": task.base_task.expected_tools,
                    "complexity": "depth_extended",
                    "base_task_id": task.base_task.task_id,
                    "source": "synthesis_engine",
                    "created_at": task.created_at.isoformat()
                }
                all_tasks.append(seed_task)
            
            # å¯¼å‡ºå®½åº¦æ‰©å±•ä»»åŠ¡
            for task in synthesis_result.width_extended_tasks:
                seed_task = {
                    "task_id": task.task_id,
                    "question": task.merged_question,
                    "expected_answer": task.merged_answer,
                    "task_type": "composite",
                    "domain": "multi_domain",
                    "requires_tool": True,
                    "expected_tools": list(set(tool for comp_task in task.component_tasks for tool in comp_task.expected_tools)),
                    "complexity": "width_extended",
                    "component_task_ids": [comp_task.task_id for comp_task in task.component_tasks],
                    "source": "synthesis_engine",
                    "created_at": task.created_at.isoformat()
                }
                all_tasks.append(seed_task)
            
            # è¿½åŠ å†™å…¥seed_tasks.jsonlæ–‡ä»¶
            with open(self.seed_tasks_file, 'a', encoding='utf-8') as f:
                for task in all_tasks:
                    f.write(json.dumps(task, ensure_ascii=False) + '\n')
            
            logger.info(f"ğŸ“„ å¯¼å‡º {len(all_tasks)} ä¸ªä»»åŠ¡åˆ° seed_tasks.jsonl")
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºseed_taskså¤±è´¥: {e}")
    
    def _load_processed_trajectories(self) -> set:
        """åŠ è½½å·²å¤„ç†è½¨è¿¹è®°å½•"""
        try:
            if os.path.exists(self.processed_trajectories_file):
                with open(self.processed_trajectories_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, dict) and 'processed' in data:
                        return set(data['processed'])
                    elif isinstance(data, list):
                        return set(data)
            return set()
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å·²å¤„ç†è½¨è¿¹è®°å½•å¤±è´¥: {e}")
            return set()
    
    async def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è¯»å–ç§å­ä»»åŠ¡æ–‡ä»¶ç»Ÿè®¡
            seed_count = 0
            if os.path.exists(self.seed_tasks_file):
                with open(self.seed_tasks_file, 'r', encoding='utf-8') as f:
                    seed_count = sum(1 for line in f if line.strip())
            
            return {
                "processed_trajectories": len(self.processed_trajectories),
                "total_seed_tasks": seed_count,
                "files": {
                    "trajectories_file": self.trajectories_collection_file,
                    "seed_tasks_file": self.seed_tasks_file,
                    "processed_record": self.processed_trajectories_file
                },
                "monitoring_status": self.observer.is_alive() if hasattr(self, 'observer') else False
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}


# å…¨å±€ç›‘æ§å™¨å®ä¾‹
_simple_monitor = None

def get_simple_monitor():
    """è·å–å…¨å±€ç®€åŒ–ç›‘æ§å™¨å®ä¾‹"""
    global _simple_monitor
    if _simple_monitor is None:
        _simple_monitor = SimpleTrajectoryMonitor()
    return _simple_monitor

async def initialize_simple_monitor():
    """åˆå§‹åŒ–å¹¶å¯åŠ¨ç®€åŒ–ç›‘æ§å™¨"""
    monitor = get_simple_monitor()
    await monitor.initialize()
    await monitor.start_monitoring()
    return monitor

async def stop_simple_monitor():
    """åœæ­¢ç®€åŒ–ç›‘æ§å™¨"""
    global _simple_monitor
    if _simple_monitor:
        await _simple_monitor.stop_monitoring()