#!/usr/bin/env python3
"""
Enhanced Synthesis Controller - Â¢ûÂº∫ÂêàÊàêÊéßÂà∂Âô®
ÈõÜÊàêÊâÄÊúâÊîπËøõÂäüËÉΩÁöÑÁªü‰∏ÄÊéßÂà∂Âô®ÔºåÊèê‰æõÂÆåÊï¥ÁöÑ‰ªªÂä°ÂêàÊàêÂíåÊâ©Â±ïËÉΩÂäõ
"""

import asyncio
import json
import logging
import time
from typing import Dict, List, Optional, Any, Callable
from dataclasses import asdict
from datetime import datetime

from core.interfaces import TrajectoryResult
from core.llm_client import LLMClient
from core.toolscore.mcp_client import MCPToolClient

# ÂØºÂÖ•ÊâÄÊúâÂ¢ûÂº∫ÁªÑ‰ª∂
from .enhanced_interfaces import (
    AtomicTask, ExtendedTask, CompositeTask, TaskVerificationMetrics,
    AdaptiveExtensionConfig, GenerationMetrics, TaskUnion
)
from .enhanced_synthesis_engine import EnhancedSynthesisEngine
from .depth_extender import DepthExtender
from .width_extender import WidthExtender
from .enhanced_verification_agent import EnhancedVerificationAgent, BatchVerificationProcessor
from .real_time_extension_trigger import RealTimeExtensionTrigger, RealTimeExtensionManager

logger = logging.getLogger(__name__)


class EnhancedSynthesisController:
    """Â¢ûÂº∫ÂêàÊàêÊéßÂà∂Âô® - Áªü‰∏ÄÁÆ°ÁêÜÊâÄÊúâÂêàÊàêÂíåÊâ©Â±ïÂäüËÉΩ"""
    
    def __init__(self, 
                 llm_client: LLMClient,
                 mcp_client: Optional[MCPToolClient] = None,
                 enable_real_time: bool = True):
        
        self.llm_client = llm_client
        self.mcp_client = mcp_client
        self.enable_real_time = enable_real_time
        
        # ÂàùÂßãÂåñÊ†∏ÂøÉÁªÑ‰ª∂
        self.synthesis_engine = EnhancedSynthesisEngine(llm_client, mcp_client)
        self.depth_extender = DepthExtender(llm_client, mcp_client)
        self.width_extender = WidthExtender(llm_client, mcp_client)
        self.verification_agent = EnhancedVerificationAgent(llm_client)
        self.batch_processor = BatchVerificationProcessor(self.verification_agent)
        
        # Ëá™ÈÄÇÂ∫îÈÖçÁΩÆ
        self.adaptive_config = AdaptiveExtensionConfig()
        
        # ÂÆûÊó∂Êâ©Â±ïÁªÑ‰ª∂ÔºàÂèØÈÄâÔºâ
        self.real_time_trigger = None
        self.real_time_manager = None
        if enable_real_time:
            self._initialize_real_time_components()
        
        # ÁªüËÆ°ÊåáÊ†á
        self.session_metrics = GenerationMetrics(session_id=f"enhanced_{int(time.time())}")
        
        # ‰ªªÂä°Â≠òÂÇ®ÔºàÂÜÖÂ≠ò‰∏≠ÁöÑÁºìÂ≠òÔºåÂÆûÈôÖÂ∫îËØ•ËøûÊé•Âà∞RedisÊàñÊï∞ÊçÆÂ∫ìÔºâ
        self.task_pool = {
            "atomic_tasks": [],
            "extended_tasks": [],
            "composite_tasks": []
        }
        
        # ÂõûË∞ÉÂáΩÊï∞Ê≥®ÂÜå
        self.callbacks = {
            "task_generated": [],
            "quality_report": [],
            "metrics_updated": []
        }
        
        logger.info("üöÄ Â¢ûÂº∫ÂêàÊàêÊéßÂà∂Âô®ÂàùÂßãÂåñÂÆåÊàê")
    
    def _initialize_real_time_components(self):
        """ÂàùÂßãÂåñÂÆûÊó∂Êâ©Â±ïÁªÑ‰ª∂"""
        try:
            self.real_time_trigger = RealTimeExtensionTrigger(
                self.llm_client, 
                self.mcp_client, 
                self.synthesis_engine
            )
            self.real_time_manager = RealTimeExtensionManager(self.real_time_trigger)
            
            # ËÆæÁΩÆÂõûË∞ÉÂáΩÊï∞
            self.real_time_trigger.set_task_generated_callback(self._on_real_time_task_generated)
            self.real_time_trigger.set_quality_report_callback(self._on_real_time_quality_report)
            
            logger.info("‚úÖ ÂÆûÊó∂Êâ©Â±ïÁªÑ‰ª∂ÂàùÂßãÂåñÂÆåÊàê")
            
        except Exception as e:
            logger.error(f"‚ùå ÂÆûÊó∂Êâ©Â±ïÁªÑ‰ª∂ÂàùÂßãÂåñÂ§±Ë¥•: {e}")
            self.enable_real_time = False
    
    async def start(self):
        """ÂêØÂä®ÊéßÂà∂Âô®"""
        logger.info("üöÄ ÂêØÂä®Â¢ûÂº∫ÂêàÊàêÊéßÂà∂Âô®")
        
        if self.enable_real_time and self.real_time_trigger:
            await self.real_time_trigger.start()
            logger.info("‚úÖ ÂÆûÊó∂Êâ©Â±ïÊúçÂä°Â∑≤ÂêØÂä®")
        
        logger.info("‚úÖ Â¢ûÂº∫ÂêàÊàêÊéßÂà∂Âô®ÂêØÂä®ÂÆåÊàê")
    
    async def stop(self):
        """ÂÅúÊ≠¢ÊéßÂà∂Âô®"""
        logger.info("üõë ÂÅúÊ≠¢Â¢ûÂº∫ÂêàÊàêÊéßÂà∂Âô®")
        
        if self.enable_real_time and self.real_time_trigger:
            await self.real_time_trigger.stop()
            
        if self.real_time_manager:
            self.real_time_manager.stop_monitoring()
        
        logger.info("‚úÖ Â¢ûÂº∫ÂêàÊàêÊéßÂà∂Âô®Â∑≤ÂÅúÊ≠¢")
    
    # Ê†∏ÂøÉÂêàÊàêÂäüËÉΩ
    async def process_trajectory(self, trajectory: TrajectoryResult) -> Dict[str, Any]:
        """Â§ÑÁêÜËΩ®ËøπÂπ∂ÁîüÊàê‰ªªÂä°"""
        logger.info(f"üì• Â§ÑÁêÜËΩ®Ëøπ: {trajectory.trajectory_id}")
        
        start_time = time.time()
        
        try:
            # 1. ‰ªéËΩ®Ëøπ‰∏≠ÊèêÂèñÁßçÂ≠ê‰ªªÂä°
            seed_tasks = await self.synthesis_engine.extract_atomic_tasks_from_trajectory(trajectory)
            
            if not seed_tasks:
                return {
                    "success": False,
                    "message": "Êú™ËÉΩ‰ªéËΩ®Ëøπ‰∏≠ÊèêÂèñÁßçÂ≠ê‰ªªÂä°",
                    "trajectory_id": trajectory.trajectory_id
                }
            
            self.session_metrics.atomic_tasks_generated += len(seed_tasks)
            logger.info(f"‚úÖ ÊèêÂèñÁßçÂ≠ê‰ªªÂä°: {len(seed_tasks)} ‰∏™")
            
            # 2. ÊâßË°å‰ªªÂä°Êâ©Â±ï
            extension_results = await self._perform_task_extension(seed_tasks)
            
            # 3. È™åËØÅÂíåÁ≠õÈÄâ‰ªªÂä°
            verification_results = await self._verify_and_filter_tasks(
                seed_tasks + extension_results["extended_tasks"] + extension_results["composite_tasks"]
            )
            
            # 4. Â≠òÂÇ®È´òË¥®Èáè‰ªªÂä°
            high_quality_tasks = verification_results["high_quality_tasks"]
            await self._store_tasks(high_quality_tasks)
            
            # 5. Êõ¥Êñ∞Ëá™ÈÄÇÂ∫îÈÖçÁΩÆ
            await self._update_adaptive_configuration(verification_results["verification_metrics"])
            
            # 6. Ëß¶ÂèëÂÆûÊó∂Êâ©Â±ïÔºàÂ¶ÇÊûúÂêØÁî®Ôºâ
            if self.enable_real_time and self.real_time_trigger:
                await self.real_time_trigger.on_trajectory_completed(trajectory)
            
            # 7. Êõ¥Êñ∞ÁªüËÆ°ÊåáÊ†á
            processing_time = time.time() - start_time
            self.session_metrics.processing_time_seconds += processing_time
            self.session_metrics.total_trajectories_processed += 1
            
            # 8. ÁîüÊàêÂ§ÑÁêÜÊä•Âëä
            report = self._generate_processing_report(
                trajectory, seed_tasks, extension_results, verification_results, processing_time
            )
            
            # 9. Ëß¶ÂèëÂõûË∞É
            self._trigger_callbacks("metrics_updated", self.session_metrics)
            
            logger.info(f"‚úÖ ËΩ®ËøπÂ§ÑÁêÜÂÆåÊàê: {trajectory.trajectory_id} (ËÄóÊó∂: {processing_time:.2f}Áßí)")
            return report
            
        except Exception as e:
            logger.error(f"‚ùå ËΩ®ËøπÂ§ÑÁêÜÂ§±Ë¥• {trajectory.trajectory_id}: {e}")
            return {
                "success": False,
                "error": str(e),
                "trajectory_id": trajectory.trajectory_id
            }
    
    async def _perform_task_extension(self, seed_tasks: List[AtomicTask]) -> Dict[str, Any]:
        """ÊâßË°å‰ªªÂä°Êâ©Â±ï"""
        logger.info(f"üîÑ ÂºÄÂßã‰ªªÂä°Êâ©Â±ï: {len(seed_tasks)} ‰∏™ÁßçÂ≠ê‰ªªÂä°")
        
        # Âπ∂Ë°åÊâßË°åÊ∑±Â∫¶ÂíåÂÆΩÂ∫¶Êâ©Â±ï
        depth_task = self.depth_extender.batch_extend_atomic_tasks(seed_tasks, self.adaptive_config)
        width_task = self.width_extender.extend_atomic_tasks_width(seed_tasks, self.adaptive_config)
        
        depth_results, width_results = await asyncio.gather(
            depth_task, width_task, return_exceptions=True
        )
        
        # Â§ÑÁêÜÁªìÊûú
        extended_tasks = depth_results if isinstance(depth_results, list) else []
        composite_tasks = width_results if isinstance(width_results, list) else []
        
        if isinstance(depth_results, Exception):
            logger.error(f"‚ùå Ê∑±Â∫¶Êâ©Â±ïÂ§±Ë¥•: {depth_results}")
        else:
            self.session_metrics.depth_extended_tasks += len(extended_tasks)
        
        if isinstance(width_results, Exception):
            logger.error(f"‚ùå ÂÆΩÂ∫¶Êâ©Â±ïÂ§±Ë¥•: {width_results}")
        else:
            self.session_metrics.width_extended_tasks += len(composite_tasks)
        
        logger.info(f"‚úÖ ‰ªªÂä°Êâ©Â±ïÂÆåÊàê: Ê∑±Â∫¶ {len(extended_tasks)}, ÂÆΩÂ∫¶ {len(composite_tasks)}")
        
        return {
            "extended_tasks": extended_tasks,
            "composite_tasks": composite_tasks
        }
    
    async def _verify_and_filter_tasks(self, all_tasks: List[TaskUnion]) -> Dict[str, Any]:
        """È™åËØÅÂíåÁ≠õÈÄâ‰ªªÂä°"""
        logger.info(f"üîç ÂºÄÂßãÈ™åËØÅ {len(all_tasks)} ‰∏™‰ªªÂä°")
        
        if not all_tasks:
            return {"high_quality_tasks": [], "verification_metrics": []}
        
        # ÊâπÈáèÈ™åËØÅ
        verification_metrics = await self.batch_processor.batch_verify_tasks(all_tasks)
        
        # Á≠õÈÄâÈ´òË¥®Èáè‰ªªÂä°
        high_quality_tasks = []
        for task, metrics in zip(all_tasks, verification_metrics):
            if metrics.verification_passed:
                high_quality_tasks.append(task)
                self.session_metrics.verification_passed += 1
            else:
                self.session_metrics.verification_failed += 1
        
        # ËÆ°ÁÆóÂπ≥ÂùáË¥®ÈáèÂàÜÊï∞
        if verification_metrics:
            total_score = sum(m.overall_score for m in verification_metrics)
            self.session_metrics.average_quality_score = total_score / len(verification_metrics)
        
        logger.info(f"‚úÖ ‰ªªÂä°È™åËØÅÂÆåÊàê: {len(high_quality_tasks)}/{len(all_tasks)} ÈÄöËøá")
        
        return {
            "high_quality_tasks": high_quality_tasks,
            "verification_metrics": verification_metrics
        }
    
    async def _store_tasks(self, tasks: List[TaskUnion]):
        """Â≠òÂÇ®‰ªªÂä°Âà∞‰ªªÂä°Ê±†"""
        for task in tasks:
            if isinstance(task, AtomicTask):
                self.task_pool["atomic_tasks"].append(task)
            elif isinstance(task, ExtendedTask):
                self.task_pool["extended_tasks"].append(task)
            elif isinstance(task, CompositeTask):
                self.task_pool["composite_tasks"].append(task)
        
        logger.info(f"üìù Â≠òÂÇ®‰ªªÂä°: {len(tasks)} ‰∏™")
        
        # Ëß¶Âèë‰ªªÂä°ÁîüÊàêÂõûË∞É
        self._trigger_callbacks("task_generated", tasks)
    
    async def _update_adaptive_configuration(self, verification_metrics: List[TaskVerificationMetrics]):
        """Êõ¥Êñ∞Ëá™ÈÄÇÂ∫îÈÖçÁΩÆ"""
        if not verification_metrics:
            return
        
        # ËÆ°ÁÆóÊàêÂäüÁéáÂíåÊïàÁéáÊåáÊ†á
        success_count = sum(1 for m in verification_metrics if m.verification_passed)
        success_rate = success_count / len(verification_metrics)
        
        # ËÆ∞ÂΩïÊàêÂäüÂéÜÂè≤
        for metrics in verification_metrics:
            self.adaptive_config.record_success(metrics.verification_passed)
        
        # ËÆ°ÁÆóÊïàÁéáÊåáÊ†á
        efficiency_metrics = {
            "verification_success_rate": success_rate,
            "average_quality_score": sum(m.overall_score for m in verification_metrics) / len(verification_metrics),
            "generation_efficiency": self.session_metrics.generation_efficiency
        }
        
        # Ë∞ÉÊï¥ÈòàÂÄº
        self.adaptive_config.adjust_thresholds(success_rate, efficiency_metrics)
        
        logger.debug(f"üîß Ëá™ÈÄÇÂ∫îÈÖçÁΩÆÊõ¥Êñ∞ÂÆåÊàê (ÊàêÂäüÁéá: {success_rate:.3f})")
    
    def _generate_processing_report(self, trajectory: TrajectoryResult, 
                                  seed_tasks: List[AtomicTask],
                                  extension_results: Dict[str, Any],
                                  verification_results: Dict[str, Any],
                                  processing_time: float) -> Dict[str, Any]:
        """ÁîüÊàêÂ§ÑÁêÜÊä•Âëä"""
        return {
            "success": True,
            "trajectory_id": trajectory.trajectory_id,
            "processing_time_seconds": processing_time,
            "task_generation": {
                "seed_tasks": len(seed_tasks),
                "extended_tasks": len(extension_results["extended_tasks"]),
                "composite_tasks": len(extension_results["composite_tasks"]),
                "total_generated": len(seed_tasks) + len(extension_results["extended_tasks"]) + len(extension_results["composite_tasks"])
            },
            "quality_assessment": {
                "total_verified": len(verification_results["verification_metrics"]),
                "high_quality_count": len(verification_results["high_quality_tasks"]),
                "pass_rate": len(verification_results["high_quality_tasks"]) / max(len(verification_results["verification_metrics"]), 1),
                "average_quality_score": sum(m.overall_score for m in verification_results["verification_metrics"]) / max(len(verification_results["verification_metrics"]), 1)
            },
            "adaptive_config_snapshot": {
                "current_success_rate": self.adaptive_config.get_current_success_rate(),
                "depth_threshold": self.adaptive_config.depth_config["superset_confidence_threshold"],
                "width_threshold": self.adaptive_config.width_config["semantic_similarity_threshold"],
                "batch_size": self.adaptive_config.batch_config["batch_size"]
            },
            "session_metrics": asdict(self.session_metrics)
        }
    
    # ÊâπÈáèÂ§ÑÁêÜÂäüËÉΩ
    async def batch_process_trajectories(self, trajectories: List[TrajectoryResult], 
                                       max_concurrent: int = 3) -> List[Dict[str, Any]]:
        """ÊâπÈáèÂ§ÑÁêÜËΩ®Ëøπ"""
        logger.info(f"üîÑ ÂºÄÂßãÊâπÈáèÂ§ÑÁêÜ {len(trajectories)} ‰∏™ËΩ®Ëøπ")
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_with_semaphore(trajectory):
            async with semaphore:
                return await self.process_trajectory(trajectory)
        
        results = await asyncio.gather(
            *[process_with_semaphore(t) for t in trajectories],
            return_exceptions=True
        )
        
        # Â§ÑÁêÜÁªìÊûú
        successful_results = []
        for result in results:
            if isinstance(result, dict) and result.get("success", False):
                successful_results.append(result)
            elif isinstance(result, Exception):
                logger.error(f"‚ùå ÊâπÈáèÂ§ÑÁêÜÂºÇÂ∏∏: {result}")
        
        logger.info(f"‚úÖ ÊâπÈáèÂ§ÑÁêÜÂÆåÊàê: {len(successful_results)}/{len(trajectories)} ÊàêÂäü")
        return successful_results
    
    # ÂõûË∞ÉÁ≥ªÁªü
    def register_callback(self, event_type: str, callback: Callable):
        """Ê≥®ÂÜåÂõûË∞ÉÂáΩÊï∞"""
        if event_type in self.callbacks:
            self.callbacks[event_type].append(callback)
            logger.info(f"‚úÖ Ê≥®ÂÜåÂõûË∞ÉÂáΩÊï∞: {event_type}")
        else:
            logger.warning(f"‚ö†Ô∏è Êú™Áü•ÁöÑ‰∫ã‰ª∂Á±ªÂûã: {event_type}")
    
    def _trigger_callbacks(self, event_type: str, data: Any):
        """Ëß¶ÂèëÂõûË∞ÉÂáΩÊï∞"""
        for callback in self.callbacks.get(event_type, []):
            try:
                callback(data)
            except Exception as e:
                logger.error(f"‚ùå ÂõûË∞ÉÂáΩÊï∞ÊâßË°åÂ§±Ë¥• ({event_type}): {e}")
    
    # ÂÆûÊó∂Êâ©Â±ïÂõûË∞ÉÂ§ÑÁêÜ
    def _on_real_time_task_generated(self, tasks: List[TaskUnion]):
        """ÂÆûÊó∂‰ªªÂä°ÁîüÊàêÂõûË∞É"""
        logger.info(f"üî• ÂÆûÊó∂ÁîüÊàê‰ªªÂä°: {len(tasks)} ‰∏™")
        
        # Â≠òÂÇ®Âà∞‰ªªÂä°Ê±†
        asyncio.create_task(self._store_tasks(tasks))
    
    def _on_real_time_quality_report(self, report: Dict[str, Any]):
        """ÂÆûÊó∂Ë¥®ÈáèÊä•ÂëäÂõûË∞É"""
        logger.info(f"üìä ÂÆûÊó∂Ë¥®ÈáèÊä•Âëä: ÈÄöËøáÁéá {report.get('processing_summary', {}).get('verification_pass_rate', 0):.3f}")
        
        # Ëß¶ÂèëË¥®ÈáèÊä•ÂëäÂõûË∞É
        self._trigger_callbacks("quality_report", report)
    
    # Áä∂ÊÄÅÊü•ËØ¢ÂíåÁõëÊéß
    def get_status(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÊéßÂà∂Âô®Áä∂ÊÄÅ"""
        status = {
            "controller_status": "running",
            "session_metrics": asdict(self.session_metrics),
            "adaptive_config": {
                "current_success_rate": self.adaptive_config.get_current_success_rate(),
                "depth_threshold": self.adaptive_config.depth_config["superset_confidence_threshold"],
                "width_threshold": self.adaptive_config.width_config["semantic_similarity_threshold"],
                "batch_size": self.adaptive_config.batch_config["batch_size"]
            },
            "task_pool_size": {
                "atomic_tasks": len(self.task_pool["atomic_tasks"]),
                "extended_tasks": len(self.task_pool["extended_tasks"]),
                "composite_tasks": len(self.task_pool["composite_tasks"]),
                "total": sum(len(pool) for pool in self.task_pool.values())
            },
            "real_time_enabled": self.enable_real_time
        }
        
        # Ê∑ªÂä†ÂÆûÊó∂Êâ©Â±ïÁä∂ÊÄÅ
        if self.enable_real_time and self.real_time_trigger:
            status["real_time_status"] = self.real_time_trigger.get_status()
        
        return status
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÊÄßËÉΩÊëòË¶Å"""
        summary = {
            "session_id": self.session_metrics.session_id,
            "total_trajectories_processed": self.session_metrics.total_trajectories_processed,
            "total_tasks_generated": self.session_metrics.total_tasks_generated,
            "generation_efficiency": self.session_metrics.generation_efficiency,
            "verification_pass_rate": self.session_metrics.verification_pass_rate,
            "average_quality_score": self.session_metrics.average_quality_score,
            "session_duration_seconds": time.time() - float(self.session_metrics.start_time.split("T")[1].replace(":", "").replace("-", "").replace("Z", "")[:6])
        }
        
        # Ê∑ªÂä†ÂÆûÊó∂Êâ©Â±ïÊÄßËÉΩ
        if self.enable_real_time and self.real_time_trigger:
            summary["real_time_performance"] = self.real_time_trigger.get_performance_summary()
        
        return summary
    
    def get_task_pool_statistics(self) -> Dict[str, Any]:
        """Ëé∑Âèñ‰ªªÂä°Ê±†ÁªüËÆ°"""
        stats = {}
        
        for task_type, tasks in self.task_pool.items():
            if not tasks:
                stats[task_type] = {"count": 0}
                continue
            
            # ËÆ°ÁÆóÁªüËÆ°‰ø°ÊÅØ
            quality_scores = []
            difficulty_distribution = {"simple": 0, "medium": 0, "complex": 0}
            
            for task in tasks:
                # ÊèêÂèñË¥®ÈáèÂàÜÊï∞ÔºàÂ¶ÇÊûúÊúâÁöÑËØùÔºâ
                if hasattr(task, 'verification_score'):
                    quality_scores.append(task.verification_score)
                
                # ÁªüËÆ°ÈöæÂ∫¶ÂàÜÂ∏É
                if hasattr(task, 'difficulty_level'):
                    difficulty_distribution[task.difficulty_level.value] += 1
            
            stats[task_type] = {
                "count": len(tasks),
                "average_quality": sum(quality_scores) / len(quality_scores) if quality_scores else 0.0,
                "difficulty_distribution": difficulty_distribution
            }
        
        return stats
    
    # ÈÖçÁΩÆÁÆ°ÁêÜ
    def update_adaptive_config(self, new_config: Dict[str, Any]):
        """Êõ¥Êñ∞Ëá™ÈÄÇÂ∫îÈÖçÁΩÆ"""
        try:
            if "depth_config" in new_config:
                self.adaptive_config.depth_config.update(new_config["depth_config"])
            
            if "width_config" in new_config:
                self.adaptive_config.width_config.update(new_config["width_config"])
            
            if "batch_config" in new_config:
                self.adaptive_config.batch_config.update(new_config["batch_config"])
            
            logger.info("‚úÖ Ëá™ÈÄÇÂ∫îÈÖçÁΩÆÊõ¥Êñ∞ÂÆåÊàê")
            
        except Exception as e:
            logger.error(f"‚ùå Êõ¥Êñ∞Ëá™ÈÄÇÂ∫îÈÖçÁΩÆÂ§±Ë¥•: {e}")
    
    def export_configuration(self) -> Dict[str, Any]:
        """ÂØºÂá∫ÂΩìÂâçÈÖçÁΩÆ"""
        return {
            "adaptive_config": {
                "depth_config": self.adaptive_config.depth_config.copy(),
                "width_config": self.adaptive_config.width_config.copy(),
                "batch_config": self.adaptive_config.batch_config.copy()
            },
            "session_metrics": asdict(self.session_metrics),
            "controller_settings": {
                "enable_real_time": self.enable_real_time,
                "task_pool_limits": {
                    "max_atomic_tasks": 1000,
                    "max_extended_tasks": 500,
                    "max_composite_tasks": 200
                }
            }
        }