#!/usr/bin/env python3
"""
Corpus Ingestor - è¯­æ–™å¯¼å…¥å™¨
åŸºäºTaskCraftç®—æ³•ï¼Œå®ç°ä¸»åŠ¨è¯­æ–™é‡‡æ ·å’Œè½¨è¿¹å¤„ç†
"""

import asyncio
import json
import logging
import os
import re
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse

from core.interfaces import TrajectoryResult, ExecutionStep, ActionType
from core.toolscore.mcp_client import MCPToolClient
from .enhanced_interfaces import CorpusContent, ContentType, EnhancedSynthesisConfig

logger = logging.getLogger(__name__)


class TrajectoryCorpusExtractor:
    """è½¨è¿¹è¯­æ–™æå–å™¨"""
    
    def __init__(self):
        self.config = EnhancedSynthesisConfig()
    
    async def extract_from_trajectories(self, trajectories: List[TrajectoryResult]) -> List[CorpusContent]:
        """ä»è½¨è¿¹ä¸­æå–åŸå­è¯­æ–™"""
        corpus_contents = []
        
        for trajectory in trajectories:
            try:
                # æå–è½¨è¿¹çº§åˆ«çš„è¯­æ–™
                trajectory_corpus = await self._extract_trajectory_corpus(trajectory)
                if trajectory_corpus:
                    corpus_contents.append(trajectory_corpus)
                
                # æå–æ­¥éª¤çº§åˆ«çš„è¯­æ–™
                step_corpus_list = await self._extract_step_corpus(trajectory)
                corpus_contents.extend(step_corpus_list)
                
            except Exception as e:
                # å¢å¼ºæ—¥å¿—è®°å½•ï¼ŒåŒ…å«å¯¼è‡´é”™è¯¯çš„è½¨è¿¹å¯¹è±¡
                logger.error(f"âŒ æå–è½¨è¿¹è¯­æ–™å¤±è´¥ {trajectory.task_id}: {e}")
                try:
                    # å°è¯•å°†è½¨è¿¹å¯¹è±¡åºåˆ—åŒ–ä¸ºJSONè¿›è¡Œè®°å½•
                    trajectory_dump = json.dumps(trajectory.to_dict() if hasattr(trajectory, 'to_dict') else trajectory.__dict__, indent=2, ensure_ascii=False)
                    logger.error(f" problematic_trajectory: {trajectory_dump}")
                except Exception as dump_error:
                    logger.error(f"æ— æ³•åºåˆ—åŒ–è½¨è¿¹å¯¹è±¡: {dump_error}")
                continue
        
        logger.info(f"âœ… ä» {len(trajectories)} ä¸ªè½¨è¿¹ä¸­æå–äº† {len(corpus_contents)} ä¸ªè¯­æ–™")
        return corpus_contents
    
    async def _extract_trajectory_corpus(self, trajectory: TrajectoryResult) -> Optional[CorpusContent]:
        """æå–è½¨è¿¹çº§åˆ«çš„è¯­æ–™"""
        if not trajectory.final_result or len(trajectory.final_result.strip()) < 30:
            return None
        
        return CorpusContent(
            source=f"trajectory_{trajectory.task_id}",
            content_type=ContentType.TRAJECTORY,
            text_content=trajectory.final_result,
            metadata={
                "task_id": trajectory.task_id,
                "task_description": trajectory.task_description,
                "runtime_id": trajectory.runtime_id,
                "success": trajectory.success,
                "total_duration": trajectory.total_duration,
                "steps_count": len(trajectory.steps)
            }
        )
    
    async def _extract_step_corpus(self, trajectory: TrajectoryResult) -> List[CorpusContent]:
        """æå–æ­¥éª¤çº§åˆ«çš„è¯­æ–™"""
        corpus_contents = []
        
        for step in trajectory.steps:
            try:
                if step.action_type == ActionType.TOOL_CALL:
                    tool_id = step.action_params.get('tool_id', '')
                    extracted_content = None
                    
                    if 'browser' in tool_id.lower():
                        # ä»æµè§ˆå™¨å·¥å…·çš„è¾“å‡ºä¸­æå–ç½‘é¡µå†…å®¹
                        extracted_content = await self._extract_web_content(step)
                    
                    elif 'python' in tool_id.lower() or 'code' in tool_id.lower():
                        # ä»ä»£ç æ‰§è¡Œç»“æœä¸­æå–æ•°æ®
                        extracted_content = await self._extract_code_results(step)
                    
                    elif 'search' in tool_id.lower():
                        # ä»æœç´¢ç»“æœä¸­æå–å†…å®¹
                        extracted_content = await self._extract_search_results(step)
                    
                    # å¦‚æœæ²¡æœ‰åŒ¹é…ç‰¹å®šå·¥å…·ç±»å‹ï¼Œå°è¯•é€šç”¨æå–
                    if not extracted_content and step.observation and len(step.observation.strip()) > 30:
                        extracted_content = await self._extract_generic_tool_output(step)
                    
                    if extracted_content:
                        corpus_contents.append(extracted_content)
                
            except Exception as e:
                logger.warning(f"âš ï¸ æå–æ­¥éª¤è¯­æ–™å¤±è´¥ {trajectory.task_id}#{step.step_id}: {e}")
                try:
                    # å°è¯•å°†æ­¥éª¤å¯¹è±¡åºåˆ—åŒ–ä¸ºJSONè¿›è¡Œè®°å½•
                    step_dump = json.dumps(step.to_dict() if hasattr(step, 'to_dict') else step.__dict__, indent=2, ensure_ascii=False)
                    logger.warning(f"  problematic_step: {step_dump}")
                except Exception as dump_error:
                    logger.warning(f"  æ— æ³•åºåˆ—åŒ–æ­¥éª¤å¯¹è±¡: {dump_error}")
                continue
        
        return corpus_contents
    
    async def _extract_web_content(self, step: ExecutionStep) -> Optional[CorpusContent]:
        """æå–ç½‘é¡µå†…å®¹"""
        if not step.observation or len(step.observation.strip()) < 50:
            return None
        
        # æ¸…ç†å’Œç»“æ„åŒ–ç½‘é¡µå†…å®¹
        cleaned_content = self._clean_web_content(step.observation)
        if len(cleaned_content) < 100:
            return None
        
        return CorpusContent(
            source=f"web_step_{step.step_id}",
            content_type=ContentType.WEB,
            text_content=cleaned_content,
            metadata={
                "url": step.action_params.get('url', ''),
                "action": step.action_params.get('action', ''),
                "step_id": step.step_id,
                "success": step.success,
                "thinking": step.thinking
            }
        )
    
    async def _extract_code_results(self, step: ExecutionStep) -> Optional[CorpusContent]:
        """æå–ä»£ç æ‰§è¡Œç»“æœ"""
        if not step.observation or not step.success:
            return None
        
        # æå–æœ‰ä»·å€¼çš„ä»£ç è¾“å‡º
        code_output = self._extract_valuable_code_output(step.observation)
        if not code_output:
            return None
        
        return CorpusContent(
            source=f"code_step_{step.step_id}",
            content_type=ContentType.CODE_OUTPUT,
            text_content=code_output,
            metadata={
                "code": step.action_params.get('code', ''),
                "execution_time": step.duration,
                "step_id": step.step_id,
                "thinking": step.thinking
            }
        )
    
    async def _extract_search_results(self, step: ExecutionStep) -> Optional[CorpusContent]:
        """æå–æœç´¢ç»“æœ"""
        if not step.observation:
            return None
        
        try:
            observation = step.observation
            
            # å¤„ç†"å·¥å…·æ‰§è¡ŒæˆåŠŸ: "å‰ç¼€
            if observation.startswith("å·¥å…·æ‰§è¡ŒæˆåŠŸ: "):
                observation = observation[len("å·¥å…·æ‰§è¡ŒæˆåŠŸ: "):]
            
            # å¤„ç†å•å¼•å·æ ¼å¼ï¼ˆè½¬æ¢ä¸ºæœ‰æ•ˆJSONï¼‰
            if observation.strip().startswith("{") and "'" in observation:
                observation = observation.replace("'", '"')
            
            # å°è¯•è§£ææœç´¢ç»“æœJSON
            search_data = json.loads(observation) if isinstance(observation, str) else observation
            
            if not isinstance(search_data, dict):
                return None
            
            # æå–æœ‰ä»·å€¼å†…å®¹ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
            valuable_content = None
            
            # å°è¯•ä»'answer'å­—æ®µæå–ï¼ˆæ–°æ ¼å¼ï¼‰
            if 'answer' in search_data and search_data['answer']:
                valuable_content = search_data['answer']
            
            # å°è¯•ä»'results'å­—æ®µæå–ï¼ˆæ—§æ ¼å¼ï¼‰
            elif 'results' in search_data:
                valuable_content = self._extract_search_valuable_content(search_data)
            
            # å°è¯•ä»'search_results'å­—æ®µæå–
            elif 'search_results' in search_data:
                valuable_content = str(search_data['search_results'])[:1000]
            
            if not valuable_content or len(valuable_content.strip()) < 30:
                return None
            
            return CorpusContent(
                source=f"search_step_{step.step_id}",
                content_type=ContentType.WEB,
                text_content=valuable_content,
                metadata={
                    "query": step.action_params.get('query', ''),
                    "step_id": step.step_id,
                    "thinking": step.thinking
                }
            )
        except (json.JSONDecodeError, Exception) as e:
            logger.debug(f"æœç´¢ç»“æœè§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥æå–: {e}")
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨è§‚å¯Ÿç»“æœ
            if len(step.observation.strip()) > 50:
                return CorpusContent(
                    source=f"search_step_{step.step_id}",
                    content_type=ContentType.WEB,
                    text_content=step.observation[:1000],
                    metadata={
                        "query": step.action_params.get('query', ''),
                        "step_id": step.step_id,
                        "thinking": step.thinking,
                        "extraction_method": "direct"
                    }
                )
            return None
    
    def _clean_web_content(self, raw_content: str) -> str:
        """æ¸…ç†ç½‘é¡µå†…å®¹"""
        # ç§»é™¤HTMLæ ‡ç­¾
        content = re.sub(r'<[^>]+>', '', raw_content)
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½
        content = re.sub(r'\s+', ' ', content)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        content = re.sub(r'[^\w\s\.\,\!\?\;\:\-\(\)]', '', content)
        
        return content.strip()
    
    def _extract_valuable_code_output(self, output: str) -> Optional[str]:
        """æå–æœ‰ä»·å€¼çš„ä»£ç è¾“å‡º"""
        # åŸºæœ¬é•¿åº¦æ£€æŸ¥
        if len(output.strip()) < 30:
            return None
        
        # åªæœ‰åœ¨è¾“å‡ºä¸»è¦æ˜¯é”™è¯¯ä¿¡æ¯æ—¶æ‰è¿‡æ»¤ï¼ˆè€Œä¸æ˜¯åŒ…å«é”™è¯¯å…³é”®è¯ï¼‰
        error_indicators = ['traceback', 'exception occurred', 'error:']
        if any(indicator in output.lower() for indicator in error_indicators):
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰ç”¨çš„ä¿¡æ¯ä¸é”™è¯¯ä¿¡æ¯æ··åˆ
            if len([line for line in output.split('\n') if line.strip() and not any(err in line.lower() for err in error_indicators)]) < 3:
                return None
        
        # æå–æ•°å€¼ç»“æœã€è¡¨æ ¼æ•°æ®ç­‰
        valuable_patterns = [
            r'\d+\.\d+',  # æµ®ç‚¹æ•°
            r'\d+',       # æ•´æ•°
            r'[A-Z][a-z]+:\s*\d+',  # æ ‡ç­¾:æ•°å€¼
            r'\w+\s*\|\s*\w+',      # è¡¨æ ¼æ ¼å¼
        ]
        
        valuable_parts = []
        for pattern in valuable_patterns:
            matches = re.findall(pattern, output)
            valuable_parts.extend(matches)
        
        if valuable_parts:
            return ' '.join(valuable_parts[:20])  # é™åˆ¶æå–æ•°é‡
        
        # è¿”å›æœ‰æ„ä¹‰çš„å†…å®¹ï¼ˆæ‰©å¤§é•¿åº¦é™åˆ¶ï¼‰
        return output[:1000] if len(output) > 50 else output
    
    def _extract_search_valuable_content(self, search_data: dict) -> Optional[str]:
        """æå–æœç´¢ç»“æœä¸­çš„æœ‰ä»·å€¼å†…å®¹"""
        valuable_content = []
        
        for result in search_data.get('results', [])[:5]:  # åªå–å‰5ä¸ªç»“æœ
            title = result.get('title', '')
            snippet = result.get('snippet', '')
            
            if title:
                valuable_content.append(f"æ ‡é¢˜: {title}")
            if snippet:
                valuable_content.append(f"æ‘˜è¦: {snippet}")
        
        return '\n'.join(valuable_content) if valuable_content else None
    
    async def _extract_generic_tool_output(self, step: ExecutionStep) -> Optional[CorpusContent]:
        """æå–é€šç”¨å·¥å…·è¾“å‡º"""
        if not step.observation or len(step.observation.strip()) < 30:
            return None
        
        # æ¸…ç†è¾“å‡ºå†…å®¹
        cleaned_output = step.observation.strip()
        
        # å¤„ç†"å·¥å…·æ‰§è¡ŒæˆåŠŸ: "å‰ç¼€
        if cleaned_output.startswith("å·¥å…·æ‰§è¡ŒæˆåŠŸ: "):
            cleaned_output = cleaned_output[len("å·¥å…·æ‰§è¡ŒæˆåŠŸ: "):]
        
        # é™åˆ¶é•¿åº¦
        if len(cleaned_output) > 2000:
            cleaned_output = cleaned_output[:2000] + "..."
        
        return CorpusContent(
            source=f"generic_tool_step_{step.step_id}",
            content_type=ContentType.CODE_OUTPUT,
            text_content=cleaned_output,
            metadata={
                "tool_id": step.action_params.get('tool_id', 'unknown'),
                "step_id": step.step_id,
                "thinking": step.thinking,
                "extraction_method": "generic"
            }
        )


class ExternalCorpusLoader:
    """å¤–éƒ¨è¯­æ–™åŠ è½½å™¨"""
    
    def __init__(self, mcp_client: Optional[MCPToolClient] = None):
        self.mcp_client = mcp_client
        self.config = EnhancedSynthesisConfig()
    
    async def active_corpus_sampling(self, domains: List[str]) -> List[CorpusContent]:
        """ä¸»åŠ¨è¯­æ–™é‡‡æ ·"""
        if not self.mcp_client:
            logger.warning("âš ï¸ MCPå®¢æˆ·ç«¯æœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œä¸»åŠ¨è¯­æ–™é‡‡æ ·")
            return []
        
        corpus_contents = []
        
        for domain in domains:
            try:
                logger.info(f"ğŸ” å¼€å§‹é‡‡æ ·é¢†åŸŸ: {domain}")
                
                # ç”Ÿæˆé¢†åŸŸç›¸å…³çš„æœç´¢æŸ¥è¯¢
                search_queries = await self._generate_domain_queries(domain)
                
                for query in search_queries:
                    # ä½¿ç”¨æœç´¢å·¥å…·è·å–å†…å®¹
                    search_results = await self._search_domain_content(query)
                    
                    # å¤„ç†æœç´¢ç»“æœ
                    for result in search_results:
                        content = await self._fetch_content_from_url(result['url'])
                        if content:
                            corpus_content = CorpusContent(
                                source=f"active_sampling_{domain}",
                                content_type=ContentType.WEB,
                                text_content=content,
                                metadata={
                                    "domain": domain,
                                    "search_query": query,
                                    "url": result['url'],
                                    "title": result.get('title', ''),
                                    "snippet": result.get('snippet', '')
                                }
                            )
                            corpus_contents.append(corpus_content)
                
                logger.info(f"âœ… é¢†åŸŸ {domain} é‡‡æ ·å®Œæˆï¼Œè·å¾— {len([c for c in corpus_contents if c.metadata.get('domain') == domain])} ä¸ªè¯­æ–™")
                
            except Exception as e:
                logger.error(f"âŒ é¢†åŸŸ {domain} é‡‡æ ·å¤±è´¥: {e}")
                continue
        
        return corpus_contents
    
    async def _generate_domain_queries(self, domain: str) -> List[str]:
        """ç”Ÿæˆé¢†åŸŸç›¸å…³çš„æœç´¢æŸ¥è¯¢"""
        domain_query_templates = {
            "algorithm": [
                "ç®—æ³•å®ç°æ•™ç¨‹",
                "æ•°æ®ç»“æ„ä¸ç®—æ³•",
                "ç¼–ç¨‹ç®—æ³•é¢˜è§£",
                "ç®—æ³•å¤æ‚åº¦åˆ†æ"
            ],
            "data_analysis": [
                "æ•°æ®åˆ†ææ–¹æ³•",
                "Pythonæ•°æ®åˆ†æ",
                "ç»Ÿè®¡åˆ†æå®ä¾‹",
                "æ•°æ®å¯è§†åŒ–æ•™ç¨‹"
            ],
            "web_automation": [
                "ç½‘é¡µè‡ªåŠ¨åŒ–å·¥å…·",
                "çˆ¬è™«æŠ€æœ¯æ•™ç¨‹",
                "æµè§ˆå™¨è‡ªåŠ¨åŒ–",
                "Webæµ‹è¯•è‡ªåŠ¨åŒ–"
            ],
            "research": [
                "å­¦æœ¯ç ”ç©¶æ–¹æ³•",
                "è®ºæ–‡å†™ä½œæŒ‡å—",
                "ç ”ç©¶æ•°æ®æ”¶é›†",
                "æ–‡çŒ®ç»¼è¿°æ–¹æ³•"
            ],
            "machine_learning": [
                "æœºå™¨å­¦ä¹ ç®—æ³•",
                "æ·±åº¦å­¦ä¹ æ•™ç¨‹",
                "AIæ¨¡å‹è®­ç»ƒ",
                "ç¥ç»ç½‘ç»œå®ç°"
            ]
        }
        
        return domain_query_templates.get(domain, [f"{domain} æ•™ç¨‹", f"{domain} å®ä¾‹"])
    
    async def _search_domain_content(self, query: str) -> List[Dict]:
        """æœç´¢é¢†åŸŸå†…å®¹"""
        try:
            search_result = await self.mcp_client.call_tool("deepsearch", {
                "query": query,
                "max_results": 5
            })
            
            if search_result and 'results' in search_result:
                return search_result['results']
                
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥ '{query}': {e}")
        
        return []
    
    async def _fetch_content_from_url(self, url: str) -> Optional[str]:
        """ä»URLè·å–å†…å®¹"""
        try:
            content_result = await self.mcp_client.call_tool("browser_navigator", {
                "action": "navigate",
                "url": url
            })
            
            if content_result and 'page_text' in content_result:
                page_text = content_result['page_text']
                # æ¸…ç†å’Œé™åˆ¶å†…å®¹é•¿åº¦
                cleaned_text = self._clean_web_content(page_text)
                return cleaned_text[:2000] if len(cleaned_text) > 2000 else cleaned_text
                
        except Exception as e:
            logger.error(f"âŒ è·å–URLå†…å®¹å¤±è´¥ {url}: {e}")
        
        return None
    
    def _clean_web_content(self, content: str) -> str:
        """æ¸…ç†ç½‘é¡µå†…å®¹ï¼ˆå¤ç”¨TrajectoryCorpusExtractorçš„æ–¹æ³•ï¼‰"""
        extractor = TrajectoryCorpusExtractor()
        return extractor._clean_web_content(content)


class ContentProcessor:
    """å†…å®¹é¢„å¤„ç†å™¨"""
    
    def __init__(self):
        self.config = EnhancedSynthesisConfig()
    
    async def preprocess_corpus_batch(self, corpus_contents: List[CorpusContent]) -> List[CorpusContent]:
        """æ‰¹é‡é¢„å¤„ç†è¯­æ–™"""
        processed_contents = []
        
        for corpus in corpus_contents:
            try:
                processed_corpus = await self._preprocess_single_corpus(corpus)
                if processed_corpus:
                    processed_contents.append(processed_corpus)
            except Exception as e:
                logger.error(f"âŒ é¢„å¤„ç†è¯­æ–™å¤±è´¥ {corpus.corpus_id}: {e}")
                continue
        
        logger.info(f"âœ… é¢„å¤„ç†å®Œæˆ: {len(processed_contents)}/{len(corpus_contents)} ä¸ªè¯­æ–™")
        return processed_contents
    
    async def _preprocess_single_corpus(self, corpus: CorpusContent) -> Optional[CorpusContent]:
        """é¢„å¤„ç†å•ä¸ªè¯­æ–™"""
        # 1. å†…å®¹é•¿åº¦æ£€æŸ¥
        if len(corpus.text_content.strip()) < 50:
            logger.debug(f"â© è·³è¿‡è¿‡çŸ­å†…å®¹: {corpus.corpus_id}")
            return None
        
        # 2. å†…å®¹è´¨é‡æ£€æŸ¥
        if not self._is_quality_content(corpus.text_content):
            logger.debug(f"â© è·³è¿‡ä½è´¨é‡å†…å®¹: {corpus.corpus_id}")
            return None
        
        # 3. å†…å®¹æ¸…ç†å’Œæ ‡å‡†åŒ–
        cleaned_content = self._clean_and_normalize_content(corpus.text_content)
        
        # 4. æ›´æ–°è¯­æ–™å†…å®¹
        corpus.text_content = cleaned_content
        corpus.processing_status = "completed"
        
        # 5. å¢å¼ºå…ƒæ•°æ®
        corpus.metadata.update({
            "content_length": len(cleaned_content),
            "estimated_reading_time": len(cleaned_content) // 200,  # å‡è®¾æ¯åˆ†é’Ÿ200å­—
            "content_quality_score": self._calculate_content_quality_score(cleaned_content)
        })
        
        return corpus
    
    def _is_quality_content(self, content: str) -> bool:
        """æ£€æŸ¥å†…å®¹è´¨é‡"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤ªå¤šé‡å¤å†…å®¹ï¼ˆæ”¾å®½æ ‡å‡†ï¼‰
        words = content.split()
        if len(words) > 10 and len(set(words)) / len(words) < 0.2:  # è¯æ±‡å¤šæ ·æ€§ä½äº20%ï¼Œä¸”ä»…å¯¹è¾ƒé•¿å†…å®¹æ£€æŸ¥
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„ä¿¡æ¯
        meaningful_patterns = [
            r'\d+',           # æ•°å­—
            r'[A-Z][a-z]+',   # ä¸“æœ‰åè¯
            r'https?://',     # URL
            r'\w+@\w+\.\w+',  # é‚®ç®±
        ]
        
        meaningful_count = 0
        for pattern in meaningful_patterns:
            if re.search(pattern, content):
                meaningful_count += 1
        
        return meaningful_count >= 2  # è‡³å°‘åŒ…å«2ç§æœ‰æ„ä¹‰çš„ä¿¡æ¯
    
    def _clean_and_normalize_content(self, content: str) -> str:
        """æ¸…ç†å’Œæ ‡å‡†åŒ–å†…å®¹"""
        # 1. ç§»é™¤å¤šä½™çš„ç©ºç™½
        content = re.sub(r'\s+', ' ', content)
        
        # 2. æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·
        content = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]', lambda m: {'ï¼Œ': ',', 'ã€‚': '.', 'ï¼': '!', 'ï¼Ÿ': '?', 'ï¼›': ';', 'ï¼š': ':'}[m.group()], content)
        
        # 3. ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™åŸºæœ¬æ ‡ç‚¹
        content = re.sub(r'[^\w\s\.\,\!\?\;\:\-\(\)\[\]\"\'\/]', '', content)
        
        # 4. é¦–å°¾å»ç©ºæ ¼
        return content.strip()
    
    def _calculate_content_quality_score(self, content: str) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°"""
        score = 0.0
        
        # é•¿åº¦åˆ†æ•° (0.3æƒé‡)
        length_score = min(len(content) / 1000, 1.0) * 0.3
        score += length_score
        
        # è¯æ±‡å¤šæ ·æ€§åˆ†æ•° (0.3æƒé‡)
        words = content.split()
        if words:
            diversity_score = len(set(words)) / len(words) * 0.3
            score += diversity_score
        
        # ä¿¡æ¯å¯†åº¦åˆ†æ•° (0.4æƒé‡)
        info_patterns = [
            r'\d+\.\d+',      # å°æ•°
            r'\d+%',          # ç™¾åˆ†æ¯”
            r'\d{4}',         # å¹´ä»½
            r'[A-Z][a-z]+',   # ä¸“æœ‰åè¯
        ]
        
        info_count = 0
        for pattern in info_patterns:
            info_count += len(re.findall(pattern, content))
        
        info_density = min(info_count / 10, 1.0) * 0.4
        score += info_density
        
        return score


class CorpusIngestor:
    """ç»Ÿä¸€è¯­æ–™å¯¼å…¥å™¨"""
    
    def __init__(self, mcp_client: Optional[MCPToolClient] = None):
        self.trajectory_extractor = TrajectoryCorpusExtractor()
        self.external_loader = ExternalCorpusLoader(mcp_client)
        self.content_processor = ContentProcessor()
        self.config = EnhancedSynthesisConfig()
    
    async def ingest_from_trajectories(self, trajectories: List[TrajectoryResult]) -> List[CorpusContent]:
        """ä»è½¨è¿¹ä¸­å¯¼å…¥è¯­æ–™"""
        logger.info(f"ğŸ”„ å¼€å§‹ä» {len(trajectories)} ä¸ªè½¨è¿¹ä¸­æå–è¯­æ–™")
        
        # 1. æå–åŸå§‹è¯­æ–™
        raw_corpus = await self.trajectory_extractor.extract_from_trajectories(trajectories)
        
        # 2. é¢„å¤„ç†è¯­æ–™
        processed_corpus = await self.content_processor.preprocess_corpus_batch(raw_corpus)
        
        logger.info(f"âœ… è½¨è¿¹è¯­æ–™å¯¼å…¥å®Œæˆ: {len(processed_corpus)} ä¸ªé«˜è´¨é‡è¯­æ–™")
        return processed_corpus
    
    async def ingest_external_corpus(self, domains: List[str]) -> List[CorpusContent]:
        """å¯¼å…¥å¤–éƒ¨è¯­æ–™"""
        logger.info(f"ğŸ”„ å¼€å§‹ä¸»åŠ¨é‡‡æ ·å¤–éƒ¨è¯­æ–™: {domains}")
        
        # 1. ä¸»åŠ¨é‡‡æ ·
        raw_corpus = await self.external_loader.active_corpus_sampling(domains)
        
        # 2. é¢„å¤„ç†è¯­æ–™
        processed_corpus = await self.content_processor.preprocess_corpus_batch(raw_corpus)
        
        logger.info(f"âœ… å¤–éƒ¨è¯­æ–™å¯¼å…¥å®Œæˆ: {len(processed_corpus)} ä¸ªé«˜è´¨é‡è¯­æ–™")
        return processed_corpus
    
    async def ingest_mixed_corpus(self, trajectories: List[TrajectoryResult], domains: List[str]) -> List[CorpusContent]:
        """æ··åˆå¯¼å…¥è¯­æ–™"""
        logger.info(f"ğŸ”„ å¼€å§‹æ··åˆè¯­æ–™å¯¼å…¥: {len(trajectories)} ä¸ªè½¨è¿¹ + {len(domains)} ä¸ªå¤–éƒ¨é¢†åŸŸ")
        
        # å¹¶è¡Œå¤„ç†è½¨è¿¹å’Œå¤–éƒ¨è¯­æ–™
        tasks = [
            self.ingest_from_trajectories(trajectories),
            self.ingest_external_corpus(domains)
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_corpus = []
        for result in results:
            if isinstance(result, list):
                all_corpus.extend(result)
            else:
                logger.error(f"âŒ è¯­æ–™å¯¼å…¥ä»»åŠ¡å¤±è´¥: {result}")
        
        logger.info(f"âœ… æ··åˆè¯­æ–™å¯¼å…¥å®Œæˆ: æ€»è®¡ {len(all_corpus)} ä¸ªé«˜è´¨é‡è¯­æ–™")
        return all_corpus