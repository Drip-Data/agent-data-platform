#!/usr/bin/env python3
"""
TaskCraft ä»»åŠ¡å­˜å‚¨ç®¡ç†å™¨
ç»Ÿä¸€ç®¡ç†ä¸åŒç±»å‹ä»»åŠ¡çš„å­˜å‚¨å’Œæ£€ç´¢
"""

import asyncio
import json
import logging
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import aiofiles
from dataclasses import asdict

from .interfaces import (
    AtomicTask, DepthExtendedTask, WidthExtendedTask, TaskUnion,
    TaskValidationResult, SynthesisResult, TaskType, TaskComplexity
)

logger = logging.getLogger(__name__)


class TaskStorage:
    """
    Synthesis ä»»åŠ¡å­˜å‚¨ç®¡ç†å™¨
    
    åŠŸèƒ½ï¼š
    1. ç®€åŒ–å­˜å‚¨ï¼šä»…ä¿ç•™ä¸¤ä¸ªæ–‡ä»¶ - åŸå­ä»»åŠ¡å’Œç»¼åˆä»»åŠ¡
    2. åŸå­ä»»åŠ¡ï¼šåŸºç¡€å•å…ƒä»»åŠ¡
    3. ç»¼åˆä»»åŠ¡ï¼šåŸºäºåŸå­ä»»åŠ¡åº“æ‰©å±•çš„å¤åˆä»»åŠ¡ï¼ˆæ·±åº¦+å®½åº¦ï¼‰
    4. éªŒè¯å­˜å‚¨ï¼šä¿å­˜éªŒè¯ç»“æœ
    5. ç»Ÿä¸€æ¥å£ï¼šæä¾›ç»Ÿä¸€çš„å­˜å‚¨å’Œæ£€ç´¢æ¥å£
    """
    
    def __init__(self, storage_dir: str = "output/SynthesisTask"):
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        # ç®€åŒ–å­˜å‚¨æ–‡ä»¶è·¯å¾„ - åªä¿ç•™ä¸¤ä¸ªæ ¸å¿ƒæ–‡ä»¶
        self.files = {
            # åŸå­ä»»åŠ¡å­˜å‚¨
            "atomic_tasks": self.storage_dir / "atomic_tasks.jsonl",
            
            # ç»¼åˆä»»åŠ¡å­˜å‚¨ï¼ˆæ·±åº¦æ‰©å±•+å®½åº¦æ‰©å±•ï¼‰
            "composite_tasks": self.storage_dir / "composite_tasks.jsonl"
        }
        
        # åˆå§‹åŒ–æ–‡ä»¶
        self._initialize_storage_files()
        
        logger.info(f"âœ… TaskStorage åˆå§‹åŒ–å®Œæˆï¼Œå­˜å‚¨ç›®å½•: {self.storage_dir}")
    
    def _initialize_storage_files(self):
        """åˆå§‹åŒ–å­˜å‚¨æ–‡ä»¶"""
        for file_path in self.files.values():
            if not file_path.exists():
                file_path.touch()  # åªåˆ›å»ºJSONLæ–‡ä»¶
    
    async def store_atomic_task(self, task: AtomicTask, validation_result: Optional[TaskValidationResult] = None) -> bool:
        """å­˜å‚¨åŸå­ä»»åŠ¡"""
        try:
            # æ‰€æœ‰åŸå­ä»»åŠ¡å­˜å‚¨åˆ°åŒä¸€ä¸ªæ–‡ä»¶
            file_path = self.files["atomic_tasks"]
            
            # å‡†å¤‡å­˜å‚¨æ•°æ®
            task_data = asdict(task)
            task_data["created_at"] = task.created_at.isoformat()
            task_data["task_category"] = "atomic"  # æ ‡è®°ä»»åŠ¡ç±»åˆ«
            
            # ä¿®å¤æšä¸¾åºåˆ—åŒ–é—®é¢˜
            task_data["task_type"] = task.task_type.value
            task_data["complexity"] = task.complexity.value
            
            # å¼‚æ­¥å†™å…¥
            async with aiofiles.open(file_path, 'a', encoding='utf-8') as f:
                await f.write(json.dumps(task_data, ensure_ascii=False) + '\n')
            
            logger.debug(f"âœ… å­˜å‚¨åŸå­ä»»åŠ¡: {task.task_id} -> {file_path.name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨åŸå­ä»»åŠ¡å¤±è´¥ {task.task_id}: {e}")
            return False
    
    async def store_depth_extended_task(self, task: DepthExtendedTask, validation_result: Optional[TaskValidationResult] = None) -> bool:
        """å­˜å‚¨æ·±åº¦æ‰©å±•ä»»åŠ¡"""
        try:
            # æ‰€æœ‰ç»¼åˆä»»åŠ¡å­˜å‚¨åˆ°åŒä¸€ä¸ªæ–‡ä»¶
            file_path = self.files["composite_tasks"]
            
            # å‡†å¤‡å­˜å‚¨æ•°æ®
            task_data = {
                "task_id": task.task_id,
                "complexity": task.complexity.value,
                "task_category": "composite_depth",  # æ ‡è®°ä»»åŠ¡ç±»åˆ«
                "base_task": asdict(task.base_task),
                "intermediate_task": asdict(task.intermediate_task),
                "superset_input": asdict(task.superset_input),
                "superset_relation": asdict(task.superset_relation),
                "combined_question": task.combined_question,
                "combined_answer": task.combined_answer,
                "created_at": task.created_at.isoformat()
            }
            
            # ä¿®å¤datetimeå­—æ®µå’Œæšä¸¾å­—æ®µ
            task_data["base_task"]["created_at"] = task.base_task.created_at.isoformat()
            task_data["base_task"]["task_type"] = task.base_task.task_type.value
            task_data["base_task"]["complexity"] = task.base_task.complexity.value
            
            task_data["intermediate_task"]["created_at"] = task.intermediate_task.created_at.isoformat()
            task_data["intermediate_task"]["task_type"] = task.intermediate_task.task_type.value
            task_data["intermediate_task"]["complexity"] = task.intermediate_task.complexity.value
            
            # å¼‚æ­¥å†™å…¥
            async with aiofiles.open(file_path, 'a', encoding='utf-8') as f:
                await f.write(json.dumps(task_data, ensure_ascii=False) + '\n')
            
            logger.debug(f"âœ… å­˜å‚¨æ·±åº¦æ‰©å±•ä»»åŠ¡: {task.task_id} -> {file_path.name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨æ·±åº¦æ‰©å±•ä»»åŠ¡å¤±è´¥ {task.task_id}: {e}")
            return False
    
    async def store_width_extended_task(self, task: WidthExtendedTask, validation_result: Optional[TaskValidationResult] = None) -> bool:
        """å­˜å‚¨å®½åº¦æ‰©å±•ä»»åŠ¡"""
        try:
            # æ‰€æœ‰ç»¼åˆä»»åŠ¡å­˜å‚¨åˆ°åŒä¸€ä¸ªæ–‡ä»¶
            file_path = self.files["composite_tasks"]
            
            # å‡†å¤‡å­˜å‚¨æ•°æ®
            task_data = {
                "task_id": task.task_id,
                "complexity": task.complexity.value,
                "task_category": "composite_width",  # æ ‡è®°ä»»åŠ¡ç±»åˆ«
                "component_tasks": [asdict(t) for t in task.component_tasks],
                "merged_question": task.merged_question,
                "merged_answer": task.merged_answer,
                "merge_strategy": task.merge_strategy,
                "created_at": task.created_at.isoformat()
            }
            
            # ä¿®å¤datetimeå­—æ®µå’Œæšä¸¾å­—æ®µ
            for i, comp_task_data in enumerate(task_data["component_tasks"]):
                comp_task_data["created_at"] = task.component_tasks[i].created_at.isoformat()
                comp_task_data["task_type"] = task.component_tasks[i].task_type.value
                comp_task_data["complexity"] = task.component_tasks[i].complexity.value
            
            # å¼‚æ­¥å†™å…¥
            async with aiofiles.open(file_path, 'a', encoding='utf-8') as f:
                await f.write(json.dumps(task_data, ensure_ascii=False) + '\n')
            
            logger.debug(f"âœ… å­˜å‚¨å®½åº¦æ‰©å±•ä»»åŠ¡: {task.task_id} -> {file_path.name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨å®½åº¦æ‰©å±•ä»»åŠ¡å¤±è´¥ {task.task_id}: {e}")
            return False
    
    
    async def load_tasks_by_type(self, task_type: TaskType, complexity: Optional[TaskComplexity] = None) -> List[Dict]:
        """æŒ‰ç±»å‹åŠ è½½ä»»åŠ¡"""
        tasks = []
        
        try:
            # ç¡®å®šè¦è¯»å–çš„æ–‡ä»¶
            if complexity is None or complexity == TaskComplexity.ATOMIC:
                files_to_read = [self.files["atomic_tasks"]]
            else:
                files_to_read = []
                
            if complexity is None or complexity in [TaskComplexity.DEPTH, TaskComplexity.WIDTH]:
                files_to_read.append(self.files["composite_tasks"])
            
            # è¯»å–æ–‡ä»¶
            for file_path in files_to_read:
                if file_path.exists():
                    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                        async for line in f:
                            if line.strip():
                                try:
                                    task_data = json.loads(line)
                                    
                                    # æ ¹æ®ä»»åŠ¡ç±»å‹è¿‡æ»¤
                                    if task_type == TaskType.TOOL_REQUIRED:
                                        if 'task_type' in task_data and task_data['task_type'] == 'tool_required':
                                            tasks.append(task_data)
                                        elif 'base_task' in task_data and task_data['base_task'].get('task_type') == 'tool_required':
                                            tasks.append(task_data)
                                    else:
                                        if 'task_type' in task_data and task_data['task_type'] == 'reasoning_only':
                                            tasks.append(task_data)
                                        elif 'base_task' in task_data and task_data['base_task'].get('task_type') == 'reasoning_only':
                                            tasks.append(task_data)
                                    
                                    # æ ¹æ®å¤æ‚åº¦è¿‡æ»¤
                                    if complexity == TaskComplexity.DEPTH:
                                        if task_data.get('task_category') != 'composite_depth':
                                            continue
                                    elif complexity == TaskComplexity.WIDTH:
                                        if task_data.get('task_category') != 'composite_width':
                                            continue
                                    elif complexity == TaskComplexity.ATOMIC:
                                        if task_data.get('task_category') != 'atomic':
                                            continue
                                    
                                except json.JSONDecodeError as e:
                                    logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆJSONè¡Œ: {e}")
            
            logger.debug(f"ğŸ“– åŠ è½½ä»»åŠ¡: {task_type.value}, æ•°é‡: {len(tasks)}")
            return tasks
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ä»»åŠ¡å¤±è´¥: {e}")
            return []
    
    async def get_statistics(self) -> Dict[str, Any]:
        """è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {
                "total_tasks": 0,
                "atomic_tasks": 0,
                "composite_tasks": {
                    "depth_extended": 0,
                    "width_extended": 0,
                    "total": 0
                },
                "storage_files": {}
            }
            
            # ç»Ÿè®¡åŸå­ä»»åŠ¡æ•°é‡
            if self.files["atomic_tasks"].exists():
                atomic_count = await self._count_lines(self.files["atomic_tasks"])
                stats["atomic_tasks"] = atomic_count
                stats["total_tasks"] += atomic_count
                stats["storage_files"]["atomic_tasks"] = {
                    "path": str(self.files["atomic_tasks"]),
                    "count": atomic_count,
                    "size_bytes": self.files["atomic_tasks"].stat().st_size
                }
            
            # ç»Ÿè®¡ç»¼åˆä»»åŠ¡æ•°é‡
            if self.files["composite_tasks"].exists():
                composite_count = await self._count_lines(self.files["composite_tasks"])
                # éœ€è¦è¯»å–æ–‡ä»¶å†…å®¹æ¥åŒºåˆ†æ·±åº¦å’Œå®½åº¦ä»»åŠ¡
                depth_count = 0
                width_count = 0
                
                async with aiofiles.open(self.files["composite_tasks"], 'r', encoding='utf-8') as f:
                    async for line in f:
                        if line.strip():
                            try:
                                task_data = json.loads(line)
                                if task_data.get('task_category') == 'composite_depth':
                                    depth_count += 1
                                elif task_data.get('task_category') == 'composite_width':
                                    width_count += 1
                            except json.JSONDecodeError:
                                pass
                
                stats["composite_tasks"]["depth_extended"] = depth_count
                stats["composite_tasks"]["width_extended"] = width_count
                stats["composite_tasks"]["total"] = depth_count + width_count
                stats["total_tasks"] += composite_count
                
                stats["storage_files"]["composite_tasks"] = {
                    "path": str(self.files["composite_tasks"]),
                    "count": composite_count,
                    "size_bytes": self.files["composite_tasks"].stat().st_size
                }
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    async def store_synthesis_session(self, result) -> bool:
        """å­˜å‚¨åˆæˆä¼šè¯ç»“æœï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            # ç®€åŒ–çš„ä¼šè¯è®°å½•ï¼Œåªè®°å½•ç»Ÿè®¡ä¿¡æ¯
            logger.info(f"ğŸ“Š åˆæˆä¼šè¯å®Œæˆ: {getattr(result, 'session_id', 'unknown')}")
            logger.info(f"  ç”Ÿæˆä»»åŠ¡: {getattr(result, 'total_tasks_generated', 0)} ä¸ª")
            logger.info(f"  æœ‰æ•ˆä»»åŠ¡: {getattr(result, 'valid_tasks_count', 0)} ä¸ª")
            return True
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨åˆæˆä¼šè¯å¤±è´¥: {e}")
            return False
    
    async def _count_lines(self, file_path: Path) -> int:
        """è®¡ç®—æ–‡ä»¶è¡Œæ•°"""
        try:
            count = 0
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                async for line in f:
                    if line.strip():
                        count += 1
            return count
        except Exception:
            return 0
    
    def store_validation_result(self, validation_result: 'TaskValidationResult'):
        """å­˜å‚¨éªŒè¯ç»“æœï¼ˆå·²ç¦ç”¨æ–‡ä»¶è¾“å‡ºï¼‰"""
        try:
            # ä¸å†ç”Ÿæˆç‹¬ç«‹çš„validation.jsonæ–‡ä»¶ï¼Œåªè®°å½•æ—¥å¿—
            logger.debug(f"ğŸ” éªŒè¯å®Œæˆ - ä»»åŠ¡ {validation_result.task_id}: "
                        f"æœ‰æ•ˆ={validation_result.is_valid}, "
                        f"åˆ†æ•°={validation_result.validation_score:.2f}")
            
            # å¦‚æœéœ€è¦è¯¦ç»†ä¿¡æ¯ï¼Œåªåœ¨è°ƒè¯•çº§åˆ«è®°å½•
            if not validation_result.is_valid and validation_result.errors:
                logger.debug(f"  éªŒè¯é”™è¯¯: {validation_result.errors}")
            
        except Exception as e:
            logger.error(f"âŒ è®°å½•éªŒè¯ç»“æœå¤±è´¥: {e}")

    async def clear_storage(self, confirm: bool = False) -> bool:
        """æ¸…ç©ºå­˜å‚¨ï¼ˆå±é™©æ“ä½œï¼‰"""
        if not confirm:
            logger.warning("âš ï¸ æ¸…ç©ºå­˜å‚¨éœ€è¦ç¡®è®¤å‚æ•° confirm=True")
            return False
        
        try:
            for file_path in self.files.values():
                if file_path.exists():
                    file_path.write_text('', encoding='utf-8')
            
            logger.warning("ğŸ—‘ï¸ æ‰€æœ‰å­˜å‚¨æ–‡ä»¶å·²æ¸…ç©º")
            return True
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºå­˜å‚¨å¤±è´¥: {e}")
            return False
    
    def get_storage_info(self) -> Dict[str, str]:
        """è·å–å­˜å‚¨ä¿¡æ¯"""
        return {
            "storage_directory": str(self.storage_dir),
            "files": {key: str(path) for key, path in self.files.items()},
            "description": {
                "atomic_tasks": "åŸå­ä»»åŠ¡ï¼ˆåŸºç¡€å•å…ƒä»»åŠ¡ï¼‰",
                "composite_tasks": "ç»¼åˆä»»åŠ¡ï¼ˆæ·±åº¦æ‰©å±•+å®½åº¦æ‰©å±•ï¼ŒåŸºäºåŸå­ä»»åŠ¡åº“æ‰©å±•ï¼‰"
            }
        }