#!/usr/bin/env python3
"""
Prompt æ¨¡æ¿ç®¡ç†å·¥å…·
æä¾›æ¨¡æ¿ç›‘æ§ã€ä¼˜åŒ–å»ºè®®ã€A/Bæµ‹è¯•ç­‰é«˜çº§åŠŸèƒ½
"""

import logging
import json
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
from dataclasses import dataclass, field
from .base import PromptManager, PromptTemplate, PromptType

logger = logging.getLogger(__name__)


@dataclass
class PromptPerformanceMetrics:
    """Promptæ€§èƒ½æŒ‡æ ‡"""
    template_name: str
    total_usage: int = 0
    success_rate: float = 0.0
    avg_response_time: float = 0.0
    error_count: int = 0
    last_used: Optional[datetime] = None
    performance_trend: List[float] = field(default_factory=list)
    
    def update_metrics(self, success: bool, response_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.total_usage += 1
        self.last_used = datetime.now()
        
        if not success:
            self.error_count += 1
        
        # æ›´æ–°æˆåŠŸç‡
        self.success_rate = (self.total_usage - self.error_count) / self.total_usage
        
        # æ›´æ–°å“åº”æ—¶é—´
        if self.avg_response_time == 0:
            self.avg_response_time = response_time
        else:
            self.avg_response_time = (self.avg_response_time * 0.8) + (response_time * 0.2)
        
        # æ›´æ–°è¶‹åŠ¿ï¼ˆä¿ç•™æœ€è¿‘10æ¬¡çš„æˆåŠŸç‡ï¼‰
        current_success = 1.0 if success else 0.0
        self.performance_trend.append(current_success)
        if len(self.performance_trend) > 10:
            self.performance_trend.pop(0)


class PromptPerformanceManager:
    """Promptæ€§èƒ½ç®¡ç†å™¨"""
    
    def __init__(self, storage_path: str = "output/SynthesisTask/prompt_metrics.json"):
        self.storage_path = Path(storage_path)
        self.metrics: Dict[str, PromptPerformanceMetrics] = {}
        self.load_metrics()
        
        logger.info("âœ… PromptPerformanceManager åˆå§‹åŒ–å®Œæˆ")
    
    def record_usage(self, template_name: str, success: bool, response_time: float):
        """è®°å½•æ¨¡æ¿ä½¿ç”¨æƒ…å†µ"""
        if template_name not in self.metrics:
            self.metrics[template_name] = PromptPerformanceMetrics(template_name)
        
        self.metrics[template_name].update_metrics(success, response_time)
        
        # å®šæœŸä¿å­˜
        if self.metrics[template_name].total_usage % 10 == 0:
            self.save_metrics()
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        report = {
            "summary": {
                "total_templates": len(self.metrics),
                "total_usage": sum(m.total_usage for m in self.metrics.values()),
                "overall_success_rate": 0.0,
                "avg_response_time": 0.0
            },
            "top_performers": [],
            "underperformers": [],
            "usage_trends": {},
            "recommendations": []
        }
        
        if self.metrics:
            # è®¡ç®—æ€»ä½“æŒ‡æ ‡
            total_usage = sum(m.total_usage for m in self.metrics.values())
            if total_usage > 0:
                weighted_success_rate = sum(
                    m.success_rate * m.total_usage for m in self.metrics.values()
                ) / total_usage
                report["summary"]["overall_success_rate"] = weighted_success_rate
                
                weighted_response_time = sum(
                    m.avg_response_time * m.total_usage for m in self.metrics.values()
                ) / total_usage
                report["summary"]["avg_response_time"] = weighted_response_time
            
            # æ’åºæ¨¡æ¿
            sorted_metrics = sorted(
                self.metrics.values(),
                key=lambda x: x.success_rate * (x.total_usage + 1),  # è€ƒè™‘ä½¿ç”¨é¢‘ç‡
                reverse=True
            )
            
            # æœ€ä½³è¡¨ç°
            report["top_performers"] = [
                {
                    "name": m.template_name,
                    "success_rate": m.success_rate,
                    "usage_count": m.total_usage,
                    "avg_response_time": m.avg_response_time
                }
                for m in sorted_metrics[:5]
            ]
            
            # è¡¨ç°ä¸ä½³
            underperformers = [m for m in sorted_metrics if m.success_rate < 0.7 and m.total_usage > 5]
            report["underperformers"] = [
                {
                    "name": m.template_name,
                    "success_rate": m.success_rate,
                    "error_count": m.error_count,
                    "issues": self._analyze_issues(m)
                }
                for m in underperformers[-3:]
            ]
            
            # ä½¿ç”¨è¶‹åŠ¿
            for name, metrics in self.metrics.items():
                if metrics.performance_trend:
                    recent_trend = sum(metrics.performance_trend[-5:]) / len(metrics.performance_trend[-5:])
                    report["usage_trends"][name] = {
                        "recent_success_rate": recent_trend,
                        "trend_direction": "improving" if recent_trend > metrics.success_rate else "declining"
                    }
            
            # ä¼˜åŒ–å»ºè®®
            report["recommendations"] = self._generate_recommendations()
        
        return report
    
    def _analyze_issues(self, metrics: PromptPerformanceMetrics) -> List[str]:
        """åˆ†ææ€§èƒ½é—®é¢˜"""
        issues = []
        
        if metrics.success_rate < 0.5:
            issues.append("æˆåŠŸç‡è¿‡ä½ï¼Œéœ€è¦é‡æ–°è®¾è®¡æ¨¡æ¿")
        elif metrics.success_rate < 0.7:
            issues.append("æˆåŠŸç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–æç¤ºè¯")
        
        if metrics.avg_response_time > 30:
            issues.append("å“åº”æ—¶é—´è¿‡é•¿ï¼Œè€ƒè™‘ç®€åŒ–æ¨¡æ¿")
        
        if len(metrics.performance_trend) >= 5:
            recent_avg = sum(metrics.performance_trend[-5:]) / 5
            if recent_avg < metrics.success_rate - 0.1:
                issues.append("æ€§èƒ½å‘ˆä¸‹é™è¶‹åŠ¿")
        
        return issues
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åˆ†ææ€»ä½“æ¨¡å¼
        total_templates = len(self.metrics)
        low_performers = sum(1 for m in self.metrics.values() if m.success_rate < 0.7)
        
        if low_performers / total_templates > 0.3:
            recommendations.append("è¶…è¿‡30%çš„æ¨¡æ¿è¡¨ç°ä¸ä½³ï¼Œå»ºè®®è¿›è¡Œå…¨é¢çš„æ¨¡æ¿è´¨é‡å®¡æ ¸")
        
        # æ£€æŸ¥ä½¿ç”¨ä¸å¹³è¡¡
        usage_counts = [m.total_usage for m in self.metrics.values()]
        if usage_counts:
            max_usage = max(usage_counts)
            min_usage = min(usage_counts)
            if max_usage > min_usage * 10:
                recommendations.append("æ¨¡æ¿ä½¿ç”¨ä¸å¹³è¡¡ï¼Œè€ƒè™‘æ•´åˆä½ä½¿ç”¨ç‡æ¨¡æ¿")
        
        # æ£€æŸ¥å“åº”æ—¶é—´
        slow_templates = [m for m in self.metrics.values() if m.avg_response_time > 25]
        if len(slow_templates) > 0:
            recommendations.append(f"å‘ç°{len(slow_templates)}ä¸ªå“åº”ç¼“æ…¢çš„æ¨¡æ¿ï¼Œå»ºè®®ä¼˜åŒ–æˆ–åˆ†è§£")
        
        return recommendations
    
    def save_metrics(self):
        """ä¿å­˜æ€§èƒ½æŒ‡æ ‡"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            self.storage_path.parent.mkdir(parents=True, exist_ok=True)
            
            # åºåˆ—åŒ–æ•°æ®
            data = {}
            for name, metrics in self.metrics.items():
                data[name] = {
                    "template_name": metrics.template_name,
                    "total_usage": metrics.total_usage,
                    "success_rate": metrics.success_rate,
                    "avg_response_time": metrics.avg_response_time,
                    "error_count": metrics.error_count,
                    "last_used": metrics.last_used.isoformat() if metrics.last_used else None,
                    "performance_trend": metrics.performance_trend
                }
            
            with open(self.storage_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
            logger.debug(f"ğŸ’¾ æ€§èƒ½æŒ‡æ ‡å·²ä¿å­˜åˆ°: {self.storage_path}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
    
    def load_metrics(self):
        """åŠ è½½æ€§èƒ½æŒ‡æ ‡"""
        try:
            if self.storage_path.exists():
                with open(self.storage_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                for name, metrics_data in data.items():
                    metrics = PromptPerformanceMetrics(
                        template_name=metrics_data["template_name"],
                        total_usage=metrics_data["total_usage"],
                        success_rate=metrics_data["success_rate"],
                        avg_response_time=metrics_data["avg_response_time"],
                        error_count=metrics_data["error_count"],
                        performance_trend=metrics_data.get("performance_trend", [])
                    )
                    
                    if metrics_data.get("last_used"):
                        metrics.last_used = datetime.fromisoformat(metrics_data["last_used"])
                    
                    self.metrics[name] = metrics
                
                logger.info(f"ğŸ“Š åŠ è½½äº† {len(self.metrics)} ä¸ªæ¨¡æ¿çš„æ€§èƒ½æŒ‡æ ‡")
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")


class EnhancedPromptManager(PromptManager):
    """å¢å¼ºçš„Promptç®¡ç†å™¨ï¼Œé›†æˆæ€§èƒ½ç›‘æ§"""
    
    def __init__(self):
        super().__init__()
        self.performance_manager = PromptPerformanceManager()
        
    async def render_template_with_monitoring(self, template_name: str, **kwargs) -> Tuple[str, bool, float]:
        """å¸¦æ€§èƒ½ç›‘æ§çš„æ¨¡æ¿æ¸²æŸ“"""
        start_time = datetime.now()
        success = False
        
        try:
            result = self.render_template(template_name, **kwargs)
            success = True
            return result, success, 0.0  # æ¸²æŸ“æœ¬èº«å¾ˆå¿«
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡æ¿æ¸²æŸ“å¤±è´¥ {template_name}: {e}")
            return "", success, 0.0
        finally:
            # è®°å½•æ€§èƒ½ï¼ˆè¿™é‡Œä¸»è¦è®°å½•æ¨¡æ¿ä½¿ç”¨æƒ…å†µï¼‰
            response_time = (datetime.now() - start_time).total_seconds()
            self.performance_manager.record_usage(template_name, success, response_time)
    
    async def render_and_call_llm(self, template_name: str, llm_client, **kwargs) -> Tuple[str, bool, float]:
        """æ¸²æŸ“æ¨¡æ¿å¹¶è°ƒç”¨LLMï¼Œè®°å½•å®Œæ•´æ€§èƒ½"""
        start_time = datetime.now()
        success = False
        
        try:
            # æ¸²æŸ“æ¨¡æ¿
            prompt = self.render_template(template_name, **kwargs)
            
            # è°ƒç”¨LLM
            messages = [{"role": "user", "content": prompt}]
            response = await llm_client._call_api(messages, timeout=60)
            
            success = bool(response and len(response) > 10)  # ç®€å•çš„æˆåŠŸåˆ¤æ–­
            return response, success, 0.0
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡æ¿ {template_name} LLMè°ƒç”¨å¤±è´¥: {e}")
            return "", success, 0.0
        finally:
            # è®°å½•å®Œæ•´æ€§èƒ½
            response_time = (datetime.now() - start_time).total_seconds()
            self.performance_manager.record_usage(template_name, success, response_time)
    
    def get_performance_dashboard(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ä»ªè¡¨æ¿"""
        template_stats = self.get_usage_statistics()
        performance_report = self.performance_manager.get_performance_report()
        
        return {
            "template_statistics": template_stats,
            "performance_metrics": performance_report,
            "health_status": self._assess_health_status(performance_report),
            "optimization_suggestions": self._get_optimization_suggestions(performance_report)
        }
    
    def _assess_health_status(self, performance_report: Dict) -> str:
        """è¯„ä¼°ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        overall_success_rate = performance_report["summary"]["overall_success_rate"]
        underperformers_count = len(performance_report["underperformers"])
        
        if overall_success_rate >= 0.9 and underperformers_count <= 1:
            return "excellent"
        elif overall_success_rate >= 0.8 and underperformers_count <= 2:
            return "good"
        elif overall_success_rate >= 0.7:
            return "fair"
        else:
            return "poor"
    
    def _get_optimization_suggestions(self, performance_report: Dict) -> List[str]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        suggestions = performance_report.get("recommendations", [])
        
        # æ·»åŠ å…·ä½“çš„æ“ä½œå»ºè®®
        if performance_report["summary"]["avg_response_time"] > 20:
            suggestions.append("è€ƒè™‘ç¼“å­˜å¸¸ç”¨æ¨¡æ¿çš„æ¸²æŸ“ç»“æœä»¥æé«˜æ€§èƒ½")
        
        underperformers = performance_report.get("underperformers", [])
        if len(underperformers) > 0:
            template_names = [u["name"] for u in underperformers]
            suggestions.append(f"ä¼˜å…ˆä¼˜åŒ–ä»¥ä¸‹æ¨¡æ¿: {', '.join(template_names)}")
        
        return suggestions


# åˆ›å»ºå…¨å±€å¢å¼ºç®¡ç†å™¨å®ä¾‹
enhanced_prompt_manager = EnhancedPromptManager()