#!/usr/bin/env python3
"""
ç®€å•ä»»åŠ¡åˆæˆå™¨MVP - é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ
å¤ç”¨ç°æœ‰LLMå®¢æˆ·ç«¯ï¼Œå®ç°åŸºç¡€çš„è½¨è¿¹åé¦ˆå’Œä»»åŠ¡ç”Ÿæˆ
"""

import asyncio
import json
import logging
import os
import time
import sqlite3
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import redis.asyncio as redis
import threading
from pathlib import Path
from collections import defaultdict
import uuid

from ..interfaces import TaskSpec, TrajectoryResult, TaskType, ExecutionStep, ActionType, ErrorType
from ..llm_client import LLMClient

logger = logging.getLogger(__name__)

@dataclass
class TaskEssence:
    """ä»»åŠ¡æœ¬è´¨æ•°æ®ç»“æ„"""
    essence_id: str
    task_type: str
    domain: str
    query: str
    complexity_level: str
    success_pattern: Dict
    extracted_at: str
    source_trajectory_id: str

class SimpleSynthesizer:
    """ç®€å•ä»»åŠ¡åˆæˆå™¨ - MVPç‰ˆæœ¬"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.redis = redis.from_url(config["redis_url"])
        self.llm_client = LLMClient(config)
        self.db_path = config.get("synthesis_db", "/app/output/synthesis.db")
        self.enabled = config.get("synthesis_enabled", False)
        self.processed_json_path = "/app/output/processed_trajectories.json"
        self._processed_lock = threading.Lock()
        self._ensure_processed_json()
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        max_retries = 5
        retry_delay = 2  # ç§’
        
        for attempt in range(max_retries):
            try:
                os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
                
                with sqlite3.connect(self.db_path) as conn:
                    # è®¾ç½®æ•°æ®åº“å‚æ•°æé«˜ç¨³å®šæ€§
                    conn.execute("PRAGMA journal_mode=WAL")
                    conn.execute("PRAGMA synchronous=NORMAL")
                    conn.execute("PRAGMA cache_size=10000")
                    
                    # åˆ›å»ºä»»åŠ¡æœ¬è´¨è¡¨
                    conn.execute('''
                        CREATE TABLE IF NOT EXISTS task_essences (
                            essence_id TEXT PRIMARY KEY,
                            task_type TEXT,
                            domain TEXT,
                            query TEXT,
                            complexity_level TEXT,
                            success_pattern TEXT,
                            extracted_at TEXT,
                            source_trajectory_id TEXT
                        )
                    ''')
                    
                    # åˆ›å»ºç”Ÿæˆä»»åŠ¡è¡¨
                    conn.execute('''
                        CREATE TABLE IF NOT EXISTS generated_tasks (
                            task_id TEXT PRIMARY KEY,
                            source_essence_id TEXT,
                            task_spec TEXT,
                            generated_at TEXT,
                            executed BOOLEAN DEFAULT FALSE
                        )
                    ''')
                    
                    # éªŒè¯è¡¨æ˜¯å¦åˆ›å»ºæˆåŠŸ
                    cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
                    tables = [row[0] for row in cursor.fetchall()]
                    
                    if 'task_essences' not in tables or 'generated_tasks' not in tables:
                        raise Exception("Failed to create required tables")
                    
                    # æµ‹è¯•å†™å…¥æƒé™
                    conn.execute("INSERT OR IGNORE INTO task_essences VALUES (?, ?, ?, ?, ?, ?, ?, ?)", 
                               ("test_init", "test", "test", "test", "test", "{}", 
                                datetime.now().isoformat(), "test"))
                    conn.execute("DELETE FROM task_essences WHERE essence_id = ?", ("test_init",))
                    
                    conn.commit()
                    
                logger.info(f"Database initialized successfully at {self.db_path}")
                return
                
            except Exception as e:
                logger.warning(f"Database initialization attempt {attempt + 1}/{max_retries} failed: {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    retry_delay *= 2  # æŒ‡æ•°é€€é¿
                else:
                    raise Exception(f"Failed to initialize database after {max_retries} attempts: {e}")

    def _verify_database_ready(self) -> bool:
        """éªŒè¯æ•°æ®åº“æ˜¯å¦å‡†å¤‡å°±ç»ª"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ä¸”å¯ç”¨
                conn.execute("SELECT COUNT(*) FROM task_essences LIMIT 1")
                conn.execute("SELECT COUNT(*) FROM generated_tasks LIMIT 1")
                return True
        except Exception as e:
            logger.error(f"Database verification failed: {e}")
            return False

    async def start(self):
        """å¯åŠ¨åˆæˆå™¨ï¼Œç­‰å¾…è§¦å‘æŒ‡ä»¤è€Œéè‡ªåŠ¨å¤„ç†"""
        if not self.enabled:
            logger.info("Task synthesis is disabled")
            return
            
        logger.info("Starting simple task synthesizer...")
        
        # ç­‰å¾…æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ
        max_wait = 30  # æœ€å¤§ç­‰å¾…30ç§’
        wait_interval = 2
        elapsed = 0
        
        while elapsed < max_wait:
            if self._verify_database_ready():
                logger.info("Database verification passed, ready for manual synthesis")
                break
            
            logger.info(f"Waiting for database initialization... ({elapsed}s/{max_wait}s)")
            await asyncio.sleep(wait_interval)
            elapsed += wait_interval
        else:
            logger.error("Database initialization timeout, exiting")
            return
        
        # å¯åŠ¨æŒ‡ä»¤ç›‘å¬å™¨ï¼Œè€Œéè‡ªåŠ¨å¤„ç†
        await self._listen_for_synthesis_commands()

    async def _listen_for_synthesis_commands(self):
        """ç›‘å¬åˆæˆæŒ‡ä»¤"""
        logger.info("ğŸ¯ Synthesis service ready - waiting for manual triggers")
        logger.info("Available trigger methods:")
        logger.info("1. Redis command: XADD synthesis:commands command trigger_synthesis")
        logger.info("2. Redis command: XADD synthesis:commands command process_trajectories")
        logger.info("3. Redis command: XADD synthesis:commands command process_specific trajectory_file.json")
        
        while True:
            try:
                # ç›‘å¬synthesis:commandsé˜Ÿåˆ—
                streams = {"synthesis:commands": "$"}
                result = await self.redis.xread(streams, count=1, block=5000)  # 5ç§’è¶…æ—¶
                
                if result:
                    for stream_name, messages in result:
                        for message_id, fields in messages:
                            await self._handle_synthesis_command(fields)
                            # ç¡®è®¤å¤„ç†å®Œæˆ
                            await self.redis.xdel("synthesis:commands", message_id)
                
            except Exception as e:
                logger.error(f"Error listening for synthesis commands: {e}")
                await asyncio.sleep(10)

    async def _handle_synthesis_command(self, command_fields: dict):
        """å¤„ç†åˆæˆæŒ‡ä»¤"""
        try:
            command = command_fields.get(b'command', b'').decode('utf-8')
            logger.info(f"ğŸ“¨ Received synthesis command: {command}")
            
            if command == "trigger_synthesis":
                # è§¦å‘å®Œæ•´çš„è½¨è¿¹å¤„ç†
                await self._process_all_trajectories_once()
                
            elif command == "process_trajectories":
                # å¤„ç†æ‰€æœ‰æœªå¤„ç†çš„è½¨è¿¹
                await self._process_unprocessed_trajectories()
                
            elif command.startswith("process_specific"):
                # å¤„ç†æŒ‡å®šçš„è½¨è¿¹æ–‡ä»¶
                parts = command.split(" ", 1)
                if len(parts) > 1:
                    filename = parts[1]
                    await self._process_specific_trajectory(filename)
                    
            elif command == "generate_tasks":
                # æ‰‹åŠ¨ç”Ÿæˆä»»åŠ¡
                count = int(command_fields.get(b'count', b'3').decode('utf-8'))
                tasks = await self.generate_tasks_manually(count)
                logger.info(f"Generated {len(tasks)} tasks manually")
                
            elif command == "status":
                # æŠ¥å‘ŠçŠ¶æ€
                await self._report_synthesis_status()
                
            else:
                logger.warning(f"Unknown synthesis command: {command}")
                
        except Exception as e:
            logger.error(f"Error handling synthesis command: {e}")

    async def _process_all_trajectories_once(self):
        """ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰è½¨è¿¹ï¼ˆä¸å¾ªç¯ï¼‰"""
        logger.info("ğŸ”„ Starting one-time trajectory processing...")
        
        try:
            trajectories_dir = "/app/output/trajectories"
            if not os.path.exists(trajectories_dir):
                logger.warning("Trajectories directory not found")
                return
            
            processed_count = 0
            skipped_count = 0
            
            # å¤„ç†æ‰€æœ‰è½¨è¿¹æ–‡ä»¶
            for filename in os.listdir(trajectories_dir):
                if filename.endswith('.json'):
                    trajectory_path = os.path.join(trajectories_dir, filename)
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
                    if not self._is_trajectory_processed(filename):
                        await self._process_trajectory_file(trajectory_path)
                        self._mark_trajectory_processed(filename)
                        processed_count += 1
                        logger.info(f"âœ… Processed: {filename}")
                    else:
                        skipped_count += 1
                        logger.debug(f"â© Skipped (already processed): {filename}")
            
            logger.info(f"ğŸ¯ Trajectory processing completed: {processed_count} processed, {skipped_count} skipped")
            
        except Exception as e:
            logger.error(f"Error in one-time trajectory processing: {e}")

    async def _process_unprocessed_trajectories(self):
        """åªå¤„ç†æœªå¤„ç†çš„è½¨è¿¹"""
        logger.info("ğŸ”„ Processing only unprocessed trajectories...")
        
        try:
            trajectories_dir = "/app/output/trajectories"
            if not os.path.exists(trajectories_dir):
                logger.warning("Trajectories directory not found")
                return
            
            processed_count = 0
            
            # è·å–æ‰€æœ‰æœªå¤„ç†çš„è½¨è¿¹
            for filename in os.listdir(trajectories_dir):
                if filename.endswith('.json') and not self._is_trajectory_processed(filename):
                    trajectory_path = os.path.join(trajectories_dir, filename)
                    await self._process_trajectory_file(trajectory_path)
                    self._mark_trajectory_processed(filename)
                    processed_count += 1
                    logger.info(f"âœ… Processed new trajectory: {filename}")
            
            logger.info(f"ğŸ¯ Unprocessed trajectories completed: {processed_count} new trajectories processed")
            
        except Exception as e:
            logger.error(f"Error processing unprocessed trajectories: {e}")

    async def _process_specific_trajectory(self, filename: str):
        """å¤„ç†æŒ‡å®šçš„è½¨è¿¹æ–‡ä»¶"""
        logger.info(f"ğŸ¯ Processing specific trajectory: {filename}")
        
        try:
            trajectories_dir = "/app/output/trajectories"
            trajectory_path = os.path.join(trajectories_dir, filename)
            
            if not os.path.exists(trajectory_path):
                logger.error(f"Trajectory file not found: {filename}")
                return
            
            await self._process_trajectory_file(trajectory_path)
            self._mark_trajectory_processed(filename)
            logger.info(f"âœ… Successfully processed specific trajectory: {filename}")
            
        except Exception as e:
            logger.error(f"Error processing specific trajectory {filename}: {e}")

    async def _report_synthesis_status(self):
        """æŠ¥å‘ŠåˆæˆæœåŠ¡çŠ¶æ€"""
        try:
            # ç»Ÿè®¡æ•°æ®åº“ä¸­çš„æœ¬è´¨æ•°é‡
            with sqlite3.connect(self.db_path, timeout=10) as conn:
                cursor = conn.execute("SELECT COUNT(*) FROM task_essences")
                essence_count = cursor.fetchone()[0]
                
                cursor = conn.execute("SELECT COUNT(*) FROM generated_tasks")
                generated_count = cursor.fetchone()[0]
                
                cursor = conn.execute("SELECT task_type, COUNT(*) FROM task_essences GROUP BY task_type")
                type_distribution = dict(cursor.fetchall())
            
            # ç»Ÿè®¡å¤„ç†çš„è½¨è¿¹æ•°é‡
            processed = self._load_processed()
            processed_count = len(processed)
            
            # ç»Ÿè®¡è½¨è¿¹ç›®å½•ä¸­çš„æ–‡ä»¶æ•°é‡
            trajectories_dir = "/app/output/trajectories"
            total_trajectories = 0
            if os.path.exists(trajectories_dir):
                total_trajectories = len([f for f in os.listdir(trajectories_dir) if f.endswith('.json')])
            
            status_info = {
                "synthesis_enabled": self.enabled,
                "database_ready": self._verify_database_ready(),
                "total_essences": essence_count,
                "generated_tasks": generated_count,
                "essence_distribution": type_distribution,
                "processed_trajectories": processed_count,
                "total_trajectory_files": total_trajectories,
                "unprocessed_count": total_trajectories - processed_count
            }
            
            logger.info("ğŸ“Š Synthesis Status Report:")
            for key, value in status_info.items():
                logger.info(f"  {key}: {value}")
                
            # å‘å¸ƒçŠ¶æ€åˆ°Redisä¾›å¤–éƒ¨æŸ¥è¯¢
            await self.redis.xadd(
                "synthesis:status",
                {
                    "timestamp": time.time(),
                    "status": json.dumps(status_info)
                }
            )
            
        except Exception as e:
            logger.error(f"Error generating status report: {e}")

    # ä¿ç•™åŸæœ‰çš„è‡ªåŠ¨å¤„ç†æ–¹æ³•ä½œä¸ºå¤‡ç”¨
    async def _process_trajectory_feedback(self):
        """å¤„ç†è½¨è¿¹åé¦ˆï¼Œæå–ä»»åŠ¡æœ¬è´¨ï¼ˆåŸè‡ªåŠ¨å¤„ç†æ–¹æ³•ï¼Œç°åœ¨ä½œä¸ºå¤‡ç”¨ï¼‰"""
        logger.info("âš ï¸  Note: This is the old auto-processing method, now used as backup")
        
        while True:
            try:
                # æ‰«æè½¨è¿¹è¾“å‡ºç›®å½•
                trajectories_dir = "/app/output/trajectories"
                if not os.path.exists(trajectories_dir):
                    await asyncio.sleep(30)
                    continue
                
                # å¤„ç†æ–°çš„è½¨è¿¹æ–‡ä»¶
                for filename in os.listdir(trajectories_dir):
                    if filename.endswith('.json'):
                        trajectory_path = os.path.join(trajectories_dir, filename)
                        
                        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
                        if not self._is_trajectory_processed(filename):
                            await self._process_trajectory_file(trajectory_path)
                            self._mark_trajectory_processed(filename)
                
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"Error processing trajectory feedback: {e}")
                await asyncio.sleep(30)
    
    async def _process_trajectory_file(self, trajectory_path: str):
        """å¤„ç†å•ä¸ªè½¨è¿¹æ–‡ä»¶"""
        try:
            with open(trajectory_path, 'r', encoding='utf-8') as f:
                trajectory_data = json.load(f)
            
            # å¦‚æœæ˜¯è½¨è¿¹åˆ—è¡¨ï¼Œå¤„ç†æ¯ä¸€ä¸ªè½¨è¿¹
            if isinstance(trajectory_data, list):
                logger.info(f"Processing trajectory collection with {len(trajectory_data)} trajectories")
                for i, single_trajectory in enumerate(trajectory_data):
                    if i >= 5:  # é™åˆ¶å¤„ç†å‰5ä¸ªè½¨è¿¹
                        break
                    try:
                        trajectory = self._convert_trajectory_format(single_trajectory)
                        if trajectory and trajectory.success:
                            essence = await self._extract_essence(trajectory)
                            if essence:
                                self._store_essence(essence)
                                logger.info(f"Extracted essence from trajectory {trajectory.task_id}")
                    except Exception as e:
                        logger.error(f"Error processing trajectory {i}: {e}")
            else:
                # å•ä¸ªè½¨è¿¹å¯¹è±¡
                trajectory = self._convert_trajectory_format(trajectory_data)
                if trajectory and trajectory.success:
                    essence = await self._extract_essence(trajectory)
                    if essence:
                        self._store_essence(essence)
                        logger.info(f"Extracted essence from trajectory {trajectory.task_id}")
        
        except Exception as e:
            logger.error(f"Error processing trajectory file {trajectory_path}: {e}")
    
    def _convert_trajectory_format(self, data: Dict) -> Optional[TrajectoryResult]:
        """å°†è½¨è¿¹æ•°æ®è½¬æ¢ä¸ºTrajectoryResultæ ¼å¼"""
        try:
            # è½¬æ¢stepsæ ¼å¼
            converted_steps = []
            for step_data in data.get('steps', []):
                # æ˜ å°„å­—æ®µåç§°
                converted_step = ExecutionStep(
                    step_id=step_data.get('step_id', 0),
                    action_type=ActionType(step_data.get('action_type', 'code_generation')),
                    action_params=step_data.get('tool_input', {}),
                    observation=step_data.get('tool_output', ''),
                    success=step_data.get('success', True),
                    thinking=step_data.get('thinking'),
                    execution_code=step_data.get('execution_code'),
                    error_type=ErrorType(step_data['error_type']) if step_data.get('error_type') else None,
                    error_message=step_data.get('error_message'),
                    timestamp=step_data.get('timestamp', time.time()),
                    duration=step_data.get('duration', 0.0)
                )
                converted_steps.append(converted_step)
            
            # åˆ›å»ºTrajectoryResultå¯¹è±¡
            return TrajectoryResult(
                task_id=data.get('task_id', str(uuid.uuid4())),
                task_name=data.get('task_name', data.get('task_id', 'unknown')),
                task_description=data.get('task_description', ''),
                runtime_id=data.get('runtime_id', 'unknown'),
                success=data.get('success', False),
                steps=converted_steps,
                final_result=data.get('final_result', ''),
                error_type=ErrorType(data['error_type']) if data.get('error_type') else None,
                error_message=data.get('error_message'),
                total_duration=data.get('total_duration', 0.0),
                metadata=data.get('metadata', {}),
                created_at=data.get('created_at', time.time())
            )
            
        except Exception as e:
            logger.error(f"Error converting trajectory format: {e}")
            return None
    
    async def _extract_essence(self, trajectory: TrajectoryResult) -> Optional[TaskEssence]:
        """ä½¿ç”¨LLMæå–è½¨è¿¹æœ¬è´¨"""
        try:
            # æ„å»ºåˆ†ææç¤º
            prompt = self._build_extraction_prompt(trajectory)
            
            # è°ƒç”¨LLM
            response = await self.llm_client._call_api(prompt)
            
            # è§£æå“åº”
            return self._parse_extraction_response(response, trajectory)
            
        except Exception as e:
            logger.error(f"Error extracting essence: {e}")
            return None
    
    def _build_extraction_prompt(self, trajectory: TrajectoryResult) -> str:
        """æ„å»ºæœ¬è´¨æå–æç¤º"""
        # ç®€åŒ–è½¨è¿¹ä¿¡æ¯
        steps_summary = []
        for i, step in enumerate(trajectory.steps[:3]):  # åªå–å‰3æ­¥
            steps_summary.append(f"æ­¥éª¤{i+1}: {step.action_type} - {step.observation[:100]}...")
        
        return f"""åˆ†æä»¥ä¸‹ä»»åŠ¡æ‰§è¡Œè½¨è¿¹ï¼Œæå–ä»»åŠ¡æœ¬è´¨ä¿¡æ¯ï¼š

ä»»åŠ¡ID: {trajectory.task_id}
æ‰§è¡ŒæˆåŠŸ: {trajectory.success}
æ€»è€—æ—¶: {trajectory.total_duration:.1f}ç§’
æœ€ç»ˆç»“æœ: {trajectory.final_result[:200]}...

æ‰§è¡Œæ­¥éª¤:
{chr(10).join(steps_summary)}

è¯·æå–ï¼š
1. task_type: ä»»åŠ¡ç±»å‹ (code/web/reasoning)
2. domain: é¢†åŸŸ (algorithm/data_processing/web_automation/searchç­‰)
3. query: æ ¸å¿ƒä»»åŠ¡æè¿°ï¼ˆ20å­—ä»¥å†…ï¼‰
4. complexity: å¤æ‚åº¦ (simple/medium/complex)

è¿”å›JSONæ ¼å¼ï¼š
{{"task_type": "...", "domain": "...", "query": "...", "complexity": "..."}}"""
    
    def _parse_extraction_response(self, response: str, trajectory: TrajectoryResult) -> Optional[TaskEssence]:
        """è§£æLLMå“åº”"""
        try:
            # æå–JSON
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group())
                
                return TaskEssence(
                    essence_id=f"essence_{int(time.time())}_{trajectory.task_id}",
                    task_type=parsed.get("task_type", "code"),
                    domain=parsed.get("domain", "general"),
                    query=parsed.get("query", "åŸºç¡€ä»»åŠ¡"),
                    complexity_level=parsed.get("complexity", "medium"),
                    success_pattern={"duration": trajectory.total_duration},
                    extracted_at=datetime.now().isoformat(),
                    source_trajectory_id=trajectory.task_id
                )
        except Exception as e:
            logger.error(f"Error parsing extraction response: {e}")
        
        return None
    
    def _store_essence(self, essence: TaskEssence):
        """å­˜å‚¨ä»»åŠ¡æœ¬è´¨åˆ°æ•°æ®åº“ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                with sqlite3.connect(self.db_path, timeout=10) as conn:
                    # ç¡®ä¿æ•°æ®åº“è¿æ¥æ­£å¸¸
                    conn.execute("PRAGMA journal_mode=WAL")
                    
                    conn.execute('''
                        INSERT OR REPLACE INTO task_essences 
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        essence.essence_id,
                        essence.task_type,
                        essence.domain,
                        essence.query,
                        essence.complexity_level,
                        json.dumps(essence.success_pattern),
                        essence.extracted_at,
                        essence.source_trajectory_id
                    ))
                    
                    conn.commit()
                    logger.info(f"Successfully stored essence {essence.essence_id}")
                    return
                    
            except sqlite3.OperationalError as e:
                if "database is locked" in str(e).lower() or "no such table" in str(e).lower():
                    logger.warning(f"Database operation failed (attempt {attempt + 1}/{max_retries}): {e}")
                    if attempt < max_retries - 1:
                        time.sleep(retry_delay)
                        retry_delay *= 2
                        # å°è¯•é‡æ–°åˆå§‹åŒ–æ•°æ®åº“
                        if "no such table" in str(e).lower():
                            try:
                                self._init_database()
                            except Exception as init_e:
                                logger.error(f"Failed to re-initialize database: {init_e}")
                        continue
                else:
                    logger.error(f"Database error: {e}")
                    break
            except Exception as e:
                logger.error(f"Unexpected error storing essence: {e}")
                break
        
        logger.error(f"Failed to store essence {essence.essence_id} after {max_retries} attempts")
    
    async def generate_tasks_manually(self, count: int = 3) -> List[TaskSpec]:
        """æ‰‹åŠ¨ç”ŸæˆæŒ‡å®šæ•°é‡çš„ä»»åŠ¡"""
        generated_tasks = []
        
        try:
            # è·å–ç§å­æ•°æ®
            essences = self._get_random_essences(count)
            if not essences:
                logger.warning("No task essences available for generation")
                return generated_tasks
            
            # ä¸ºæ¯ä¸ªessenceç”Ÿæˆå˜å¼‚ä»»åŠ¡
            for essence in essences:
                new_task = await self._generate_task_from_essence(essence)
                if new_task:
                    await self._publish_task(new_task)
                    self._record_generated_task(new_task, essence.essence_id)
                    generated_tasks.append(new_task)
                    logger.info(f"Generated new task: {new_task.task_id}")
            
            return generated_tasks
            
        except Exception as e:
            logger.error(f"Error generating manual tasks: {e}")
            return generated_tasks

    async def generate_task_from_specific_essence(self, essence_id: str) -> Optional[TaskSpec]:
        """åŸºäºæŒ‡å®šçš„ä»»åŠ¡æœ¬è´¨ç”Ÿæˆæ–°ä»»åŠ¡"""
        try:
            # ä»æ•°æ®åº“è·å–æŒ‡å®šçš„æœ¬è´¨
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute('''
                    SELECT * FROM task_essences WHERE essence_id = ?
                ''', (essence_id,))
                
                row = cursor.fetchone()
                if not row:
                    logger.warning(f"Essence {essence_id} not found")
                    return None
                
                essence = TaskEssence(
                    essence_id=row[0],
                    task_type=row[1],
                    domain=row[2],
                    query=row[3],
                    complexity_level=row[4],
                    success_pattern=json.loads(row[5]),
                    extracted_at=row[6],
                    source_trajectory_id=row[7]
                )
            
            # ç”Ÿæˆä»»åŠ¡
            new_task = await self._generate_task_from_essence(essence)
            if new_task:
                await self._publish_task(new_task)
                self._record_generated_task(new_task, essence.essence_id)
                logger.info(f"Generated task {new_task.task_id} from essence {essence_id}")
                return new_task
            
        except Exception as e:
            logger.error(f"Error generating task from essence {essence_id}: {e}")
        
        return None
    
    def _get_random_essences(self, count: int) -> List[TaskEssence]:
        """è·å–éšæœºçš„ç§å­æœ¬è´¨ï¼Œå¸¦é”™è¯¯å¤„ç†"""
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                with sqlite3.connect(self.db_path, timeout=10) as conn:
                    cursor = conn.execute('''
                        SELECT * FROM task_essences 
                        ORDER BY RANDOM() 
                        LIMIT ?
                    ''', (count,))
                    
                    essences = []
                    for row in cursor.fetchall():
                        essences.append(TaskEssence(
                            essence_id=row[0],
                            task_type=row[1],
                            domain=row[2],
                            query=row[3],
                            complexity_level=row[4],
                            success_pattern=json.loads(row[5]),
                            extracted_at=row[6],
                            source_trajectory_id=row[7]
                        ))
                    
                    return essences
                    
            except sqlite3.OperationalError as e:
                logger.warning(f"Database read failed (attempt {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                else:
                    logger.error(f"Failed to read essences after {max_retries} attempts")
                    return []
            except Exception as e:
                logger.error(f"Unexpected error reading essences: {e}")
                return []
        
        return []
    
    async def _generate_task_from_essence(self, essence: TaskEssence) -> Optional[TaskSpec]:
        """åŸºäºæœ¬è´¨ç”Ÿæˆæ–°ä»»åŠ¡"""
        try:
            # æ„å»ºå˜å¼‚æç¤º
            prompt = f"""åŸºäºä»¥ä¸‹ä»»åŠ¡æœ¬è´¨ï¼Œç”Ÿæˆä¸€ä¸ªç›¸ä¼¼ä½†ä¸åŒçš„æ–°ä»»åŠ¡ï¼š

åŸä»»åŠ¡ç±»å‹: {essence.task_type}
åŸä»»åŠ¡é¢†åŸŸ: {essence.domain}
åŸä»»åŠ¡æè¿°: {essence.query}
å¤æ‚åº¦: {essence.complexity_level}

è¯·ç”Ÿæˆä¸€ä¸ªæ–°çš„ç±»ä¼¼ä»»åŠ¡ï¼Œè¦æ±‚ï¼š
1. ä¿æŒç›¸åŒçš„ä»»åŠ¡ç±»å‹å’Œé¢†åŸŸ
2. é€‚å½“è°ƒæ•´å‚æ•°æˆ–åœºæ™¯
3. ä¿æŒç›¸ä¼¼çš„å¤æ‚åº¦

è¿”å›JSONæ ¼å¼ï¼š
{{"description": "æ–°ä»»åŠ¡æè¿°", "expected_tools": ["å·¥å…·åˆ—è¡¨"]}}"""
            
            response = await self.llm_client._call_api(prompt)
            
            # è§£æå“åº”
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group())
                
                # æ„å»ºTaskSpec
                task_id = f"synth_{int(time.time())}_{essence.essence_id.split('_')[-1]}"
                
                return TaskSpec(
                    task_id=task_id,
                    task_type=TaskType(essence.task_type),
                    description=parsed.get("description", essence.query),
                    expected_tools=parsed.get("expected_tools", ["python_executor"]),
                    constraints={},
                    max_steps=5,
                    priority=1
                )
        
        except Exception as e:
            logger.error(f"Error generating task from essence: {e}")
        
        return None
    
    async def _publish_task(self, task: TaskSpec):
        """å‘å¸ƒä»»åŠ¡åˆ°å¯¹åº”é˜Ÿåˆ—"""
        queue_mapping = {
            TaskType.CODE: "tasks:code",
            TaskType.WEB: "tasks:web",
            TaskType.REASONING: "tasks:reasoning"
        }
        
        queue_name = queue_mapping.get(task.task_type, "tasks:code")
        
        await self.redis.xadd(
            queue_name,
            {
                "task": task.json(),
                "submitted_at": time.time(),
                "priority": task.priority,
                "source": "synthesis"
            }
        )
    
    def _record_generated_task(self, task: TaskSpec, essence_id: str):
        """è®°å½•ç”Ÿæˆçš„ä»»åŠ¡ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                with sqlite3.connect(self.db_path, timeout=10) as conn:
                    conn.execute('''
                        INSERT OR REPLACE INTO generated_tasks 
                        VALUES (?, ?, ?, ?, ?)
                    ''', (
                        task.task_id,
                        essence_id,
                        task.json(),
                        datetime.now().isoformat(),
                        False
                    ))
                    conn.commit()
                    logger.info(f"Successfully recorded generated task {task.task_id}")
                    return
                    
            except sqlite3.OperationalError as e:
                logger.warning(f"Database write failed (attempt {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                else:
                    logger.error(f"Failed to record task {task.task_id} after {max_retries} attempts")
            except Exception as e:
                logger.error(f"Unexpected error recording task {task.task_id}: {e}")
                break
    
    def _ensure_processed_json(self):
        """ç¡®ä¿é›†ä¸­æ ‡è®°æ–‡ä»¶å­˜åœ¨"""
        if not os.path.exists(self.processed_json_path):
            with open(self.processed_json_path, 'w', encoding='utf-8') as f:
                json.dump({}, f)
    
    def _load_processed(self) -> dict:
        with self._processed_lock:
            try:
                with open(self.processed_json_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return {}
    
    def _save_processed(self, processed: dict):
        with self._processed_lock:
            with open(self.processed_json_path, 'w', encoding='utf-8') as f:
                json.dump(processed, f, ensure_ascii=False, indent=2)
    
    def _is_trajectory_processed(self, filename: str) -> bool:
        """æ£€æŸ¥è½¨è¿¹æ˜¯å¦å·²å¤„ç†ï¼ˆé›†ä¸­ç®¡ç†ï¼‰"""
        processed = self._load_processed()
        return filename in processed
    
    def _mark_trajectory_processed(self, filename: str):
        """æ ‡è®°è½¨è¿¹å·²å¤„ç†ï¼ˆé›†ä¸­ç®¡ç†ï¼‰"""
        processed = self._load_processed()
        processed[filename] = int(time.time())
        self._save_processed(processed)

async def main():
    """ä»»åŠ¡åˆæˆå™¨ä¸»ç¨‹åº"""
    config = {
        "redis_url": os.getenv("REDIS_URL", "redis://redis:6379"),
        "synthesis_db": os.getenv("SYNTHESIS_DB", "/app/output/synthesis.db"),
        "synthesis_enabled": os.getenv("SYNTHESIS_ENABLED", "false").lower() == "true",
        "vllm_url": os.getenv("VLLM_URL", "http://vllm:8000")
    }
    
    synthesizer = SimpleSynthesizer(config)
    
    try:
        if config["synthesis_enabled"]:
            logger.info("Starting task synthesis service...")
            await synthesizer.start()
        else:
            logger.info("Task synthesis is disabled, service will wait...")
            # ä¿æŒæœåŠ¡è¿è¡Œï¼Œä½†ä¸æ‰§è¡Œåˆæˆé€»è¾‘
            while True:
                await asyncio.sleep(60)
                logger.info("Synthesis service waiting (disabled)...")
    except KeyboardInterrupt:
        logger.info("Synthesis service interrupted")
    finally:
        try:
            await synthesizer.redis.aclose()  # ä½¿ç”¨aclose()æ›¿ä»£close()
        except Exception as e:
            logger.warning(f"Error closing Redis connection: {e}")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main()) 