#!/usr/bin/env python3
"""
ä»»åŠ¡åˆæˆå™¨æ¨¡å— - åŸºäºè½¨è¿¹å­¦ä¹ çš„ç§å­ä»»åŠ¡ç”Ÿæˆç³»ç»Ÿ

ä¸“æ³¨äºé€šè¿‡åˆ†æagentæ‰§è¡Œè½¨è¿¹ï¼Œæå–ä»»åŠ¡æœ¬è´¨ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„ç§å­ä»»åŠ¡ã€‚
å®Œå…¨ç§»é™¤æ•°æ®åº“ä¾èµ–ï¼Œä½¿ç”¨JSONæ–‡ä»¶è¿›è¡Œæ•°æ®å­˜å‚¨ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. è½¨è¿¹åˆ†æï¼šæ·±åº¦ç†è§£agentè¡Œä¸ºæ¨¡å¼
2. æœ¬è´¨æå–ï¼šè¯†åˆ«ä»»åŠ¡çš„æ ¸å¿ƒç‰¹å¾å’ŒæˆåŠŸè¦ç´   
3. ç§å­ç”Ÿæˆï¼šåŸºäºæœ¬è´¨åˆ›é€ æ–°çš„è®­ç»ƒä»»åŠ¡
4. è‡ªåŠ¨ç›‘æ§ï¼šå®æ—¶è·Ÿè¸ªè½¨è¿¹æ–‡ä»¶å˜åŒ–
"""

import os
import sys
import json
import asyncio
import logging
import threading
import time
import uuid
import hashlib
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Any
from collections import defaultdict
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

import aiofiles
import redis  # For synchronous operations in threading contexts
import redis.asyncio as async_redis

from ..interfaces import TaskSpec, TrajectoryResult, TaskType, ExecutionStep, ActionType, ErrorType, LLMInteraction
from ..llm_client import LLMClient
from ..toolscore.unified_tool_library import UnifiedToolLibrary
from ..toolscore.interfaces import ToolType, FunctionToolSpec, ToolCapability
from ..utils.path_utils import get_output_dir, get_trajectories_dir

logger = logging.getLogger(__name__)

@dataclass
class TaskEssence:
    """ä»»åŠ¡æœ¬è´¨æ•°æ®ç»“æ„"""
    essence_id: str
    task_type: str
    domain: str
    query: str
    complexity_level: str
    success_pattern: Dict
    extracted_at: str
    source_trajectory_id: str

class TrajectoryHandler(FileSystemEventHandler):
    """è½¨è¿¹æ–‡ä»¶å˜åŒ–å¤„ç†å™¨"""
    
    def __init__(self, synthesis_instance, target_file_path):
        self.synthesis = synthesis_instance
        self.target_file_path = target_file_path
        
    def on_created(self, event):
        if not event.is_directory and event.src_path == self.target_file_path:
            logger.info(f"ğŸ”” æ£€æµ‹åˆ°è½¨è¿¹é›†åˆæ–‡ä»¶åˆ›å»º: {event.src_path}")
            # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ–¹å¼è§¦å‘å¤„ç†
            self._trigger_processing()
    
    def on_modified(self, event):
        if not event.is_directory and event.src_path == self.target_file_path:
            logger.info(f"ğŸ”” æ£€æµ‹åˆ°è½¨è¿¹é›†åˆæ–‡ä»¶ä¿®æ”¹: {event.src_path}")
            # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ–¹å¼è§¦å‘å¤„ç†
            self._trigger_processing()
    
    def _trigger_processing(self):
        """çº¿ç¨‹å®‰å…¨åœ°è§¦å‘è½¨è¿¹å¤„ç†"""
        try:
            # ä½¿ç”¨Rediså‘é€å¤„ç†å‘½ä»¤ï¼Œè€Œä¸æ˜¯ç›´æ¥è°ƒç”¨å¼‚æ­¥å‡½æ•°
            redis_client = redis.from_url(self.synthesis.config["redis_url"])
            redis_client.xadd(
                "synthesis:commands",
                {
                    "command": "process_trajectories",
                    "timestamp": time.time(),
                    "source": "file_watcher"
                }
            )
            logger.info("ğŸ“¨ å·²å‘é€è½¨è¿¹å¤„ç†å‘½ä»¤åˆ°Redisé˜Ÿåˆ—")
        except Exception as e:
            logger.error(f"âŒ å‘é€å¤„ç†å‘½ä»¤å¤±è´¥: {e}")

from ..unified_tool_manager import UnifiedToolManager

class SynthesisService:
    """ç®€å•ä»»åŠ¡åˆæˆå™¨ - åŸºäºJSONæ–‡ä»¶å­˜å‚¨"""
    
    def __init__(self, config: Dict, tool_manager: UnifiedToolManager):
        self.config = config
        self.redis = async_redis.from_url(config["redis_url"])  # ä½¿ç”¨å¼‚æ­¥rediså®¢æˆ·ç«¯
        self.llm_client = LLMClient(config, tool_manager=tool_manager)
        self.enabled = config.get("synthesis_enabled", False)
        self.tool_library = UnifiedToolLibrary() # åˆå§‹åŒ–UnifiedToolLibrary
          # ä½¿ç”¨ç»Ÿä¸€çš„è·¯å¾„ç®¡ç†
        self.task_essences_path = str(get_output_dir() / "task_essences.json")
        self.seed_tasks_path = str(get_output_dir() / "seed_tasks.jsonl")
        self.processed_trajectories_path = str(get_output_dir() / "processed_trajectories.json")
        self.auto_monitor_enabled = config.get("auto_monitor_trajectories", True)
        self.auto_export_seeds = config.get("auto_export_seeds", True)
        
        # æŒ‡å®šç›‘æ§çš„è½¨è¿¹é›†åˆæ–‡ä»¶
        self.trajectories_collection_path = str(get_output_dir("trajectories") / "trajectories_collection.json")
        self.observer = None
        
        # æ–‡ä»¶é”
        self._file_lock = threading.Lock()
        
        # å·²å¤„ç†è½¨è¿¹çš„è®°å½•ï¼ˆä»æ–‡ä»¶åŠ è½½ï¼‰
        self.processed_trajectories = set()
        
        # åˆå§‹åŒ–JSONæ–‡ä»¶
        self._init_json_files()
        
        # åŠ è½½å·²å¤„ç†çš„è½¨è¿¹åˆ—è¡¨
        self._load_processed_trajectories()
    
    def _init_json_files(self):
        """åˆå§‹åŒ–JSONå­˜å‚¨æ–‡ä»¶"""
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(os.path.dirname(self.task_essences_path), exist_ok=True)
            os.makedirs(os.path.dirname(self.seed_tasks_path), exist_ok=True)
            
            # åˆå§‹åŒ–ä»»åŠ¡æœ¬è´¨æ–‡ä»¶
            if not os.path.exists(self.task_essences_path):
                self._save_json_file(self.task_essences_path, [])
                logger.info(f"âœ… åˆå§‹åŒ–ä»»åŠ¡æœ¬è´¨æ–‡ä»¶: {self.task_essences_path}")
            
            # åˆå§‹åŒ–ç§å­ä»»åŠ¡æ–‡ä»¶ç›®å½•
            if not os.path.exists(self.seed_tasks_path):
                Path(self.seed_tasks_path).touch()
                logger.info(f"âœ… åˆå§‹åŒ–ç§å­ä»»åŠ¡æ–‡ä»¶: {self.seed_tasks_path}")
            
            # åˆå§‹åŒ–å·²å¤„ç†è½¨è¿¹è®°å½•æ–‡ä»¶
            if not os.path.exists(self.processed_trajectories_path):
                self._save_json_file(self.processed_trajectories_path, [])
                logger.info(f"âœ… åˆå§‹åŒ–å·²å¤„ç†è½¨è¿¹è®°å½•æ–‡ä»¶: {self.processed_trajectories_path}")
                
            logger.info("âœ… JSONæ–‡ä»¶å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ JSONæ–‡ä»¶å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _load_processed_trajectories(self):
        """ä»æ–‡ä»¶åŠ è½½å·²å¤„ç†çš„è½¨è¿¹åˆ—è¡¨"""
        try:
            processed_list = self._load_json_file(self.processed_trajectories_path, [])
            self.processed_trajectories = set(processed_list)
            
            if self.processed_trajectories:
                logger.info(f"ğŸ“‹ ä»æ–‡ä»¶åŠ è½½äº† {len(self.processed_trajectories)} ä¸ªå·²å¤„ç†è½¨è¿¹è®°å½•")
                logger.debug(f"å·²å¤„ç†è½¨è¿¹åˆ—è¡¨: {list(self.processed_trajectories)[:5]}{'...' if len(self.processed_trajectories) > 5 else ''}")
            else:
                logger.info("ğŸ“‹ æœªå‘ç°å·²å¤„ç†è½¨è¿¹è®°å½•ï¼Œä»ç©ºç™½çŠ¶æ€å¼€å§‹")
                
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å·²å¤„ç†è½¨è¿¹è®°å½•å¤±è´¥: {e}")
            self.processed_trajectories = set()

    def _save_processed_trajectories(self):
        """å°†å·²å¤„ç†è½¨è¿¹åˆ—è¡¨ä¿å­˜åˆ°æ–‡ä»¶"""
        try:
            processed_list = list(self.processed_trajectories)
            success = self._save_json_file(self.processed_trajectories_path, processed_list)
            if success:
                logger.debug(f"ğŸ’¾ å·²ä¿å­˜ {len(processed_list)} ä¸ªå·²å¤„ç†è½¨è¿¹è®°å½•åˆ°æ–‡ä»¶")
            else:
                logger.error("âŒ ä¿å­˜å·²å¤„ç†è½¨è¿¹è®°å½•å¤±è´¥")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å·²å¤„ç†è½¨è¿¹è®°å½•æ—¶å‡ºé”™: {e}")

    def _load_json_file(self, filepath: str, default_value=None):
        """çº¿ç¨‹å®‰å…¨åœ°åŠ è½½JSONæ–‡ä»¶"""
        with self._file_lock:
            try:
                if os.path.exists(filepath):
                    with open(filepath, 'r', encoding='utf-8') as f:
                        return json.load(f)
                else:
                    return default_value if default_value is not None else []
            except Exception as e:
                logger.error(f"åŠ è½½JSONæ–‡ä»¶å¤±è´¥ {filepath}: {e}")
                return default_value if default_value is not None else []
    
    def _save_json_file(self, filepath: str, data):
        """çº¿ç¨‹å®‰å…¨åœ°ä¿å­˜JSONæ–‡ä»¶"""
        with self._file_lock:
            try:
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ï¼Œç¡®ä¿åŸå­å†™å…¥
                temp_filepath = filepath + '.tmp'
                with open(temp_filepath, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                # åŸå­æ›¿æ¢
                os.replace(temp_filepath, filepath)
                return True
            except Exception as e:
                logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥ {filepath}: {e}")
                return False

    async def start(self):
        """å¯åŠ¨åˆæˆå™¨ï¼Œæ”¯æŒè‡ªåŠ¨è½¨è¿¹ç›‘æ§å’Œç§å­æ•°æ®å¯¼å‡º"""
        if not self.enabled:
            logger.info("Task synthesis is disabled")
            return
            
        logger.info("ğŸš€ å¯åŠ¨åŸºäºJSONçš„ä»»åŠ¡åˆæˆå™¨...")
        
        await self.tool_library.initialize() # åˆå§‹åŒ–UnifiedToolLibrary
        
        # å¯åŠ¨è‡ªåŠ¨è½¨è¿¹ç›‘æ§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.auto_monitor_enabled:
            await self._start_trajectory_monitoring()
        
        # å¯åŠ¨æŒ‡ä»¤ç›‘å¬å™¨
        await self._listen_for_synthesis_commands()

    async def _start_trajectory_monitoring(self):
        """å¯åŠ¨è½¨è¿¹æ–‡ä»¶è‡ªåŠ¨ç›‘æ§ - ä¸“é—¨ç›‘æ§trajectories_collection.json"""
        try:
            logger.info("ğŸ” å¯åŠ¨è½¨è¿¹é›†åˆæ–‡ä»¶ç›‘æ§...")
            
            # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            target_dir = os.path.dirname(self.trajectories_collection_path)
            if not os.path.exists(target_dir):
                logger.warning(f"âš ï¸ è½¨è¿¹ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºç›®å½•: {target_dir}")
                os.makedirs(target_dir, exist_ok=True)
            
            logger.info(f"ğŸ“ ç›‘æ§æ–‡ä»¶: {self.trajectories_collection_path}")
            
            # åˆ›å»ºæ–‡ä»¶ç›‘æ§å™¨
            self.observer = Observer()
            handler = TrajectoryHandler(self, self.trajectories_collection_path)
            
            # ç›‘æ§è½¨è¿¹é›†åˆæ–‡ä»¶æ‰€åœ¨ç›®å½•
            self.observer.schedule(handler, target_dir, recursive=False)
            self.observer.start()
            
            logger.info(f"âœ… è‡ªåŠ¨è½¨è¿¹ç›‘æ§å·²å¯åŠ¨ï¼Œç›‘æ§æ–‡ä»¶: {self.trajectories_collection_path}")
            
            # å¤„ç†ç°æœ‰çš„è½¨è¿¹é›†åˆæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if os.path.exists(self.trajectories_collection_path):
                await self._process_trajectories_collection()
            else:
                logger.info(f"ğŸ“ è½¨è¿¹é›†åˆæ–‡ä»¶å°šä¸å­˜åœ¨: {self.trajectories_collection_path}")
                
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨è½¨è¿¹ç›‘æ§å¤±è´¥: {e}")
    
    async def _process_trajectories_collection(self):
        """å¤„ç†trajectories_collection.jsonæ–‡ä»¶ä¸­çš„è½¨è¿¹"""
        try:
            if not os.path.exists(self.trajectories_collection_path):
                logger.warning(f"âš ï¸ è½¨è¿¹é›†åˆæ–‡ä»¶ä¸å­˜åœ¨: {self.trajectories_collection_path}")
                return
            
            logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†è½¨è¿¹é›†åˆæ–‡ä»¶: {self.trajectories_collection_path}")

            if not os.path.exists(self.trajectories_collection_path) or os.path.getsize(self.trajectories_collection_path) == 0:
                logger.info(f"ğŸ“ Trajectory collection file is empty or does not exist: {self.trajectories_collection_path}")
                return

            # è¯»å–è½¨è¿¹é›†åˆæ•°æ®
            try:
                with open(self.trajectories_collection_path, 'r', encoding='utf-8') as f:
                    trajectories_data = json.load(f)
            except json.JSONDecodeError as e:
                logger.error(f"âŒ Error decoding JSON from {self.trajectories_collection_path}: {e}")
                return
            
            if not isinstance(trajectories_data, list):
                logger.error("âŒ è½¨è¿¹é›†åˆæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºè½¨è¿¹æ•°ç»„")
                return
            
            new_essences = []
            new_seed_tasks = []
            processed_count = 0
            skipped_count = 0
            
            logger.info(f"ğŸ“Š è½¨è¿¹é›†åˆåŒ…å« {len(trajectories_data)} ä¸ªè½¨è¿¹")
            
            for i, trajectory_data in enumerate(trajectories_data):
                try:
                    # ç”Ÿæˆè½¨è¿¹å”¯ä¸€æ ‡è¯†ç¬¦
                    trajectory_id = trajectory_data.get('task_id', f'trajectory_{i}')
                    
                    # é¿å…é‡å¤å¤„ç†
                    if self._is_trajectory_processed(trajectory_id):
                        skipped_count += 1
                        logger.debug(f"â© è·³è¿‡å·²å¤„ç†çš„è½¨è¿¹: {trajectory_id}")
                        continue
                    
                    # è½¬æ¢è½¨è¿¹æ ¼å¼
                    trajectory = self._convert_trajectory_format(trajectory_data)
                    if trajectory and self._should_process_trajectory(trajectory):
                        # æå–ä»»åŠ¡æœ¬è´¨
                        essence = await self._extract_essence(trajectory)
                        if essence:
                            # ä¿å­˜æœ¬è´¨åˆ°JSONæ–‡ä»¶
                            new_essences.append(asdict(essence))
                            
                            # ç›´æ¥è½¬æ¢ä¸ºç§å­ä»»åŠ¡
                            seed_task = await self._convert_essence_to_seed(essence)
                            if seed_task:
                                new_seed_tasks.append(seed_task)
                                processed_count += 1
                                logger.info(f"âœ… ç”Ÿæˆä»»åŠ¡æœ¬è´¨å’Œç§å­ä»»åŠ¡: {trajectory_id}")
                                
                                # æ ‡è®°ä¸ºå·²å¤„ç†
                                self._mark_trajectory_processed(trajectory_id)
                        
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†ç¬¬{i+1}ä¸ªè½¨è¿¹æ—¶å‡ºé”™: {e}")
                    continue
            
            # ä¿å­˜æ–°çš„ä»»åŠ¡æœ¬è´¨
            if new_essences:
                await self._save_new_essences(new_essences)
                logger.info(f"ğŸ’¾ ä¿å­˜ {len(new_essences)} ä¸ªä»»åŠ¡æœ¬è´¨")
            
            # ç›´æ¥è¿½åŠ åˆ°ç§å­æ–‡ä»¶
            if new_seed_tasks:
                await self._append_seed_tasks(new_seed_tasks)
                logger.info(f"ğŸ“¤ æˆåŠŸæ·»åŠ  {len(new_seed_tasks)} ä¸ªç§å­ä»»åŠ¡")
            
            logger.info(f"âœ… è½¨è¿¹é›†åˆå¤„ç†å®Œæˆ: æ–°å¤„ç† {processed_count} ä¸ªï¼Œè·³è¿‡ {skipped_count} ä¸ª")
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†è½¨è¿¹é›†åˆæ–‡ä»¶å¤±è´¥: {e}")
    
    async def _save_new_essences(self, new_essences: List[Dict]):
        """ä¿å­˜æ–°çš„ä»»åŠ¡æœ¬è´¨åˆ°JSONæ–‡ä»¶"""
        try:
            # è¯»å–ç°æœ‰æœ¬è´¨
            existing_essences = self._load_json_file(self.task_essences_path, [])
            
            # æ·»åŠ æ–°æœ¬è´¨
            existing_essences.extend(new_essences)
            
            # ä¿å­˜å›æ–‡ä»¶
            self._save_json_file(self.task_essences_path, existing_essences)
            
            logger.info(f"ğŸ’¾ å·²ä¿å­˜ {len(new_essences)} ä¸ªæ–°ä»»åŠ¡æœ¬è´¨åˆ° {self.task_essences_path}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            type_stats = defaultdict(int)
            domain_stats = defaultdict(int)
            for essence in new_essences:
                type_stats[essence['task_type']] += 1
                domain_stats[essence['domain']] += 1
            
            logger.info(f"ğŸ“Š æ–°å¢æœ¬è´¨åˆ†å¸ƒ - ç±»å‹: {dict(type_stats)}, é¢†åŸŸ: {dict(domain_stats)}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ä»»åŠ¡æœ¬è´¨å¤±è´¥: {e}")

    async def _process_existing_trajectories(self):
        """å¤„ç†ç°æœ‰çš„è½¨è¿¹é›†åˆæ–‡ä»¶"""
        logger.info("ğŸ”„ æ£€æŸ¥ç°æœ‰è½¨è¿¹é›†åˆæ–‡ä»¶...")
        
        if os.path.exists(self.trajectories_collection_path):
            await self._process_trajectories_collection()
        else:
            logger.info("ğŸ“ æ²¡æœ‰ç°æœ‰çš„è½¨è¿¹é›†åˆæ–‡ä»¶")

    async def _process_new_trajectory_file(self, trajectory_path: str):
        """å¤„ç†è½¨è¿¹é›†åˆæ–‡ä»¶ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
        if trajectory_path == self.trajectories_collection_path:
            await self._process_trajectories_collection()
        else:
            logger.debug(f"â© å¿½ç•¥éç›®æ ‡æ–‡ä»¶: {trajectory_path}")
    
    async def _convert_essence_to_seed(self, essence: TaskEssence) -> Optional[Dict]:
        """å°†ä»»åŠ¡æœ¬è´¨ç›´æ¥è½¬æ¢ä¸ºç§å­ä»»åŠ¡"""
        try:
            # ç”Ÿæˆç§å­ä»»åŠ¡ID
            task_id = f"seed_{essence.task_type}_{self._generate_task_id_suffix(essence.query)}"
            
            # æ¨æ–­é¢„æœŸå·¥å…·
            success_pattern = essence.success_pattern
            expected_tools = success_pattern.get('tools_used', [])
            if not expected_tools:
                expected_tools = await self._infer_expected_tools(essence.task_type, essence.domain)
            
            # æ¨æ–­æœ€å¤§æ­¥æ•°
            max_steps = self._infer_max_steps(essence.complexity_level, essence.task_type)
            
            seed_task = {
                "task_id": task_id,
                "task_type": essence.task_type,
                "description": essence.query,
                "expected_tools": expected_tools,
                "max_steps": max_steps,
                "domain": essence.domain,
                "complexity": essence.complexity_level,
                "confidence": success_pattern.get('confidence', 0.8),
                "source_essence_id": essence.essence_id,
                "source_trajectory": essence.source_trajectory_id,
                "extracted_at": essence.extracted_at
            }
            
            return seed_task
            
        except Exception as e:
            logger.error(f"è½¬æ¢ä»»åŠ¡æœ¬è´¨ä¸ºç§å­ä»»åŠ¡æ—¶å‡ºé”™: {e}")
            return None
    
    async def _append_seed_tasks(self, seed_tasks: List[Dict]):
        """è¿½åŠ ç§å­ä»»åŠ¡åˆ°æ–‡ä»¶"""
        with self._file_lock:
            try:
                async with aiofiles.open(self.seed_tasks_path, 'a', encoding='utf-8') as f:
                    for seed_task in seed_tasks:
                        await f.write(json.dumps(seed_task, ensure_ascii=False) + '\n')
                
                logger.info(f"âœ… æˆåŠŸè¿½åŠ  {len(seed_tasks)} ä¸ªç§å­ä»»åŠ¡åˆ° {self.seed_tasks_path}")
                
                # ç»Ÿè®¡ä¿¡æ¯
                type_stats = defaultdict(int)
                for task in seed_tasks:
                    type_stats[task['task_type']] += 1
                
                logger.info(f"ğŸ“Š æ–°å¢ç§å­ä»»åŠ¡åˆ†å¸ƒ: {dict(type_stats)}")
                
            except Exception as e:
                logger.error(f"âŒ è¿½åŠ ç§å­ä»»åŠ¡å¤±è´¥: {e}")

    async def _export_seed_tasks(self):
        """å¯¼å‡ºç§å­ä»»åŠ¡ç»Ÿè®¡å’ŒçŠ¶æ€æŠ¥å‘Š"""
        try:
            if not os.path.exists(self.seed_tasks_path):
                logger.info("ğŸ“ ç§å­ä»»åŠ¡æ–‡ä»¶å°šä¸å­˜åœ¨")
                return
            
            # è¯»å–å¹¶ç»Ÿè®¡ç§å­ä»»åŠ¡
            seed_count = 0
            type_stats = defaultdict(int)
            domain_stats = defaultdict(int)
            
            with open(self.seed_tasks_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        try:
                            task = json.loads(line.strip())
                            seed_count += 1
                            type_stats[task.get('task_type', 'unknown')] += 1
                            domain_stats[task.get('domain', 'unknown')] += 1
                        except json.JSONDecodeError:
                            continue
            
            logger.info("ğŸ“Š ç§å­ä»»åŠ¡å¯¼å‡ºç»Ÿè®¡:")
            logger.info(f"  æ€»æ•°é‡: {seed_count}")
            logger.info(f"  ä»»åŠ¡ç±»å‹åˆ†å¸ƒ: {dict(type_stats)}")
            logger.info(f"  é¢†åŸŸåˆ†å¸ƒ: {dict(domain_stats)}")
            logger.info(f"  æ–‡ä»¶è·¯å¾„: {self.seed_tasks_path}")
            
            # å‘å¸ƒç»Ÿè®¡ä¿¡æ¯åˆ°Redis
            export_stats = {
                "total_seeds": seed_count,
                "type_distribution": dict(type_stats),
                "domain_distribution": dict(domain_stats),
                "file_path": self.seed_tasks_path,
                "exported_at": datetime.now().isoformat()
            }
            
            await self.redis.xadd(
                "synthesis:seed_export",
                {
                    "timestamp": time.time(),
                    "stats": json.dumps(export_stats)
                }
            )
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºç§å­ä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {e}")

    def _generate_task_id_suffix(self, description: str) -> str:
        """æ ¹æ®æè¿°ç”Ÿæˆä»»åŠ¡IDåç¼€"""
        # ä½¿ç”¨æè¿°çš„å“ˆå¸Œå€¼ç”ŸæˆçŸ­åç¼€
        hash_obj = hashlib.md5(description.encode('utf-8'))
        return hash_obj.hexdigest()[:8]
    
    async def _infer_expected_tools(self, task_type: str, domain: str) -> List[str]:
        """æ ¹æ®ä»»åŠ¡ç±»å‹å’Œé¢†åŸŸæ¨æ–­é¢„æœŸå·¥å…· - åŠ¨æ€ä»UnifiedToolLibraryè·å–"""
        
        all_tools = await self.tool_library.get_all_tools()
        available_tool_ids = {tool.tool_id for tool in all_tools}
        
        inferred_tools = set()

        # ä¼˜å…ˆåŒ¹é…æ˜ç¡®çš„å·¥å…·ID
        if task_type == 'code':
            if "python_executor" in available_tool_ids:
                inferred_tools.add("python_executor")
        elif task_type == 'web':
            if "browser_navigator" in available_tool_ids:
                inferred_tools.add("browser_navigator")
            # å‡è®¾æœ‰å…¶ä»–webå·¥å…·ï¼Œä¾‹å¦‚"web_scraper"
            # if "web_scraper" in available_tool_ids:
            #     inferred_tools.add("web_scraper")
        elif task_type == 'reasoning':
            if "browser_navigator" in available_tool_ids:
                inferred_tools.add("browser_navigator")
            if "python_executor" in available_tool_ids:
                inferred_tools.add("python_executor")

        # æ ¹æ®é¢†åŸŸè¿›ä¸€æ­¥ç»†åŒ–ï¼ŒåŒ¹é…å·¥å…·çš„tagsæˆ–description
        # è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„åŒ¹é…é€»è¾‘
        domain_keywords_map = {
            'data_analysis': ['data', 'analysis', 'pandas', 'numpy', 'matplotlib'],
            'web_automation': ['web', 'browser', 'scrape', 'requests', 'BeautifulSoup'],
            'algorithm': ['algorithm', 'math', 'calculate'],
            'research': ['search', 'research', 'query'],
            'stock_analysis': ['stock', 'finance', 'market']
        }

        domain_keywords = domain_keywords_map.get(domain, [])
        
        for tool in all_tools:
            tool_description_lower = tool.description.lower()
            tool_name_lower = tool.name.lower()
            tool_tags_lower = [tag.lower() for tag in tool.tags] if tool.tags else []

            # æ£€æŸ¥å·¥å…·æè¿°ã€åç§°æˆ–æ ‡ç­¾æ˜¯å¦åŒ…å«é¢†åŸŸå…³é”®è¯
            if any(keyword in tool_description_lower or
                   keyword in tool_name_lower or
                   any(keyword in tag for tag in tool_tags_lower)
                   for keyword in domain_keywords):
                inferred_tools.add(tool.tool_id)
        
        # å¦‚æœæ²¡æœ‰æ¨æ–­å‡ºä»»ä½•å·¥å…·ï¼Œåˆ™æ ¹æ®ä»»åŠ¡ç±»å‹æä¾›ä¸€ä¸ªé»˜è®¤å·¥å…·
        if not inferred_tools:
            if task_type == 'code' and "python_executor" in available_tool_ids:
                inferred_tools.add("python_executor")
            elif task_type == 'web' and "browser_navigator" in available_tool_ids:
                inferred_tools.add("browser_navigator")
            elif task_type == 'reasoning' and "browser_navigator" in available_tool_ids:
                inferred_tools.add("browser_navigator")
            elif task_type == 'reasoning' and "python_executor" in available_tool_ids:
                inferred_tools.add("python_executor")

        return list(inferred_tools)
    
    def _infer_max_steps(self, complexity_level: str, task_type: str) -> int:
        """æ ¹æ®å¤æ‚åº¦å’Œä»»åŠ¡ç±»å‹æ¨æ–­æœ€å¤§æ­¥æ•°"""
        base_steps = {
            'simple': 5,
            'medium': 10,
            'complex': 15
        }
        
        steps = base_steps.get(complexity_level, 8)
        
        # reasoningä»»åŠ¡é€šå¸¸éœ€è¦æ›´å¤šæ­¥éª¤
        if task_type == 'reasoning':
            steps += 5
        
        return min(steps, 20)  # æœ€å¤§ä¸è¶…è¿‡20æ­¥

    async def _listen_for_synthesis_commands(self):
        """ç›‘å¬åˆæˆæŒ‡ä»¤"""
        logger.info("ğŸ¯ Synthesis service ready - waiting for manual triggers")
        logger.info("Available trigger methods:")
        logger.info("1. Redis command: XADD synthesis:commands command trigger_synthesis")
        logger.info("2. Redis command: XADD synthesis:commands command process_trajectories")
        logger.info("3. Redis command: XADD synthesis:commands command process_specific trajectory_file.json")
        
        # é¦–å…ˆå¤„ç†é˜Ÿåˆ—ä¸­ç°æœ‰çš„å‘½ä»¤
        await self._process_pending_commands()
        
        while True:
            try:
                # ç›‘å¬synthesis:commandsé˜Ÿåˆ—ï¼Œä½¿ç”¨$è¡¨ç¤ºä»å½“å‰æœ€æ–°ä½ç½®å¼€å§‹è¯»å–æ–°æ¶ˆæ¯
                # ä½¿ç”¨ type: ignore æŠ‘åˆ¶ Pylance å¯¹ redis.asyncio.Redis.xread ç±»å‹æç¤ºçš„è¯¯æŠ¥ã€‚
                streams = {b"synthesis:commands": b"$"}
                result = await self.redis.xread(streams, count=1, block=5000)  # type: ignore # 5ç§’è¶…æ—¶
                
                if result:
                    for stream_name, messages in result:
                        for message_id, fields in messages:
                            await self._handle_synthesis_command(fields)
                            # ç¡®è®¤å¤„ç†å®Œæˆ
                            await self.redis.xdel("synthesis:commands", message_id)
                
            except Exception as e:
                logger.error(f"Error listening for synthesis commands: {e}")
                await asyncio.sleep(10)

    async def _process_pending_commands(self):
        """å¤„ç†é˜Ÿåˆ—ä¸­ç°æœ‰çš„å¾…å¤„ç†å‘½ä»¤"""
        try:
            # è¯»å–é˜Ÿåˆ—ä¸­æ‰€æœ‰ç°æœ‰å‘½ä»¤
            # ä½¿ç”¨ type: ignore æŠ‘åˆ¶ Pylance å¯¹ redis.asyncio.Redis.xread ç±»å‹æç¤ºçš„è¯¯æŠ¥ã€‚
            result = await self.redis.xread({b"synthesis:commands": b"0"}, count=100)  # type: ignore
            
            if result:
                for stream_name, messages in result:
                    logger.info(f"Found {len(messages)} pending commands in queue")
                    for message_id, fields in messages:
                        logger.info(f"Processing pending command: {message_id}")
                        await self._handle_synthesis_command(fields)
                        # åˆ é™¤å·²å¤„ç†çš„å‘½ä»¤
                        await self.redis.xdel("synthesis:commands", message_id)
                        
                logger.info("âœ… All pending commands processed")
            else:
                logger.info("No pending commands found")
                
        except Exception as e:
            logger.error(f"Error processing pending commands: {e}")

    async def _handle_synthesis_command(self, command_fields: dict):
        """å¤„ç†åˆæˆæŒ‡ä»¤"""
        try:
            command = command_fields.get(b'command', b'').decode('utf-8')
            logger.info(f"ğŸ“¨ Received synthesis command: {command}")
            
            if command == "trigger_synthesis":
                # è§¦å‘å®Œæ•´çš„è½¨è¿¹å¤„ç†
                await self._process_all_trajectories_once()
                # è‡ªåŠ¨å¯¼å‡ºç§å­æ•°æ®
                if self.auto_export_seeds:
                    await self._export_seed_tasks()
                
            elif command == "process_trajectories":
                # å¤„ç†æ‰€æœ‰æœªå¤„ç†çš„è½¨è¿¹
                await self._process_unprocessed_trajectories()
                # è‡ªåŠ¨å¯¼å‡ºç§å­æ•°æ®
                if self.auto_export_seeds:
                    await self._export_seed_tasks()
                
            elif command.startswith("process_specific"):
                # å¤„ç†æŒ‡å®šçš„è½¨è¿¹æ–‡ä»¶
                parts = command.split(" ", 1)
                if len(parts) > 1:
                    filename = parts[1]
                    await self._process_specific_trajectory(filename)
                    # è‡ªåŠ¨å¯¼å‡ºç§å­æ•°æ®
                    if self.auto_export_seeds:
                        await self._export_seed_tasks()
                    
            elif command == "export_seeds":
                # æ‰‹åŠ¨å¯¼å‡ºç§å­ä»»åŠ¡
                await self._export_seed_tasks()
                
            elif command == "start_monitoring":
                # å¯åŠ¨è½¨è¿¹ç›‘æ§
                if not self.observer or not self.observer.is_alive():
                    await self._start_trajectory_monitoring()
                    logger.info("âœ… è½¨è¿¹ç›‘æ§å·²å¯åŠ¨")
                else:
                    logger.info("âš ï¸ è½¨è¿¹ç›‘æ§å·²åœ¨è¿è¡Œ")
                    
            elif command == "stop_monitoring":
                # åœæ­¢è½¨è¿¹ç›‘æ§
                if self.observer and self.observer.is_alive():
                    self.observer.stop()
                    self.observer.join()
                    logger.info("ğŸ›‘ è½¨è¿¹ç›‘æ§å·²åœæ­¢")
                else:
                    logger.info("âš ï¸ è½¨è¿¹ç›‘æ§æœªè¿è¡Œ")
                    
            elif command == "generate_tasks":
                # æ‰‹åŠ¨ç”Ÿæˆä»»åŠ¡
                count = int(command_fields.get(b'count', b'3').decode('utf-8'))
                tasks = await self.generate_tasks_manually(count)
                logger.info(f"Generated {len(tasks)} tasks manually")
                
            elif command == "generate_seeds_from_essences":
                # ä»ç°æœ‰ä»»åŠ¡æœ¬è´¨ç”Ÿæˆç§å­ä»»åŠ¡
                await self._generate_seeds_from_existing_essences()
                
            elif command == "status":
                # æŠ¥å‘ŠçŠ¶æ€
                await self._report_synthesis_status()
                
            else:
                logger.warning(f"Unknown synthesis command: {command}")
                
        except Exception as e:
            logger.error(f"Error handling synthesis command: {e}")
    
    async def _process_all_trajectories_once(self):
        """ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰è½¨è¿¹ï¼ˆä¸å¾ªç¯ï¼‰"""
        logger.info("ğŸ”„ Starting one-time trajectory processing...")
        
        try:
            trajectories_dir = get_trajectories_dir()
            if not os.path.exists(trajectories_dir):
                logger.warning("Trajectories directory not found")
                return
            
            processed_count = 0
            skipped_count = 0
            
            # å¤„ç†æ‰€æœ‰è½¨è¿¹æ–‡ä»¶
            for filename in os.listdir(trajectories_dir):
                if filename.endswith('.json'):
                    trajectory_path = os.path.join(trajectories_dir, filename)
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
                    if not self._is_trajectory_processed(filename):
                        await self._process_trajectory_file(trajectory_path)
                        self._mark_trajectory_processed(filename)
                        processed_count += 1
                        logger.info(f"âœ… Processed: {filename}")
                    else:
                        skipped_count += 1
                        logger.debug(f"â© Skipped (already processed): {filename}")
            
            logger.info(f"ğŸ¯ Trajectory processing completed: {processed_count} processed, {skipped_count} skipped")
            
        except Exception as e:
            logger.error(f"Error in one-time trajectory processing: {e}")

    async def _process_unprocessed_trajectories(self):
        """åªå¤„ç†æœªå¤„ç†çš„è½¨è¿¹"""
        logger.info("ğŸ”„ Processing only unprocessed trajectories...")
        try:
            trajectories_dir = get_trajectories_dir()
            if not os.path.exists(trajectories_dir):
                logger.warning("Trajectories directory not found")
                return
            
            processed_count = 0
            
            # è·å–æ‰€æœ‰è½¨è¿¹æ–‡ä»¶å¹¶å¤„ç†å…¶ä¸­æœªå¤„ç†çš„è½¨è¿¹
            for filename in os.listdir(trajectories_dir):
                if filename.endswith('.json'):
                    trajectory_path = os.path.join(trajectories_dir, filename)
                    # å¤„ç†æ–‡ä»¶ä¸­æ‰€æœ‰æœªå¤„ç†çš„è½¨è¿¹
                    file_processed_count = await self._process_unprocessed_in_file(trajectory_path)
                    processed_count += file_processed_count
            
            logger.info(f"ğŸ¯ Unprocessed trajectories completed: {processed_count} new trajectories processed")
            
        except Exception as e:
            logger.error(f"Error processing unprocessed trajectories: {e}")

    async def _process_unprocessed_in_file(self, trajectory_path: str) -> int:
        """å¤„ç†å•ä¸ªæ–‡ä»¶ä¸­æœªå¤„ç†çš„è½¨è¿¹ï¼Œè¿”å›å¤„ç†æ•°é‡"""
        try:
            logger.info(f"ğŸ” Checking for unprocessed trajectories in: {trajectory_path}")
            if not os.path.exists(trajectory_path) or os.path.getsize(trajectory_path) == 0:
                logger.info(f"ğŸ“ Trajectory file is empty or does not exist: {trajectory_path}")
                return 0
            
            with open(trajectory_path, 'r', encoding='utf-8') as f:
                try:
                    trajectory_data = json.load(f)
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ Error decoding JSON from {trajectory_path}: {e}")
                    return 0 #æ— æ³•è§£ææ–‡ä»¶ï¼Œè¿”å›0
            
            processed_count = 0
            new_seed_tasks = []  # æ”¶é›†æ–°ç”Ÿæˆçš„ç§å­ä»»åŠ¡
            
            # å¦‚æœæ˜¯è½¨è¿¹åˆ—è¡¨ï¼Œå¤„ç†æ¯ä¸€ä¸ªæœªå¤„ç†çš„è½¨è¿¹
            if isinstance(trajectory_data, list):
                logger.info(f"ğŸ“Š Found trajectory collection with {len(trajectory_data)} trajectories")
                for i, single_trajectory in enumerate(trajectory_data):
                    if processed_count >= 10:  # å¤„ç†æ•°é‡é™åˆ¶
                        logger.info(f"â¹ï¸ Reached processing limit of 10 trajectories")
                        break
                    try:
                        trajectory = self._convert_trajectory_format(single_trajectory)
                        if trajectory:
                            # åŸºäºè½¨è¿¹IDæ£€æŸ¥æ˜¯å¦å·²å¤„ç†
                            if not self._is_trajectory_processed(trajectory.task_id):
                                should_process = self._should_process_trajectory(trajectory)
                                logger.info(f"ğŸ“‹ New trajectory {trajectory.task_id}: runtime={trajectory.runtime_id}, success={trajectory.success}, should_process={should_process}")
                                
                                if should_process:
                                    essence = await self._extract_essence(trajectory)
                                    if essence:
                                        self._store_essence(essence)
                                        
                                        # ç«‹å³ç”Ÿæˆç§å­ä»»åŠ¡
                                        seed_task = await self._convert_essence_to_seed(essence)
                                        if seed_task:
                                            new_seed_tasks.append(seed_task)
                                            logger.info(f"ğŸŒ± Generated seed task from essence: {essence.essence_id}")
                                        
                                        # æ ‡è®°è¿™ä¸ªè½¨è¿¹IDå·²å¤„ç†
                                        self._mark_trajectory_processed(trajectory.task_id)
                                        processed_count += 1
                                        logger.info(f"âœ… Extracted essence {essence.task_type}/{essence.domain} from trajectory {trajectory.task_id}")
                                    else:
                                        logger.warning(f"âŒ Failed to extract essence from trajectory {trajectory.task_id}")
                                        # å³ä½¿æå–å¤±è´¥ä¹Ÿæ ‡è®°ä¸ºå·²å¤„ç†ï¼Œé¿å…é‡å¤å°è¯•
                                        self._mark_trajectory_processed(trajectory.task_id)
                                else:
                                    logger.info(f"â­ï¸ Skipping trajectory {trajectory.task_id} (not worth processing)")
                                    # æ ‡è®°ä¸ºå·²å¤„ç†
                                    self._mark_trajectory_processed(trajectory.task_id)
                            else:
                                logger.debug(f"â­ï¸ Trajectory {trajectory.task_id} already processed")
                    except Exception as e:
                        logger.error(f"âŒ Error processing trajectory {i}: {e}")
                
                # æ‰¹é‡ä¿å­˜ç§å­ä»»åŠ¡
                if new_seed_tasks:
                    await self._append_seed_tasks(new_seed_tasks)
                    logger.info(f"ğŸ’¾ Saved {len(new_seed_tasks)} seed tasks to file")
                
                if processed_count > 0:
                    logger.info(f"ğŸ¯ Successfully processed {processed_count} new trajectories from {trajectory_path}")
                return processed_count
                
            else:
                # å•ä¸ªè½¨è¿¹å¯¹è±¡
                trajectory = self._convert_trajectory_format(trajectory_data)
                if trajectory and not self._is_trajectory_processed(trajectory.task_id):
                    if self._should_process_trajectory(trajectory):
                        essence = await self._extract_essence(trajectory)
                        if essence:
                            self._store_essence(essence)
                            
                            # ç«‹å³ç”Ÿæˆç§å­ä»»åŠ¡
                            seed_task = await self._convert_essence_to_seed(essence)
                            if seed_task:
                                await self._append_seed_tasks([seed_task])
                                logger.info(f"ğŸŒ± Generated and saved seed task from essence: {essence.essence_id}")
                            
                            self._mark_trajectory_processed(trajectory.task_id)
                            logger.info(f"âœ… Extracted essence from single trajectory {trajectory.task_id}")
                            return 1
                    else:
                        self._mark_trajectory_processed(trajectory.task_id)
                
                return 0
        
        except Exception as e:
            logger.error(f"âŒ Error processing trajectory file {trajectory_path}: {e}")
            return 0

    async def _process_specific_trajectory(self, filename: str):
        """å¤„ç†æŒ‡å®šçš„è½¨è¿¹æ–‡ä»¶"""
        logger.info(f"ğŸ¯ Processing specific trajectory: {filename}")
        
        try:
            trajectories_dir = "/app/output/trajectories"
            trajectory_path = os.path.join(trajectories_dir, filename)
            
            if not os.path.exists(trajectory_path):
                logger.error(f"Trajectory file not found: {filename}")
                return
            
            await self._process_trajectory_file(trajectory_path)
            self._mark_trajectory_processed(filename)
            logger.info(f"âœ… Successfully processed specific trajectory: {filename}")
            
        except Exception as e:
            logger.error(f"Error processing specific trajectory {filename}: {e}")

    async def _report_synthesis_status(self):
        """æŠ¥å‘ŠåˆæˆæœåŠ¡çŠ¶æ€"""
        try:
            # ç»Ÿè®¡ç§å­ä»»åŠ¡æ–‡ä»¶
            seed_count = 0
            seed_type_stats = defaultdict(int)
            if os.path.exists(self.seed_tasks_path):
                with open(self.seed_tasks_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            try:
                                task = json.loads(line.strip())
                                seed_count += 1
                                seed_type_stats[task.get('task_type', 'unknown')] += 1
                            except json.JSONDecodeError:
                                continue
            
            # ç»Ÿè®¡ä»»åŠ¡æœ¬è´¨æ–‡ä»¶
            essence_count = 0
            essence_type_stats = defaultdict(int)
            essence_domain_stats = defaultdict(int)
            if os.path.exists(self.task_essences_path):
                essences_data = self._load_json_file(self.task_essences_path, [])
                essence_count = len(essences_data)
                for essence in essences_data:
                    essence_type_stats[essence.get('task_type', 'unknown')] += 1
                    essence_domain_stats[essence.get('domain', 'unknown')] += 1
            
            # ç»Ÿè®¡è½¨è¿¹é›†åˆçŠ¶æ€
            collection_exists = os.path.exists(self.trajectories_collection_path)
            collection_size = 0
            if collection_exists:
                try:
                    with open(self.trajectories_collection_path, 'r', encoding='utf-8') as f:
                        collection_data = json.load(f)
                        collection_size = len(collection_data) if isinstance(collection_data, list) else 0
                except:
                    collection_size = 0
            
            processed_count = len(self.processed_trajectories)
            
            status_info = {
                "synthesis_enabled": self.enabled,
                "storage_type": "JSONæ–‡ä»¶å­˜å‚¨",
                "monitoring_enabled": self.auto_monitor_enabled,
                "target_file": self.trajectories_collection_path,
                "collection_exists": collection_exists,
                "collection_size": collection_size,
                "processed_trajectories": processed_count,
                "unprocessed_count": max(0, collection_size - processed_count),
                "total_task_essences": essence_count,
                "essence_type_distribution": dict(essence_type_stats),
                "essence_domain_distribution": dict(essence_domain_stats),
                "total_seed_tasks": seed_count,
                "seed_type_distribution": dict(seed_type_stats),
                "essence_file_path": self.task_essences_path,
                "seed_file_path": self.seed_tasks_path,
                "observer_running": self.observer.is_alive() if self.observer else False,
                "auto_export_seeds": self.auto_export_seeds
            }
            
            logger.info("ğŸ“Š Synthesis Status Report:")
            for key, value in status_info.items():
                logger.info(f"  {key}: {value}")
                
            # å‘å¸ƒçŠ¶æ€åˆ°Redisä¾›å¤–éƒ¨æŸ¥è¯¢
            await self.redis.xadd(
                "synthesis:status",
                {
                    "timestamp": time.time(),
                    "status": json.dumps(status_info)
                }
            )
            
        except Exception as e:
            logger.error(f"Error generating status report: {e}")

    # ä¿ç•™åŸæœ‰çš„è‡ªåŠ¨å¤„ç†æ–¹æ³•ä½œä¸ºå¤‡ç”¨
    async def _process_trajectory_feedback(self):
        """å¤„ç†è½¨è¿¹åé¦ˆï¼Œæå–ä»»åŠ¡æœ¬è´¨ï¼ˆåŸè‡ªåŠ¨å¤„ç†æ–¹æ³•ï¼Œç°åœ¨ä½œä¸ºå¤‡ç”¨ï¼‰"""
        logger.info("âš ï¸  Note: This is the old auto-processing method, now used as backup")
        
        while True:
            try:
                # æ‰«æè½¨è¿¹è¾“å‡ºç›®å½•
                trajectories_dir = "/app/output/trajectories"
                if not os.path.exists(trajectories_dir):
                    await asyncio.sleep(30)
                    continue
                
                # å¤„ç†æ–°çš„è½¨è¿¹æ–‡ä»¶
                for filename in os.listdir(trajectories_dir):
                    if filename.endswith('.json'):
                        trajectory_path = os.path.join(trajectories_dir, filename)
                        
                        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
                        if not self._is_trajectory_processed(filename):
                            await self._process_trajectory_file(trajectory_path)
                            self._mark_trajectory_processed(filename)
                
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"Error processing trajectory feedback: {e}")
                await asyncio.sleep(30)
    
    def _should_process_trajectory(self, trajectory: TrajectoryResult) -> bool:
        """åˆ¤æ–­è½¨è¿¹æ˜¯å¦å€¼å¾—å¤„ç†"""
        # 1. æˆåŠŸçš„è½¨è¿¹æ€»æ˜¯å¤„ç†
        if trajectory.success:
            return True
        
        # 2. reasoning runtimeçš„è½¨è¿¹ï¼Œå³ä½¿å¤±è´¥ä¹Ÿå¯èƒ½æœ‰ä»·å€¼ï¼ˆä¸è¦æ±‚æœ‰æ‰§è¡Œæ­¥éª¤ï¼‰
        runtime_id = trajectory.runtime_id.lower()
        if 'reasoning' in runtime_id:
            logger.info(f"ğŸ§  Found reasoning trajectory: {trajectory.task_id}")
            return True
        
        # 3. æœ‰æ‰§è¡Œæ­¥éª¤çš„è½¨è¿¹
        if len(trajectory.steps) > 0:
            # æœ‰å¤šä¸ªæ­¥éª¤çš„å¤æ‚ä»»åŠ¡ï¼Œå³ä½¿å¤±è´¥ä¹Ÿå¯èƒ½æœ‰ä»·å€¼
            if len(trajectory.steps) >= 2:
                return True
        
        # 4. ä»»åŠ¡æè¿°åŒ…å«ç‰¹å®šå…³é”®è¯
        task_desc = trajectory.task_description.lower()
        valuable_keywords = ['reasoning', 'æ¨ç†', 'åˆ†æ', 'analysis', 'compare', 'å¯¹æ¯”', 'ç ”ç©¶']
        if any(keyword in task_desc for keyword in valuable_keywords):
            logger.info(f"ğŸ” Found valuable keywords in task description: {trajectory.task_id}")
            return True
        
        # 5. æœ‰æœ€ç»ˆç»“æœçš„è½¨è¿¹ï¼Œå³ä½¿å¤±è´¥ä¹Ÿå¯èƒ½æœ‰ä»·å€¼
        if trajectory.final_result and len(trajectory.final_result.strip()) > 50:
            logger.info(f"ğŸ“ Found trajectory with substantial final result: {trajectory.task_id}")
            return True
        
        return False

    async def _process_trajectory_file(self, trajectory_path: str) -> bool:
        """å¤„ç†å•ä¸ªè½¨è¿¹æ–‡ä»¶ï¼Œè¿”å›å¤„ç†æ˜¯å¦æˆåŠŸ"""
        try:
            logger.info(f"ğŸ” Processing trajectory file: {trajectory_path}")
            with open(trajectory_path, 'r', encoding='utf-8') as f:
                trajectory_data = json.load(f)
            
            processed_count = 0
            
            # å¦‚æœæ˜¯è½¨è¿¹åˆ—è¡¨ï¼Œå¤„ç†æ¯ä¸€ä¸ªè½¨è¿¹
            if isinstance(trajectory_data, list):
                logger.info(f"ğŸ“Š Processing trajectory collection with {len(trajectory_data)} trajectories")
                for i, single_trajectory in enumerate(trajectory_data):
                    if processed_count >= 10:  # å¢åŠ å¤„ç†æ•°é‡é™åˆ¶
                        logger.info(f"â¹ï¸ Reached processing limit of 10 trajectories")
                        break
                    try:
                        trajectory = self._convert_trajectory_format(single_trajectory)
                        if trajectory:
                            # å¢å¼ºè½¨è¿¹å¤„ç†é€»è¾‘ï¼šå¤„ç†æ›´å¤šç±»å‹çš„è½¨è¿¹
                            should_process = self._should_process_trajectory(trajectory)
                            logger.info(f"ğŸ“‹ Trajectory {trajectory.task_id}: runtime={trajectory.runtime_id}, success={trajectory.success}, should_process={should_process}")
                            
                            if should_process:
                                essence = await self._extract_essence(trajectory)
                                if essence:
                                    self._store_essence(essence)
                                    processed_count += 1
                                    logger.info(f"âœ… Extracted essence {essence.task_type}/{essence.domain} from trajectory {trajectory.task_id}")
                                else:
                                    logger.warning(f"âŒ Failed to extract essence from trajectory {trajectory.task_id}")
                            else:
                                logger.info(f"â­ï¸ Skipping trajectory {trajectory.task_id} (not worth processing)")
                    except Exception as e:
                        logger.error(f"âŒ Error processing trajectory {i}: {e}")
                
                logger.info(f"ğŸ¯ Successfully processed {processed_count} trajectories from collection")
                return processed_count > 0
                
            else:
                # å•ä¸ªè½¨è¿¹å¯¹è±¡
                trajectory = self._convert_trajectory_format(trajectory_data)
                if trajectory and self._should_process_trajectory(trajectory):
                    essence = await self._extract_essence(trajectory)
                    if essence:
                        self._store_essence(essence)
                        logger.info(f"âœ… Extracted essence from single trajectory {trajectory.task_id}")
                        return True
                
                return False
        
        except Exception as e:
            logger.error(f"âŒ Error processing trajectory file {trajectory_path}: {e}")
            return False
    
    def _convert_trajectory_format(self, data: Dict) -> Optional[TrajectoryResult]:
        """å°†è½¨è¿¹æ•°æ®è½¬æ¢ä¸ºTrajectoryResultæ ¼å¼"""
        try:
            logger.debug(f"Attempting to convert trajectory data for task_id: {data.get('task_id', 'Unknown')}, type of data: {type(data)}")
            # è½¬æ¢stepsæ ¼å¼
            converted_steps = []
            steps_list = data.get('steps', [])
            if not isinstance(steps_list, list):
                logger.error(f"Field 'steps' is not a list for task_id: {data.get('task_id', 'Unknown')}. Got {type(steps_list)}. Skipping steps conversion.")
                steps_list = []

            for i, step_data in enumerate(steps_list):
                logger.debug(f"Processing step {i} for task_id: {data.get('task_id', 'Unknown')}: type={type(step_data)}, content='{str(step_data)[:200]}...'")
                
                # å¤„ç†ExecutionStepå¯¹è±¡
                if hasattr(step_data, 'to_dict'):
                    # å¦‚æœæ˜¯ExecutionStepå¯¹è±¡ï¼Œç›´æ¥ä½¿ç”¨
                    converted_steps.append(step_data)
                    continue
                elif isinstance(step_data, str) and step_data.startswith('ExecutionStep('):
                    # å¦‚æœæ˜¯ExecutionStepçš„å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œè·³è¿‡ï¼ˆæ— æ³•å®‰å…¨è§£æï¼‰
                    logger.warning(f"Skipping ExecutionStep string representation for step {i} in task_id: {data.get('task_id', 'Unknown')}")
                    continue
                elif isinstance(step_data, dict):
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºExecutionStepå¯¹è±¡
                    converted_step = ExecutionStep(
                        step_id=step_data.get('step_id', 0),
                        action_type=ActionType(step_data.get('action_type', 'code_generation')),
                        action_params=step_data.get('action_params', step_data.get('tool_input', {})),
                        observation=step_data.get('observation', step_data.get('tool_output', '')),
                        success=step_data.get('success', True),
                        thinking=step_data.get('thinking'),
                        execution_code=step_data.get('execution_code'),
                        error_type=self._safe_parse_error_type(step_data.get('error_type')), # ç¡®ä¿æ˜¯ErrorTypeæšä¸¾
                        error_message=step_data.get('error_message'),
                        timestamp=step_data.get('timestamp', time.time()),
                        duration=step_data.get('duration', 0.0),
                        # llm_interactions å­—æ®µåœ¨ ExecutionStep å®šä¹‰ä¸­ï¼Œä½†åŸå§‹æ•°æ®ä¸­å¯èƒ½æ²¡æœ‰ï¼Œéœ€è¦å¤„ç†
                        llm_interactions=[LLMInteraction(**interaction_dict) for interaction_dict in step_data.get('llm_interactions', []) if isinstance(interaction_dict, dict)]
                    )
                    converted_steps.append(converted_step)
                else:
                    logger.error(f"Skipping step {i} for task_id: {data.get('task_id', 'Unknown')} due to unexpected format. Expected dict or ExecutionStep, got {type(step_data)}. Content: {str(step_data)[:200]}")
                    continue
            
            # åˆ›å»ºTrajectoryResultå¯¹è±¡
            return TrajectoryResult(
                task_id=data.get('task_id', str(uuid.uuid4())),
                task_name=data.get('task_name', data.get('task_id', 'unknown')),
                task_description=data.get('task_description', ''),
                runtime_id=data.get('runtime_id', 'unknown'),
                success=data.get('success', False),
                steps=converted_steps,
                final_result=data.get('final_result', ''),
                error_type=self._safe_parse_error_type(data.get('error_type')),
                error_message=data.get('error_message'),
                total_duration=data.get('total_duration', 0.0),
                metadata=data.get('metadata', {}),
                created_at=data.get('created_at', time.time())
            )
            
        except Exception as e:
            logger.error(f"Error converting trajectory format: {e}")
            return None
    
    def _safe_parse_error_type(self, error_type_data) -> Optional[ErrorType]:
        """å®‰å…¨è§£æé”™è¯¯ç±»å‹"""
        if not error_type_data:
            return None
        
        try:
            # å¦‚æœå·²ç»æ˜¯ErrorTypeå®ä¾‹
            if isinstance(error_type_data, ErrorType):
                return error_type_data
            
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²
            if isinstance(error_type_data, str):
                # å¤„ç†é”™è¯¯åºåˆ—åŒ–çš„æƒ…å†µï¼Œå¦‚ 'ErrorType.SYSTEM_ERROR'
                if error_type_data.startswith('ErrorType.'):
                    enum_name = error_type_data.replace('ErrorType.', '')
                    # å°†å¤§å†™è½¬æ¢ä¸ºå°å†™ä¸‹åˆ’çº¿æ ¼å¼
                    error_type_value = enum_name.lower()
                    return ErrorType(error_type_value)
                else:
                    # ç›´æ¥æ˜¯æšä¸¾å€¼
                    return ErrorType(error_type_data)
            
            return None
        except (ValueError, KeyError):
            logger.warning(f"æ— æ³•è§£æé”™è¯¯ç±»å‹: {error_type_data}")
            return None
    
    async def _extract_essence(self, trajectory: TrajectoryResult) -> Optional[TaskEssence]:
        """ä½¿ç”¨LLMæå–è½¨è¿¹æœ¬è´¨"""
        try:
            # æ„å»ºåˆ†ææç¤º
            prompt = self._build_extraction_prompt(trajectory)
            
            # è°ƒç”¨LLM
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_client._call_api(messages)
            
            # è§£æå“åº”
            return self._parse_extraction_response(response, trajectory)
            
        except Exception as e:
            logger.error(f"Error extracting essence: {e}")
            return None
    
    def _build_extraction_prompt(self, trajectory: TrajectoryResult) -> str:
        """æ„å»ºæœ¬è´¨æå–æç¤º - æä¾›å®Œæ•´è½¨è¿¹ä¿¡æ¯"""
        # æ„å»ºå®Œæ•´çš„æ‰§è¡Œæ­¥éª¤ä¿¡æ¯
        steps_detail = []
        for i, step in enumerate(trajectory.steps, 1):
            step_info = f"""æ­¥éª¤ {i}:
  åŠ¨ä½œç±»å‹: {step.action_type}
  æ‰§è¡Œå‚æ•°: {json.dumps(step.action_params, ensure_ascii=False)[:300]}
  æ‰§è¡Œç»“æœ: {step.observation[:500]}{"..." if len(step.observation) > 500 else ""}
  æ˜¯å¦æˆåŠŸ: {step.success}
  æ€è€ƒè¿‡ç¨‹: {step.thinking[:300] if step.thinking else "æ— "}{"..." if step.thinking and len(step.thinking) > 300 else ""}
  è€—æ—¶: {step.duration:.2f}ç§’"""
            
            if step.error_message:
                step_info += f"\n  é”™è¯¯ä¿¡æ¯: {step.error_message[:200]}"
            
            steps_detail.append(step_info)
        
        # æå–runtimeä¿¡æ¯å’Œæ™ºèƒ½æç¤º
        runtime_analysis = ""
        if hasattr(trajectory, 'runtime_id') and trajectory.runtime_id:
            runtime_id = trajectory.runtime_id.lower()
            if 'reasoning' in runtime_id:
                runtime_analysis = """
è¿™æ˜¯ä¸€ä¸ªReasoning Runtimeæ‰§è¡Œçš„ä»»åŠ¡ï¼Œç‰¹ç‚¹ï¼š
- é€šå¸¸æ¶‰åŠå¤šä¸ªå·¥å…·çš„ååŒä½¿ç”¨ï¼ˆæµè§ˆå™¨+ä»£ç æ‰§è¡Œç­‰ï¼‰
- éœ€è¦å¤æ‚çš„å†³ç­–å’Œæ¨ç†è¿‡ç¨‹
- ä»»åŠ¡ç›®æ ‡å¾€å¾€æ˜¯åˆ†æã€å¯¹æ¯”ã€ç ”ç©¶ç±»é—®é¢˜"""
            elif 'web' in runtime_id:
                runtime_analysis = """
è¿™æ˜¯ä¸€ä¸ªWeb Runtimeæ‰§è¡Œçš„ä»»åŠ¡ï¼Œç‰¹ç‚¹ï¼š
- ä¸»è¦ä½¿ç”¨æµè§ˆå™¨è¿›è¡Œç½‘é¡µæ“ä½œ
- æ¶‰åŠä¿¡æ¯æœç´¢ã€ç½‘é¡µå¯¼èˆªã€æ•°æ®æå–
- ä»»åŠ¡ç›®æ ‡é€šå¸¸æ˜¯è·å–ç‰¹å®šç½‘é¡µä¿¡æ¯"""
            elif 'sandbox' in runtime_id or 'code' in runtime_id:
                runtime_analysis = """
è¿™æ˜¯ä¸€ä¸ªCode Runtimeæ‰§è¡Œçš„ä»»åŠ¡ï¼Œç‰¹ç‚¹ï¼š
- ä¸»è¦è¿›è¡ŒPythonä»£ç ç”Ÿæˆå’Œæ‰§è¡Œ
- è§£å†³è®¡ç®—ã€ç®—æ³•ã€æ•°æ®å¤„ç†é—®é¢˜
- ä»»åŠ¡ç›®æ ‡é€šå¸¸æ˜¯å®ç°ç‰¹å®šåŠŸèƒ½æˆ–è®¡ç®—"""
        
        # æ„å»ºå·¥å…·ä½¿ç”¨åˆ†æ
        tools_used = set()
        for step in trajectory.steps:
            if 'browser' in str(step.action_type).lower():
                tools_used.add("æµè§ˆå™¨æ“ä½œ")
            if 'python' in str(step.observation).lower() or 'code' in str(step.action_type).lower():
                tools_used.add("Pythonä»£ç ")
            if 'navigate' in str(step.action_params) or 'url' in str(step.action_params):
                tools_used.add("ç½‘é¡µå¯¼èˆª")
        
        tools_analysis = f"ä½¿ç”¨çš„å·¥å…·ç±»å‹: {', '.join(tools_used) if tools_used else 'æœªæ˜ç¡®'}"
        
        return f"""è¯·åˆ†æä»¥ä¸‹å®Œæ•´çš„ä»»åŠ¡æ‰§è¡Œè½¨è¿¹ï¼Œæå–ä»»åŠ¡çš„æœ¬è´¨ç‰¹å¾å¹¶ç”Ÿæˆä¼˜åŒ–çš„ä»»åŠ¡æè¿°ï¼š

=== ä»»åŠ¡åŸºæœ¬ä¿¡æ¯ ===
ä»»åŠ¡ID: {trajectory.task_id}
åŸå§‹æè¿°: {trajectory.task_description}
æ‰§è¡Œç¯å¢ƒ: {trajectory.runtime_id}
æ‰§è¡ŒçŠ¶æ€: {"æˆåŠŸ" if trajectory.success else "å¤±è´¥"}
æ€»æ­¥éª¤æ•°: {len(trajectory.steps)}
æ€»è€—æ—¶: {trajectory.total_duration:.2f}ç§’
æœ€ç»ˆç»“æœ: {trajectory.final_result[:400]}{"..." if len(trajectory.final_result) > 400 else ""}

{runtime_analysis}

=== å·¥å…·ä½¿ç”¨åˆ†æ ===
{tools_analysis}

=== å®Œæ•´æ‰§è¡Œè½¨è¿¹ ===
{chr(10).join(steps_detail)}

=== åˆ†æè¦æ±‚ ===
è¯·åŸºäºä»¥ä¸Šå®Œæ•´è½¨è¿¹ä¿¡æ¯ï¼Œè¿›è¡Œæ·±åº¦åˆ†æå¹¶æå–ï¼š

1. **ä»»åŠ¡ç±»å‹åˆ†ç±»** (task_type):
   - "reasoning": å¤šå·¥å…·ååŒä»»åŠ¡ï¼Œæ¶‰åŠå¤æ‚åˆ†æã€å¯¹æ¯”ç ”ç©¶ã€å†³ç­–æ¨ç†ç­‰
   - "web": çº¯ç½‘é¡µæ“ä½œä»»åŠ¡ï¼Œä¸“æ³¨äºä¿¡æ¯æœç´¢ã€ç½‘ç«™å¯¼èˆªã€æ•°æ®æå–ç­‰  
   - "code": çº¯ç¼–ç¨‹ä»»åŠ¡ï¼Œä¸“æ³¨äºç®—æ³•å®ç°ã€è®¡ç®—ã€æ•°æ®å¤„ç†ç­‰

2. **ä»»åŠ¡é¢†åŸŸ** (domain):
   - algorithm: ç®—æ³•ã€æ•°å­¦è®¡ç®—ã€æ•°æ®ç»“æ„
   - data_analysis: æ•°æ®åˆ†æã€ç»Ÿè®¡ã€å¯è§†åŒ–
   - web_automation: ç½‘é¡µè‡ªåŠ¨åŒ–ã€ä¿¡æ¯æå–
   - research: ç ”ç©¶è°ƒæŸ¥ã€å¯¹æ¯”åˆ†æ  
   - comparison: å¯¹æ¯”è¯„ä¼°ã€ç«å“åˆ†æ
   - stock_analysis: é‡‘èåˆ†æã€è‚¡ç¥¨ç ”ç©¶
   - educational: æ•™è‚²ã€å­¦ä¹ ã€çŸ¥è¯†è·å–
   - å…¶ä»–åˆé€‚çš„é¢†åŸŸ

3. **ä¼˜åŒ–ä»»åŠ¡æè¿°** (optimized_description):
   åŸºäºè½¨è¿¹åˆ†æï¼Œç”Ÿæˆä¸€ä¸ªæ¸…æ™°ã€å…·ä½“ã€å¯æ‰§è¡Œçš„ä»»åŠ¡æè¿°ï¼Œè¦æ±‚ï¼š
   - æ˜ç¡®è¯´æ˜ä»»åŠ¡ç›®æ ‡
   - æŒ‡å‡ºéœ€è¦ä½¿ç”¨çš„ä¸»è¦å·¥å…·æˆ–æ–¹æ³•
   - çªå‡ºä»»åŠ¡çš„æ ¸å¿ƒä»·å€¼å’Œéš¾ç‚¹
   - é•¿åº¦æ§åˆ¶åœ¨50-100å­—

4. **å¤æ‚åº¦è¯„ä¼°** (complexity):
   - simple: å•æ­¥éª¤æˆ–ç®€å•æ“ä½œ
   - medium: å¤šæ­¥éª¤åè°ƒæˆ–ä¸­ç­‰éš¾åº¦åˆ†æ
   - complex: æ·±åº¦åˆ†æã€å¤šå·¥å…·é›†æˆæˆ–é«˜éš¾åº¦æ¨ç†

5. **å…³é”®ç‰¹å¾** (key_features):
   åˆ—å‡ºè¿™ä¸ªä»»åŠ¡çš„3-5ä¸ªå…³é”®ç‰¹å¾

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š

{{
  "task_type": "...",
  "domain": "...", 
  "optimized_description": "...",
  "complexity": "...",
  "key_features": ["ç‰¹å¾1", "ç‰¹å¾2", "ç‰¹å¾3"],
  "confidence": 0.9
}}

æ³¨æ„ï¼šè¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€æè¿°æ¸…æ™°ã€åˆ†ç±»åˆç†ã€‚"""
    
    def _parse_extraction_response(self, response: str, trajectory: TrajectoryResult) -> Optional[TaskEssence]:
        """è§£æLLMå“åº”ï¼Œå¤„ç†ä¼˜åŒ–åçš„JSONæ ¼å¼"""
        try:
            # æå–JSON
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group())
                
                # è·å–LLMåˆ†æçš„ç»“æœ
                llm_task_type = parsed.get("task_type", "").lower()
                llm_domain = parsed.get("domain", "general")
                optimized_description = parsed.get("optimized_description", "")
                complexity = parsed.get("complexity", "medium")
                key_features = parsed.get("key_features", [])
                confidence = parsed.get("confidence", 0.8)
                
                # æ™ºèƒ½ä»»åŠ¡ç±»å‹æ¨æ–­å’ŒéªŒè¯
                final_task_type = self._infer_task_type(trajectory, llm_task_type)
                
                # æ™ºèƒ½é¢†åŸŸæ¨æ–­å’ŒéªŒè¯
                final_domain = self._infer_domain(trajectory, llm_domain, final_task_type)
                
                # ä½¿ç”¨ä¼˜åŒ–åçš„æè¿°ï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°åŸå§‹æè¿°
                final_query = optimized_description if optimized_description else trajectory.task_description[:50]
                
                # æ„å»ºå¢å¼ºçš„æˆåŠŸæ¨¡å¼
                enhanced_success_pattern = {
                    "duration": trajectory.total_duration,
                    "steps_count": len(trajectory.steps),
                    "key_features": key_features,
                    "confidence": confidence,
                    "tools_used": self._extract_tools_from_trajectory(trajectory)
                }
                
                logger.info(f"ğŸ§  Enhanced task analysis:")
                logger.info(f"   Type: {llm_task_type} â†’ {final_task_type}")
                logger.info(f"   Domain: {final_domain}")
                logger.info(f"   Optimized description: {final_query[:80]}...")
                logger.info(f"   Key features: {key_features}")
                logger.info(f"   Confidence: {confidence}")
                
                return TaskEssence(
                    essence_id=f"essence_{int(time.time())}_{trajectory.task_id}",
                    task_type=final_task_type,
                    domain=final_domain,
                    query=final_query,
                    complexity_level=complexity,
                    success_pattern=enhanced_success_pattern,
                    extracted_at=datetime.now().isoformat(),
                    source_trajectory_id=trajectory.task_id
                )
            else:
                logger.error(f"Failed to parse JSON response from LLM for essence {trajectory.task_id}")
                logger.warning(f"Response content: {response[:200]}...")
                return None
                
        except Exception as e:
            logger.error(f"Error parsing enhanced extraction response: {e}")
            logger.warning(f"Response content: {response[:500]}...")
            # å³ä½¿è§£æå¤±è´¥ï¼Œä¹Ÿå°è¯•åŸºäºè½¨è¿¹ç‰¹å¾åˆ›å»ºåŸºç¡€æœ¬è´¨
            return self._create_fallback_essence(trajectory)
        
        return None
    
    def _extract_tools_from_trajectory(self, trajectory: TrajectoryResult) -> List[str]:
        """ä»è½¨è¿¹ä¸­æå–ä½¿ç”¨çš„å·¥å…·"""
        tools = set()
        
        for step in trajectory.steps:
            # ä¼˜å…ˆä» action_params ä¸­æå– tool_id
            if step.action_type == ActionType.TOOL_CALL and 'tool_id' in step.action_params:
                tools.add(step.action_params['tool_id'])
            
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„ tool_idï¼Œåˆ™åŸºäº action_type å’Œ action_params è¿›è¡Œæ¨æ–­
            action_type_str = str(step.action_type).lower()
            
            if 'browser_action' in action_type_str:
                tools.add("browser_navigator") # ä½¿ç”¨æ–‡æ¡£ä¸­å®šä¹‰çš„tool_id
            elif 'code_execution' in action_type_str:
                tools.add("python_executor") # ä½¿ç”¨æ–‡æ¡£ä¸­å®šä¹‰çš„tool_id
            
            # è¿›ä¸€æ­¥åŸºäº action_params ä¸­çš„å…³é”®è¯æ¨æ–­
            if step.action_params:
                params_str = str(step.action_params).lower()
                if 'url' in params_str or 'navigate' in params_str:
                    tools.add("browser_navigator")
                if 'code' in params_str or 'python' in params_str:
                    tools.add("python_executor")
                # å‡è®¾æœ‰å…¶ä»–å·¥å…·ï¼Œä¾‹å¦‚æ–‡ä»¶å¤„ç†å·¥å…·
                if 'file' in params_str or 'path' in params_str:
                    tools.add("file_processor")
            
            # åŸºäº observation è¯†åˆ«å·¥å…·è¾“å‡º (ä½œä¸ºè¡¥å……)
            if step.observation:
                obs_str = str(step.observation).lower()
                if 'browser' in obs_str or 'page' in obs_str or 'website' in obs_str:
                    tools.add("browser_navigator")
                if 'python' in obs_str or 'execution' in obs_str:
                    tools.add("python_executor")
        
        return list(tools)
    
    def _infer_task_type(self, trajectory: TrajectoryResult, llm_suggestion: str) -> str:
        """æ™ºèƒ½æ¨æ–­ä»»åŠ¡ç±»å‹"""
        # 1. åŸºäºruntime_idçš„å¼ºè§„åˆ™
        if hasattr(trajectory, 'runtime_id') and trajectory.runtime_id:
            runtime_id = trajectory.runtime_id.lower()
            if 'reasoning' in runtime_id:
                return "reasoning"
            elif 'web' in runtime_id:
                return "web"  
            elif 'sandbox' in runtime_id or 'code' in runtime_id:
                return "code"
        
        # 2. åŸºäºä»»åŠ¡æè¿°çš„å…³é”®è¯
        desc = trajectory.task_description.lower()
        reasoning_keywords = ['åˆ†æ', 'å¯¹æ¯”', 'ç ”ç©¶', 'æ¨ç†', 'analysis', 'compare', 'research', 'reasoning', 'å½±å“']
        web_keywords = ['æœç´¢', 'è®¿é—®', 'æµè§ˆå™¨', 'search', 'visit', 'browser', 'google', 'github']
        code_keywords = ['è®¡ç®—', 'ç®—æ³•', 'ä»£ç ', 'å‡½æ•°', 'calculate', 'algorithm', 'function', 'code', 'çŸ©é˜µ']
        
        if any(keyword in desc for keyword in reasoning_keywords):
            return "reasoning"
        elif any(keyword in desc for keyword in web_keywords):
            return "web"
        elif any(keyword in desc for keyword in code_keywords):
            return "code"
        
        # 3. åŸºäºæ‰§è¡Œæ­¥éª¤åˆ†æ
        if len(trajectory.steps) > 0:
            # æ£€æŸ¥æ˜¯å¦æœ‰æµè§ˆå™¨æ“ä½œ
            has_browser = any('browser' in str(step.action_type).lower() or 'navigate' in str(step.action_params) for step in trajectory.steps)
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»£ç æ‰§è¡Œ
            has_code = any('code' in str(step.action_type).lower() or 'python' in str(step.observation).lower() for step in trajectory.steps)
            
            if has_browser and has_code:
                return "reasoning"  # å¤šå·¥å…·ååŒ
            elif has_browser:
                return "web"
            elif has_code:
                return "code"
        
        # 4. ä½¿ç”¨LLMå»ºè®®ï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
        if llm_suggestion in ['reasoning', 'web', 'code']:
            return llm_suggestion
        
        # 5. é»˜è®¤å€¼
        return "code"
    
    def _infer_domain(self, trajectory: TrajectoryResult, llm_domain: str, task_type: str) -> str:
        """æ™ºèƒ½æ¨æ–­ä»»åŠ¡é¢†åŸŸ"""
        desc = trajectory.task_description.lower()
        
        # åŸºäºå…³é”®è¯çš„é¢†åŸŸæ˜ å°„
        domain_keywords = {
            'algorithm': ['ç®—æ³•', 'è®¡ç®—', 'æ•°å­¦', 'algorithm', 'calculate', 'math', 'æ’åº', 'æœç´¢'],
            'data_analysis': ['æ•°æ®', 'åˆ†æ', 'ç»Ÿè®¡', 'data', 'analysis', 'statistics', 'å›¾è¡¨'],
            'web_automation': ['ç½‘é¡µ', 'æµè§ˆå™¨', 'æœç´¢', 'web', 'browser', 'search', 'google'],
            'research': ['ç ”ç©¶', 'å¯¹æ¯”', 'è°ƒæŸ¥', 'research', 'compare', 'study', 'å½±å“'],
            'comparison': ['å¯¹æ¯”', 'æ¯”è¾ƒ', 'vs', 'compare', 'comparison', 'åŒºåˆ«'],
            'stock_analysis': ['è‚¡ç¥¨', 'è‚¡ä»·', 'stock', 'price', 'æŠ•èµ„', 'investment']
        }
        
        for domain, keywords in domain_keywords.items():
            if any(keyword in desc for keyword in keywords):
                return domain
        
        # åŸºäºä»»åŠ¡ç±»å‹çš„é»˜è®¤é¢†åŸŸ
        type_domain_map = {
            'reasoning': 'research',
            'web': 'web_automation', 
            'code': 'algorithm'
        }
        
        return type_domain_map.get(task_type, llm_domain if llm_domain != "general" else "general")
    
    def _create_fallback_essence(self, trajectory: TrajectoryResult) -> Optional[TaskEssence]:
        """åˆ›å»ºå¤‡ç”¨æœ¬è´¨ï¼ˆå½“LLMè§£æå¤±è´¥æ—¶ï¼‰"""
        try:
            task_type = self._infer_task_type(trajectory, "")
            domain = self._infer_domain(trajectory, "general", task_type)
            
            return TaskEssence(
                essence_id=f"essence_{int(time.time())}_{trajectory.task_id}",
                task_type=task_type,
                domain=domain,
                query=trajectory.task_description[:20],
                complexity_level="medium",
                success_pattern={"duration": trajectory.total_duration},
                extracted_at=datetime.now().isoformat(),
                source_trajectory_id=trajectory.task_id
            )
        except Exception as e:
            logger.error(f"Failed to create fallback essence: {e}")
            return None
    
    def _store_essence(self, essence: TaskEssence):
        """å­˜å‚¨ä»»åŠ¡æœ¬è´¨åˆ°JSONæ–‡ä»¶"""
        try:
            # è¯»å–ç°æœ‰æœ¬è´¨
            existing_essences = self._load_json_file(self.task_essences_path, [])
            
            # æ·»åŠ æ–°æœ¬è´¨
            essence_dict = asdict(essence)
            existing_essences.append(essence_dict)
            
            # ä¿å­˜å›æ–‡ä»¶
            if self._save_json_file(self.task_essences_path, existing_essences):
                logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜ä»»åŠ¡æœ¬è´¨: {essence.essence_id}")
                logger.info(f"  ç±»å‹: {essence.task_type}")
                logger.info(f"  é¢†åŸŸ: {essence.domain}")
                logger.info(f"  æè¿°: {essence.query[:50]}...")
                logger.info(f"  å¤æ‚åº¦: {essence.complexity_level}")
                logger.info(f"  æ¥æºè½¨è¿¹: {essence.source_trajectory_id}")
            else:
                logger.error(f"âŒ ä¿å­˜ä»»åŠ¡æœ¬è´¨å¤±è´¥: {essence.essence_id}")
            
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨ä»»åŠ¡æœ¬è´¨æ—¶å‡ºé”™: {e}")

    async def _generate_seeds_from_existing_essences(self):
        """ä»ç°æœ‰çš„ä»»åŠ¡æœ¬è´¨ç”Ÿæˆç§å­ä»»åŠ¡"""
        try:
            logger.info("ğŸŒ± å¼€å§‹ä»ç°æœ‰ä»»åŠ¡æœ¬è´¨ç”Ÿæˆç§å­ä»»åŠ¡...")
            
            # è¯»å–ç°æœ‰ä»»åŠ¡æœ¬è´¨
            essences_data = self._load_json_file(self.task_essences_path, [])
            if not essences_data:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»åŠ¡æœ¬è´¨æ•°æ®")
                return
            
            logger.info(f"ğŸ“Š æ‰¾åˆ° {len(essences_data)} ä¸ªä»»åŠ¡æœ¬è´¨")
            
            # è½¬æ¢ä¸ºTaskEssenceå¯¹è±¡å¹¶ç”Ÿæˆç§å­ä»»åŠ¡
            seed_tasks = []
            for essence_data in essences_data:
                try:
                    # é‡æ„TaskEssenceå¯¹è±¡
                    essence = TaskEssence(
                        essence_id=essence_data['essence_id'],
                        task_type=essence_data['task_type'],
                        domain=essence_data['domain'],
                        query=essence_data['query'],
                        complexity_level=essence_data['complexity_level'],
                        success_pattern=essence_data['success_pattern'],
                        extracted_at=essence_data['extracted_at'],
                        source_trajectory_id=essence_data['source_trajectory_id']
                    )
                    
                    # ç”Ÿæˆç§å­ä»»åŠ¡
                    seed_task = await self._convert_essence_to_seed(essence)
                    if seed_task:
                        seed_tasks.append(seed_task)
                        logger.debug(f"âœ… ä»æœ¬è´¨ {essence.essence_id} ç”Ÿæˆç§å­ä»»åŠ¡")
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†ä»»åŠ¡æœ¬è´¨æ—¶å‡ºé”™: {e}")
                    continue
            
            # ä¿å­˜ç”Ÿæˆçš„ç§å­ä»»åŠ¡
            if seed_tasks:
                await self._append_seed_tasks(seed_tasks)
                logger.info(f"ğŸ¯ æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜ {len(seed_tasks)} ä¸ªç§å­ä»»åŠ¡")
                
                # ç»Ÿè®¡ä¿¡æ¯
                type_stats = defaultdict(int)
                domain_stats = defaultdict(int)
                for task in seed_tasks:
                    type_stats[task['task_type']] += 1
                    domain_stats[task['domain']] += 1
                
                logger.info(f"ğŸ“Š ç§å­ä»»åŠ¡ç±»å‹åˆ†å¸ƒ: {dict(type_stats)}")
                logger.info(f"ğŸ“Š ç§å­ä»»åŠ¡é¢†åŸŸåˆ†å¸ƒ: {dict(domain_stats)}")
            else:
                logger.warning("âš ï¸ æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•ç§å­ä»»åŠ¡")
                
        except Exception as e:
            logger.error(f"âŒ ä»ä»»åŠ¡æœ¬è´¨ç”Ÿæˆç§å­ä»»åŠ¡å¤±è´¥: {e}")

    async def generate_tasks_manually(self, count: int = 3) -> List[TaskSpec]:
        """æ‰‹åŠ¨ç”ŸæˆæŒ‡å®šæ•°é‡çš„ä»»åŠ¡ï¼ˆæš‚æ—¶ç¦ç”¨ï¼Œå› ä¸ºæ²¡æœ‰æœ¬è´¨å­˜å‚¨ï¼‰"""
        logger.warning("âš ï¸ æ‰‹åŠ¨ä»»åŠ¡ç”ŸæˆåŠŸèƒ½å·²ç¦ç”¨ï¼Œè¯·ä½¿ç”¨è½¨è¿¹å¤„ç†ç”Ÿæˆç§å­ä»»åŠ¡")
        return []

    async def generate_task_from_specific_essence(self, essence_id: str) -> Optional[TaskSpec]:
        """åŸºäºæŒ‡å®šçš„ä»»åŠ¡æœ¬è´¨ç”Ÿæˆæ–°ä»»åŠ¡ï¼ˆæš‚æ—¶ç¦ç”¨ï¼‰"""
        logger.warning("âš ï¸ æŒ‡å®šæœ¬è´¨ä»»åŠ¡ç”ŸæˆåŠŸèƒ½å·²ç¦ç”¨ï¼Œè¯·ä½¿ç”¨è½¨è¿¹å¤„ç†ç”Ÿæˆç§å­ä»»åŠ¡")
        return None
    
    def _get_random_essences(self, count: int) -> List[TaskEssence]:
        """è·å–éšæœºçš„ç§å­æœ¬è´¨ï¼ˆæš‚æ—¶ç¦ç”¨ï¼‰"""
        logger.warning("âš ï¸ éšæœºæœ¬è´¨è·å–åŠŸèƒ½å·²ç¦ç”¨ï¼Œè¯·ä½¿ç”¨è½¨è¿¹å¤„ç†ç”Ÿæˆç§å­ä»»åŠ¡")
        return []
        
    def _record_generated_task(self, task: TaskSpec, essence_id: str):
        """è®°å½•ç”Ÿæˆçš„ä»»åŠ¡ï¼ˆç®€åŒ–ç‰ˆï¼Œä»…è®°å½•æ—¥å¿—ï¼‰"""
        try:
            logger.info(f"âœ… è®°å½•ç”Ÿæˆä»»åŠ¡: {task.task_id}")
            logger.info(f"  æ¥æºæœ¬è´¨: {essence_id}")
            logger.info(f"  ä»»åŠ¡ç±»å‹: {task.task_type}")
            logger.info(f"  ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}")
                
        except Exception as e:
            logger.error(f"âŒ è®°å½•ç”Ÿæˆä»»åŠ¡æ—¶å‡ºé”™: {e}")

    def _is_trajectory_processed(self, trajectory_id: str) -> bool:
        """æ£€æŸ¥è½¨è¿¹æ˜¯å¦å·²å¤„ç†ï¼ˆåŸºäºæŒä¹…åŒ–å­˜å‚¨ï¼‰"""
        is_processed = trajectory_id in self.processed_trajectories
        if is_processed:
            logger.debug(f"ğŸ” è½¨è¿¹å·²å¤„ç†è¿‡: {trajectory_id}")
        return is_processed
    
    def _mark_trajectory_processed(self, trajectory_id: str):
        """æ ‡è®°è½¨è¿¹å·²å¤„ç†ï¼ˆæŒä¹…åŒ–åˆ°æ–‡ä»¶ï¼‰"""
        if trajectory_id not in self.processed_trajectories:
            self.processed_trajectories.add(trajectory_id)
            # ç«‹å³ä¿å­˜åˆ°æ–‡ä»¶
            self._save_processed_trajectories()
            logger.info(f"âœ… æ ‡è®°è½¨è¿¹å·²å¤„ç†å¹¶ä¿å­˜: {trajectory_id}")
        else:
            logger.debug(f"âš ï¸ è½¨è¿¹å·²ç»åœ¨å¤„ç†è®°å½•ä¸­: {trajectory_id}")
    
    async def _generate_task_from_essence(self, essence: TaskEssence) -> Optional[TaskSpec]:
        """åŸºäºæœ¬è´¨ç”Ÿæˆæ–°ä»»åŠ¡"""
        try:
            # è§£æå¢å¼ºçš„æˆåŠŸæ¨¡å¼
            success_pattern = essence.success_pattern
            key_features = success_pattern.get("key_features", [])
            tools_used = success_pattern.get("tools_used", [])
            confidence = success_pattern.get("confidence", 0.8)
            
            # æ„å»ºå¢å¼ºçš„å˜å¼‚æç¤º
            prompt = f"""åŸºäºä»¥ä¸‹é«˜è´¨é‡ä»»åŠ¡æœ¬è´¨ï¼Œç”Ÿæˆä¸€ä¸ªç›¸ä¼¼ä½†åˆ›æ–°çš„æ–°ä»»åŠ¡ï¼š

=== åŸä»»åŠ¡åˆ†æ ===
ä»»åŠ¡ç±»å‹: {essence.task_type}
ä»»åŠ¡é¢†åŸŸ: {essence.domain}
ä¼˜åŒ–æè¿°: {essence.query}
å¤æ‚åº¦ç­‰çº§: {essence.complexity_level}
æå–ç½®ä¿¡åº¦: {confidence}

=== å…³é”®ç‰¹å¾ ===
{chr(10).join(f"- {feature}" for feature in key_features)}

=== å·¥å…·ä½¿ç”¨æ¨¡å¼ ===
ä¸»è¦å·¥å…·: {', '.join(tools_used) if tools_used else 'æœªæŒ‡å®š'}

=== ä»»åŠ¡ç”Ÿæˆè¦æ±‚ ===
è¯·åŸºäºä¸Šè¿°åˆ†æï¼Œåˆ›é€ ä¸€ä¸ªæ–°çš„åŒç±»å‹ä»»åŠ¡ï¼Œè¦æ±‚ï¼š

1. **ä¿æŒæ ¸å¿ƒç‰¹å¾**ï¼š
   - ä»»åŠ¡ç±»å‹å¿…é¡»æ˜¯ {essence.task_type}
   - é¢†åŸŸåº”è¯¥æ˜¯ {essence.domain} æˆ–ç›¸å…³é¢†åŸŸ
   - å¤æ‚åº¦ä¿æŒåœ¨ {essence.complexity_level} çº§åˆ«

2. **åˆ›æ–°å˜åŒ–**ï¼š
   - æ”¹å˜å…·ä½“çš„ç›®æ ‡å¯¹è±¡æˆ–å‚æ•°
   - è°ƒæ•´åœºæ™¯è®¾å®šæˆ–åº”ç”¨èƒŒæ™¯
   - ä¿æŒä»»åŠ¡çš„æŒ‘æˆ˜æ€§å’Œä»·å€¼

3. **å®ç”¨æ€§è¦æ±‚**ï¼š
   - ä»»åŠ¡æè¿°æ¸…æ™°å…·ä½“ï¼Œå¯ç›´æ¥æ‰§è¡Œ
   - è¯´æ˜é¢„æœŸä½¿ç”¨çš„å·¥å…·å’Œæ–¹æ³•
   - ç¡®ä¿ä»»åŠ¡æœ‰æ˜ç¡®çš„æˆåŠŸæ ‡å‡†

4. **è´¨é‡æ ‡å‡†**ï¼š
   - ä»»åŠ¡æè¿°é•¿åº¦60-120å­—
   - é¿å…è¿‡äºç®€å•æˆ–è¿‡äºå¤æ‚
   - ç¡®ä¿ä»»åŠ¡å…·æœ‰å®é™…åº”ç”¨ä»·å€¼

=== ç¤ºä¾‹æ ¼å¼å‚è€ƒ ===
{essence.task_type}ç±»å‹ä»»åŠ¡ç¤ºä¾‹ï¼š
- reasoning: "ä½¿ç”¨æµè§ˆå™¨æœç´¢å’ŒPythonåˆ†æï¼Œå¯¹æ¯”åˆ†æChatGPTå’ŒClaudeåœ¨ä»£ç ç”Ÿæˆèƒ½åŠ›ä¸Šçš„å·®å¼‚ï¼Œä»å‡†ç¡®æ€§ã€æ•ˆç‡å’Œå¯è¯»æ€§ä¸‰ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°"
- web: "è®¿é—®GitHubæœç´¢æœ€å—æ¬¢è¿çš„æœºå™¨å­¦ä¹ é¡¹ç›®ï¼Œç­›é€‰staræ•°é‡è¶…è¿‡10kçš„é¡¹ç›®ï¼Œæå–é¡¹ç›®åç§°ã€ç®€ä»‹å’Œä¸»è¦æŠ€æœ¯æ ˆä¿¡æ¯"  
- code: "å®ç°ä¸€ä¸ªé«˜æ•ˆçš„å¿«é€Ÿæ’åºç®—æ³•ï¼Œè¦æ±‚æ”¯æŒè‡ªå®šä¹‰æ¯”è¾ƒå‡½æ•°ï¼Œå¹¶æ·»åŠ æ€§èƒ½æµ‹è¯•ä»£ç éªŒè¯åœ¨ä¸åŒæ•°æ®è§„æ¨¡ä¸‹çš„æ‰§è¡Œæ•ˆç‡"

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š

{{
  "description": "æ–°ä»»åŠ¡çš„è¯¦ç»†æè¿°",
  "expected_tools": ["å·¥å…·1", "å·¥å…·2"],
  "success_criteria": "æˆåŠŸæ ‡å‡†æè¿°",
  "estimated_steps": æ•°å­—,
  "innovation_points": ["åˆ›æ–°ç‚¹1", "åˆ›æ–°ç‚¹2"]
}}

æ³¨æ„ï¼šç¡®ä¿ç”Ÿæˆçš„ä»»åŠ¡æ—¢ä¿æŒåŸæœ‰ç‰¹å¾ï¼Œåˆå…·æœ‰åˆ›æ–°æ€§å’Œå®ç”¨ä»·å€¼ã€‚"""
            
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_client._call_api(messages)
            
            # è§£æå“åº”
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group())
                
                # æ„å»ºå¢å¼ºçš„TaskSpec
                task_id = f"synth_{essence.task_type}_{essence.domain}_{int(time.time())}"
                
                # ç¡®å®šexpected_toolsï¼Œç»“åˆåŸæœ‰å·¥å…·å’Œæ–°å»ºè®®
                expected_tools = parsed.get("expected_tools", tools_used if tools_used else ["python_executor"])
                
                # æ ¹æ®å¤æ‚åº¦è°ƒæ•´max_steps
                complexity_steps_map = {
                    "simple": 3,
                    "medium": 6, 
                    "complex": 10
                }
                max_steps = complexity_steps_map.get(essence.complexity_level, 5)
                
                # å¦‚æœLLMæä¾›äº†ä¼°è®¡æ­¥éª¤æ•°ï¼Œä½¿ç”¨å®ƒ
                if "estimated_steps" in parsed:
                    max_steps = min(parsed["estimated_steps"], 15)  # æœ€å¤§ä¸è¶…è¿‡15æ­¥
                
                logger.info(f"âœ¨ Generated enhanced task:")
                logger.info(f"   Task ID: {task_id}")
                logger.info(f"   Description: {parsed.get('description', '')[:80]}...")
                logger.info(f"   Expected tools: {expected_tools}")
                logger.info(f"   Max steps: {max_steps}")
                
                return TaskSpec(
                    task_id=task_id,
                    task_type=TaskType(essence.task_type),
                    description=parsed.get("description", essence.query),
                    expected_tools=expected_tools,
                    constraints={
                        "success_criteria": parsed.get("success_criteria", ""),
                        "innovation_points": parsed.get("innovation_points", []),
                        "source_essence": essence.essence_id
                    },
                    max_steps=max_steps,
                    priority=1
                )
            else:
                logger.error(f"Failed to parse JSON response from LLM for essence {essence.essence_id}")
                logger.warning(f"Response content: {response[:200]}...")
                return None
        
        except Exception as e:
            logger.error(f"Error generating task from essence {essence.essence_id}: {e}")
        return None
    
    async def _publish_task(self, task: TaskSpec):
        """å‘å¸ƒä»»åŠ¡åˆ°å¯¹åº”é˜Ÿåˆ—"""
        queue_mapping = {
            TaskType.CODE: "tasks:code",
            TaskType.WEB: "tasks:web",
            TaskType.REASONING: "tasks:reasoning"
        }
        
        queue_name = queue_mapping.get(task.task_type, "tasks:code")
        
        await self.redis.xadd(
            queue_name,
            {
                "task": task.json(),
                "submitted_at": time.time(),
                "priority": task.priority,
                "source": "synthesis"
            }
        )

async def main():
    """ä»»åŠ¡åˆæˆå™¨ä¸»ç¨‹åº"""
    config = {
        "redis_url": os.getenv("REDIS_URL", "redis://redis:6379"),
        "synthesis_enabled": os.getenv("SYNTHESIS_ENABLED", "false").lower() == "true",
        "auto_monitor_trajectories": os.getenv("AUTO_MONITOR_TRAJECTORIES", "true").lower() == "true",
        "auto_export_seeds": os.getenv("AUTO_EXPORT_SEEDS", "true").lower() == "true",
        # "vllm_url": os.getenv("VLLM_URL", "http://vllm:8000") # ç”¨æˆ·ä¸ä½¿ç”¨vLLMï¼Œæ³¨é‡Šæ‰æ­¤é…ç½®
    }
    
    # è¾“å‡ºé…ç½®ä¿¡æ¯
    logger.info("ğŸš€ å¯åŠ¨åŸºäºJSONçš„ä»»åŠ¡åˆæˆå™¨ï¼Œé…ç½®å¦‚ä¸‹:")
    logger.info(f"  åˆæˆå™¨å¯ç”¨: {config['synthesis_enabled']}")
    logger.info(f"  è‡ªåŠ¨è½¨è¿¹ç›‘æ§: {config['auto_monitor_trajectories']}")
    logger.info(f"  è‡ªåŠ¨ç§å­å¯¼å‡º: {config['auto_export_seeds']}")
    logger.info(f"  å­˜å‚¨æ–¹å¼: JSONæ–‡ä»¶")
    
    synthesizer = SynthesisService(config)
    
    try:
        if config["synthesis_enabled"]:
            logger.info("ğŸ¯ å¼€å§‹å¯åŠ¨ä»»åŠ¡åˆæˆæœåŠ¡...")
            await synthesizer.start()
        else:
            logger.info("âš ï¸ ä»»åŠ¡åˆæˆåŠŸèƒ½å·²ç¦ç”¨ï¼ŒæœåŠ¡å°†ç­‰å¾…...")
            # ä¿æŒæœåŠ¡è¿è¡Œï¼Œä½†ä¸æ‰§è¡Œåˆæˆé€»è¾‘
            while True:
                await asyncio.sleep(60)
                logger.info("Synthesis service waiting (disabled)...")
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ åˆæˆæœåŠ¡è¢«ä¸­æ–­")
    finally:
        try:
            # åœæ­¢æ–‡ä»¶ç›‘æ§
            if hasattr(synthesizer, 'observer') and synthesizer.observer and synthesizer.observer.is_alive():
                synthesizer.observer.stop()
                synthesizer.observer.join()
                logger.info("ğŸ“ æ–‡ä»¶ç›‘æ§å·²åœæ­¢")
            
            # æ¸…ç†UnifiedToolLibraryç®¡ç†çš„èµ„æº
            if hasattr(synthesizer, 'tool_library'):
               
               
               
                await synthesizer.tool_library.cleanup()
                logger.info("ğŸ§¹ UnifiedToolLibraryèµ„æºå·²æ¸…ç†")

            await synthesizer.redis.aclose()  # ä½¿ç”¨aclose()æ›¿ä»£close()
            logger.info("ğŸ”Œ Redisè¿æ¥å·²å…³é—­")
        except Exception as e:
            logger.warning(f"âš ï¸ å…³é—­èµ„æºæ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())