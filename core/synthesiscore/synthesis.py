#!/usr/bin/env python3
"""
‰ªªÂä°ÂêàÊàêÂô®Ê®°Âùó - Âü∫‰∫éËΩ®ËøπÂ≠¶‰π†ÁöÑÁßçÂ≠ê‰ªªÂä°ÁîüÊàêÁ≥ªÁªü

‰∏ìÊ≥®‰∫éÈÄöËøáÂàÜÊûêagentÊâßË°åËΩ®ËøπÔºåÊèêÂèñ‰ªªÂä°Êú¨Ë¥®ÔºåÂπ∂ÁîüÊàêÈ´òË¥®ÈáèÁöÑÁßçÂ≠ê‰ªªÂä°„ÄÇ
ÂÆåÂÖ®ÁßªÈô§Êï∞ÊçÆÂ∫ì‰æùËµñÔºå‰ΩøÁî®JSONÊñá‰ª∂ËøõË°åÊï∞ÊçÆÂ≠òÂÇ®„ÄÇ

‰∏ªË¶ÅÂäüËÉΩÔºö
1. ËΩ®ËøπÂàÜÊûêÔºöÊ∑±Â∫¶ÁêÜËß£agentË°å‰∏∫Ê®°Âºè
2. Êú¨Ë¥®ÊèêÂèñÔºöËØÜÂà´‰ªªÂä°ÁöÑÊ†∏ÂøÉÁâπÂæÅÂíåÊàêÂäüË¶ÅÁ¥†  
3. ÁßçÂ≠êÁîüÊàêÔºöÂü∫‰∫éÊú¨Ë¥®ÂàõÈÄ†Êñ∞ÁöÑËÆ≠ÁªÉ‰ªªÂä°
4. Ëá™Âä®ÁõëÊéßÔºöÂÆûÊó∂Ë∑üË∏™ËΩ®ËøπÊñá‰ª∂ÂèòÂåñ
"""

import os
import sys
import json
import asyncio
import logging
import threading
import time
import uuid
import hashlib
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Any
from collections import defaultdict
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

import aiofiles
import redis  # For synchronous operations in threading contexts
import redis.asyncio as async_redis

from ..interfaces import TaskSpec, TrajectoryResult, TaskType, ExecutionStep, ActionType, ErrorType, LLMInteraction
from ..llm_client import LLMClient
from ..toolscore.unified_tool_library import UnifiedToolLibrary
from ..toolscore.interfaces import ToolType, FunctionToolSpec, ToolCapability
from ..utils.path_utils import get_output_dir, get_trajectories_dir

logger = logging.getLogger(__name__)

@dataclass
class TaskEssence:
    """‰ªªÂä°Êú¨Ë¥®Êï∞ÊçÆÁªìÊûÑ"""
    essence_id: str
    task_type: str
    domain: str
    query: str
    complexity_level: str
    success_pattern: Dict
    extracted_at: str
    source_trajectory_id: str

class TrajectoryHandler(FileSystemEventHandler):
    """ËΩ®ËøπÊñá‰ª∂ÂèòÂåñÂ§ÑÁêÜÂô®"""
    
    def __init__(self, synthesis_instance, target_file_path):
        self.synthesis = synthesis_instance
        self.target_file_path = target_file_path
        
    def on_created(self, event):
        if not event.is_directory and event.src_path == self.target_file_path:
            logger.info(f"üîî Ê£ÄÊµãÂà∞ËΩ®ËøπÈõÜÂêàÊñá‰ª∂ÂàõÂª∫: {event.src_path}")
            # ‰ΩøÁî®Á∫øÁ®ãÂÆâÂÖ®ÁöÑÊñπÂºèËß¶ÂèëÂ§ÑÁêÜ
            self._trigger_processing()
    
    def on_modified(self, event):
        if not event.is_directory and event.src_path == self.target_file_path:
            logger.info(f"üîî Ê£ÄÊµãÂà∞ËΩ®ËøπÈõÜÂêàÊñá‰ª∂‰øÆÊîπ: {event.src_path}")
            # ‰ΩøÁî®Á∫øÁ®ãÂÆâÂÖ®ÁöÑÊñπÂºèËß¶ÂèëÂ§ÑÁêÜ
            self._trigger_processing()
    
    def _trigger_processing(self):
        """Á∫øÁ®ãÂÆâÂÖ®Âú∞Ëß¶ÂèëËΩ®ËøπÂ§ÑÁêÜ"""
        try:
            # ‰ΩøÁî®RedisÂèëÈÄÅÂ§ÑÁêÜÂëΩ‰ª§ÔºåËÄå‰∏çÊòØÁõ¥Êé•Ë∞ÉÁî®ÂºÇÊ≠•ÂáΩÊï∞
            redis_client = redis.from_url(self.synthesis.config["redis_url"])
            redis_client.xadd(
                "synthesis:commands",
                {
                    "command": "process_trajectories",
                    "timestamp": time.time(),
                    "source": "file_watcher"
                }
            )
            logger.info("üì® Â∑≤ÂèëÈÄÅËΩ®ËøπÂ§ÑÁêÜÂëΩ‰ª§Âà∞RedisÈòüÂàó")
        except Exception as e:
            logger.error(f"‚ùå ÂèëÈÄÅÂ§ÑÁêÜÂëΩ‰ª§Â§±Ë¥•: {e}")

class SynthesisService:
    """ÁÆÄÂçï‰ªªÂä°ÂêàÊàêÂô® - Âü∫‰∫éJSONÊñá‰ª∂Â≠òÂÇ®"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.redis = async_redis.from_url(config["redis_url"])  # ‰ΩøÁî®ÂºÇÊ≠•redisÂÆ¢Êà∑Á´Ø
        self.llm_client = LLMClient(config)
        self.enabled = config.get("synthesis_enabled", False)
        self.tool_library = UnifiedToolLibrary() # ÂàùÂßãÂåñUnifiedToolLibrary
          # ‰ΩøÁî®Áªü‰∏ÄÁöÑË∑ØÂæÑÁÆ°ÁêÜ
        self.task_essences_path = str(get_output_dir() / "task_essences.json")
        self.seed_tasks_path = str(get_output_dir() / "seed_tasks.jsonl")
        self.processed_trajectories_path = str(get_output_dir() / "processed_trajectories.json")
        self.auto_monitor_enabled = config.get("auto_monitor_trajectories", True)
        self.auto_export_seeds = config.get("auto_export_seeds", True)
        
        # ÊåáÂÆöÁõëÊéßÁöÑËΩ®ËøπÈõÜÂêàÊñá‰ª∂
        self.trajectories_collection_path = str(get_output_dir("trajectories") / "trajectories_collection.json")
        self.observer = None
        
        # Êñá‰ª∂ÈîÅ
        self._file_lock = threading.Lock()
        
        # Â∑≤Â§ÑÁêÜËΩ®ËøπÁöÑËÆ∞ÂΩïÔºà‰ªéÊñá‰ª∂Âä†ËΩΩÔºâ
        self.processed_trajectories = set()
        
        # ÂàùÂßãÂåñJSONÊñá‰ª∂
        self._init_json_files()
        
        # Âä†ËΩΩÂ∑≤Â§ÑÁêÜÁöÑËΩ®ËøπÂàóË°®
        self._load_processed_trajectories()
    
    def _init_json_files(self):
        """ÂàùÂßãÂåñJSONÂ≠òÂÇ®Êñá‰ª∂"""
        try:
            # ÂàõÂª∫ËæìÂá∫ÁõÆÂΩï
            os.makedirs(os.path.dirname(self.task_essences_path), exist_ok=True)
            os.makedirs(os.path.dirname(self.seed_tasks_path), exist_ok=True)
            
            # ÂàùÂßãÂåñ‰ªªÂä°Êú¨Ë¥®Êñá‰ª∂
            if not os.path.exists(self.task_essences_path):
                self._save_json_file(self.task_essences_path, [])
                logger.info(f"‚úÖ ÂàùÂßãÂåñ‰ªªÂä°Êú¨Ë¥®Êñá‰ª∂: {self.task_essences_path}")
            
            # ÂàùÂßãÂåñÁßçÂ≠ê‰ªªÂä°Êñá‰ª∂ÁõÆÂΩï
            if not os.path.exists(self.seed_tasks_path):
                Path(self.seed_tasks_path).touch()
                logger.info(f"‚úÖ ÂàùÂßãÂåñÁßçÂ≠ê‰ªªÂä°Êñá‰ª∂: {self.seed_tasks_path}")
            
            # ÂàùÂßãÂåñÂ∑≤Â§ÑÁêÜËΩ®ËøπËÆ∞ÂΩïÊñá‰ª∂
            if not os.path.exists(self.processed_trajectories_path):
                self._save_json_file(self.processed_trajectories_path, [])
                logger.info(f"‚úÖ ÂàùÂßãÂåñÂ∑≤Â§ÑÁêÜËΩ®ËøπËÆ∞ÂΩïÊñá‰ª∂: {self.processed_trajectories_path}")
                
            logger.info("‚úÖ JSONÊñá‰ª∂Â≠òÂÇ®ÂàùÂßãÂåñÂÆåÊàê")
        except Exception as e:
            logger.error(f"‚ùå JSONÊñá‰ª∂Â≠òÂÇ®ÂàùÂßãÂåñÂ§±Ë¥•: {e}")
            raise

    def _load_processed_trajectories(self):
        """‰ªéÊñá‰ª∂Âä†ËΩΩÂ∑≤Â§ÑÁêÜÁöÑËΩ®ËøπÂàóË°®"""
        try:
            processed_list = self._load_json_file(self.processed_trajectories_path, [])
            self.processed_trajectories = set(processed_list)
            
            if self.processed_trajectories:
                logger.info(f"üìã ‰ªéÊñá‰ª∂Âä†ËΩΩ‰∫Ü {len(self.processed_trajectories)} ‰∏™Â∑≤Â§ÑÁêÜËΩ®ËøπËÆ∞ÂΩï")
                logger.debug(f"Â∑≤Â§ÑÁêÜËΩ®ËøπÂàóË°®: {list(self.processed_trajectories)[:5]}{'...' if len(self.processed_trajectories) > 5 else ''}")
            else:
                logger.info("üìã Êú™ÂèëÁé∞Â∑≤Â§ÑÁêÜËΩ®ËøπËÆ∞ÂΩïÔºå‰ªéÁ©∫ÁôΩÁä∂ÊÄÅÂºÄÂßã")
                
        except Exception as e:
            logger.error(f"‚ùå Âä†ËΩΩÂ∑≤Â§ÑÁêÜËΩ®ËøπËÆ∞ÂΩïÂ§±Ë¥•: {e}")
            self.processed_trajectories = set()

    def _save_processed_trajectories(self):
        """Â∞ÜÂ∑≤Â§ÑÁêÜËΩ®ËøπÂàóË°®‰øùÂ≠òÂà∞Êñá‰ª∂"""
        try:
            processed_list = list(self.processed_trajectories)
            success = self._save_json_file(self.processed_trajectories_path, processed_list)
            if success:
                logger.debug(f"üíæ Â∑≤‰øùÂ≠ò {len(processed_list)} ‰∏™Â∑≤Â§ÑÁêÜËΩ®ËøπËÆ∞ÂΩïÂà∞Êñá‰ª∂")
            else:
                logger.error("‚ùå ‰øùÂ≠òÂ∑≤Â§ÑÁêÜËΩ®ËøπËÆ∞ÂΩïÂ§±Ë¥•")
        except Exception as e:
            logger.error(f"‚ùå ‰øùÂ≠òÂ∑≤Â§ÑÁêÜËΩ®ËøπËÆ∞ÂΩïÊó∂Âá∫Èîô: {e}")

    def _load_json_file(self, filepath: str, default_value=None):
        """Á∫øÁ®ãÂÆâÂÖ®Âú∞Âä†ËΩΩJSONÊñá‰ª∂"""
        with self._file_lock:
            try:
                if os.path.exists(filepath):
                    with open(filepath, 'r', encoding='utf-8') as f:
                        return json.load(f)
                else:
                    return default_value if default_value is not None else []
            except Exception as e:
                logger.error(f"Âä†ËΩΩJSONÊñá‰ª∂Â§±Ë¥• {filepath}: {e}")
                return default_value if default_value is not None else []
    
    def _save_json_file(self, filepath: str, data):
        """Á∫øÁ®ãÂÆâÂÖ®Âú∞‰øùÂ≠òJSONÊñá‰ª∂"""
        with self._file_lock:
            try:
                # ÂàõÂª∫‰∏¥Êó∂Êñá‰ª∂ÔºåÁ°Æ‰øùÂéüÂ≠êÂÜôÂÖ•
                temp_filepath = filepath + '.tmp'
                with open(temp_filepath, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                # ÂéüÂ≠êÊõøÊç¢
                os.replace(temp_filepath, filepath)
                return True
            except Exception as e:
                logger.error(f"‰øùÂ≠òJSONÊñá‰ª∂Â§±Ë¥• {filepath}: {e}")
                return False

    async def start(self):
        """ÂêØÂä®ÂêàÊàêÂô®ÔºåÊîØÊåÅËá™Âä®ËΩ®ËøπÁõëÊéßÂíåÁßçÂ≠êÊï∞ÊçÆÂØºÂá∫"""
        if not self.enabled:
            logger.info("Task synthesis is disabled")
            return
            
        logger.info("üöÄ ÂêØÂä®Âü∫‰∫éJSONÁöÑ‰ªªÂä°ÂêàÊàêÂô®...")
        
        await self.tool_library.initialize() # ÂàùÂßãÂåñUnifiedToolLibrary
        
        # ÂêØÂä®Ëá™Âä®ËΩ®ËøπÁõëÊéßÔºàÂ¶ÇÊûúÂêØÁî®Ôºâ
        if self.auto_monitor_enabled:
            await self._start_trajectory_monitoring()
        
        # ÂêØÂä®Êåá‰ª§ÁõëÂê¨Âô®
        await self._listen_for_synthesis_commands()

    async def _start_trajectory_monitoring(self):
        """ÂêØÂä®ËΩ®ËøπÊñá‰ª∂Ëá™Âä®ÁõëÊéß - ‰∏ìÈó®ÁõëÊéßtrajectories_collection.json"""
        try:
            logger.info("üîç ÂêØÂä®ËΩ®ËøπÈõÜÂêàÊñá‰ª∂ÁõëÊéß...")
            
            # Ê£ÄÊü•ÁõÆÊ†áÊñá‰ª∂ÊòØÂê¶Â≠òÂú®
            target_dir = os.path.dirname(self.trajectories_collection_path)
            if not os.path.exists(target_dir):
                logger.warning(f"‚ö†Ô∏è ËΩ®ËøπÁõÆÂΩï‰∏çÂ≠òÂú®ÔºåÂàõÂª∫ÁõÆÂΩï: {target_dir}")
                os.makedirs(target_dir, exist_ok=True)
            
            logger.info(f"üìÅ ÁõëÊéßÊñá‰ª∂: {self.trajectories_collection_path}")
            
            # ÂàõÂª∫Êñá‰ª∂ÁõëÊéßÂô®
            self.observer = Observer()
            handler = TrajectoryHandler(self, self.trajectories_collection_path)
            
            # ÁõëÊéßËΩ®ËøπÈõÜÂêàÊñá‰ª∂ÊâÄÂú®ÁõÆÂΩï
            self.observer.schedule(handler, target_dir, recursive=False)
            self.observer.start()
            
            logger.info(f"‚úÖ Ëá™Âä®ËΩ®ËøπÁõëÊéßÂ∑≤ÂêØÂä®ÔºåÁõëÊéßÊñá‰ª∂: {self.trajectories_collection_path}")
            
            # Â§ÑÁêÜÁé∞ÊúâÁöÑËΩ®ËøπÈõÜÂêàÊñá‰ª∂ÔºàÂ¶ÇÊûúÂ≠òÂú®Ôºâ
            if os.path.exists(self.trajectories_collection_path):
                await self._process_trajectories_collection()
            else:
                logger.info(f"üìù ËΩ®ËøπÈõÜÂêàÊñá‰ª∂Â∞ö‰∏çÂ≠òÂú®: {self.trajectories_collection_path}")
                
        except Exception as e:
            logger.error(f"‚ùå ÂêØÂä®ËΩ®ËøπÁõëÊéßÂ§±Ë¥•: {e}")
    
    async def _process_trajectories_collection(self):
        """Â§ÑÁêÜtrajectories_collection.jsonÊñá‰ª∂‰∏≠ÁöÑËΩ®Ëøπ"""
        try:
            if not os.path.exists(self.trajectories_collection_path):
                logger.warning(f"‚ö†Ô∏è ËΩ®ËøπÈõÜÂêàÊñá‰ª∂‰∏çÂ≠òÂú®: {self.trajectories_collection_path}")
                return
            
            logger.info(f"üîÑ ÂºÄÂßãÂ§ÑÁêÜËΩ®ËøπÈõÜÂêàÊñá‰ª∂: {self.trajectories_collection_path}")

            if not os.path.exists(self.trajectories_collection_path) or os.path.getsize(self.trajectories_collection_path) == 0:
                logger.info(f"üìù Trajectory collection file is empty or does not exist: {self.trajectories_collection_path}")
                return

            # ËØªÂèñËΩ®ËøπÈõÜÂêàÊï∞ÊçÆ
            try:
                with open(self.trajectories_collection_path, 'r', encoding='utf-8') as f:
                    trajectories_data = json.load(f)
            except json.JSONDecodeError as e:
                logger.error(f"‚ùå Error decoding JSON from {self.trajectories_collection_path}: {e}")
                return
            
            if not isinstance(trajectories_data, list):
                logger.error("‚ùå ËΩ®ËøπÈõÜÂêàÊñá‰ª∂Ê†ºÂºèÈîôËØØÔºåÂ∫î‰∏∫ËΩ®ËøπÊï∞ÁªÑ")
                return
            
            new_essences = []
            new_seed_tasks = []
            processed_count = 0
            skipped_count = 0
            
            logger.info(f"üìä ËΩ®ËøπÈõÜÂêàÂåÖÂê´ {len(trajectories_data)} ‰∏™ËΩ®Ëøπ")
            
            for i, trajectory_data in enumerate(trajectories_data):
                try:
                    # ÁîüÊàêËΩ®ËøπÂîØ‰∏ÄÊ†áËØÜÁ¨¶
                    trajectory_id = trajectory_data.get('task_id', f'trajectory_{i}')
                    
                    # ÈÅøÂÖçÈáçÂ§çÂ§ÑÁêÜ
                    if self._is_trajectory_processed(trajectory_id):
                        skipped_count += 1
                        logger.debug(f"‚è© Ë∑≥ËøáÂ∑≤Â§ÑÁêÜÁöÑËΩ®Ëøπ: {trajectory_id}")
                        continue
                    
                    # ËΩ¨Êç¢ËΩ®ËøπÊ†ºÂºè
                    trajectory = self._convert_trajectory_format(trajectory_data)
                    if trajectory and self._should_process_trajectory(trajectory):
                        # ÊèêÂèñ‰ªªÂä°Êú¨Ë¥®
                        essence = await self._extract_essence(trajectory)
                        if essence:
                            # ‰øùÂ≠òÊú¨Ë¥®Âà∞JSONÊñá‰ª∂
                            new_essences.append(asdict(essence))
                            
                            # Áõ¥Êé•ËΩ¨Êç¢‰∏∫ÁßçÂ≠ê‰ªªÂä°
                            seed_task = await self._convert_essence_to_seed(essence)
                            if seed_task:
                                new_seed_tasks.append(seed_task)
                                processed_count += 1
                                logger.info(f"‚úÖ ÁîüÊàê‰ªªÂä°Êú¨Ë¥®ÂíåÁßçÂ≠ê‰ªªÂä°: {trajectory_id}")
                                
                                # Ê†áËÆ∞‰∏∫Â∑≤Â§ÑÁêÜ
                                self._mark_trajectory_processed(trajectory_id)
                        
                except Exception as e:
                    logger.error(f"‚ùå Â§ÑÁêÜÁ¨¨{i+1}‰∏™ËΩ®ËøπÊó∂Âá∫Èîô: {e}")
                    continue
            
            # ‰øùÂ≠òÊñ∞ÁöÑ‰ªªÂä°Êú¨Ë¥®
            if new_essences:
                await self._save_new_essences(new_essences)
                logger.info(f"üíæ ‰øùÂ≠ò {len(new_essences)} ‰∏™‰ªªÂä°Êú¨Ë¥®")
            
            # Áõ¥Êé•ËøΩÂä†Âà∞ÁßçÂ≠êÊñá‰ª∂
            if new_seed_tasks:
                await self._append_seed_tasks(new_seed_tasks)
                logger.info(f"üì§ ÊàêÂäüÊ∑ªÂä† {len(new_seed_tasks)} ‰∏™ÁßçÂ≠ê‰ªªÂä°")
            
            logger.info(f"‚úÖ ËΩ®ËøπÈõÜÂêàÂ§ÑÁêÜÂÆåÊàê: Êñ∞Â§ÑÁêÜ {processed_count} ‰∏™ÔºåË∑≥Ëøá {skipped_count} ‰∏™")
                
        except Exception as e:
            logger.error(f"‚ùå Â§ÑÁêÜËΩ®ËøπÈõÜÂêàÊñá‰ª∂Â§±Ë¥•: {e}")
    
    async def _save_new_essences(self, new_essences: List[Dict]):
        """‰øùÂ≠òÊñ∞ÁöÑ‰ªªÂä°Êú¨Ë¥®Âà∞JSONÊñá‰ª∂"""
        try:
            # ËØªÂèñÁé∞ÊúâÊú¨Ë¥®
            existing_essences = self._load_json_file(self.task_essences_path, [])
            
            # Ê∑ªÂä†Êñ∞Êú¨Ë¥®
            existing_essences.extend(new_essences)
            
            # ‰øùÂ≠òÂõûÊñá‰ª∂
            self._save_json_file(self.task_essences_path, existing_essences)
            
            logger.info(f"üíæ Â∑≤‰øùÂ≠ò {len(new_essences)} ‰∏™Êñ∞‰ªªÂä°Êú¨Ë¥®Âà∞ {self.task_essences_path}")
            
            # ÁªüËÆ°‰ø°ÊÅØ
            type_stats = defaultdict(int)
            domain_stats = defaultdict(int)
            for essence in new_essences:
                type_stats[essence['task_type']] += 1
                domain_stats[essence['domain']] += 1
            
            logger.info(f"üìä Êñ∞Â¢ûÊú¨Ë¥®ÂàÜÂ∏É - Á±ªÂûã: {dict(type_stats)}, È¢ÜÂüü: {dict(domain_stats)}")
            
        except Exception as e:
            logger.error(f"‚ùå ‰øùÂ≠ò‰ªªÂä°Êú¨Ë¥®Â§±Ë¥•: {e}")

    async def _process_existing_trajectories(self):
        """Â§ÑÁêÜÁé∞ÊúâÁöÑËΩ®ËøπÈõÜÂêàÊñá‰ª∂"""
        logger.info("üîÑ Ê£ÄÊü•Áé∞ÊúâËΩ®ËøπÈõÜÂêàÊñá‰ª∂...")
        
        if os.path.exists(self.trajectories_collection_path):
            await self._process_trajectories_collection()
        else:
            logger.info("üìù Ê≤°ÊúâÁé∞ÊúâÁöÑËΩ®ËøπÈõÜÂêàÊñá‰ª∂")

    async def _process_new_trajectory_file(self, trajectory_path: str):
        """Â§ÑÁêÜËΩ®ËøπÈõÜÂêàÊñá‰ª∂Ôºà‰øùÊåÅÂÖºÂÆπÊÄßÔºâ"""
        if trajectory_path == self.trajectories_collection_path:
            await self._process_trajectories_collection()
        else:
            logger.debug(f"‚è© ÂøΩÁï•ÈùûÁõÆÊ†áÊñá‰ª∂: {trajectory_path}")
    
    async def _convert_essence_to_seed(self, essence: TaskEssence) -> Optional[Dict]:
        """Â∞Ü‰ªªÂä°Êú¨Ë¥®Áõ¥Êé•ËΩ¨Êç¢‰∏∫ÁßçÂ≠ê‰ªªÂä°"""
        try:
            # ÁîüÊàêÁßçÂ≠ê‰ªªÂä°ID
            task_id = f"seed_{essence.task_type}_{self._generate_task_id_suffix(essence.query)}"
            
            # Êé®Êñ≠È¢ÑÊúüÂ∑•ÂÖ∑
            success_pattern = essence.success_pattern
            expected_tools = success_pattern.get('tools_used', [])
            if not expected_tools:
                expected_tools = await self._infer_expected_tools(essence.task_type, essence.domain)
            
            # Êé®Êñ≠ÊúÄÂ§ßÊ≠•Êï∞
            max_steps = self._infer_max_steps(essence.complexity_level, essence.task_type)
            
            seed_task = {
                "task_id": task_id,
                "task_type": essence.task_type,
                "description": essence.query,
                "expected_tools": expected_tools,
                "max_steps": max_steps,
                "domain": essence.domain,
                "complexity": essence.complexity_level,
                "confidence": success_pattern.get('confidence', 0.8),
                "source_essence_id": essence.essence_id,
                "source_trajectory": essence.source_trajectory_id,
                "extracted_at": essence.extracted_at
            }
            
            return seed_task
            
        except Exception as e:
            logger.error(f"ËΩ¨Êç¢‰ªªÂä°Êú¨Ë¥®‰∏∫ÁßçÂ≠ê‰ªªÂä°Êó∂Âá∫Èîô: {e}")
            return None
    
    async def _append_seed_tasks(self, seed_tasks: List[Dict]):
        """ËøΩÂä†ÁßçÂ≠ê‰ªªÂä°Âà∞Êñá‰ª∂"""
        with self._file_lock:
            try:
                async with aiofiles.open(self.seed_tasks_path, 'a', encoding='utf-8') as f:
                    for seed_task in seed_tasks:
                        await f.write(json.dumps(seed_task, ensure_ascii=False) + '\n')
                
                logger.info(f"‚úÖ ÊàêÂäüËøΩÂä† {len(seed_tasks)} ‰∏™ÁßçÂ≠ê‰ªªÂä°Âà∞ {self.seed_tasks_path}")
                
                # ÁªüËÆ°‰ø°ÊÅØ
                type_stats = defaultdict(int)
                for task in seed_tasks:
                    type_stats[task['task_type']] += 1
                
                logger.info(f"üìä Êñ∞Â¢ûÁßçÂ≠ê‰ªªÂä°ÂàÜÂ∏É: {dict(type_stats)}")
                
            except Exception as e:
                logger.error(f"‚ùå ËøΩÂä†ÁßçÂ≠ê‰ªªÂä°Â§±Ë¥•: {e}")

    async def _export_seed_tasks(self):
        """ÂØºÂá∫ÁßçÂ≠ê‰ªªÂä°ÁªüËÆ°ÂíåÁä∂ÊÄÅÊä•Âëä"""
        try:
            if not os.path.exists(self.seed_tasks_path):
                logger.info("üìù ÁßçÂ≠ê‰ªªÂä°Êñá‰ª∂Â∞ö‰∏çÂ≠òÂú®")
                return
            
            # ËØªÂèñÂπ∂ÁªüËÆ°ÁßçÂ≠ê‰ªªÂä°
            seed_count = 0
            type_stats = defaultdict(int)
            domain_stats = defaultdict(int)
            
            with open(self.seed_tasks_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        try:
                            task = json.loads(line.strip())
                            seed_count += 1
                            type_stats[task.get('task_type', 'unknown')] += 1
                            domain_stats[task.get('domain', 'unknown')] += 1
                        except json.JSONDecodeError:
                            continue
            
            logger.info("üìä ÁßçÂ≠ê‰ªªÂä°ÂØºÂá∫ÁªüËÆ°:")
            logger.info(f"  ÊÄªÊï∞Èáè: {seed_count}")
            logger.info(f"  ‰ªªÂä°Á±ªÂûãÂàÜÂ∏É: {dict(type_stats)}")
            logger.info(f"  È¢ÜÂüüÂàÜÂ∏É: {dict(domain_stats)}")
            logger.info(f"  Êñá‰ª∂Ë∑ØÂæÑ: {self.seed_tasks_path}")
            
            # ÂèëÂ∏ÉÁªüËÆ°‰ø°ÊÅØÂà∞Redis
            export_stats = {
                "total_seeds": seed_count,
                "type_distribution": dict(type_stats),
                "domain_distribution": dict(domain_stats),
                "file_path": self.seed_tasks_path,
                "exported_at": datetime.now().isoformat()
            }
            
            await self.redis.xadd(
                "synthesis:seed_export",
                {
                    "timestamp": time.time(),
                    "stats": json.dumps(export_stats)
                }
            )
            
        except Exception as e:
            logger.error(f"‚ùå ÂØºÂá∫ÁßçÂ≠ê‰ªªÂä°ÁªüËÆ°Â§±Ë¥•: {e}")

    def _generate_task_id_suffix(self, description: str) -> str:
        """Ê†πÊçÆÊèèËø∞ÁîüÊàê‰ªªÂä°IDÂêéÁºÄ"""
        # ‰ΩøÁî®ÊèèËø∞ÁöÑÂìàÂ∏åÂÄºÁîüÊàêÁü≠ÂêéÁºÄ
        hash_obj = hashlib.md5(description.encode('utf-8'))
        return hash_obj.hexdigest()[:8]
    
    async def _infer_expected_tools(self, task_type: str, domain: str) -> List[str]:
        """Ê†πÊçÆ‰ªªÂä°Á±ªÂûãÂíåÈ¢ÜÂüüÊé®Êñ≠È¢ÑÊúüÂ∑•ÂÖ∑ - Âä®ÊÄÅ‰ªéUnifiedToolLibraryËé∑Âèñ"""
        
        all_tools = await self.tool_library.get_all_tools()
        available_tool_ids = {tool.tool_id for tool in all_tools}
        
        inferred_tools = set()

        # ‰ºòÂÖàÂåπÈÖçÊòéÁ°ÆÁöÑÂ∑•ÂÖ∑ID
        if task_type == 'code':
            if "python_executor" in available_tool_ids:
                inferred_tools.add("python_executor")
        elif task_type == 'web':
            if "browser_navigator" in available_tool_ids:
                inferred_tools.add("browser_navigator")
            # ÂÅáËÆæÊúâÂÖ∂‰ªñwebÂ∑•ÂÖ∑Ôºå‰æãÂ¶Ç"web_scraper"
            # if "web_scraper" in available_tool_ids:
            #     inferred_tools.add("web_scraper")
        elif task_type == 'reasoning':
            if "browser_navigator" in available_tool_ids:
                inferred_tools.add("browser_navigator")
            if "python_executor" in available_tool_ids:
                inferred_tools.add("python_executor")

        # Ê†πÊçÆÈ¢ÜÂüüËøõ‰∏ÄÊ≠•ÁªÜÂåñÔºåÂåπÈÖçÂ∑•ÂÖ∑ÁöÑtagsÊàñdescription
        # ËøôÊòØ‰∏Ä‰∏™Á§∫‰æãÔºåÂÆûÈôÖÂèØËÉΩÈúÄË¶ÅÊõ¥Â§çÊùÇÁöÑÂåπÈÖçÈÄªËæë
        domain_keywords_map = {
            'data_analysis': ['data', 'analysis', 'pandas', 'numpy', 'matplotlib'],
            'web_automation': ['web', 'browser', 'scrape', 'requests', 'BeautifulSoup'],
            'algorithm': ['algorithm', 'math', 'calculate'],
            'research': ['search', 'research', 'query'],
            'stock_analysis': ['stock', 'finance', 'market']
        }

        domain_keywords = domain_keywords_map.get(domain, [])
        
        for tool in all_tools:
            tool_description_lower = tool.description.lower()
            tool_name_lower = tool.name.lower()
            tool_tags_lower = [tag.lower() for tag in tool.tags] if tool.tags else []

            # Ê£ÄÊü•Â∑•ÂÖ∑ÊèèËø∞„ÄÅÂêçÁß∞ÊàñÊ†áÁ≠æÊòØÂê¶ÂåÖÂê´È¢ÜÂüüÂÖ≥ÈîÆËØç
            if any(keyword in tool_description_lower or
                   keyword in tool_name_lower or
                   any(keyword in tag for tag in tool_tags_lower)
                   for keyword in domain_keywords):
                inferred_tools.add(tool.tool_id)
        
        # Â¶ÇÊûúÊ≤°ÊúâÊé®Êñ≠Âá∫‰ªª‰ΩïÂ∑•ÂÖ∑ÔºåÂàôÊ†πÊçÆ‰ªªÂä°Á±ªÂûãÊèê‰æõ‰∏Ä‰∏™ÈªòËÆ§Â∑•ÂÖ∑
        if not inferred_tools:
            if task_type == 'code' and "python_executor" in available_tool_ids:
                inferred_tools.add("python_executor")
            elif task_type == 'web' and "browser_navigator" in available_tool_ids:
                inferred_tools.add("browser_navigator")
            elif task_type == 'reasoning' and "browser_navigator" in available_tool_ids:
                inferred_tools.add("browser_navigator")
            elif task_type == 'reasoning' and "python_executor" in available_tool_ids:
                inferred_tools.add("python_executor")

        return list(inferred_tools)
    
    def _infer_max_steps(self, complexity_level: str, task_type: str) -> int:
        """Ê†πÊçÆÂ§çÊùÇÂ∫¶Âíå‰ªªÂä°Á±ªÂûãÊé®Êñ≠ÊúÄÂ§ßÊ≠•Êï∞"""
        base_steps = {
            'simple': 5,
            'medium': 10,
            'complex': 15
        }
        
        steps = base_steps.get(complexity_level, 8)
        
        # reasoning‰ªªÂä°ÈÄöÂ∏∏ÈúÄË¶ÅÊõ¥Â§öÊ≠•È™§
        if task_type == 'reasoning':
            steps += 5
        
        return min(steps, 20)  # ÊúÄÂ§ß‰∏çË∂ÖËøá20Ê≠•

    async def _listen_for_synthesis_commands(self):
        """ÁõëÂê¨ÂêàÊàêÊåá‰ª§"""
        logger.info("üéØ Synthesis service ready - waiting for manual triggers")
        logger.info("Available trigger methods:")
        logger.info("1. Redis command: XADD synthesis:commands command trigger_synthesis")
        logger.info("2. Redis command: XADD synthesis:commands command process_trajectories")
        logger.info("3. Redis command: XADD synthesis:commands command process_specific trajectory_file.json")
        
        # È¶ñÂÖàÂ§ÑÁêÜÈòüÂàó‰∏≠Áé∞ÊúâÁöÑÂëΩ‰ª§
        await self._process_pending_commands()
        
        while True:
            try:
                # ÁõëÂê¨synthesis:commandsÈòüÂàóÔºå‰ΩøÁî®$Ë°®Á§∫‰ªéÂΩìÂâçÊúÄÊñ∞‰ΩçÁΩÆÂºÄÂßãËØªÂèñÊñ∞Ê∂àÊÅØ
                # ‰ΩøÁî® type: ignore ÊäëÂà∂ Pylance ÂØπ redis.asyncio.Redis.xread Á±ªÂûãÊèêÁ§∫ÁöÑËØØÊä•„ÄÇ
                streams = {b"synthesis:commands": b"$"}
                result = await self.redis.xread(streams, count=1, block=5000)  # type: ignore # 5ÁßíË∂ÖÊó∂
                
                if result:
                    for stream_name, messages in result:
                        for message_id, fields in messages:
                            await self._handle_synthesis_command(fields)
                            # Á°ÆËÆ§Â§ÑÁêÜÂÆåÊàê
                            await self.redis.xdel("synthesis:commands", message_id)
                
            except Exception as e:
                logger.error(f"Error listening for synthesis commands: {e}")
                await asyncio.sleep(10)

    async def _process_pending_commands(self):
        """Â§ÑÁêÜÈòüÂàó‰∏≠Áé∞ÊúâÁöÑÂæÖÂ§ÑÁêÜÂëΩ‰ª§"""
        try:
            # ËØªÂèñÈòüÂàó‰∏≠ÊâÄÊúâÁé∞ÊúâÂëΩ‰ª§
            # ‰ΩøÁî® type: ignore ÊäëÂà∂ Pylance ÂØπ redis.asyncio.Redis.xread Á±ªÂûãÊèêÁ§∫ÁöÑËØØÊä•„ÄÇ
            result = await self.redis.xread({b"synthesis:commands": b"0"}, count=100)  # type: ignore
            
            if result:
                for stream_name, messages in result:
                    logger.info(f"Found {len(messages)} pending commands in queue")
                    for message_id, fields in messages:
                        logger.info(f"Processing pending command: {message_id}")
                        await self._handle_synthesis_command(fields)
                        # Âà†Èô§Â∑≤Â§ÑÁêÜÁöÑÂëΩ‰ª§
                        await self.redis.xdel("synthesis:commands", message_id)
                        
                logger.info("‚úÖ All pending commands processed")
            else:
                logger.info("No pending commands found")
                
        except Exception as e:
            logger.error(f"Error processing pending commands: {e}")

    async def _handle_synthesis_command(self, command_fields: dict):
        """Â§ÑÁêÜÂêàÊàêÊåá‰ª§"""
        try:
            command = command_fields.get(b'command', b'').decode('utf-8')
            logger.info(f"üì® Received synthesis command: {command}")
            
            if command == "trigger_synthesis":
                # Ëß¶ÂèëÂÆåÊï¥ÁöÑËΩ®ËøπÂ§ÑÁêÜ
                await self._process_all_trajectories_once()
                # Ëá™Âä®ÂØºÂá∫ÁßçÂ≠êÊï∞ÊçÆ
                if self.auto_export_seeds:
                    await self._export_seed_tasks()
                
            elif command == "process_trajectories":
                # Â§ÑÁêÜÊâÄÊúâÊú™Â§ÑÁêÜÁöÑËΩ®Ëøπ
                await self._process_unprocessed_trajectories()
                # Ëá™Âä®ÂØºÂá∫ÁßçÂ≠êÊï∞ÊçÆ
                if self.auto_export_seeds:
                    await self._export_seed_tasks()
                
            elif command.startswith("process_specific"):
                # Â§ÑÁêÜÊåáÂÆöÁöÑËΩ®ËøπÊñá‰ª∂
                parts = command.split(" ", 1)
                if len(parts) > 1:
                    filename = parts[1]
                    await self._process_specific_trajectory(filename)
                    # Ëá™Âä®ÂØºÂá∫ÁßçÂ≠êÊï∞ÊçÆ
                    if self.auto_export_seeds:
                        await self._export_seed_tasks()
                    
            elif command == "export_seeds":
                # ÊâãÂä®ÂØºÂá∫ÁßçÂ≠ê‰ªªÂä°
                await self._export_seed_tasks()
                
            elif command == "start_monitoring":
                # ÂêØÂä®ËΩ®ËøπÁõëÊéß
                if not self.observer or not self.observer.is_alive():
                    await self._start_trajectory_monitoring()
                    logger.info("‚úÖ ËΩ®ËøπÁõëÊéßÂ∑≤ÂêØÂä®")
                else:
                    logger.info("‚ö†Ô∏è ËΩ®ËøπÁõëÊéßÂ∑≤Âú®ËøêË°å")
                    
            elif command == "stop_monitoring":
                # ÂÅúÊ≠¢ËΩ®ËøπÁõëÊéß
                if self.observer and self.observer.is_alive():
                    self.observer.stop()
                    self.observer.join()
                    logger.info("üõë ËΩ®ËøπÁõëÊéßÂ∑≤ÂÅúÊ≠¢")
                else:
                    logger.info("‚ö†Ô∏è ËΩ®ËøπÁõëÊéßÊú™ËøêË°å")
                    
            elif command == "generate_tasks":
                # ÊâãÂä®ÁîüÊàê‰ªªÂä°
                count = int(command_fields.get(b'count', b'3').decode('utf-8'))
                tasks = await self.generate_tasks_manually(count)
                logger.info(f"Generated {len(tasks)} tasks manually")
                
            elif command == "generate_seeds_from_essences":
                # ‰ªéÁé∞Êúâ‰ªªÂä°Êú¨Ë¥®ÁîüÊàêÁßçÂ≠ê‰ªªÂä°
                await self._generate_seeds_from_existing_essences()
                
            elif command == "status":
                # Êä•ÂëäÁä∂ÊÄÅ
                await self._report_synthesis_status()
                
            else:
                logger.warning(f"Unknown synthesis command: {command}")
                
        except Exception as e:
            logger.error(f"Error handling synthesis command: {e}")
    
    async def _process_all_trajectories_once(self):
        """‰∏ÄÊ¨°ÊÄßÂ§ÑÁêÜÊâÄÊúâËΩ®ËøπÔºà‰∏çÂæ™ÁéØÔºâ"""
        logger.info("üîÑ Starting one-time trajectory processing...")
        
        try:
            trajectories_dir = get_trajectories_dir()
            if not os.path.exists(trajectories_dir):
                logger.warning("Trajectories directory not found")
                return
            
            processed_count = 0
            skipped_count = 0
            
            # Â§ÑÁêÜÊâÄÊúâËΩ®ËøπÊñá‰ª∂
            for filename in os.listdir(trajectories_dir):
                if filename.endswith('.json'):
                    trajectory_path = os.path.join(trajectories_dir, filename)
                    
                    # Ê£ÄÊü•ÊòØÂê¶Â∑≤Â§ÑÁêÜ
                    if not self._is_trajectory_processed(filename):
                        await self._process_trajectory_file(trajectory_path)
                        self._mark_trajectory_processed(filename)
                        processed_count += 1
                        logger.info(f"‚úÖ Processed: {filename}")
                    else:
                        skipped_count += 1
                        logger.debug(f"‚è© Skipped (already processed): {filename}")
            
            logger.info(f"üéØ Trajectory processing completed: {processed_count} processed, {skipped_count} skipped")
            
        except Exception as e:
            logger.error(f"Error in one-time trajectory processing: {e}")

    async def _process_unprocessed_trajectories(self):
        """Âè™Â§ÑÁêÜÊú™Â§ÑÁêÜÁöÑËΩ®Ëøπ"""
        logger.info("üîÑ Processing only unprocessed trajectories...")
        try:
            trajectories_dir = get_trajectories_dir()
            if not os.path.exists(trajectories_dir):
                logger.warning("Trajectories directory not found")
                return
            
            processed_count = 0
            
            # Ëé∑ÂèñÊâÄÊúâËΩ®ËøπÊñá‰ª∂Âπ∂Â§ÑÁêÜÂÖ∂‰∏≠Êú™Â§ÑÁêÜÁöÑËΩ®Ëøπ
            for filename in os.listdir(trajectories_dir):
                if filename.endswith('.json'):
                    trajectory_path = os.path.join(trajectories_dir, filename)
                    # Â§ÑÁêÜÊñá‰ª∂‰∏≠ÊâÄÊúâÊú™Â§ÑÁêÜÁöÑËΩ®Ëøπ
                    file_processed_count = await self._process_unprocessed_in_file(trajectory_path)
                    processed_count += file_processed_count
            
            logger.info(f"üéØ Unprocessed trajectories completed: {processed_count} new trajectories processed")
            
        except Exception as e:
            logger.error(f"Error processing unprocessed trajectories: {e}")

    async def _process_unprocessed_in_file(self, trajectory_path: str) -> int:
        """Â§ÑÁêÜÂçï‰∏™Êñá‰ª∂‰∏≠Êú™Â§ÑÁêÜÁöÑËΩ®ËøπÔºåËøîÂõûÂ§ÑÁêÜÊï∞Èáè"""
        try:
            logger.info(f"üîç Checking for unprocessed trajectories in: {trajectory_path}")
            if not os.path.exists(trajectory_path) or os.path.getsize(trajectory_path) == 0:
                logger.info(f"üìù Trajectory file is empty or does not exist: {trajectory_path}")
                return 0
            
            with open(trajectory_path, 'r', encoding='utf-8') as f:
                try:
                    trajectory_data = json.load(f)
                except json.JSONDecodeError as e:
                    logger.error(f"‚ùå Error decoding JSON from {trajectory_path}: {e}")
                    return 0 #Êó†Ê≥ïËß£ÊûêÊñá‰ª∂ÔºåËøîÂõû0
            
            processed_count = 0
            new_seed_tasks = []  # Êî∂ÈõÜÊñ∞ÁîüÊàêÁöÑÁßçÂ≠ê‰ªªÂä°
            
            # Â¶ÇÊûúÊòØËΩ®ËøπÂàóË°®ÔºåÂ§ÑÁêÜÊØè‰∏Ä‰∏™Êú™Â§ÑÁêÜÁöÑËΩ®Ëøπ
            if isinstance(trajectory_data, list):
                logger.info(f"üìä Found trajectory collection with {len(trajectory_data)} trajectories")
                for i, single_trajectory in enumerate(trajectory_data):
                    if processed_count >= 10:  # Â§ÑÁêÜÊï∞ÈáèÈôêÂà∂
                        logger.info(f"‚èπÔ∏è Reached processing limit of 10 trajectories")
                        break
                    try:
                        trajectory = self._convert_trajectory_format(single_trajectory)
                        if trajectory:
                            # Âü∫‰∫éËΩ®ËøπIDÊ£ÄÊü•ÊòØÂê¶Â∑≤Â§ÑÁêÜ
                            if not self._is_trajectory_processed(trajectory.task_id):
                                should_process = self._should_process_trajectory(trajectory)
                                logger.info(f"üìã New trajectory {trajectory.task_id}: runtime={trajectory.runtime_id}, success={trajectory.success}, should_process={should_process}")
                                
                                if should_process:
                                    essence = await self._extract_essence(trajectory)
                                    if essence:
                                        self._store_essence(essence)
                                        
                                        # Á´ãÂç≥ÁîüÊàêÁßçÂ≠ê‰ªªÂä°
                                        seed_task = await self._convert_essence_to_seed(essence)
                                        if seed_task:
                                            new_seed_tasks.append(seed_task)
                                            logger.info(f"üå± Generated seed task from essence: {essence.essence_id}")
                                        
                                        # Ê†áËÆ∞Ëøô‰∏™ËΩ®ËøπIDÂ∑≤Â§ÑÁêÜ
                                        self._mark_trajectory_processed(trajectory.task_id)
                                        processed_count += 1
                                        logger.info(f"‚úÖ Extracted essence {essence.task_type}/{essence.domain} from trajectory {trajectory.task_id}")
                                    else:
                                        logger.warning(f"‚ùå Failed to extract essence from trajectory {trajectory.task_id}")
                                        # Âç≥‰ΩøÊèêÂèñÂ§±Ë¥•‰πüÊ†áËÆ∞‰∏∫Â∑≤Â§ÑÁêÜÔºåÈÅøÂÖçÈáçÂ§çÂ∞ùËØï
                                        self._mark_trajectory_processed(trajectory.task_id)
                                else:
                                    logger.info(f"‚è≠Ô∏è Skipping trajectory {trajectory.task_id} (not worth processing)")
                                    # Ê†áËÆ∞‰∏∫Â∑≤Â§ÑÁêÜ
                                    self._mark_trajectory_processed(trajectory.task_id)
                            else:
                                logger.debug(f"‚è≠Ô∏è Trajectory {trajectory.task_id} already processed")
                    except Exception as e:
                        logger.error(f"‚ùå Error processing trajectory {i}: {e}")
                
                # ÊâπÈáè‰øùÂ≠òÁßçÂ≠ê‰ªªÂä°
                if new_seed_tasks:
                    await self._append_seed_tasks(new_seed_tasks)
                    logger.info(f"üíæ Saved {len(new_seed_tasks)} seed tasks to file")
                
                if processed_count > 0:
                    logger.info(f"üéØ Successfully processed {processed_count} new trajectories from {trajectory_path}")
                return processed_count
                
            else:
                # Âçï‰∏™ËΩ®ËøπÂØπË±°
                trajectory = self._convert_trajectory_format(trajectory_data)
                if trajectory and not self._is_trajectory_processed(trajectory.task_id):
                    if self._should_process_trajectory(trajectory):
                        essence = await self._extract_essence(trajectory)
                        if essence:
                            self._store_essence(essence)
                            
                            # Á´ãÂç≥ÁîüÊàêÁßçÂ≠ê‰ªªÂä°
                            seed_task = await self._convert_essence_to_seed(essence)
                            if seed_task:
                                await self._append_seed_tasks([seed_task])
                                logger.info(f"üå± Generated and saved seed task from essence: {essence.essence_id}")
                            
                            self._mark_trajectory_processed(trajectory.task_id)
                            logger.info(f"‚úÖ Extracted essence from single trajectory {trajectory.task_id}")
                            return 1
                    else:
                        self._mark_trajectory_processed(trajectory.task_id)
                
                return 0
        
        except Exception as e:
            logger.error(f"‚ùå Error processing trajectory file {trajectory_path}: {e}")
            return 0

    async def _process_specific_trajectory(self, filename: str):
        """Â§ÑÁêÜÊåáÂÆöÁöÑËΩ®ËøπÊñá‰ª∂"""
        logger.info(f"üéØ Processing specific trajectory: {filename}")
        
        try:
            trajectories_dir = "/app/output/trajectories"
            trajectory_path = os.path.join(trajectories_dir, filename)
            
            if not os.path.exists(trajectory_path):
                logger.error(f"Trajectory file not found: {filename}")
                return
            
            await self._process_trajectory_file(trajectory_path)
            self._mark_trajectory_processed(filename)
            logger.info(f"‚úÖ Successfully processed specific trajectory: {filename}")
            
        except Exception as e:
            logger.error(f"Error processing specific trajectory {filename}: {e}")

    async def _report_synthesis_status(self):
        """Êä•ÂëäÂêàÊàêÊúçÂä°Áä∂ÊÄÅ"""
        try:
            # ÁªüËÆ°ÁßçÂ≠ê‰ªªÂä°Êñá‰ª∂
            seed_count = 0
            seed_type_stats = defaultdict(int)
            if os.path.exists(self.seed_tasks_path):
                with open(self.seed_tasks_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            try:
                                task = json.loads(line.strip())
                                seed_count += 1
                                seed_type_stats[task.get('task_type', 'unknown')] += 1
                            except json.JSONDecodeError:
                                continue
            
            # ÁªüËÆ°‰ªªÂä°Êú¨Ë¥®Êñá‰ª∂
            essence_count = 0
            essence_type_stats = defaultdict(int)
            essence_domain_stats = defaultdict(int)
            if os.path.exists(self.task_essences_path):
                essences_data = self._load_json_file(self.task_essences_path, [])
                essence_count = len(essences_data)
                for essence in essences_data:
                    essence_type_stats[essence.get('task_type', 'unknown')] += 1
                    essence_domain_stats[essence.get('domain', 'unknown')] += 1
            
            # ÁªüËÆ°ËΩ®ËøπÈõÜÂêàÁä∂ÊÄÅ
            collection_exists = os.path.exists(self.trajectories_collection_path)
            collection_size = 0
            if collection_exists:
                try:
                    with open(self.trajectories_collection_path, 'r', encoding='utf-8') as f:
                        collection_data = json.load(f)
                        collection_size = len(collection_data) if isinstance(collection_data, list) else 0
                except:
                    collection_size = 0
            
            processed_count = len(self.processed_trajectories)
            
            status_info = {
                "synthesis_enabled": self.enabled,
                "storage_type": "JSONÊñá‰ª∂Â≠òÂÇ®",
                "monitoring_enabled": self.auto_monitor_enabled,
                "target_file": self.trajectories_collection_path,
                "collection_exists": collection_exists,
                "collection_size": collection_size,
                "processed_trajectories": processed_count,
                "unprocessed_count": max(0, collection_size - processed_count),
                "total_task_essences": essence_count,
                "essence_type_distribution": dict(essence_type_stats),
                "essence_domain_distribution": dict(essence_domain_stats),
                "total_seed_tasks": seed_count,
                "seed_type_distribution": dict(seed_type_stats),
                "essence_file_path": self.task_essences_path,
                "seed_file_path": self.seed_tasks_path,
                "observer_running": self.observer.is_alive() if self.observer else False,
                "auto_export_seeds": self.auto_export_seeds
            }
            
            logger.info("üìä Synthesis Status Report:")
            for key, value in status_info.items():
                logger.info(f"  {key}: {value}")
                
            # ÂèëÂ∏ÉÁä∂ÊÄÅÂà∞Redis‰æõÂ§ñÈÉ®Êü•ËØ¢
            await self.redis.xadd(
                "synthesis:status",
                {
                    "timestamp": time.time(),
                    "status": json.dumps(status_info)
                }
            )
            
        except Exception as e:
            logger.error(f"Error generating status report: {e}")

    # ‰øùÁïôÂéüÊúâÁöÑËá™Âä®Â§ÑÁêÜÊñπÊ≥ï‰Ωú‰∏∫Â§áÁî®
    async def _process_trajectory_feedback(self):
        """Â§ÑÁêÜËΩ®ËøπÂèçÈ¶àÔºåÊèêÂèñ‰ªªÂä°Êú¨Ë¥®ÔºàÂéüËá™Âä®Â§ÑÁêÜÊñπÊ≥ïÔºåÁé∞Âú®‰Ωú‰∏∫Â§áÁî®Ôºâ"""
        logger.info("‚ö†Ô∏è  Note: This is the old auto-processing method, now used as backup")
        
        while True:
            try:
                # Êâ´ÊèèËΩ®ËøπËæìÂá∫ÁõÆÂΩï
                trajectories_dir = "/app/output/trajectories"
                if not os.path.exists(trajectories_dir):
                    await asyncio.sleep(30)
                    continue
                
                # Â§ÑÁêÜÊñ∞ÁöÑËΩ®ËøπÊñá‰ª∂
                for filename in os.listdir(trajectories_dir):
                    if filename.endswith('.json'):
                        trajectory_path = os.path.join(trajectories_dir, filename)
                        
                        # Ê£ÄÊü•ÊòØÂê¶Â∑≤Â§ÑÁêÜ
                        if not self._is_trajectory_processed(filename):
                            await self._process_trajectory_file(trajectory_path)
                            self._mark_trajectory_processed(filename)
                
                await asyncio.sleep(60)  # ÊØèÂàÜÈíüÊ£ÄÊü•‰∏ÄÊ¨°
                
            except Exception as e:
                logger.error(f"Error processing trajectory feedback: {e}")
                await asyncio.sleep(30)
    
    def _should_process_trajectory(self, trajectory: TrajectoryResult) -> bool:
        """Âà§Êñ≠ËΩ®ËøπÊòØÂê¶ÂÄºÂæóÂ§ÑÁêÜ"""
        # 1. ÊàêÂäüÁöÑËΩ®ËøπÊÄªÊòØÂ§ÑÁêÜ
        if trajectory.success:
            return True
        
        # 2. reasoning runtimeÁöÑËΩ®ËøπÔºåÂç≥‰ΩøÂ§±Ë¥•‰πüÂèØËÉΩÊúâ‰ª∑ÂÄºÔºà‰∏çË¶ÅÊ±ÇÊúâÊâßË°åÊ≠•È™§Ôºâ
        runtime_id = trajectory.runtime_id.lower()
        if 'reasoning' in runtime_id:
            logger.info(f"üß† Found reasoning trajectory: {trajectory.task_id}")
            return True
        
        # 3. ÊúâÊâßË°åÊ≠•È™§ÁöÑËΩ®Ëøπ
        if len(trajectory.steps) > 0:
            # ÊúâÂ§ö‰∏™Ê≠•È™§ÁöÑÂ§çÊùÇ‰ªªÂä°ÔºåÂç≥‰ΩøÂ§±Ë¥•‰πüÂèØËÉΩÊúâ‰ª∑ÂÄº
            if len(trajectory.steps) >= 2:
                return True
        
        # 4. ‰ªªÂä°ÊèèËø∞ÂåÖÂê´ÁâπÂÆöÂÖ≥ÈîÆËØç
        task_desc = trajectory.task_description.lower()
        valuable_keywords = ['reasoning', 'Êé®ÁêÜ', 'ÂàÜÊûê', 'analysis', 'compare', 'ÂØπÊØî', 'Á†îÁ©∂']
        if any(keyword in task_desc for keyword in valuable_keywords):
            logger.info(f"üîé Found valuable keywords in task description: {trajectory.task_id}")
            return True
        
        # 5. ÊúâÊúÄÁªàÁªìÊûúÁöÑËΩ®ËøπÔºåÂç≥‰ΩøÂ§±Ë¥•‰πüÂèØËÉΩÊúâ‰ª∑ÂÄº
        if trajectory.final_result and len(trajectory.final_result.strip()) > 50:
            logger.info(f"üìù Found trajectory with substantial final result: {trajectory.task_id}")
            return True
        
        return False

    async def _process_trajectory_file(self, trajectory_path: str) -> bool:
        """Â§ÑÁêÜÂçï‰∏™ËΩ®ËøπÊñá‰ª∂ÔºåËøîÂõûÂ§ÑÁêÜÊòØÂê¶ÊàêÂäü"""
        try:
            logger.info(f"üîç Processing trajectory file: {trajectory_path}")
            with open(trajectory_path, 'r', encoding='utf-8') as f:
                trajectory_data = json.load(f)
            
            processed_count = 0
            
            # Â¶ÇÊûúÊòØËΩ®ËøπÂàóË°®ÔºåÂ§ÑÁêÜÊØè‰∏Ä‰∏™ËΩ®Ëøπ
            if isinstance(trajectory_data, list):
                logger.info(f"üìä Processing trajectory collection with {len(trajectory_data)} trajectories")
                for i, single_trajectory in enumerate(trajectory_data):
                    if processed_count >= 10:  # Â¢ûÂä†Â§ÑÁêÜÊï∞ÈáèÈôêÂà∂
                        logger.info(f"‚èπÔ∏è Reached processing limit of 10 trajectories")
                        break
                    try:
                        trajectory = self._convert_trajectory_format(single_trajectory)
                        if trajectory:
                            # Â¢ûÂº∫ËΩ®ËøπÂ§ÑÁêÜÈÄªËæëÔºöÂ§ÑÁêÜÊõ¥Â§öÁ±ªÂûãÁöÑËΩ®Ëøπ
                            should_process = self._should_process_trajectory(trajectory)
                            logger.info(f"üìã Trajectory {trajectory.task_id}: runtime={trajectory.runtime_id}, success={trajectory.success}, should_process={should_process}")
                            
                            if should_process:
                                essence = await self._extract_essence(trajectory)
                                if essence:
                                    self._store_essence(essence)
                                    processed_count += 1
                                    logger.info(f"‚úÖ Extracted essence {essence.task_type}/{essence.domain} from trajectory {trajectory.task_id}")
                                else:
                                    logger.warning(f"‚ùå Failed to extract essence from trajectory {trajectory.task_id}")
                            else:
                                logger.info(f"‚è≠Ô∏è Skipping trajectory {trajectory.task_id} (not worth processing)")
                    except Exception as e:
                        logger.error(f"‚ùå Error processing trajectory {i}: {e}")
                
                logger.info(f"üéØ Successfully processed {processed_count} trajectories from collection")
                return processed_count > 0
                
            else:
                # Âçï‰∏™ËΩ®ËøπÂØπË±°
                trajectory = self._convert_trajectory_format(trajectory_data)
                if trajectory and self._should_process_trajectory(trajectory):
                    essence = await self._extract_essence(trajectory)
                    if essence:
                        self._store_essence(essence)
                        logger.info(f"‚úÖ Extracted essence from single trajectory {trajectory.task_id}")
                        return True
                
                return False
        
        except Exception as e:
            logger.error(f"‚ùå Error processing trajectory file {trajectory_path}: {e}")
            return False
    
    def _convert_trajectory_format(self, data: Dict) -> Optional[TrajectoryResult]:
        """Â∞ÜËΩ®ËøπÊï∞ÊçÆËΩ¨Êç¢‰∏∫TrajectoryResultÊ†ºÂºè"""
        try:
            logger.debug(f"Attempting to convert trajectory data for task_id: {data.get('task_id', 'Unknown')}, type of data: {type(data)}")
            # ËΩ¨Êç¢stepsÊ†ºÂºè
            converted_steps = []
            steps_list = data.get('steps', [])
            if not isinstance(steps_list, list):
                logger.error(f"Field 'steps' is not a list for task_id: {data.get('task_id', 'Unknown')}. Got {type(steps_list)}. Skipping steps conversion.")
                steps_list = []

            for i, step_data in enumerate(steps_list):
                logger.debug(f"Processing step {i} for task_id: {data.get('task_id', 'Unknown')}: type={type(step_data)}, content='{str(step_data)[:200]}...'")
                
                # Â§ÑÁêÜExecutionStepÂØπË±°
                if hasattr(step_data, 'to_dict'):
                    # Â¶ÇÊûúÊòØExecutionStepÂØπË±°ÔºåÁõ¥Êé•‰ΩøÁî®
                    converted_steps.append(step_data)
                    continue
                elif isinstance(step_data, str) and step_data.startswith('ExecutionStep('):
                    # Â¶ÇÊûúÊòØExecutionStepÁöÑÂ≠óÁ¨¶‰∏≤Ë°®Á§∫ÔºåË∑≥ËøáÔºàÊó†Ê≥ïÂÆâÂÖ®Ëß£ÊûêÔºâ
                    logger.warning(f"Skipping ExecutionStep string representation for step {i} in task_id: {data.get('task_id', 'Unknown')}")
                    continue
                elif isinstance(step_data, dict):
                    # Â¶ÇÊûúÊòØÂ≠óÂÖ∏ÔºåËΩ¨Êç¢‰∏∫ExecutionStepÂØπË±°
                    converted_step = ExecutionStep(
                        step_id=step_data.get('step_id', 0),
                        action_type=ActionType(step_data.get('action_type', 'code_generation')),
                        action_params=step_data.get('action_params', step_data.get('tool_input', {})),
                        observation=step_data.get('observation', step_data.get('tool_output', '')),
                        success=step_data.get('success', True),
                        thinking=step_data.get('thinking'),
                        execution_code=step_data.get('execution_code'),
                        error_type=self._safe_parse_error_type(step_data.get('error_type')), # Á°Æ‰øùÊòØErrorTypeÊûö‰∏æ
                        error_message=step_data.get('error_message'),
                        timestamp=step_data.get('timestamp', time.time()),
                        duration=step_data.get('duration', 0.0),
                        # llm_interactions Â≠óÊÆµÂú® ExecutionStep ÂÆö‰πâ‰∏≠Ôºå‰ΩÜÂéüÂßãÊï∞ÊçÆ‰∏≠ÂèØËÉΩÊ≤°ÊúâÔºåÈúÄË¶ÅÂ§ÑÁêÜ
                        llm_interactions=[LLMInteraction(**interaction_dict) for interaction_dict in step_data.get('llm_interactions', []) if isinstance(interaction_dict, dict)]
                    )
                    converted_steps.append(converted_step)
                else:
                    logger.error(f"Skipping step {i} for task_id: {data.get('task_id', 'Unknown')} due to unexpected format. Expected dict or ExecutionStep, got {type(step_data)}. Content: {str(step_data)[:200]}")
                    continue
            
            # ÂàõÂª∫TrajectoryResultÂØπË±°
            return TrajectoryResult(
                task_id=data.get('task_id', str(uuid.uuid4())),
                task_name=data.get('task_name', data.get('task_id', 'unknown')),
                task_description=data.get('task_description', ''),
                runtime_id=data.get('runtime_id', 'unknown'),
                success=data.get('success', False),
                steps=converted_steps,
                final_result=data.get('final_result', ''),
                error_type=self._safe_parse_error_type(data.get('error_type')),
                error_message=data.get('error_message'),
                total_duration=data.get('total_duration', 0.0),
                metadata=data.get('metadata', {}),
                created_at=data.get('created_at', time.time())
            )
            
        except Exception as e:
            logger.error(f"Error converting trajectory format: {e}")
            return None
    
    def _safe_parse_error_type(self, error_type_data) -> Optional[ErrorType]:
        """ÂÆâÂÖ®Ëß£ÊûêÈîôËØØÁ±ªÂûã"""
        if not error_type_data:
            return None
        
        try:
            # Â¶ÇÊûúÂ∑≤ÁªèÊòØErrorTypeÂÆû‰æã
            if isinstance(error_type_data, ErrorType):
                return error_type_data
            
            # Â¶ÇÊûúÊòØÂ≠óÁ¨¶‰∏≤
            if isinstance(error_type_data, str):
                # Â§ÑÁêÜÈîôËØØÂ∫èÂàóÂåñÁöÑÊÉÖÂÜµÔºåÂ¶Ç 'ErrorType.SYSTEM_ERROR'
                if error_type_data.startswith('ErrorType.'):
                    enum_name = error_type_data.replace('ErrorType.', '')
                    # Â∞ÜÂ§ßÂÜôËΩ¨Êç¢‰∏∫Â∞èÂÜô‰∏ãÂàíÁ∫øÊ†ºÂºè
                    error_type_value = enum_name.lower()
                    return ErrorType(error_type_value)
                else:
                    # Áõ¥Êé•ÊòØÊûö‰∏æÂÄº
                    return ErrorType(error_type_data)
            
            return None
        except (ValueError, KeyError):
            logger.warning(f"Êó†Ê≥ïËß£ÊûêÈîôËØØÁ±ªÂûã: {error_type_data}")
            return None
    
    async def _extract_essence(self, trajectory: TrajectoryResult) -> Optional[TaskEssence]:
        """‰ΩøÁî®LLMÊèêÂèñËΩ®ËøπÊú¨Ë¥®"""
        try:
            # ÊûÑÂª∫ÂàÜÊûêÊèêÁ§∫
            prompt = self._build_extraction_prompt(trajectory)
            
            # Ë∞ÉÁî®LLM
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_client._call_api(messages)
            
            # Ëß£ÊûêÂìçÂ∫î
            return self._parse_extraction_response(response, trajectory)
            
        except Exception as e:
            logger.error(f"Error extracting essence: {e}")
            return None
    
    def _build_extraction_prompt(self, trajectory: TrajectoryResult) -> str:
        """ÊûÑÂª∫Êú¨Ë¥®ÊèêÂèñÊèêÁ§∫ - Êèê‰æõÂÆåÊï¥ËΩ®Ëøπ‰ø°ÊÅØ"""
        # ÊûÑÂª∫ÂÆåÊï¥ÁöÑÊâßË°åÊ≠•È™§‰ø°ÊÅØ
        steps_detail = []
        for i, step in enumerate(trajectory.steps, 1):
            step_info = f"""Ê≠•È™§ {i}:
  Âä®‰ΩúÁ±ªÂûã: {step.action_type}
  ÊâßË°åÂèÇÊï∞: {json.dumps(step.action_params, ensure_ascii=False)[:300]}
  ÊâßË°åÁªìÊûú: {step.observation[:500]}{"..." if len(step.observation) > 500 else ""}
  ÊòØÂê¶ÊàêÂäü: {step.success}
  ÊÄùËÄÉËøáÁ®ã: {step.thinking[:300] if step.thinking else "Êó†"}{"..." if step.thinking and len(step.thinking) > 300 else ""}
  ËÄóÊó∂: {step.duration:.2f}Áßí"""
            
            if step.error_message:
                step_info += f"\n  ÈîôËØØ‰ø°ÊÅØ: {step.error_message[:200]}"
            
            steps_detail.append(step_info)
        
        # ÊèêÂèñruntime‰ø°ÊÅØÂíåÊô∫ËÉΩÊèêÁ§∫
        runtime_analysis = ""
        if hasattr(trajectory, 'runtime_id') and trajectory.runtime_id:
            runtime_id = trajectory.runtime_id.lower()
            if 'reasoning' in runtime_id:
                runtime_analysis = """
ËøôÊòØ‰∏Ä‰∏™Reasoning RuntimeÊâßË°åÁöÑ‰ªªÂä°ÔºåÁâπÁÇπÔºö
- ÈÄöÂ∏∏Ê∂âÂèäÂ§ö‰∏™Â∑•ÂÖ∑ÁöÑÂçèÂêå‰ΩøÁî®ÔºàÊµèËßàÂô®+‰ª£Á†ÅÊâßË°åÁ≠âÔºâ
- ÈúÄË¶ÅÂ§çÊùÇÁöÑÂÜ≥Á≠ñÂíåÊé®ÁêÜËøáÁ®ã
- ‰ªªÂä°ÁõÆÊ†áÂæÄÂæÄÊòØÂàÜÊûê„ÄÅÂØπÊØî„ÄÅÁ†îÁ©∂Á±ªÈóÆÈ¢ò"""
            elif 'web' in runtime_id:
                runtime_analysis = """
ËøôÊòØ‰∏Ä‰∏™Web RuntimeÊâßË°åÁöÑ‰ªªÂä°ÔºåÁâπÁÇπÔºö
- ‰∏ªË¶Å‰ΩøÁî®ÊµèËßàÂô®ËøõË°åÁΩëÈ°µÊìç‰Ωú
- Ê∂âÂèä‰ø°ÊÅØÊêúÁ¥¢„ÄÅÁΩëÈ°µÂØºËà™„ÄÅÊï∞ÊçÆÊèêÂèñ
- ‰ªªÂä°ÁõÆÊ†áÈÄöÂ∏∏ÊòØËé∑ÂèñÁâπÂÆöÁΩëÈ°µ‰ø°ÊÅØ"""
            elif 'sandbox' in runtime_id or 'code' in runtime_id:
                runtime_analysis = """
ËøôÊòØ‰∏Ä‰∏™Code RuntimeÊâßË°åÁöÑ‰ªªÂä°ÔºåÁâπÁÇπÔºö
- ‰∏ªË¶ÅËøõË°åPython‰ª£Á†ÅÁîüÊàêÂíåÊâßË°å
- Ëß£ÂÜ≥ËÆ°ÁÆó„ÄÅÁÆóÊ≥ï„ÄÅÊï∞ÊçÆÂ§ÑÁêÜÈóÆÈ¢ò
- ‰ªªÂä°ÁõÆÊ†áÈÄöÂ∏∏ÊòØÂÆûÁé∞ÁâπÂÆöÂäüËÉΩÊàñËÆ°ÁÆó"""
        
        # ÊûÑÂª∫Â∑•ÂÖ∑‰ΩøÁî®ÂàÜÊûê
        tools_used = set()
        for step in trajectory.steps:
            if 'browser' in str(step.action_type).lower():
                tools_used.add("ÊµèËßàÂô®Êìç‰Ωú")
            if 'python' in str(step.observation).lower() or 'code' in str(step.action_type).lower():
                tools_used.add("Python‰ª£Á†Å")
            if 'navigate' in str(step.action_params) or 'url' in str(step.action_params):
                tools_used.add("ÁΩëÈ°µÂØºËà™")
        
        tools_analysis = f"‰ΩøÁî®ÁöÑÂ∑•ÂÖ∑Á±ªÂûã: {', '.join(tools_used) if tools_used else 'Êú™ÊòéÁ°Æ'}"
        
        return f"""ËØ∑ÂàÜÊûê‰ª•‰∏ãÂÆåÊï¥ÁöÑ‰ªªÂä°ÊâßË°åËΩ®ËøπÔºåÊèêÂèñ‰ªªÂä°ÁöÑÊú¨Ë¥®ÁâπÂæÅÂπ∂ÁîüÊàê‰ºòÂåñÁöÑ‰ªªÂä°ÊèèËø∞Ôºö

=== ‰ªªÂä°Âü∫Êú¨‰ø°ÊÅØ ===
‰ªªÂä°ID: {trajectory.task_id}
ÂéüÂßãÊèèËø∞: {trajectory.task_description}
ÊâßË°åÁéØÂ¢É: {trajectory.runtime_id}
ÊâßË°åÁä∂ÊÄÅ: {"ÊàêÂäü" if trajectory.success else "Â§±Ë¥•"}
ÊÄªÊ≠•È™§Êï∞: {len(trajectory.steps)}
ÊÄªËÄóÊó∂: {trajectory.total_duration:.2f}Áßí
ÊúÄÁªàÁªìÊûú: {trajectory.final_result[:400]}{"..." if len(trajectory.final_result) > 400 else ""}

{runtime_analysis}

=== Â∑•ÂÖ∑‰ΩøÁî®ÂàÜÊûê ===
{tools_analysis}

=== ÂÆåÊï¥ÊâßË°åËΩ®Ëøπ ===
{chr(10).join(steps_detail)}

=== ÂàÜÊûêË¶ÅÊ±Ç ===
ËØ∑Âü∫‰∫é‰ª•‰∏äÂÆåÊï¥ËΩ®Ëøπ‰ø°ÊÅØÔºåËøõË°åÊ∑±Â∫¶ÂàÜÊûêÂπ∂ÊèêÂèñÔºö

1. **‰ªªÂä°Á±ªÂûãÂàÜÁ±ª** (task_type):
   - "reasoning": Â§öÂ∑•ÂÖ∑ÂçèÂêå‰ªªÂä°ÔºåÊ∂âÂèäÂ§çÊùÇÂàÜÊûê„ÄÅÂØπÊØîÁ†îÁ©∂„ÄÅÂÜ≥Á≠ñÊé®ÁêÜÁ≠â
   - "web": Á∫ØÁΩëÈ°µÊìç‰Ωú‰ªªÂä°Ôºå‰∏ìÊ≥®‰∫é‰ø°ÊÅØÊêúÁ¥¢„ÄÅÁΩëÁ´ôÂØºËà™„ÄÅÊï∞ÊçÆÊèêÂèñÁ≠â  
   - "code": Á∫ØÁºñÁ®ã‰ªªÂä°Ôºå‰∏ìÊ≥®‰∫éÁÆóÊ≥ïÂÆûÁé∞„ÄÅËÆ°ÁÆó„ÄÅÊï∞ÊçÆÂ§ÑÁêÜÁ≠â

2. **‰ªªÂä°È¢ÜÂüü** (domain):
   - algorithm: ÁÆóÊ≥ï„ÄÅÊï∞Â≠¶ËÆ°ÁÆó„ÄÅÊï∞ÊçÆÁªìÊûÑ
   - data_analysis: Êï∞ÊçÆÂàÜÊûê„ÄÅÁªüËÆ°„ÄÅÂèØËßÜÂåñ
   - web_automation: ÁΩëÈ°µËá™Âä®Âåñ„ÄÅ‰ø°ÊÅØÊèêÂèñ
   - research: Á†îÁ©∂Ë∞ÉÊü•„ÄÅÂØπÊØîÂàÜÊûê  
   - comparison: ÂØπÊØîËØÑ‰º∞„ÄÅÁ´ûÂìÅÂàÜÊûê
   - stock_analysis: ÈáëËûçÂàÜÊûê„ÄÅËÇ°Á•®Á†îÁ©∂
   - educational: ÊïôËÇ≤„ÄÅÂ≠¶‰π†„ÄÅÁü•ËØÜËé∑Âèñ
   - ÂÖ∂‰ªñÂêàÈÄÇÁöÑÈ¢ÜÂüü

3. **‰ºòÂåñ‰ªªÂä°ÊèèËø∞** (optimized_description):
   Âü∫‰∫éËΩ®ËøπÂàÜÊûêÔºåÁîüÊàê‰∏Ä‰∏™Ê∏ÖÊô∞„ÄÅÂÖ∑‰Ωì„ÄÅÂèØÊâßË°åÁöÑ‰ªªÂä°ÊèèËø∞ÔºåË¶ÅÊ±ÇÔºö
   - ÊòéÁ°ÆËØ¥Êòé‰ªªÂä°ÁõÆÊ†á
   - ÊåáÂá∫ÈúÄË¶Å‰ΩøÁî®ÁöÑ‰∏ªË¶ÅÂ∑•ÂÖ∑ÊàñÊñπÊ≥ï
   - Á™ÅÂá∫‰ªªÂä°ÁöÑÊ†∏ÂøÉ‰ª∑ÂÄºÂíåÈöæÁÇπ
   - ÈïøÂ∫¶ÊéßÂà∂Âú®50-100Â≠ó

4. **Â§çÊùÇÂ∫¶ËØÑ‰º∞** (complexity):
   - simple: ÂçïÊ≠•È™§ÊàñÁÆÄÂçïÊìç‰Ωú
   - medium: Â§öÊ≠•È™§ÂçèË∞ÉÊàñ‰∏≠Á≠âÈöæÂ∫¶ÂàÜÊûê
   - complex: Ê∑±Â∫¶ÂàÜÊûê„ÄÅÂ§öÂ∑•ÂÖ∑ÈõÜÊàêÊàñÈ´òÈöæÂ∫¶Êé®ÁêÜ

5. **ÂÖ≥ÈîÆÁâπÂæÅ** (key_features):
   ÂàóÂá∫Ëøô‰∏™‰ªªÂä°ÁöÑ3-5‰∏™ÂÖ≥ÈîÆÁâπÂæÅ

ËØ∑‰∏•Ê†ºÊåâÁÖß‰ª•‰∏ãJSONÊ†ºÂºèËøîÂõûÂàÜÊûêÁªìÊûúÔºö

{{
  "task_type": "...",
  "domain": "...", 
  "optimized_description": "...",
  "complexity": "...",
  "key_features": ["ÁâπÂæÅ1", "ÁâπÂæÅ2", "ÁâπÂæÅ3"],
  "confidence": 0.9
}}

Ê≥®ÊÑèÔºöËØ∑Á°Æ‰øùÂàÜÊûêÂáÜÁ°Æ„ÄÅÊèèËø∞Ê∏ÖÊô∞„ÄÅÂàÜÁ±ªÂêàÁêÜ„ÄÇ"""
    
    def _parse_extraction_response(self, response: str, trajectory: TrajectoryResult) -> Optional[TaskEssence]:
        """Ëß£ÊûêLLMÂìçÂ∫îÔºåÂ§ÑÁêÜ‰ºòÂåñÂêéÁöÑJSONÊ†ºÂºè"""
        try:
            # ÊèêÂèñJSON
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group())
                
                # Ëé∑ÂèñLLMÂàÜÊûêÁöÑÁªìÊûú
                llm_task_type = parsed.get("task_type", "").lower()
                llm_domain = parsed.get("domain", "general")
                optimized_description = parsed.get("optimized_description", "")
                complexity = parsed.get("complexity", "medium")
                key_features = parsed.get("key_features", [])
                confidence = parsed.get("confidence", 0.8)
                
                # Êô∫ËÉΩ‰ªªÂä°Á±ªÂûãÊé®Êñ≠ÂíåÈ™åËØÅ
                final_task_type = self._infer_task_type(trajectory, llm_task_type)
                
                # Êô∫ËÉΩÈ¢ÜÂüüÊé®Êñ≠ÂíåÈ™åËØÅ
                final_domain = self._infer_domain(trajectory, llm_domain, final_task_type)
                
                # ‰ΩøÁî®‰ºòÂåñÂêéÁöÑÊèèËø∞ÔºåÂ¶ÇÊûúÊ≤°ÊúâÂàôÂõûÈÄÄÂà∞ÂéüÂßãÊèèËø∞
                final_query = optimized_description if optimized_description else trajectory.task_description[:50]
                
                # ÊûÑÂª∫Â¢ûÂº∫ÁöÑÊàêÂäüÊ®°Âºè
                enhanced_success_pattern = {
                    "duration": trajectory.total_duration,
                    "steps_count": len(trajectory.steps),
                    "key_features": key_features,
                    "confidence": confidence,
                    "tools_used": self._extract_tools_from_trajectory(trajectory)
                }
                
                logger.info(f"üß† Enhanced task analysis:")
                logger.info(f"   Type: {llm_task_type} ‚Üí {final_task_type}")
                logger.info(f"   Domain: {final_domain}")
                logger.info(f"   Optimized description: {final_query[:80]}...")
                logger.info(f"   Key features: {key_features}")
                logger.info(f"   Confidence: {confidence}")
                
                return TaskEssence(
                    essence_id=f"essence_{int(time.time())}_{trajectory.task_id}",
                    task_type=final_task_type,
                    domain=final_domain,
                    query=final_query,
                    complexity_level=complexity,
                    success_pattern=enhanced_success_pattern,
                    extracted_at=datetime.now().isoformat(),
                    source_trajectory_id=trajectory.task_id
                )
            else:
                logger.error(f"Failed to parse JSON response from LLM for essence {trajectory.task_id}")
                logger.warning(f"Response content: {response[:200]}...")
                return None
                
        except Exception as e:
            logger.error(f"Error parsing enhanced extraction response: {e}")
            logger.warning(f"Response content: {response[:500]}...")
            # Âç≥‰ΩøËß£ÊûêÂ§±Ë¥•Ôºå‰πüÂ∞ùËØïÂü∫‰∫éËΩ®ËøπÁâπÂæÅÂàõÂª∫Âü∫Á°ÄÊú¨Ë¥®
            return self._create_fallback_essence(trajectory)
        
        return None
    
    def _extract_tools_from_trajectory(self, trajectory: TrajectoryResult) -> List[str]:
        """‰ªéËΩ®Ëøπ‰∏≠ÊèêÂèñ‰ΩøÁî®ÁöÑÂ∑•ÂÖ∑"""
        tools = set()
        
        for step in trajectory.steps:
            # ‰ºòÂÖà‰ªé action_params ‰∏≠ÊèêÂèñ tool_id
            if step.action_type == ActionType.TOOL_CALL and 'tool_id' in step.action_params:
                tools.add(step.action_params['tool_id'])
            
            # Â¶ÇÊûúÊ≤°ÊúâÊòéÁ°ÆÁöÑ tool_idÔºåÂàôÂü∫‰∫é action_type Âíå action_params ËøõË°åÊé®Êñ≠
            action_type_str = str(step.action_type).lower()
            
            if 'browser_action' in action_type_str:
                tools.add("browser_navigator") # ‰ΩøÁî®ÊñáÊ°£‰∏≠ÂÆö‰πâÁöÑtool_id
            elif 'code_execution' in action_type_str:
                tools.add("python_executor") # ‰ΩøÁî®ÊñáÊ°£‰∏≠ÂÆö‰πâÁöÑtool_id
            
            # Ëøõ‰∏ÄÊ≠•Âü∫‰∫é action_params ‰∏≠ÁöÑÂÖ≥ÈîÆËØçÊé®Êñ≠
            if step.action_params:
                params_str = str(step.action_params).lower()
                if 'url' in params_str or 'navigate' in params_str:
                    tools.add("browser_navigator")
                if 'code' in params_str or 'python' in params_str:
                    tools.add("python_executor")
                # ÂÅáËÆæÊúâÂÖ∂‰ªñÂ∑•ÂÖ∑Ôºå‰æãÂ¶ÇÊñá‰ª∂Â§ÑÁêÜÂ∑•ÂÖ∑
                if 'file' in params_str or 'path' in params_str:
                    tools.add("file_processor")
            
            # Âü∫‰∫é observation ËØÜÂà´Â∑•ÂÖ∑ËæìÂá∫ (‰Ωú‰∏∫Ë°•ÂÖÖ)
            if step.observation:
                obs_str = str(step.observation).lower()
                if 'browser' in obs_str or 'page' in obs_str or 'website' in obs_str:
                    tools.add("browser_navigator")
                if 'python' in obs_str or 'execution' in obs_str:
                    tools.add("python_executor")
        
        return list(tools)
    
    def _infer_task_type(self, trajectory: TrajectoryResult, llm_suggestion: str) -> str:
        """Êô∫ËÉΩÊé®Êñ≠‰ªªÂä°Á±ªÂûã"""
        # 1. Âü∫‰∫éruntime_idÁöÑÂº∫ËßÑÂàô
        if hasattr(trajectory, 'runtime_id') and trajectory.runtime_id:
            runtime_id = trajectory.runtime_id.lower()
            if 'reasoning' in runtime_id:
                return "reasoning"
            elif 'web' in runtime_id:
                return "web"  
            elif 'sandbox' in runtime_id or 'code' in runtime_id:
                return "code"
        
        # 2. Âü∫‰∫é‰ªªÂä°ÊèèËø∞ÁöÑÂÖ≥ÈîÆËØç
        desc = trajectory.task_description.lower()
        reasoning_keywords = ['ÂàÜÊûê', 'ÂØπÊØî', 'Á†îÁ©∂', 'Êé®ÁêÜ', 'analysis', 'compare', 'research', 'reasoning', 'ÂΩ±Âìç']
        web_keywords = ['ÊêúÁ¥¢', 'ËÆøÈóÆ', 'ÊµèËßàÂô®', 'search', 'visit', 'browser', 'google', 'github']
        code_keywords = ['ËÆ°ÁÆó', 'ÁÆóÊ≥ï', '‰ª£Á†Å', 'ÂáΩÊï∞', 'calculate', 'algorithm', 'function', 'code', 'Áü©Èòµ']
        
        if any(keyword in desc for keyword in reasoning_keywords):
            return "reasoning"
        elif any(keyword in desc for keyword in web_keywords):
            return "web"
        elif any(keyword in desc for keyword in code_keywords):
            return "code"
        
        # 3. Âü∫‰∫éÊâßË°åÊ≠•È™§ÂàÜÊûê
        if len(trajectory.steps) > 0:
            # Ê£ÄÊü•ÊòØÂê¶ÊúâÊµèËßàÂô®Êìç‰Ωú
            has_browser = any('browser' in str(step.action_type).lower() or 'navigate' in str(step.action_params) for step in trajectory.steps)
            # Ê£ÄÊü•ÊòØÂê¶Êúâ‰ª£Á†ÅÊâßË°å
            has_code = any('code' in str(step.action_type).lower() or 'python' in str(step.observation).lower() for step in trajectory.steps)
            
            if has_browser and has_code:
                return "reasoning"  # Â§öÂ∑•ÂÖ∑ÂçèÂêå
            elif has_browser:
                return "web"
            elif has_code:
                return "code"
        
        # 4. ‰ΩøÁî®LLMÂª∫ËÆÆÔºàÂ¶ÇÊûúÊúâÊïàÔºâ
        if llm_suggestion in ['reasoning', 'web', 'code']:
            return llm_suggestion
        
        # 5. ÈªòËÆ§ÂÄº
        return "code"
    
    def _infer_domain(self, trajectory: TrajectoryResult, llm_domain: str, task_type: str) -> str:
        """Êô∫ËÉΩÊé®Êñ≠‰ªªÂä°È¢ÜÂüü"""
        desc = trajectory.task_description.lower()
        
        # Âü∫‰∫éÂÖ≥ÈîÆËØçÁöÑÈ¢ÜÂüüÊò†Â∞Ñ
        domain_keywords = {
            'algorithm': ['ÁÆóÊ≥ï', 'ËÆ°ÁÆó', 'Êï∞Â≠¶', 'algorithm', 'calculate', 'math', 'ÊéíÂ∫è', 'ÊêúÁ¥¢'],
            'data_analysis': ['Êï∞ÊçÆ', 'ÂàÜÊûê', 'ÁªüËÆ°', 'data', 'analysis', 'statistics', 'ÂõæË°®'],
            'web_automation': ['ÁΩëÈ°µ', 'ÊµèËßàÂô®', 'ÊêúÁ¥¢', 'web', 'browser', 'search', 'google'],
            'research': ['Á†îÁ©∂', 'ÂØπÊØî', 'Ë∞ÉÊü•', 'research', 'compare', 'study', 'ÂΩ±Âìç'],
            'comparison': ['ÂØπÊØî', 'ÊØîËæÉ', 'vs', 'compare', 'comparison', 'Âå∫Âà´'],
            'stock_analysis': ['ËÇ°Á•®', 'ËÇ°‰ª∑', 'stock', 'price', 'ÊäïËµÑ', 'investment']
        }
        
        for domain, keywords in domain_keywords.items():
            if any(keyword in desc for keyword in keywords):
                return domain
        
        # Âü∫‰∫é‰ªªÂä°Á±ªÂûãÁöÑÈªòËÆ§È¢ÜÂüü
        type_domain_map = {
            'reasoning': 'research',
            'web': 'web_automation', 
            'code': 'algorithm'
        }
        
        return type_domain_map.get(task_type, llm_domain if llm_domain != "general" else "general")
    
    def _create_fallback_essence(self, trajectory: TrajectoryResult) -> Optional[TaskEssence]:
        """ÂàõÂª∫Â§áÁî®Êú¨Ë¥®ÔºàÂΩìLLMËß£ÊûêÂ§±Ë¥•Êó∂Ôºâ"""
        try:
            task_type = self._infer_task_type(trajectory, "")
            domain = self._infer_domain(trajectory, "general", task_type)
            
            return TaskEssence(
                essence_id=f"essence_{int(time.time())}_{trajectory.task_id}",
                task_type=task_type,
                domain=domain,
                query=trajectory.task_description[:20],
                complexity_level="medium",
                success_pattern={"duration": trajectory.total_duration},
                extracted_at=datetime.now().isoformat(),
                source_trajectory_id=trajectory.task_id
            )
        except Exception as e:
            logger.error(f"Failed to create fallback essence: {e}")
            return None
    
    def _store_essence(self, essence: TaskEssence):
        """Â≠òÂÇ®‰ªªÂä°Êú¨Ë¥®Âà∞JSONÊñá‰ª∂"""
        try:
            # ËØªÂèñÁé∞ÊúâÊú¨Ë¥®
            existing_essences = self._load_json_file(self.task_essences_path, [])
            
            # Ê∑ªÂä†Êñ∞Êú¨Ë¥®
            essence_dict = asdict(essence)
            existing_essences.append(essence_dict)
            
            # ‰øùÂ≠òÂõûÊñá‰ª∂
            if self._save_json_file(self.task_essences_path, existing_essences):
                logger.info(f"üíæ ÊàêÂäü‰øùÂ≠ò‰ªªÂä°Êú¨Ë¥®: {essence.essence_id}")
                logger.info(f"  Á±ªÂûã: {essence.task_type}")
                logger.info(f"  È¢ÜÂüü: {essence.domain}")
                logger.info(f"  ÊèèËø∞: {essence.query[:50]}...")
                logger.info(f"  Â§çÊùÇÂ∫¶: {essence.complexity_level}")
                logger.info(f"  Êù•Ê∫êËΩ®Ëøπ: {essence.source_trajectory_id}")
            else:
                logger.error(f"‚ùå ‰øùÂ≠ò‰ªªÂä°Êú¨Ë¥®Â§±Ë¥•: {essence.essence_id}")
            
        except Exception as e:
            logger.error(f"‚ùå Â≠òÂÇ®‰ªªÂä°Êú¨Ë¥®Êó∂Âá∫Èîô: {e}")

    async def _generate_seeds_from_existing_essences(self):
        """‰ªéÁé∞ÊúâÁöÑ‰ªªÂä°Êú¨Ë¥®ÁîüÊàêÁßçÂ≠ê‰ªªÂä°"""
        try:
            logger.info("üå± ÂºÄÂßã‰ªéÁé∞Êúâ‰ªªÂä°Êú¨Ë¥®ÁîüÊàêÁßçÂ≠ê‰ªªÂä°...")
            
            # ËØªÂèñÁé∞Êúâ‰ªªÂä°Êú¨Ë¥®
            essences_data = self._load_json_file(self.task_essences_path, [])
            if not essences_data:
                logger.warning("‚ö†Ô∏è Ê≤°ÊúâÊâæÂà∞‰ªªÂä°Êú¨Ë¥®Êï∞ÊçÆ")
                return
            
            logger.info(f"üìä ÊâæÂà∞ {len(essences_data)} ‰∏™‰ªªÂä°Êú¨Ë¥®")
            
            # ËΩ¨Êç¢‰∏∫TaskEssenceÂØπË±°Âπ∂ÁîüÊàêÁßçÂ≠ê‰ªªÂä°
            seed_tasks = []
            for essence_data in essences_data:
                try:
                    # ÈáçÊûÑTaskEssenceÂØπË±°
                    essence = TaskEssence(
                        essence_id=essence_data['essence_id'],
                        task_type=essence_data['task_type'],
                        domain=essence_data['domain'],
                        query=essence_data['query'],
                        complexity_level=essence_data['complexity_level'],
                        success_pattern=essence_data['success_pattern'],
                        extracted_at=essence_data['extracted_at'],
                        source_trajectory_id=essence_data['source_trajectory_id']
                    )
                    
                    # ÁîüÊàêÁßçÂ≠ê‰ªªÂä°
                    seed_task = await self._convert_essence_to_seed(essence)
                    if seed_task:
                        seed_tasks.append(seed_task)
                        logger.debug(f"‚úÖ ‰ªéÊú¨Ë¥® {essence.essence_id} ÁîüÊàêÁßçÂ≠ê‰ªªÂä°")
                    
                except Exception as e:
                    logger.error(f"‚ùå Â§ÑÁêÜ‰ªªÂä°Êú¨Ë¥®Êó∂Âá∫Èîô: {e}")
                    continue
            
            # ‰øùÂ≠òÁîüÊàêÁöÑÁßçÂ≠ê‰ªªÂä°
            if seed_tasks:
                await self._append_seed_tasks(seed_tasks)
                logger.info(f"üéØ ÊàêÂäüÁîüÊàêÂπ∂‰øùÂ≠ò {len(seed_tasks)} ‰∏™ÁßçÂ≠ê‰ªªÂä°")
                
                # ÁªüËÆ°‰ø°ÊÅØ
                type_stats = defaultdict(int)
                domain_stats = defaultdict(int)
                for task in seed_tasks:
                    type_stats[task['task_type']] += 1
                    domain_stats[task['domain']] += 1
                
                logger.info(f"üìä ÁßçÂ≠ê‰ªªÂä°Á±ªÂûãÂàÜÂ∏É: {dict(type_stats)}")
                logger.info(f"üìä ÁßçÂ≠ê‰ªªÂä°È¢ÜÂüüÂàÜÂ∏É: {dict(domain_stats)}")
            else:
                logger.warning("‚ö†Ô∏è Ê≤°ÊúâÊàêÂäüÁîüÊàê‰ªª‰ΩïÁßçÂ≠ê‰ªªÂä°")
                
        except Exception as e:
            logger.error(f"‚ùå ‰ªé‰ªªÂä°Êú¨Ë¥®ÁîüÊàêÁßçÂ≠ê‰ªªÂä°Â§±Ë¥•: {e}")

    async def generate_tasks_manually(self, count: int = 3) -> List[TaskSpec]:
        """ÊâãÂä®ÁîüÊàêÊåáÂÆöÊï∞ÈáèÁöÑ‰ªªÂä°ÔºàÊöÇÊó∂Á¶ÅÁî®ÔºåÂõ†‰∏∫Ê≤°ÊúâÊú¨Ë¥®Â≠òÂÇ®Ôºâ"""
        logger.warning("‚ö†Ô∏è ÊâãÂä®‰ªªÂä°ÁîüÊàêÂäüËÉΩÂ∑≤Á¶ÅÁî®ÔºåËØ∑‰ΩøÁî®ËΩ®ËøπÂ§ÑÁêÜÁîüÊàêÁßçÂ≠ê‰ªªÂä°")
        return []

    async def generate_task_from_specific_essence(self, essence_id: str) -> Optional[TaskSpec]:
        """Âü∫‰∫éÊåáÂÆöÁöÑ‰ªªÂä°Êú¨Ë¥®ÁîüÊàêÊñ∞‰ªªÂä°ÔºàÊöÇÊó∂Á¶ÅÁî®Ôºâ"""
        logger.warning("‚ö†Ô∏è ÊåáÂÆöÊú¨Ë¥®‰ªªÂä°ÁîüÊàêÂäüËÉΩÂ∑≤Á¶ÅÁî®ÔºåËØ∑‰ΩøÁî®ËΩ®ËøπÂ§ÑÁêÜÁîüÊàêÁßçÂ≠ê‰ªªÂä°")
        return None
    
    def _get_random_essences(self, count: int) -> List[TaskEssence]:
        """Ëé∑ÂèñÈöèÊú∫ÁöÑÁßçÂ≠êÊú¨Ë¥®ÔºàÊöÇÊó∂Á¶ÅÁî®Ôºâ"""
        logger.warning("‚ö†Ô∏è ÈöèÊú∫Êú¨Ë¥®Ëé∑ÂèñÂäüËÉΩÂ∑≤Á¶ÅÁî®ÔºåËØ∑‰ΩøÁî®ËΩ®ËøπÂ§ÑÁêÜÁîüÊàêÁßçÂ≠ê‰ªªÂä°")
        return []
        
    def _record_generated_task(self, task: TaskSpec, essence_id: str):
        """ËÆ∞ÂΩïÁîüÊàêÁöÑ‰ªªÂä°ÔºàÁÆÄÂåñÁâàÔºå‰ªÖËÆ∞ÂΩïÊó•ÂøóÔºâ"""
        try:
            logger.info(f"‚úÖ ËÆ∞ÂΩïÁîüÊàê‰ªªÂä°: {task.task_id}")
            logger.info(f"  Êù•Ê∫êÊú¨Ë¥®: {essence_id}")
            logger.info(f"  ‰ªªÂä°Á±ªÂûã: {task.task_type}")
            logger.info(f"  ÁîüÊàêÊó∂Èó¥: {datetime.now().isoformat()}")
                
        except Exception as e:
            logger.error(f"‚ùå ËÆ∞ÂΩïÁîüÊàê‰ªªÂä°Êó∂Âá∫Èîô: {e}")

    def _is_trajectory_processed(self, trajectory_id: str) -> bool:
        """Ê£ÄÊü•ËΩ®ËøπÊòØÂê¶Â∑≤Â§ÑÁêÜÔºàÂü∫‰∫éÊåÅ‰πÖÂåñÂ≠òÂÇ®Ôºâ"""
        is_processed = trajectory_id in self.processed_trajectories
        if is_processed:
            logger.debug(f"üîç ËΩ®ËøπÂ∑≤Â§ÑÁêÜËøá: {trajectory_id}")
        return is_processed
    
    def _mark_trajectory_processed(self, trajectory_id: str):
        """Ê†áËÆ∞ËΩ®ËøπÂ∑≤Â§ÑÁêÜÔºàÊåÅ‰πÖÂåñÂà∞Êñá‰ª∂Ôºâ"""
        if trajectory_id not in self.processed_trajectories:
            self.processed_trajectories.add(trajectory_id)
            # Á´ãÂç≥‰øùÂ≠òÂà∞Êñá‰ª∂
            self._save_processed_trajectories()
            logger.info(f"‚úÖ Ê†áËÆ∞ËΩ®ËøπÂ∑≤Â§ÑÁêÜÂπ∂‰øùÂ≠ò: {trajectory_id}")
        else:
            logger.debug(f"‚ö†Ô∏è ËΩ®ËøπÂ∑≤ÁªèÂú®Â§ÑÁêÜËÆ∞ÂΩï‰∏≠: {trajectory_id}")
    
    async def _generate_task_from_essence(self, essence: TaskEssence) -> Optional[TaskSpec]:
        """Âü∫‰∫éÊú¨Ë¥®ÁîüÊàêÊñ∞‰ªªÂä°"""
        try:
            # Ëß£ÊûêÂ¢ûÂº∫ÁöÑÊàêÂäüÊ®°Âºè
            success_pattern = essence.success_pattern
            key_features = success_pattern.get("key_features", [])
            tools_used = success_pattern.get("tools_used", [])
            confidence = success_pattern.get("confidence", 0.8)
            
            # ÊûÑÂª∫Â¢ûÂº∫ÁöÑÂèòÂºÇÊèêÁ§∫
            prompt = f"""Âü∫‰∫é‰ª•‰∏ãÈ´òË¥®Èáè‰ªªÂä°Êú¨Ë¥®ÔºåÁîüÊàê‰∏Ä‰∏™Áõ∏‰ºº‰ΩÜÂàõÊñ∞ÁöÑÊñ∞‰ªªÂä°Ôºö

=== Âéü‰ªªÂä°ÂàÜÊûê ===
‰ªªÂä°Á±ªÂûã: {essence.task_type}
‰ªªÂä°È¢ÜÂüü: {essence.domain}
‰ºòÂåñÊèèËø∞: {essence.query}
Â§çÊùÇÂ∫¶Á≠âÁ∫ß: {essence.complexity_level}
ÊèêÂèñÁΩÆ‰ø°Â∫¶: {confidence}

=== ÂÖ≥ÈîÆÁâπÂæÅ ===
{chr(10).join(f"- {feature}" for feature in key_features)}

=== Â∑•ÂÖ∑‰ΩøÁî®Ê®°Âºè ===
‰∏ªË¶ÅÂ∑•ÂÖ∑: {', '.join(tools_used) if tools_used else 'Êú™ÊåáÂÆö'}

=== ‰ªªÂä°ÁîüÊàêË¶ÅÊ±Ç ===
ËØ∑Âü∫‰∫é‰∏äËø∞ÂàÜÊûêÔºåÂàõÈÄ†‰∏Ä‰∏™Êñ∞ÁöÑÂêåÁ±ªÂûã‰ªªÂä°ÔºåË¶ÅÊ±ÇÔºö

1. **‰øùÊåÅÊ†∏ÂøÉÁâπÂæÅ**Ôºö
   - ‰ªªÂä°Á±ªÂûãÂøÖÈ°ªÊòØ {essence.task_type}
   - È¢ÜÂüüÂ∫îËØ•ÊòØ {essence.domain} ÊàñÁõ∏ÂÖ≥È¢ÜÂüü
   - Â§çÊùÇÂ∫¶‰øùÊåÅÂú® {essence.complexity_level} Á∫ßÂà´

2. **ÂàõÊñ∞ÂèòÂåñ**Ôºö
   - ÊîπÂèòÂÖ∑‰ΩìÁöÑÁõÆÊ†áÂØπË±°ÊàñÂèÇÊï∞
   - Ë∞ÉÊï¥Âú∫ÊôØËÆæÂÆöÊàñÂ∫îÁî®ËÉåÊôØ
   - ‰øùÊåÅ‰ªªÂä°ÁöÑÊåëÊàòÊÄßÂíå‰ª∑ÂÄº

3. **ÂÆûÁî®ÊÄßË¶ÅÊ±Ç**Ôºö
   - ‰ªªÂä°ÊèèËø∞Ê∏ÖÊô∞ÂÖ∑‰ΩìÔºåÂèØÁõ¥Êé•ÊâßË°å
   - ËØ¥ÊòéÈ¢ÑÊúü‰ΩøÁî®ÁöÑÂ∑•ÂÖ∑ÂíåÊñπÊ≥ï
   - Á°Æ‰øù‰ªªÂä°ÊúâÊòéÁ°ÆÁöÑÊàêÂäüÊ†áÂáÜ

4. **Ë¥®ÈáèÊ†áÂáÜ**Ôºö
   - ‰ªªÂä°ÊèèËø∞ÈïøÂ∫¶60-120Â≠ó
   - ÈÅøÂÖçËøá‰∫éÁÆÄÂçïÊàñËøá‰∫éÂ§çÊùÇ
   - Á°Æ‰øù‰ªªÂä°ÂÖ∑ÊúâÂÆûÈôÖÂ∫îÁî®‰ª∑ÂÄº

=== Á§∫‰æãÊ†ºÂºèÂèÇËÄÉ ===
{essence.task_type}Á±ªÂûã‰ªªÂä°Á§∫‰æãÔºö
- reasoning: "‰ΩøÁî®ÊµèËßàÂô®ÊêúÁ¥¢ÂíåPythonÂàÜÊûêÔºåÂØπÊØîÂàÜÊûêChatGPTÂíåClaudeÂú®‰ª£Á†ÅÁîüÊàêËÉΩÂäõ‰∏äÁöÑÂ∑ÆÂºÇÔºå‰ªéÂáÜÁ°ÆÊÄß„ÄÅÊïàÁéáÂíåÂèØËØªÊÄß‰∏â‰∏™Áª¥Â∫¶ËøõË°åËØÑ‰º∞"
- web: "ËÆøÈóÆGitHubÊêúÁ¥¢ÊúÄÂèóÊ¨¢ËøéÁöÑÊú∫Âô®Â≠¶‰π†È°πÁõÆÔºåÁ≠õÈÄâstarÊï∞ÈáèË∂ÖËøá10kÁöÑÈ°πÁõÆÔºåÊèêÂèñÈ°πÁõÆÂêçÁß∞„ÄÅÁÆÄ‰ªãÂíå‰∏ªË¶ÅÊäÄÊúØÊ†à‰ø°ÊÅØ"  
- code: "ÂÆûÁé∞‰∏Ä‰∏™È´òÊïàÁöÑÂø´ÈÄüÊéíÂ∫èÁÆóÊ≥ïÔºåË¶ÅÊ±ÇÊîØÊåÅËá™ÂÆö‰πâÊØîËæÉÂáΩÊï∞ÔºåÂπ∂Ê∑ªÂä†ÊÄßËÉΩÊµãËØï‰ª£Á†ÅÈ™åËØÅÂú®‰∏çÂêåÊï∞ÊçÆËßÑÊ®°‰∏ãÁöÑÊâßË°åÊïàÁéá"

ËØ∑‰∏•Ê†ºÊåâÁÖß‰ª•‰∏ãJSONÊ†ºÂºèËøîÂõûÔºö

{{
  "description": "Êñ∞‰ªªÂä°ÁöÑËØ¶ÁªÜÊèèËø∞",
  "expected_tools": ["Â∑•ÂÖ∑1", "Â∑•ÂÖ∑2"],
  "success_criteria": "ÊàêÂäüÊ†áÂáÜÊèèËø∞",
  "estimated_steps": Êï∞Â≠ó,
  "innovation_points": ["ÂàõÊñ∞ÁÇπ1", "ÂàõÊñ∞ÁÇπ2"]
}}

Ê≥®ÊÑèÔºöÁ°Æ‰øùÁîüÊàêÁöÑ‰ªªÂä°Êó¢‰øùÊåÅÂéüÊúâÁâπÂæÅÔºåÂèàÂÖ∑ÊúâÂàõÊñ∞ÊÄßÂíåÂÆûÁî®‰ª∑ÂÄº„ÄÇ"""
            
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_client._call_api(messages)
            
            # Ëß£ÊûêÂìçÂ∫î
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group())
                
                # ÊûÑÂª∫Â¢ûÂº∫ÁöÑTaskSpec
                task_id = f"synth_{essence.task_type}_{essence.domain}_{int(time.time())}"
                
                # Á°ÆÂÆöexpected_toolsÔºåÁªìÂêàÂéüÊúâÂ∑•ÂÖ∑ÂíåÊñ∞Âª∫ËÆÆ
                expected_tools = parsed.get("expected_tools", tools_used if tools_used else ["python_executor"])
                
                # Ê†πÊçÆÂ§çÊùÇÂ∫¶Ë∞ÉÊï¥max_steps
                complexity_steps_map = {
                    "simple": 3,
                    "medium": 6, 
                    "complex": 10
                }
                max_steps = complexity_steps_map.get(essence.complexity_level, 5)
                
                # Â¶ÇÊûúLLMÊèê‰æõ‰∫Ü‰º∞ËÆ°Ê≠•È™§Êï∞Ôºå‰ΩøÁî®ÂÆÉ
                if "estimated_steps" in parsed:
                    max_steps = min(parsed["estimated_steps"], 15)  # ÊúÄÂ§ß‰∏çË∂ÖËøá15Ê≠•
                
                logger.info(f"‚ú® Generated enhanced task:")
                logger.info(f"   Task ID: {task_id}")
                logger.info(f"   Description: {parsed.get('description', '')[:80]}...")
                logger.info(f"   Expected tools: {expected_tools}")
                logger.info(f"   Max steps: {max_steps}")
                
                return TaskSpec(
                    task_id=task_id,
                    task_type=TaskType(essence.task_type),
                    description=parsed.get("description", essence.query),
                    expected_tools=expected_tools,
                    constraints={
                        "success_criteria": parsed.get("success_criteria", ""),
                        "innovation_points": parsed.get("innovation_points", []),
                        "source_essence": essence.essence_id
                    },
                    max_steps=max_steps,
                    priority=1
                )
            else:
                logger.error(f"Failed to parse JSON response from LLM for essence {essence.essence_id}")
                logger.warning(f"Response content: {response[:200]}...")
                return None
        
        except Exception as e:
            logger.error(f"Error generating task from essence {essence.essence_id}: {e}")
        return None
    
    async def _publish_task(self, task: TaskSpec):
        """ÂèëÂ∏É‰ªªÂä°Âà∞ÂØπÂ∫îÈòüÂàó"""
        queue_mapping = {
            TaskType.CODE: "tasks:code",
            TaskType.WEB: "tasks:web",
            TaskType.REASONING: "tasks:reasoning"
        }
        
        queue_name = queue_mapping.get(task.task_type, "tasks:code")
        
        await self.redis.xadd(
            queue_name,
            {
                "task": task.json(),
                "submitted_at": time.time(),
                "priority": task.priority,
                "source": "synthesis"
            }
        )

async def main():
    """‰ªªÂä°ÂêàÊàêÂô®‰∏ªÁ®ãÂ∫è"""
    config = {
        "redis_url": os.getenv("REDIS_URL", "redis://redis:6379"),
        "synthesis_enabled": os.getenv("SYNTHESIS_ENABLED", "false").lower() == "true",
        "auto_monitor_trajectories": os.getenv("AUTO_MONITOR_TRAJECTORIES", "true").lower() == "true",
        "auto_export_seeds": os.getenv("AUTO_EXPORT_SEEDS", "true").lower() == "true",
        # "vllm_url": os.getenv("VLLM_URL", "http://vllm:8000") # Áî®Êà∑‰∏ç‰ΩøÁî®vLLMÔºåÊ≥®ÈáäÊéâÊ≠§ÈÖçÁΩÆ
    }
    
    # ËæìÂá∫ÈÖçÁΩÆ‰ø°ÊÅØ
    logger.info("üöÄ ÂêØÂä®Âü∫‰∫éJSONÁöÑ‰ªªÂä°ÂêàÊàêÂô®ÔºåÈÖçÁΩÆÂ¶Ç‰∏ã:")
    logger.info(f"  ÂêàÊàêÂô®ÂêØÁî®: {config['synthesis_enabled']}")
    logger.info(f"  Ëá™Âä®ËΩ®ËøπÁõëÊéß: {config['auto_monitor_trajectories']}")
    logger.info(f"  Ëá™Âä®ÁßçÂ≠êÂØºÂá∫: {config['auto_export_seeds']}")
    logger.info(f"  Â≠òÂÇ®ÊñπÂºè: JSONÊñá‰ª∂")
    
    synthesizer = SynthesisService(config)
    
    try:
        if config["synthesis_enabled"]:
            logger.info("üéØ ÂºÄÂßãÂêØÂä®‰ªªÂä°ÂêàÊàêÊúçÂä°...")
            await synthesizer.start()
        else:
            logger.info("‚ö†Ô∏è ‰ªªÂä°ÂêàÊàêÂäüËÉΩÂ∑≤Á¶ÅÁî®ÔºåÊúçÂä°Â∞ÜÁ≠âÂæÖ...")
            # ‰øùÊåÅÊúçÂä°ËøêË°åÔºå‰ΩÜ‰∏çÊâßË°åÂêàÊàêÈÄªËæë
            while True:
                await asyncio.sleep(60)
                logger.info("Synthesis service waiting (disabled)...")
    except KeyboardInterrupt:
        logger.info("üõë ÂêàÊàêÊúçÂä°Ë¢´‰∏≠Êñ≠")
    finally:
        try:
            # ÂÅúÊ≠¢Êñá‰ª∂ÁõëÊéß
            if hasattr(synthesizer, 'observer') and synthesizer.observer and synthesizer.observer.is_alive():
                synthesizer.observer.stop()
                synthesizer.observer.join()
                logger.info("üìÅ Êñá‰ª∂ÁõëÊéßÂ∑≤ÂÅúÊ≠¢")
            
            # Ê∏ÖÁêÜUnifiedToolLibraryÁÆ°ÁêÜÁöÑËµÑÊ∫ê
            if hasattr(synthesizer, 'tool_library'):
               
               
               
                await synthesizer.tool_library.cleanup()
                logger.info("üßπ UnifiedToolLibraryËµÑÊ∫êÂ∑≤Ê∏ÖÁêÜ")

            await synthesizer.redis.aclose()  # ‰ΩøÁî®aclose()Êõø‰ª£close()
            logger.info("üîå RedisËøûÊé•Â∑≤ÂÖ≥Èó≠")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÂÖ≥Èó≠ËµÑÊ∫êÊó∂Âá∫Èîô: {e}")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())