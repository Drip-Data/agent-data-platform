import os
import logging
from typing import Any, Dict, List, Optional
import httpx
import tiktoken # Geminiä¹Ÿå¯èƒ½éœ€è¦tiktokenæ¥è®¡ç®—token

from .interfaces import ILLMProvider

logger = logging.getLogger(__name__)

class GeminiProvider(ILLMProvider):
    """
    Google Gemini LLM æä¾›å•†çš„å®ç°ã€‚
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.api_key = self.config.get('gemini_api_key') or os.getenv('GEMINI_API_KEY')
        self.api_url = self.config.get('gemini_api_url') or os.getenv('GEMINI_API_URL', 'https://generativelanguage.googleapis.com/v1beta')
        self.client = httpx.AsyncClient(timeout=60.0)
        
        # éªŒè¯å¹¶ä½¿ç”¨æœ‰æ•ˆçš„Geminiæ¨¡å‹åç§°
        # ä¼˜å…ˆä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æ¨¡å‹ï¼Œç„¶åæ˜¯gemini_default_modelï¼Œæœ€åæ˜¯é»˜è®¤å€¼
        self._default_model = (
            self.config.get('model') or 
            self.config.get('gemini_default_model') or 
            'gemini-2.5-flash-preview-05-20'
        )
        self._supported_models = self.config.get('gemini_supported_models', [
            'gemini-2.5-flash-lite-preview-06-17',
            'gemini-2.5-flash-preview-05-20',
            'gemini-2.0-flash', 'gemini-2.0-pro', 
            'gemini-1.0-pro', 'gemini-pro', 'gemini-1.5-flash' # æ·»åŠ ç¨³å®šæ¨¡å‹
        ])
        
        if not self.api_key:
            logger.warning("GEMINI_API_KEY ç¯å¢ƒå˜é‡æˆ–é…ç½®ä¸­æœªè®¾ç½®ã€‚GeminiProvider å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚")

    async def generate_response(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        temperature: float = 0.1, # Geminiçš„é»˜è®¤æ¸©åº¦
        max_tokens: int = 4096, # Geminiçš„é»˜è®¤max_tokens
        stream: bool = False,
        tools: Optional[List[Dict[str, Any]]] = None, # Geminiçš„å·¥å…·è°ƒç”¨å¯èƒ½ä¸åŒ
        tool_choice: Optional[str] = None,
        **kwargs
    ) -> Any:
        """
        æ ¹æ®ç»™å®šçš„æ¶ˆæ¯ç”Ÿæˆ Google Gemini LLM å“åº”ã€‚
        """
        if model not in self._supported_models:
            logger.warning(f"æ¨¡å‹ {model} ä¸å— GeminiProvider æ”¯æŒï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡å‹ {self._default_model}ã€‚")
            model = self._default_model
        
        # å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºGeminiçš„contentsæ ¼å¼
        contents = []
        for msg in messages:
            if msg["role"] == "user":
                contents.append({"parts": [{"text": msg["content"]}]})
            elif msg["role"] == "assistant":
                # Geminiçš„assistantè§’è‰²éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œé€šå¸¸æ˜¯æ¨¡å‹è¾“å‡º
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå¦‚æœéœ€è¦æ›´å¤æ‚çš„å¯¹è¯å†å²ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´
                contents.append({"parts": [{"text": msg["content"]}]})
        
        payload = {
            "contents": contents,
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens,
                "candidateCount": 1,
                "stopSequences": [],
                "topP": 0.9,
                "topK": 40
            },
            **kwargs
        }
        
        # Geminiçš„å·¥å…·è°ƒç”¨æ ¼å¼ä¸OpenAIä¸åŒï¼Œè¿™é‡Œæš‚æ—¶ä¸å¤„ç†toolså’Œtool_choice
        # if tools:
        #     payload["tools"] = tools
        # if tool_choice:
        #     payload["tool_choice"] = tool_choice

        try:
            # ğŸ”§ æ·»åŠ DNSè§£æé‡è¯•æœºåˆ¶ (ä»llm_client.pyä¸­è¿ç§»è¿‡æ¥)
            from httpx import Timeout
            import asyncio # å°½ç®¡è¿™é‡Œä¸ç›´æ¥ä½¿ç”¨asyncio.sleepï¼Œä½†ä¸ºäº†ä¿æŒåŸé€»è¾‘çš„å®Œæ•´æ€§
            
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶å®¢æˆ·ç«¯ï¼Œé…ç½®æ›´é•¿çš„è¶…æ—¶å’Œé‡è¯•
            temp_client = httpx.AsyncClient(
                timeout=Timeout(timeout=120.0, connect=30.0),
                limits=httpx.Limits(max_connections=10, max_keepalive_connections=5),
                trust_env=False  # ä¸ä½¿ç”¨ç¯å¢ƒä»£ç†è®¾ç½®
            )
            
            response = await temp_client.post(
                f"{self.api_url}/models/{model}:generateContent?key={self.api_key}",
                json=payload
            )
            response.raise_for_status()
            
            result = response.json()
            
            # æ£€æŸ¥å“åº”æ ¼å¼ (ä»llm_client.pyä¸­è¿ç§»è¿‡æ¥)
            if "candidates" not in result:
                raise ValueError(f"Invalid Gemini response format: missing 'candidates' field")
            
            if not result["candidates"]:
                raise ValueError(f"Empty candidates in Gemini response")
                
            candidate = result["candidates"][0]
            if "content" not in candidate:
                raise ValueError(f"Invalid candidate format: missing 'content' field")
                
            content = candidate["content"]
            if "parts" not in content:
                raise ValueError(f"Invalid content format: missing 'parts' field")
                
            if not content["parts"]:
                raise ValueError(f"Empty parts in content")
                
            part = content["parts"][0]
            if "text" not in part:
                raise ValueError(f"Invalid part format: missing 'text' field")
                
            await temp_client.aclose()
            return part["text"]
            
        except httpx.HTTPStatusError as e:
            logger.error(f"Gemini API HTTP é”™è¯¯: {e.response.status_code} - {e.response.text}")
            # ğŸ”§ å¼ºåŒ–çš„é‡è¯•æœºåˆ¶ï¼šä½¿ç”¨ç¨³å®šæ¨¡å‹+ç›´æ¥IP (ä»llm_client.pyä¸­è¿ç§»è¿‡æ¥)
            if "name resolution" in str(e).lower() or "connection" in str(e).lower():
                logger.info("å°è¯•ä½¿ç”¨å¤‡ç”¨ç½‘ç»œé…ç½®é‡è¯•...")
                try:
                    # ä½¿ç”¨å¤‡ç”¨DNSé…ç½®
                    backup_client = httpx.AsyncClient(
                        timeout=Timeout(timeout=180.0, connect=60.0),
                        limits=httpx.Limits(max_connections=5, max_keepalive_connections=2),
                        trust_env=False
                    )
                    
                    # ä½¿ç”¨ç¨³å®šæ¨¡å‹é‡è¯•
                    stable_model = 'gemini-1.5-flash'
                    response = await backup_client.post(
                        f"{self.api_url}/models/{stable_model}:generateContent?key={self.api_key}",
                        json=payload
                    )
                    response.raise_for_status()
                    result = response.json()
                    
                    # åŒæ ·çš„æ ¼å¼æ£€æŸ¥
                    if ("candidates" in result and result["candidates"] and 
                        "content" in result["candidates"][0] and
                        "parts" in result["candidates"][0]["content"] and
                        result["candidates"][0]["content"]["parts"] and
                        "text" in result["candidates"][0]["content"]["parts"][0]):
                        await backup_client.aclose()
                        logger.info("âœ… ä½¿ç”¨å¤‡ç”¨ç½‘ç»œé…ç½®æˆåŠŸæ¢å¤")
                        return result["candidates"][0]["content"]["parts"][0]["text"]
                    else:
                        raise ValueError(f"Invalid backup response format")
                        
                except Exception as backup_e:
                    logger.error(f"å¤‡ç”¨ç½‘ç»œé…ç½®ä¹Ÿå¤±è´¥: {backup_e}")
                    raise e
            else:
                raise e
        except httpx.RequestError as e:
            logger.error(f"Gemini API è¯·æ±‚é”™è¯¯: {e}")
            raise
        except Exception as e:
            logger.error(f"è°ƒç”¨ Gemini API å¤±è´¥: {e}")
            raise

    async def count_tokens(self, text: str, model: str) -> int:
        """
        è®¡ç®—ç»™å®šæ–‡æœ¬åœ¨ç‰¹å®š Gemini æ¨¡å‹ä¸­çš„ä»¤ç‰Œæ•°ã€‚
        Geminiæ²¡æœ‰ç›´æ¥çš„tiktokenï¼Œè¿™é‡Œä½¿ç”¨ç²—ç•¥ä¼°è®¡æˆ–æœªæ¥é›†æˆGoogleçš„tokenè®¡æ•°APIã€‚
        """
        # æš‚æ—¶ä½¿ç”¨ç²—ç•¥ä¼°è®¡ï¼Œæˆ–è€…å¦‚æœtiktokenæ”¯æŒGeminiæ¨¡å‹ï¼Œåˆ™ä½¿ç”¨tiktoken
        try:
            encoding = tiktoken.encoding_for_model(model)
            return len(encoding.encode(text))
        except KeyError:
            logger.warning(f"æœªæ‰¾åˆ°æ¨¡å‹ {model} çš„ tiktoken ç¼–ç ï¼Œå°†ä½¿ç”¨ cl100k_base ç¼–ç è¿›è¡Œç²—ç•¥ä¼°è®¡ã€‚")
            encoding = tiktoken.get_encoding("cl100k_base")
            return len(encoding.encode(text))
        except Exception as e:
            logger.error(f"è®¡ç®—ä»¤ç‰Œæ•°å¤±è´¥: {e}")
            return int(len(text) / 4) # ç²—ç•¥ä¼°è®¡ï¼Œå¹¶è½¬æ¢ä¸ºæ•´æ•°

    def get_supported_models(self) -> List[str]:
        """
        è·å–æ­¤æä¾›å•†æ”¯æŒçš„ Gemini æ¨¡å‹åˆ—è¡¨ã€‚
        """
        return self._supported_models

    def get_default_model(self) -> str:
        """
        è·å–æ­¤æä¾›å•†çš„é»˜è®¤ Gemini æ¨¡å‹ã€‚
        """
        return self._default_model