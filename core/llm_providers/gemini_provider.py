import os
import logging
import json
from typing import Any, Dict, List, Optional
import httpx
import tiktoken # Geminiä¹Ÿå¯èƒ½éœ€è¦tiktokenæ¥è®¡ç®—token

from .interfaces import ILLMProvider

logger = logging.getLogger(__name__)

class GeminiProvider(ILLMProvider):
    """
    Google Gemini LLM æä¾›å•†çš„å®ç°ã€‚
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.api_key = self.config.get('gemini_api_key') or os.getenv('GEMINI_API_KEY')
        self.api_url = self.config.get('gemini_api_url') or os.getenv('GEMINI_API_URL', 'https://generativelanguage.googleapis.com/v1beta')
        self.client = httpx.AsyncClient(
            timeout=httpx.Timeout(timeout=120.0, connect=30.0),
            limits=httpx.Limits(max_connections=10, max_keepalive_connections=5),
            trust_env=False
        )
        
        # éªŒè¯å¹¶ä½¿ç”¨æœ‰æ•ˆçš„Geminiæ¨¡å‹åç§°
        # ä¼˜å…ˆä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æ¨¡å‹ï¼Œç„¶åæ˜¯gemini_default_modelï¼Œæœ€åæ˜¯é»˜è®¤å€¼
        self._default_model = (
            self.config.get('model') or 
            self.config.get('gemini_default_model') or 
            'gemini-2.5-flash-preview-05-20'
        )
        self._supported_models = self.config.get('gemini_supported_models', [
            'gemini-2.5-pro',
            'gemini-2.5-flash-lite-preview-06-17',
            'gemini-2.5-flash-preview-05-20',
            'gemini-2.0-flash', 'gemini-2.0-pro', 
            'gemini-1.0-pro', 'gemini-pro', 'gemini-1.5-flash' # æ·»åŠ ç¨³å®šæ¨¡å‹
        ])
        
        if not self.api_key:
            logger.warning("GEMINI_API_KEY ç¯å¢ƒå˜é‡æˆ–é…ç½®ä¸­æœªè®¾ç½®ã€‚GeminiProvider å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚")

    async def generate_response(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        temperature: float = 0.1, # Geminiçš„é»˜è®¤æ¸©åº¦
        max_tokens: int = 32768, # Increased default for complex tasks
        stream: bool = False,
        tools: Optional[List[Dict[str, Any]]] = None, # Geminiçš„å·¥å…·è°ƒç”¨å¯èƒ½ä¸åŒ
        tool_choice: Optional[str] = None,
        timeout: int = 120,
        **kwargs
    ) -> Any:
        """
        æ ¹æ®ç»™å®šçš„æ¶ˆæ¯ç”Ÿæˆ Google Gemini LLM å“åº”ã€‚
        æ³¨æ„ï¼šGemini APIä¸æ”¯æŒstop_sequenceså‚æ•°ï¼Œä¼šè¢«è‡ªåŠ¨å¿½ç•¥ã€‚
        """
        # æå– stop_sequences
        stop_sequences = kwargs.pop('stop_sequences', [])
        if stop_sequences:
            logger.debug(f"ğŸ”§ Applying stop_sequences for Gemini: {stop_sequences}")

        # è¿‡æ»¤æ‰Geminiä¸æ”¯æŒçš„å‚æ•°
        if model not in self._supported_models:
            logger.warning(f"æ¨¡å‹ {model} ä¸å— GeminiProvider æ”¯æŒï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡å‹ {self._default_model}ã€‚")
            model = self._default_model
        
        # ğŸ”§ ä¿®å¤æ¶ˆæ¯æ ¼å¼è½¬æ¢ - æ”¯æŒè§’è‰²äº¤æ›¿å’Œå†…å®¹åˆå¹¶
        logger.debug(f"ğŸ” Geminiæ¶ˆæ¯è½¬æ¢å¼€å§‹ - è¾“å…¥æ¶ˆæ¯æ•°: {len(messages)}")
        
        contents = []
        current_role = None
        merged_content = []

        for msg in messages:
            role = msg.get("role")
            content = msg.get("content", "")

            if not role:
                logger.warning(f"æ¶ˆæ¯ç¼ºå°‘ 'role' å­—æ®µ, è·³è¿‡: {msg}")
                continue

            # å°† 'assistant' æ˜ å°„åˆ° 'model'
            if role == "assistant":
                role = "model"
            
            # ç¡®ä¿è§’è‰²æ˜¯ 'user' æˆ– 'model'
            if role not in ["user", "model"]:
                logger.warning(f"æœªçŸ¥çš„æ¶ˆæ¯è§’è‰²: {role}, è·³è¿‡è¯¥æ¶ˆæ¯")
                continue

            if current_role is None:
                current_role = role

            if role == current_role:
                # å¦‚æœè§’è‰²ç›¸åŒï¼Œåˆå¹¶å†…å®¹
                merged_content.append(str(content))
            else:
                # å¦‚æœè§’è‰²åˆ‡æ¢ï¼Œæ·»åŠ å‰ä¸€ä¸ªè§’è‰²çš„åˆå¹¶å†…å®¹
                if merged_content:
                    contents.append({
                        "role": current_role,
                        "parts": [{"text": "\n\n".join(merged_content)}]
                    })
                # å¼€å§‹æ–°çš„è§’è‰²å†…å®¹
                current_role = role
                merged_content = [str(content)]

        # æ·»åŠ æœ€åä¸€éƒ¨åˆ†åˆå¹¶çš„å†…å®¹
        if merged_content:
            contents.append({
                "role": current_role,
                "parts": [{"text": "\n\n".join(merged_content)}]
            })

        logger.debug(f"ğŸ” Geminiæ¶ˆæ¯è½¬æ¢å®Œæˆ - è½¬æ¢åæ¶ˆæ¯æ•°: {len(contents)}")
        
        payload = {
            "contents": contents,
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens,
                "candidateCount": 1,
                "stopSequences": stop_sequences,
                "topP": 0.9,
                "topK": 40
            },
            **kwargs
        }
        
        # Geminiçš„å·¥å…·è°ƒç”¨æ ¼å¼ä¸OpenAIä¸åŒï¼Œè¿™é‡Œæš‚æ—¶ä¸å¤„ç†toolså’Œtool_choice
        # if tools:
        #     payload["tools"] = tools
        # if tool_choice:
        #     payload["tool_choice"] = tool_choice

        logger.debug(f"ğŸ” Gemini APIè°ƒç”¨å¼€å§‹ - æ¨¡å‹: {model}, payloadå¤§å°: {len(json.dumps(payload))} å­—ç¬¦")
        
        try:
            response = await self.client.post(
                f"{self.api_url}/models/{model}:generateContent?key={self.api_key}",
                json=payload,
                timeout=timeout
            )
            response.raise_for_status()
            
            result = response.json()
            
            # ğŸ”§ å¢å¼ºå“åº”æ ¼å¼æ£€æŸ¥ - å®‰å…¨å¤„ç†åµŒå¥—æ•°æ®ç»“æ„
            try:
                logger.debug(f"Gemini APIåŸå§‹å“åº”ç»“æ„: {json.dumps(result, ensure_ascii=False, indent=2)[:500]}...")
                
                # æ£€æŸ¥å“åº”æ ¼å¼ - å®‰å…¨å­—å…¸è®¿é—®
                if not isinstance(result, dict):
                    raise ValueError(f"å“åº”ä¸æ˜¯å­—å…¸ç±»å‹: {type(result)}")
                
                if "candidates" not in result:
                    raise ValueError(f"å“åº”ç¼ºå°‘'candidates'å­—æ®µ: {list(result.keys())}")
                
                candidates = result["candidates"]
                if not isinstance(candidates, list) or not candidates:
                    raise ValueError(f"candidateså­—æ®µæ— æ•ˆ: {type(candidates)}, é•¿åº¦: {len(candidates) if isinstance(candidates, list) else 'N/A'}")
                    
                candidate = candidates[0]
                if not isinstance(candidate, dict):
                    raise ValueError(f"å€™é€‰é¡¹ä¸æ˜¯å­—å…¸ç±»å‹: {type(candidate)}")
                
                if "content" not in candidate:
                    raise ValueError(f"å€™é€‰é¡¹ç¼ºå°‘'content'å­—æ®µ: {list(candidate.keys())}")
                    
                content = candidate["content"]
                if not isinstance(content, dict):
                    raise ValueError(f"contentä¸æ˜¯å­—å…¸ç±»å‹: {type(content)}")
                
                if "parts" not in content:
                    raise ValueError(f"contentç¼ºå°‘'parts'å­—æ®µ: {list(content.keys())}")
                    
                parts = content["parts"]
                if not isinstance(parts, list) or not parts:
                    raise ValueError(f"partså­—æ®µæ— æ•ˆ: {type(parts)}, é•¿åº¦: {len(parts) if isinstance(parts, list) else 'N/A'}")
                    
                part = parts[0]
                if not isinstance(part, dict):
                    raise ValueError(f"partä¸æ˜¯å­—å…¸ç±»å‹: {type(part)}")
                
                if "text" not in part:
                    raise ValueError(f"partç¼ºå°‘'text'å­—æ®µ: {list(part.keys())}")
                    
                text_content = part["text"]
                if not isinstance(text_content, str):
                    logger.warning(f"textå­—æ®µä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹: {type(text_content)}, å°è¯•è½¬æ¢")
                    text_content = str(text_content)
                
                logger.info(f"âœ… Geminiå“åº”è§£ææˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(text_content)}")
                return text_content
                
            except Exception as parse_error:
                logger.error(f"Geminiå“åº”è§£æå¤±è´¥: {parse_error}")
                logger.error(f"åŸå§‹å“åº”å†…å®¹: {json.dumps(result, ensure_ascii=False, indent=2)}")
                raise ValueError(f"Geminiå“åº”æ ¼å¼è§£æé”™è¯¯: {parse_error}")
            
        except httpx.HTTPStatusError as e:
            logger.error(f"Gemini API HTTP é”™è¯¯: {e.response.status_code} - {e.response.text}")
            # ğŸ”§ å¼ºåŒ–çš„é‡è¯•æœºåˆ¶ï¼šä½¿ç”¨ç¨³å®šæ¨¡å‹+ç›´æ¥IP (ä»llm_client.pyä¸­è¿ç§»è¿‡æ¥)
            if "name resolution" in str(e).lower() or "connection" in str(e).lower():
                logger.info("å°è¯•ä½¿ç”¨å¤‡ç”¨ç½‘ç»œé…ç½®é‡è¯•...")
                try:
                    # ä½¿ç”¨å¤‡ç”¨DNSé…ç½®
                    backup_client = httpx.AsyncClient(
                        timeout=httpx.Timeout(timeout=180.0, connect=60.0),
                        limits=httpx.Limits(max_connections=5, max_keepalive_connections=2),
                        trust_env=False
                    )
                    
                    # ä½¿ç”¨ç¨³å®šæ¨¡å‹é‡è¯•
                    stable_model = 'gemini-1.5-flash'
                    response = await backup_client.post(
                        f"{self.api_url}/models/{stable_model}:generateContent?key={self.api_key}",
                        json=payload
                    )
                    response.raise_for_status()
                    result = response.json()
                    
                    # ğŸ”§ å¤‡ç”¨å“åº”çš„å®‰å…¨æ ¼å¼æ£€æŸ¥
                    try:
                        if (isinstance(result, dict) and
                            "candidates" in result and isinstance(result["candidates"], list) and
                            len(result["candidates"]) > 0 and isinstance(result["candidates"][0], dict) and
                            "content" in result["candidates"][0] and isinstance(result["candidates"][0]["content"], dict) and
                            "parts" in result["candidates"][0]["content"] and isinstance(result["candidates"][0]["content"]["parts"], list) and
                            len(result["candidates"][0]["content"]["parts"]) > 0 and isinstance(result["candidates"][0]["content"]["parts"][0], dict) and
                            "text" in result["candidates"][0]["content"]["parts"][0]):
                            
                            backup_text = result["candidates"][0]["content"]["parts"][0]["text"]
                            if not isinstance(backup_text, str):
                                backup_text = str(backup_text)
                            
                            await backup_client.aclose()
                            logger.info("âœ… ä½¿ç”¨å¤‡ç”¨ç½‘ç»œé…ç½®æˆåŠŸæ¢å¤")
                            return backup_text
                        else:
                            raise ValueError(f"å¤‡ç”¨å“åº”æ ¼å¼æ— æ•ˆ: {list(result.keys()) if isinstance(result, dict) else type(result)}")
                    except Exception as backup_parse_error:
                        logger.error(f"å¤‡ç”¨å“åº”è§£æå¤±è´¥: {backup_parse_error}")
                        raise ValueError(f"å¤‡ç”¨å“åº”æ ¼å¼è§£æé”™è¯¯: {backup_parse_error}")
                        
                except Exception as backup_e:
                    logger.error(f"å¤‡ç”¨ç½‘ç»œé…ç½®ä¹Ÿå¤±è´¥: {backup_e}")
                    raise e
            else:
                raise e
        except httpx.RequestError as e:
            logger.error(f"Gemini API è¯·æ±‚é”™è¯¯: {e}")
            raise
        except Exception as e:
            logger.error(f"è°ƒç”¨ Gemini API å¤±è´¥: {e}")
            raise

    async def count_tokens(self, text: str, model: str) -> int:
        """
        è®¡ç®—ç»™å®šæ–‡æœ¬åœ¨ç‰¹å®š Gemini æ¨¡å‹ä¸­çš„ä»¤ç‰Œæ•°ã€‚
        Geminiæ²¡æœ‰ç›´æ¥çš„tiktokenï¼Œè¿™é‡Œä½¿ç”¨ç²—ç•¥ä¼°è®¡æˆ–æœªæ¥é›†æˆGoogleçš„tokenè®¡æ•°APIã€‚
        """
        # æš‚æ—¶ä½¿ç”¨ç²—ç•¥ä¼°è®¡ï¼Œæˆ–è€…å¦‚æœtiktokenæ”¯æŒGeminiæ¨¡å‹ï¼Œåˆ™ä½¿ç”¨tiktoken
        try:
            encoding = tiktoken.encoding_for_model(model)
            return len(encoding.encode(text))
        except KeyError:
            logger.warning(f"æœªæ‰¾åˆ°æ¨¡å‹ {model} çš„ tiktoken ç¼–ç ï¼Œå°†ä½¿ç”¨ cl100k_base ç¼–ç è¿›è¡Œç²—ç•¥ä¼°è®¡ã€‚")
            encoding = tiktoken.get_encoding("cl100k_base")
            return len(encoding.encode(text))
        except Exception as e:
            logger.error(f"è®¡ç®—ä»¤ç‰Œæ•°å¤±è´¥: {e}")
            return int(len(text) / 4) # ç²—ç•¥ä¼°è®¡ï¼Œå¹¶è½¬æ¢ä¸ºæ•´æ•°

    def get_supported_models(self) -> List[str]:
        """
        è·å–æ­¤æä¾›å•†æ”¯æŒçš„ Gemini æ¨¡å‹åˆ—è¡¨ã€‚
        """
        return self._supported_models

    def get_default_model(self) -> str:
        """
        è·å–æ­¤æä¾›å•†çš„é»˜è®¤ Gemini æ¨¡å‹ã€‚
        """
        return self._default_model