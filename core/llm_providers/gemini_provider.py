import os
import logging
import json
from typing import Any, Dict, List, Optional
import httpx
import tiktoken # Geminiä¹Ÿå¯èƒ½éœ€è¦tiktokenæ¥è®¡ç®—token

from .interfaces import ILLMProvider

logger = logging.getLogger(__name__)

class GeminiProvider(ILLMProvider):
    """
    Google Gemini LLM æä¾›å•†çš„å®žçŽ°ã€‚
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.api_key = self.config.get('gemini_api_key') or os.getenv('GEMINI_API_KEY')
        self.api_url = self.config.get('gemini_api_url') or os.getenv('GEMINI_API_URL', 'https://generativelanguage.googleapis.com/v1beta')
        self.client = httpx.AsyncClient(
            timeout=httpx.Timeout(timeout=120.0, connect=30.0),
            limits=httpx.Limits(max_connections=10, max_keepalive_connections=5),
            trust_env=False
        )
        
        # éªŒè¯å¹¶ä½¿ç”¨æœ‰æ•ˆçš„Geminiæ¨¡åž‹åç§°
        # ä¼˜å…ˆä»Žé…ç½®æ–‡ä»¶ä¸­è¯»å–æ¨¡åž‹ï¼Œç„¶åŽæ˜¯gemini_default_modelï¼Œæœ€åŽæ˜¯é»˜è®¤å€¼
        self._default_model = (
            self.config.get('model') or 
            self.config.get('gemini_default_model') or 
            'gemini-2.5-flash-preview-05-20'
        )
        
        # ðŸ”§ åŠ¨æ€è¯»å–æ”¯æŒçš„æ¨¡åž‹åˆ—è¡¨ï¼Œé¿å…ç¡¬ç¼–ç 
        # å¦‚æžœé…ç½®ä¸­æŒ‡å®šäº†supported_modelsï¼Œä½¿ç”¨é…ç½®çš„åˆ—è¡¨
        # å¦åˆ™åˆ›å»ºåŒ…å«å½“å‰é…ç½®æ¨¡åž‹çš„åŸºç¡€åˆ—è¡¨
        config_supported_models = self.config.get('gemini_supported_models')
        if config_supported_models:
            self._supported_models = config_supported_models
        else:
            # åŸºç¡€æ”¯æŒæ¨¡åž‹åˆ—è¡¨ + å½“å‰é…ç½®çš„æ¨¡åž‹ï¼ˆé¿å…è­¦å‘Šï¼‰
            base_models = [
                'gemini-2.5-pro', 'gemini-2.5-flash', 
                'gemini-2.5-flash-lite-preview-06-17',
                'gemini-2.5-flash-preview-05-20',
                'gemini-2.0-flash', 'gemini-2.0-pro', 
                'gemini-1.0-pro', 'gemini-pro', 'gemini-1.5-flash'
            ]
            # ç¡®ä¿å½“å‰é…ç½®çš„æ¨¡åž‹åœ¨æ”¯æŒåˆ—è¡¨ä¸­
            if self._default_model and self._default_model not in base_models:
                base_models.append(self._default_model)
            self._supported_models = base_models
        
        if not self.api_key:
            logger.warning("GEMINI_API_KEY çŽ¯å¢ƒå˜é‡æˆ–é…ç½®ä¸­æœªè®¾ç½®ã€‚GeminiProvider å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚")

    async def generate_response(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        temperature: float = 0.1, # Geminiçš„é»˜è®¤æ¸©åº¦
        max_tokens: int = 32768, # Increased default for complex tasks
        stream: bool = False,
        tools: Optional[List[Dict[str, Any]]] = None, # Geminiçš„å·¥å…·è°ƒç”¨å¯èƒ½ä¸åŒ
        tool_choice: Optional[str] = None,
        timeout: int = 120,
        **kwargs
    ) -> Any:
        """
        æ ¹æ®ç»™å®šçš„æ¶ˆæ¯ç”Ÿæˆ Google Gemini LLM å“åº”ã€‚
        æ³¨æ„ï¼šGemini APIä¸æ”¯æŒstop_sequenceså‚æ•°ï¼Œä¼šè¢«è‡ªåŠ¨å¿½ç•¥ã€‚
        """
        # æå– stop_sequences
        stop_sequences = kwargs.pop('stop_sequences', [])
        if stop_sequences:
            logger.debug(f"ðŸ”§ Applying stop_sequences for Gemini: {stop_sequences}")

        # ðŸ”§ æ™ºèƒ½æ¨¡åž‹éªŒè¯ï¼šä¼˜å…ˆä½¿ç”¨é…ç½®çš„æ¨¡åž‹ï¼Œé¿å…ä¸å¿…è¦çš„è­¦å‘Š
        if model not in self._supported_models:
            # å¦‚æžœè¯·æ±‚çš„æ¨¡åž‹æ˜¯é…ç½®ä¸­æŒ‡å®šçš„æ¨¡åž‹ï¼Œç›´æŽ¥æŽ¥å—
            if model == self._default_model:
                logger.debug(f"ä½¿ç”¨é…ç½®æŒ‡å®šçš„æ¨¡åž‹: {model}")
            else:
                logger.warning(f"æ¨¡åž‹ {model} ä¸å—æ”¯æŒï¼Œä½¿ç”¨é»˜è®¤æ¨¡åž‹ {self._default_model}")
                model = self._default_model
        
        # ðŸ”§ ä¿®å¤æ¶ˆæ¯æ ¼å¼è½¬æ¢ - æ”¯æŒè§’è‰²äº¤æ›¿å’Œå†…å®¹åˆå¹¶
        logger.debug(f"ðŸ” Geminiæ¶ˆæ¯è½¬æ¢å¼€å§‹ - è¾“å…¥æ¶ˆæ¯æ•°: {len(messages)}")
        
        contents = []
        current_role = None
        merged_content = []

        for msg in messages:
            role = msg.get("role")
            content = msg.get("content", "")

            if not role:
                logger.warning(f"æ¶ˆæ¯ç¼ºå°‘ 'role' å­—æ®µ, è·³è¿‡: {msg}")
                continue

            # å°† 'assistant' æ˜ å°„åˆ° 'model'
            if role == "assistant":
                role = "model"
            
            # ç¡®ä¿è§’è‰²æ˜¯ 'user' æˆ– 'model'
            if role not in ["user", "model"]:
                logger.warning(f"æœªçŸ¥çš„æ¶ˆæ¯è§’è‰²: {role}, è·³è¿‡è¯¥æ¶ˆæ¯")
                continue

            if current_role is None:
                current_role = role

            if role == current_role:
                # å¦‚æžœè§’è‰²ç›¸åŒï¼Œåˆå¹¶å†…å®¹
                merged_content.append(str(content))
            else:
                # å¦‚æžœè§’è‰²åˆ‡æ¢ï¼Œæ·»åŠ å‰ä¸€ä¸ªè§’è‰²çš„åˆå¹¶å†…å®¹
                if merged_content:
                    contents.append({
                        "role": current_role,
                        "parts": [{"text": "\n\n".join(merged_content)}]
                    })
                # å¼€å§‹æ–°çš„è§’è‰²å†…å®¹
                current_role = role
                merged_content = [str(content)]

        # æ·»åŠ æœ€åŽä¸€éƒ¨åˆ†åˆå¹¶çš„å†…å®¹
        if merged_content:
            contents.append({
                "role": current_role,
                "parts": [{"text": "\n\n".join(merged_content)}]
            })

        logger.debug(f"ðŸ” Geminiæ¶ˆæ¯è½¬æ¢å®Œæˆ - è½¬æ¢åŽæ¶ˆæ¯æ•°: {len(contents)}")
        
        payload = {
            "contents": contents,
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens,
                "candidateCount": 1,
                "stopSequences": stop_sequences,
                "topP": 0.9,
                "topK": 40
            },
            **kwargs
        }
        
        # Geminiçš„å·¥å…·è°ƒç”¨æ ¼å¼ä¸ŽOpenAIä¸åŒï¼Œè¿™é‡Œæš‚æ—¶ä¸å¤„ç†toolså’Œtool_choice
        # if tools:
        #     payload["tools"] = tools
        # if tool_choice:
        #     payload["tool_choice"] = tool_choice

        logger.debug(f"ðŸ” Gemini APIè°ƒç”¨å¼€å§‹ - æ¨¡åž‹: {model}, payloadå¤§å°: {len(json.dumps(payload))} å­—ç¬¦")
        
        try:
            response = await self.client.post(
                f"{self.api_url}/models/{model}:generateContent?key={self.api_key}",
                json=payload,
                timeout=timeout
            )
            response.raise_for_status()
            
            result = response.json()
            
            # ðŸ”§ å¢žå¼ºå“åº”æ ¼å¼æ£€æŸ¥ - å®‰å…¨å¤„ç†åµŒå¥—æ•°æ®ç»“æž„
            try:
                logger.debug(f"Gemini APIåŽŸå§‹å“åº”ç»“æž„: {json.dumps(result, ensure_ascii=False, indent=2)[:500]}...")
                
                # æ£€æŸ¥å“åº”æ ¼å¼ - å®‰å…¨å­—å…¸è®¿é—®
                if not isinstance(result, dict):
                    raise ValueError(f"å“åº”ä¸æ˜¯å­—å…¸ç±»åž‹: {type(result)}")
                
                if "candidates" not in result:
                    raise ValueError(f"å“åº”ç¼ºå°‘'candidates'å­—æ®µ: {list(result.keys())}")
                
                candidates = result["candidates"]
                if not isinstance(candidates, list) or not candidates:
                    raise ValueError(f"candidateså­—æ®µæ— æ•ˆ: {type(candidates)}, é•¿åº¦: {len(candidates) if isinstance(candidates, list) else 'N/A'}")
                    
                candidate = candidates[0]
                if not isinstance(candidate, dict):
                    raise ValueError(f"å€™é€‰é¡¹ä¸æ˜¯å­—å…¸ç±»åž‹: {type(candidate)}")
                
                if "content" not in candidate:
                    raise ValueError(f"å€™é€‰é¡¹ç¼ºå°‘'content'å­—æ®µ: {list(candidate.keys())}")
                    
                content = candidate["content"]
                if not isinstance(content, dict):
                    raise ValueError(f"contentä¸æ˜¯å­—å…¸ç±»åž‹: {type(content)}")
                
                if "parts" not in content:
                    raise ValueError(f"contentç¼ºå°‘'parts'å­—æ®µ: {list(content.keys())}")
                    
                parts = content["parts"]
                if not isinstance(parts, list) or not parts:
                    raise ValueError(f"partså­—æ®µæ— æ•ˆ: {type(parts)}, é•¿åº¦: {len(parts) if isinstance(parts, list) else 'N/A'}")
                    
                part = parts[0]
                if not isinstance(part, dict):
                    raise ValueError(f"partä¸æ˜¯å­—å…¸ç±»åž‹: {type(part)}")
                
                if "text" not in part:
                    raise ValueError(f"partç¼ºå°‘'text'å­—æ®µ: {list(part.keys())}")
                    
                text_content = part["text"]
                if not isinstance(text_content, str):
                    logger.warning(f"textå­—æ®µä¸æ˜¯å­—ç¬¦ä¸²ç±»åž‹: {type(text_content)}, å°è¯•è½¬æ¢")
                    text_content = str(text_content)
                
                # ðŸ”§ æ–°å¢žï¼šæå–usage metadataï¼ˆtokenä¿¡æ¯ï¼‰
                usage_metadata = result.get('usageMetadata', {})
                usage_info = None
                if usage_metadata:
                    usage_info = {
                        'prompt_tokens': usage_metadata.get('promptTokenCount', 0),
                        'completion_tokens': usage_metadata.get('candidatesTokenCount', 0),
                        'total_tokens': usage_metadata.get('totalTokenCount', 0),
                        'data_source': 'real_api',
                        'model': model
                    }
                    logger.info(f"âœ… èŽ·å–åˆ°çœŸå®žtokenä½¿ç”¨æ•°æ®: prompt={usage_info['prompt_tokens']}, completion={usage_info['completion_tokens']}")
                else:
                    logger.warning("âš ï¸ Gemini APIå“åº”ä¸­æ²¡æœ‰usageMetadataå­—æ®µ")
                
                logger.info(f"âœ… Geminiå“åº”è§£æžæˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(text_content)}")
                
                # è¿”å›žæ–°æ ¼å¼ï¼šåŒ…å«contentå’Œusageä¿¡æ¯
                return {
                    'content': text_content,
                    'usage': usage_info,
                    'metadata': {
                        'model': model,
                        'provider': 'gemini',
                        'api_response': result  # ä¿ç•™åŽŸå§‹å“åº”ç”¨äºŽè°ƒè¯•
                    }
                }
                
            except Exception as parse_error:
                logger.error(f"Geminiå“åº”è§£æžå¤±è´¥: {parse_error}")
                logger.error(f"åŽŸå§‹å“åº”å†…å®¹: {json.dumps(result, ensure_ascii=False, indent=2)}")
                raise ValueError(f"Geminiå“åº”æ ¼å¼è§£æžé”™è¯¯: {parse_error}")
            
        except httpx.HTTPStatusError as e:
            logger.error(f"Gemini API HTTP é”™è¯¯: {e.response.status_code} - {e.response.text}")
            # ðŸ”§ å¼ºåŒ–çš„é‡è¯•æœºåˆ¶ï¼šä½¿ç”¨ç¨³å®šæ¨¡åž‹+ç›´æŽ¥IP (ä»Žllm_client.pyä¸­è¿ç§»è¿‡æ¥)
            if "name resolution" in str(e).lower() or "connection" in str(e).lower():
                logger.info("å°è¯•ä½¿ç”¨å¤‡ç”¨ç½‘ç»œé…ç½®é‡è¯•...")
                try:
                    # ä½¿ç”¨å¤‡ç”¨DNSé…ç½®
                    backup_client = httpx.AsyncClient(
                        timeout=httpx.Timeout(timeout=180.0, connect=60.0),
                        limits=httpx.Limits(max_connections=5, max_keepalive_connections=2),
                        trust_env=False
                    )
                    
                    # ä½¿ç”¨ç¨³å®šæ¨¡åž‹é‡è¯•
                    stable_model = 'gemini-1.5-flash'
                    response = await backup_client.post(
                        f"{self.api_url}/models/{stable_model}:generateContent?key={self.api_key}",
                        json=payload
                    )
                    response.raise_for_status()
                    result = response.json()
                    
                    # ðŸ”§ å¤‡ç”¨å“åº”çš„å®‰å…¨æ ¼å¼æ£€æŸ¥
                    try:
                        if (isinstance(result, dict) and
                            "candidates" in result and isinstance(result["candidates"], list) and
                            len(result["candidates"]) > 0 and isinstance(result["candidates"][0], dict) and
                            "content" in result["candidates"][0] and isinstance(result["candidates"][0]["content"], dict) and
                            "parts" in result["candidates"][0]["content"] and isinstance(result["candidates"][0]["content"]["parts"], list) and
                            len(result["candidates"][0]["content"]["parts"]) > 0 and isinstance(result["candidates"][0]["content"]["parts"][0], dict) and
                            "text" in result["candidates"][0]["content"]["parts"][0]):
                            
                            backup_text = result["candidates"][0]["content"]["parts"][0]["text"]
                            if not isinstance(backup_text, str):
                                backup_text = str(backup_text)
                            
                            # å¤‡ç”¨å“åº”ä¹Ÿæå–tokenä¿¡æ¯
                            backup_usage_metadata = result.get('usageMetadata', {})
                            backup_usage_info = None
                            if backup_usage_metadata:
                                backup_usage_info = {
                                    'prompt_tokens': backup_usage_metadata.get('promptTokenCount', 0),
                                    'completion_tokens': backup_usage_metadata.get('candidatesTokenCount', 0),
                                    'total_tokens': backup_usage_metadata.get('totalTokenCount', 0),
                                    'data_source': 'real_api_backup',
                                    'model': stable_model
                                }
                            
                            await backup_client.aclose()
                            logger.info("âœ… ä½¿ç”¨å¤‡ç”¨ç½‘ç»œé…ç½®æˆåŠŸæ¢å¤")
                            return {
                                'content': backup_text,
                                'usage': backup_usage_info,
                                'metadata': {
                                    'model': stable_model,
                                    'provider': 'gemini',
                                    'backup_recovery': True
                                }
                            }
                        else:
                            raise ValueError(f"å¤‡ç”¨å“åº”æ ¼å¼æ— æ•ˆ: {list(result.keys()) if isinstance(result, dict) else type(result)}")
                    except Exception as backup_parse_error:
                        logger.error(f"å¤‡ç”¨å“åº”è§£æžå¤±è´¥: {backup_parse_error}")
                        raise ValueError(f"å¤‡ç”¨å“åº”æ ¼å¼è§£æžé”™è¯¯: {backup_parse_error}")
                        
                except Exception as backup_e:
                    logger.error(f"å¤‡ç”¨ç½‘ç»œé…ç½®ä¹Ÿå¤±è´¥: {backup_e}")
                    raise e
            else:
                raise e
        except httpx.RemoteProtocolError as e:
            logger.error(f"Gemini API è¿žæŽ¥åè®®é”™è¯¯: {e}")
            # ðŸ”§ ä¸“é—¨å¤„ç† RemoteProtocolError - æœåŠ¡å™¨è¿žæŽ¥ä¸­æ–­
            return await self._handle_remote_protocol_error(e, payload, model, timeout)
        except httpx.RequestError as e:
            logger.error(f"Gemini API è¯·æ±‚é”™è¯¯: {e}")
            # ðŸ”§ å¯¹å…¶ä»–è¯·æ±‚é”™è¯¯ä¹Ÿå°è¯•é‡è¯•æœºåˆ¶
            if any(keyword in str(e).lower() for keyword in ['timeout', 'connection', 'network', 'disconnect']):
                logger.info("æ£€æµ‹åˆ°ç½‘ç»œç›¸å…³é”™è¯¯ï¼Œå°è¯•é‡è¯•...")
                return await self._handle_network_error_retry(e, payload, model, timeout)
            raise
        except Exception as e:
            logger.error(f"è°ƒç”¨ Gemini API å¤±è´¥: {e}")
            raise

    async def count_tokens(self, text: str, model: str) -> int:
        """
        ä½¿ç”¨Gemini SDKçš„çœŸå®žAPIè®¡ç®—ç»™å®šæ–‡æœ¬åœ¨ç‰¹å®šæ¨¡åž‹ä¸­çš„å‡†ç¡®tokenæ•°é‡
        """
        try:
            # ðŸ”§ æ™ºèƒ½æ¨¡åž‹éªŒè¯ï¼šä¼˜å…ˆä½¿ç”¨é…ç½®çš„æ¨¡åž‹
            if model not in self._supported_models:
                # å¦‚æžœè¯·æ±‚çš„æ¨¡åž‹æ˜¯é…ç½®ä¸­æŒ‡å®šçš„æ¨¡åž‹ï¼Œç›´æŽ¥æŽ¥å—
                if model == self._default_model:
                    logger.debug(f"Tokenè®¡æ•°ä½¿ç”¨é…ç½®æŒ‡å®šçš„æ¨¡åž‹: {model}")
                else:
                    logger.warning(f"æ¨¡åž‹ {model} ä¸å—æ”¯æŒï¼Œä½¿ç”¨é»˜è®¤æ¨¡åž‹ {self._default_model}")
                    model = self._default_model
            
            # æž„é€ è¯·æ±‚payload
            payload = {
                "contents": [{"parts": [{"text": text}]}]
            }
            
            # è°ƒç”¨Gemini count_tokens API
            response = await self.client.post(
                f"{self.api_url}/models/{model}:countTokens?key={self.api_key}",
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            
            # è§£æžå“åº”
            if isinstance(result, dict) and "totalTokens" in result:
                token_count = result["totalTokens"]
                logger.debug(f"âœ… Gemini API tokenè®¡æ•°: {token_count} tokens for {len(text)} characters")
                return int(token_count)
            else:
                logger.warning(f"Gemini tokenè®¡æ•°APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                # å›žé€€åˆ°æ”¹è¿›çš„ä¼°ç®—æ–¹æ³•
                return self._accurate_token_estimation_fallback(text)
                
        except httpx.RemoteProtocolError as e:
            logger.warning(f"Gemini tokenè®¡æ•°API è¿žæŽ¥åè®®é”™è¯¯: {e}, ä½¿ç”¨ä¼°ç®—æ–¹æ³•")
            return self._accurate_token_estimation_fallback(text)
        except httpx.HTTPStatusError as e:
            logger.warning(f"Gemini tokenè®¡æ•°API HTTPé”™è¯¯: {e.response.status_code} - {e.response.text}")
            return self._accurate_token_estimation_fallback(text)
        except httpx.RequestError as e:
            logger.warning(f"Gemini tokenè®¡æ•°API è¯·æ±‚é”™è¯¯: {e}, ä½¿ç”¨ä¼°ç®—æ–¹æ³•")
            return self._accurate_token_estimation_fallback(text)
        except Exception as e:
            logger.warning(f"Gemini tokenè®¡æ•°APIè°ƒç”¨å¤±è´¥: {e}, ä½¿ç”¨ä¼°ç®—æ–¹æ³•")
            return self._accurate_token_estimation_fallback(text)
    
    def _accurate_token_estimation_fallback(self, text: str) -> int:
        """
        å½“APIè°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨çš„é«˜ç²¾åº¦ä¼°ç®—æ–¹æ³•
        åŸºäºŽGemini tokenizerç‰¹æ€§çš„æ”¹è¿›ä¼°ç®—
        """
        if not text:
            return 0
        
        # ä¸­æ–‡å­—ç¬¦ç»Ÿè®¡
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        japanese_chars = sum(1 for c in text if '\u3040' <= c <= '\u309f' or '\u30a0' <= c <= '\u30ff')
        korean_chars = sum(1 for c in text if '\uac00' <= c <= '\ud7af')
        
        # å…¶ä»–å­—ç¬¦
        other_chars = len(text) - chinese_chars - japanese_chars - korean_chars
        
        # åŸºäºŽGemini tokenizerçš„æ”¹è¿›ä¼°ç®—
        # ä¸­æ–‡: ~1.5 chars/token, æ—¥æ–‡: ~2 chars/token, éŸ©æ–‡: ~2 chars/token, è‹±æ–‡: ~4 chars/token
        estimated_tokens = int(
            chinese_chars / 1.5 + 
            japanese_chars / 2.0 + 
            korean_chars / 2.0 + 
            other_chars / 4.0
        )
        
        # è€ƒè™‘ç‰¹æ®Šæ ‡è®°å’Œæ ¼å¼
        special_tokens = text.count('<') + text.count('>') + text.count('{') + text.count('}')
        estimated_tokens += int(special_tokens * 0.5)  # ç‰¹æ®Šæ ‡è®°é€šå¸¸å ç”¨é¢å¤–token
        
        return max(estimated_tokens, 1)

    def get_supported_models(self) -> List[str]:
        """
        èŽ·å–æ­¤æä¾›å•†æ”¯æŒçš„ Gemini æ¨¡åž‹åˆ—è¡¨ã€‚
        """
        return self._supported_models

    def get_default_model(self) -> str:
        """
        èŽ·å–æ­¤æä¾›å•†çš„é»˜è®¤ Gemini æ¨¡åž‹ã€‚
        """
        return self._default_model
    
    async def _handle_remote_protocol_error(self, error: Exception, payload: dict, model: str, timeout: int) -> str:
        """
        ðŸ”§ ä¸“é—¨å¤„ç† RemoteProtocolError - æœåŠ¡å™¨è¿žæŽ¥ä¸­æ–­
        
        è¿™ç§é”™è¯¯é€šå¸¸ç”±ä»¥ä¸‹åŽŸå› å¼•èµ·ï¼š
        1. ç½‘ç»œè¿žæŽ¥ä¸ç¨³å®š
        2. æœåŠ¡å™¨è´Ÿè½½è¿‡é«˜ä¸»åŠ¨æ–­å¼€è¿žæŽ¥
        3. è¯·æ±‚è¿‡å¤§å¯¼è‡´æœåŠ¡å™¨æå‰å…³é—­
        4. APIé€ŸçŽ‡é™åˆ¶è§¦å‘
        """
        import asyncio
        
        logger.warning(f"ðŸš¨ RemoteProtocolError detected: {error}")
        logger.info("ðŸ”„ å¯åŠ¨å¤šå±‚é‡è¯•ç­–ç•¥...")
        
        # ç­–ç•¥1: ç«‹å³é‡è¯•ä¸€æ¬¡ï¼ˆå¯èƒ½æ˜¯ä¸´æ—¶ç½‘ç»œæŠ–åŠ¨ï¼‰
        try:
            logger.info("ðŸ“¡ ç­–ç•¥1: ç«‹å³é‡è¯•ï¼ˆç½‘ç»œæŠ–åŠ¨æ¢å¤ï¼‰")
            await asyncio.sleep(1)  # çŸ­æš‚å»¶è¿Ÿ
            
            response = await self.client.post(
                f"{self.api_url}/models/{model}:generateContent?key={self.api_key}",
                json=payload,
                timeout=timeout
            )
            response.raise_for_status()
            result = response.json()
            
            return self._parse_gemini_response(result)
            
        except Exception as retry1_error:
            logger.warning(f"ç­–ç•¥1å¤±è´¥: {retry1_error}")
        
        # ç­–ç•¥2: ä½¿ç”¨æ–°çš„å®¢æˆ·ç«¯è¿žæŽ¥å’Œæ›´é•¿è¶…æ—¶
        try:
            logger.info("ðŸ”„ ç­–ç•¥2: æ–°è¿žæŽ¥+å»¶é•¿è¶…æ—¶ï¼ˆæœåŠ¡å™¨è´Ÿè½½é—®é¢˜ï¼‰")
            await asyncio.sleep(3)  # ç­‰å¾…æœåŠ¡å™¨è´Ÿè½½é™ä½Ž
            
            # åˆ›å»ºæ–°çš„å®¢æˆ·ç«¯ï¼Œé¿å…è¿žæŽ¥å¤ç”¨é—®é¢˜
            fresh_client = httpx.AsyncClient(
                timeout=httpx.Timeout(timeout=timeout*2, connect=60.0),  # å»¶é•¿è¶…æ—¶
                limits=httpx.Limits(max_connections=3, max_keepalive_connections=1),  # å‡å°‘å¹¶å‘
                trust_env=False,
                http2=False  # ç¦ç”¨HTTP/2ï¼Œä½¿ç”¨HTTP/1.1é¿å…åè®®é—®é¢˜
            )
            
            try:
                response = await fresh_client.post(
                    f"{self.api_url}/models/{model}:generateContent?key={self.api_key}",
                    json=payload,
                    timeout=timeout*2
                )
                response.raise_for_status()
                result = response.json()
                
                return self._parse_gemini_response(result)
                
            finally:
                await fresh_client.aclose()
                
        except Exception as retry2_error:
            logger.warning(f"ç­–ç•¥2å¤±è´¥: {retry2_error}")
        
        # ç­–ç•¥3: ä½¿ç”¨ç¨³å®šæ¨¡åž‹å’Œç®€åŒ–è¯·æ±‚
        try:
            logger.info("âš¡ ç­–ç•¥3: ç¨³å®šæ¨¡åž‹+ç®€åŒ–è¯·æ±‚ï¼ˆå…¼å®¹æ€§é—®é¢˜ï¼‰")
            await asyncio.sleep(5)  # æ›´é•¿ç­‰å¾…
            
            # ä½¿ç”¨æœ€ç¨³å®šçš„æ¨¡åž‹
            fallback_model = 'gemini-1.5-flash'
            simplified_payload = {
                "contents": payload.get("contents", []),
                "generationConfig": {
                    "temperature": 0.1,
                    "maxOutputTokens": min(payload.get("generationConfig", {}).get("maxOutputTokens", 8192), 8192)
                }
            }
            
            fallback_client = httpx.AsyncClient(
                timeout=httpx.Timeout(timeout=180.0, connect=90.0),
                limits=httpx.Limits(max_connections=1, max_keepalive_connections=0),  # å•è¿žæŽ¥æ— å¤ç”¨
                trust_env=False,
                http2=False
            )
            
            try:
                response = await fallback_client.post(
                    f"{self.api_url}/models/{fallback_model}:generateContent?key={self.api_key}",
                    json=simplified_payload,
                    timeout=180
                )
                response.raise_for_status()
                result = response.json()
                
                logger.info("âœ… ç­–ç•¥3æˆåŠŸï¼šä½¿ç”¨ç¨³å®šæ¨¡åž‹æ¢å¤")
                return self._parse_gemini_response(result)
                
            finally:
                await fallback_client.aclose()
                
        except Exception as retry3_error:
            logger.error(f"ç­–ç•¥3ä¹Ÿå¤±è´¥: {retry3_error}")
        
        # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºè¯¦ç»†é”™è¯¯
        error_msg = f"RemoteProtocolError å¤šé‡é‡è¯•å¤±è´¥ã€‚åŽŸå§‹é”™è¯¯: {error}"
        logger.error(f"âŒ {error_msg}")
        raise ConnectionError(error_msg)
    
    async def _handle_network_error_retry(self, error: Exception, payload: dict, model: str, timeout: int) -> str:
        """
        ðŸ”§ å¤„ç†ä¸€èˆ¬ç½‘ç»œé”™è¯¯çš„é‡è¯•æœºåˆ¶
        """
        import asyncio
        
        logger.warning(f"ðŸŒ ç½‘ç»œé”™è¯¯æ£€æµ‹: {error}")
        
        # æŒ‡æ•°é€€é¿é‡è¯•
        for attempt in range(3):
            wait_time = 2 ** attempt  # 2, 4, 8 ç§’
            logger.info(f"ðŸ”„ ç½‘ç»œé‡è¯• {attempt + 1}/3ï¼Œç­‰å¾… {wait_time} ç§’...")
            await asyncio.sleep(wait_time)
            
            try:
                response = await self.client.post(
                    f"{self.api_url}/models/{model}:generateContent?key={self.api_key}",
                    json=payload,
                    timeout=timeout + attempt * 30  # é€’å¢žè¶…æ—¶
                )
                response.raise_for_status()
                result = response.json()
                
                logger.info(f"âœ… ç½‘ç»œé‡è¯•ç¬¬ {attempt + 1} æ¬¡æˆåŠŸ")
                return self._parse_gemini_response(result)
                
            except Exception as retry_error:
                logger.warning(f"é‡è¯• {attempt + 1} å¤±è´¥: {retry_error}")
                if attempt == 2:  # æœ€åŽä¸€æ¬¡é‡è¯•
                    raise error  # æŠ›å‡ºåŽŸå§‹é”™è¯¯
        
        raise error
    
    def _parse_gemini_response(self, result: dict) -> str:
        """
        ðŸ”§ è§£æžGeminiå“åº”çš„é€šç”¨æ–¹æ³•
        """
        try:
            if (isinstance(result, dict) and
                "candidates" in result and isinstance(result["candidates"], list) and
                len(result["candidates"]) > 0 and isinstance(result["candidates"][0], dict) and
                "content" in result["candidates"][0] and isinstance(result["candidates"][0]["content"], dict) and
                "parts" in result["candidates"][0]["content"] and isinstance(result["candidates"][0]["content"]["parts"], list) and
                len(result["candidates"][0]["content"]["parts"]) > 0 and isinstance(result["candidates"][0]["content"]["parts"][0], dict) and
                "text" in result["candidates"][0]["content"]["parts"][0]):
                
                text_content = result["candidates"][0]["content"]["parts"][0]["text"]
                if not isinstance(text_content, str):
                    text_content = str(text_content)
                
                return text_content
            else:
                raise ValueError(f"Invalid response structure: {list(result.keys()) if isinstance(result, dict) else type(result)}")
                
        except Exception as parse_error:
            logger.error(f"Response parsing failed: {parse_error}")
            logger.error(f"Raw response: {json.dumps(result, ensure_ascii=False, indent=2)}")
            raise ValueError(f"Gemini response parsing error: {parse_error}")