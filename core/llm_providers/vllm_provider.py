import os
import logging
from typing import Any, Dict, List, Optional
import httpx
import tiktoken # vLLMä¹Ÿå¯èƒ½éœ€è¦tiktokenæ¥è®¡ç®—token

from .interfaces import ILLMProvider

logger = logging.getLogger(__name__)

class VLLMProvider(ILLMProvider):
    """
    vLLM æœ¬åœ°æœåŠ¡æä¾›å•†çš„å®ç°ã€‚
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.vllm_url = self.config.get("vllm_url", "http://localhost:8001") # æ›´æ”¹vLLMé»˜è®¤ç«¯å£ä»¥é¿å…å†²çª
        self.client = httpx.AsyncClient(timeout=60.0)
        self._supported_models = self.config.get('vllm_supported_models', ["default"]) # vLLMé€šå¸¸åªæœ‰ä¸€ä¸ªé»˜è®¤æ¨¡å‹
        self._default_model = self.config.get('vllm_default_model', "default")
        
        logger.info(f"Initialized VLLMProvider with URL: {self.vllm_url}")

    async def generate_response(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        temperature: float = 0.1, # vLLMçš„é»˜è®¤æ¸©åº¦
        max_tokens: int = 1024, # vLLMçš„é»˜è®¤max_tokens
        stream: bool = False,
        tools: Optional[List[Dict[str, Any]]] = None, # vLLMçš„å·¥å…·è°ƒç”¨å¯èƒ½ä¸åŒ
        tool_choice: Optional[str] = None,
        **kwargs
    ) -> Any:
        """
        æ ¹æ®ç»™å®šçš„æ¶ˆæ¯ç”Ÿæˆ vLLM å“åº”ã€‚
        """
        # å¤„ç†stop_sequenceså‚æ•° - vLLMå…¼å®¹OpenAI APIï¼Œä½¿ç”¨'stop'å‚æ•°
        if 'stop_sequences' in kwargs:
            stop_sequences = kwargs.pop('stop_sequences')
            kwargs['stop'] = stop_sequences
            logger.debug(f"ğŸ”§ è½¬æ¢stop_sequencesä¸ºvLLMçš„stopå‚æ•°: {stop_sequences}")
        if model not in self._supported_models:
            logger.warning(f"æ¨¡å‹ {model} ä¸å— VLLMProvider æ”¯æŒï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡å‹ {self._default_model}ã€‚")
            model = self._default_model

        payload = {
            "model": model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": stream,
            **kwargs
        }
        
        # vLLMçš„å·¥å…·è°ƒç”¨æ ¼å¼å¯èƒ½ä¸åŒï¼Œè¿™é‡Œæš‚æ—¶ä¸å¤„ç†toolså’Œtool_choice
        # if tools:
        #     payload["tools"] = tools
        # if tool_choice:
        #     payload["tool_choice"] = tool_choice

        try:
            response = await self.client.post(
                f"{self.vllm_url}/v1/chat/completions",
                json=payload
            )
            response.raise_for_status()
            
            if stream:
                return response
            else:
                result = response.json()
                # vLLMé€šå¸¸åªè¿”å›content
                if result["choices"] and result["choices"][0]["message"].get("content"):
                    return result["choices"][0]["message"]["content"]
                else:
                    return ""
        except httpx.HTTPStatusError as e:
            logger.error(f"vLLM API HTTP é”™è¯¯: {e.response.status_code} - {e.response.text}")
            raise
        except httpx.RequestError as e:
            logger.error(f"vLLM API è¯·æ±‚é”™è¯¯: {e}")
            raise
        except Exception as e:
            logger.error(f"è°ƒç”¨ vLLM API å¤±è´¥: {e}")
            raise

    async def count_tokens(self, text: str, model: str) -> int:
        """
        è®¡ç®—ç»™å®šæ–‡æœ¬åœ¨ç‰¹å®š vLLM æ¨¡å‹ä¸­çš„ä»¤ç‰Œæ•°ã€‚
        """
        try:
            encoding = tiktoken.encoding_for_model(model)
            return len(encoding.encode(text))
        except KeyError:
            logger.warning(f"æœªæ‰¾åˆ°æ¨¡å‹ {model} çš„ tiktoken ç¼–ç ï¼Œå°†ä½¿ç”¨ cl100k_base ç¼–ç è¿›è¡Œç²—ç•¥ä¼°è®¡ã€‚")
            encoding = tiktoken.get_encoding("cl100k_base")
            return len(encoding.encode(text))
        except Exception as e:
            logger.error(f"è®¡ç®—ä»¤ç‰Œæ•°å¤±è´¥: {e}")
            return int(len(text) / 4) # ç²—ç•¥ä¼°è®¡ï¼Œå¹¶è½¬æ¢ä¸ºæ•´æ•°

    def get_supported_models(self) -> List[str]:
        """
        è·å–æ­¤æä¾›å•†æ”¯æŒçš„ vLLM æ¨¡å‹åˆ—è¡¨ã€‚
        """
        return self._supported_models

    def get_default_model(self) -> str:
        """
        è·å–æ­¤æä¾›å•†çš„é»˜è®¤ vLLM æ¨¡å‹ã€‚
        """
        return self._default_model