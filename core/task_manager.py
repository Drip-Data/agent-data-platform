import asyncio
import json
import logging
import time
from typing import Dict, List, Optional
import redis.asyncio as redis
from .interfaces import TaskSpec, TrajectoryResult
from .metrics import EnhancedMetrics
from .utils.path_utils import get_trajectories_dir

logger = logging.getLogger(__name__)

class TaskManager:
    """ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨"""
    
    def __init__(self, redis_url: str = None, redis_manager=None):
        self.redis_manager = redis_manager
        
        if redis_manager and not redis_manager.is_fallback_mode():
            # ä½¿ç”¨çœŸå®Redis
            import redis.asyncio as redis
            self.redis = redis.from_url(redis_manager.get_redis_url())
            self.fallback_mode = False
        else:
            # ä½¿ç”¨å†…å­˜å­˜å‚¨
            self.redis = None
            self.fallback_mode = True
            logger.warning("TaskManager è¿è¡Œåœ¨å†…å­˜æ¨¡å¼")
            
        self.metrics = EnhancedMetrics()
        self._active_tasks: Dict[str, Dict] = {}

    async def submit_task(self, task: TaskSpec) -> str:
        """æäº¤ä»»åŠ¡"""
        task_data = {
            "task": task.to_dict(),
            "status": "submitted",
            "submitted_at": time.time(),
            "updated_at": time.time()
        }
        
        if not self.fallback_mode and self.redis:
            # å­˜å‚¨ä»»åŠ¡çŠ¶æ€åˆ°Redis
            await self.redis.hset(
                f"task:{task.task_id}",
                mapping={k: json.dumps(v) if isinstance(v, (dict, list)) else str(v) 
                        for k, v in task_data.items()}
            )
            
            # è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆ24å°æ—¶ï¼‰
            await self.redis.expire(f"task:{task.task_id}", 86400)
        else:
            # å†…å­˜æ¨¡å¼ï¼šä½¿ç”¨redis_managerçš„å†…å­˜å­˜å‚¨
            if self.redis_manager:
                await self.redis_manager.memory_set(
                    f"task:{task.task_id}", 
                    json.dumps(task_data)
                )
        
        self._active_tasks[task.task_id] = task_data
        
        logger.info(f"Task {task.task_id} submitted")
        return task.task_id
    
    async def update_task_status(self, task_id: str, status: str, metadata: Dict = None):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        update_data = {
            "status": status,
            "updated_at": time.time()
        }
        
        if metadata:
            update_data["metadata"] = json.dumps(metadata)
        
        await self.redis.hset(
            f"task:{task_id}",
            mapping={k: str(v) for k, v in update_data.items()}
        )
        
        if task_id in self._active_tasks:
            self._active_tasks[task_id].update(update_data)
        
        logger.info(f"Task {task_id} status updated to {status}")
    
    async def complete_task(self, task_id: str, result: TrajectoryResult):
        """å®Œæˆä»»åŠ¡"""
        # å­˜å‚¨ç»“æœ
        await self.redis.hset(
            f"task:{task_id}",
            mapping={
                "status": "completed",
                "completed_at": time.time(),
                "result": result.json(),
                "success": str(result.success)
            }
        )
        
        # ä¿å­˜è½¨è¿¹åˆ°æ–‡ä»¶
        await self._save_trajectory(result)
        
        # æ¸…ç†å†…å­˜ä¸­çš„ä»»åŠ¡
        if task_id in self._active_tasks:
            del self._active_tasks[task_id]
        
        logger.info(f"Task {task_id} completed with success={result.success}")
    
    async def get_task_status(self, task_id: str) -> Optional[Dict]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        task_data = await self.redis.hgetall(f"task:{task_id}")
        
        if not task_data:
            return None
        
        # è§£æJSONå­—æ®µ
        result = {}
        for key, value in task_data.items():
            key = key.decode() if isinstance(key, bytes) else key
            value = value.decode() if isinstance(value, bytes) else value
            
            if key in ['task', 'metadata', 'result']:
                try:
                    result[key] = json.loads(value)
                except json.JSONDecodeError:
                    result[key] = value
            else:
                result[key] = value
        
        return result
    
    async def list_active_tasks(self) -> List[Dict]:
        """åˆ—å‡ºæ´»è·ƒä»»åŠ¡"""
        return list(self._active_tasks.values())
    async def _save_trajectory(self, result: TrajectoryResult):
        """ä¿å­˜è½¨è¿¹åˆ°æ–‡ä»¶"""
        try:
            trajectories_dir = get_trajectories_dir()
            
            filename = f"{trajectories_dir}/{result.task_id}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(result.json())
            
            logger.info(f"Trajectory saved to {filename}")
            
        except Exception as e:
            logger.error(f"Error saving trajectory for {result.task_id}: {e}")
    
    async def cleanup_expired_tasks(self):
        """æ¸…ç†è¿‡æœŸä»»åŠ¡"""
        current_time = time.time()
        expired_tasks = []
        
        for task_id, task_data in self._active_tasks.items():
            # å¦‚æœä»»åŠ¡è¶…è¿‡1å°æ—¶æœªæ›´æ–°ï¼Œæ ‡è®°ä¸ºè¿‡æœŸ
            if current_time - task_data.get("updated_at", 0) > 3600:
                expired_tasks.append(task_id)
        
        for task_id in expired_tasks:
            await self.update_task_status(task_id, "expired")
            del self._active_tasks[task_id]
            logger.warning(f"Task {task_id} marked as expired")
        
        if expired_tasks:
            logger.info(f"Cleaned up {len(expired_tasks)} expired tasks")
    
def get_runtime(task_type: str):
    """è·å–æŒ‡å®šç±»å‹çš„è¿è¡Œæ—¶å®ä¾‹
    
    æ³¨æ„ï¼šå†å²è¿è¡Œæ—¶(sandbox, web_navigator)å·²è¢«ç§»é™¤
    æ‰€æœ‰åŠŸèƒ½å·²è¿ç§»è‡³MCPæœåŠ¡å™¨ï¼Œé€šè¿‡enhanced-reasoning-runtime + toolscoreè°ƒç”¨
    è¿ç§»æ—¥æœŸ: 2025-06-14
    """
    if task_type in ['reasoning', 'code', 'web']:
        # æ‰€æœ‰ä»»åŠ¡ç±»å‹ç°åœ¨éƒ½ä½¿ç”¨enhanced-reasoning-runtime
        # codeå’ŒwebåŠŸèƒ½é€šè¿‡toolscoreè°ƒç”¨ç›¸åº”çš„MCPæœåŠ¡å™¨å®ç°
        try:
            from runtimes.reasoning.enhanced_runtime import EnhancedReasoningRuntime
            return EnhancedReasoningRuntime()
        except ImportError:
            raise ImportError("EnhancedReasoningRuntime not found")
    else:
        raise ValueError(f"Unsupported task type: {task_type}. Supported types: reasoning, code, web")

async def _update_task_api_status(redis_client, task_id: str, status: str, result: TrajectoryResult = None):
    """æ›´æ–°Task APIä½¿ç”¨çš„ä»»åŠ¡çŠ¶æ€"""
    try:
        from datetime import datetime
        
        # æ›´æ–°Task APIä½¿ç”¨çš„çŠ¶æ€é”®
        status_data = {
            "task_id": task_id,
            "status": status,
            "timestamp": datetime.now().isoformat()
        }
        
        if result:
            status_data["message"] = f"Task {status} with success={result.success}"
        else:
            status_data["message"] = f"Task {status}"
        
        # è®¾ç½®çŠ¶æ€ï¼Œä¿å­˜1å°æ—¶
        await redis_client.setex(
            f"task_status:{task_id}", 
            3600, 
            json.dumps(status_data)
        )
        
        # å¦‚æœæœ‰ç»“æœï¼Œä¹Ÿä¿å­˜ç»“æœæ•°æ®
        if result and status in ["completed", "failed"]:
            result_data = {
                "success": result.success,
                "final_result": result.final_result,
                "error_message": result.error_message,
                "total_duration": result.total_duration,
                "steps_count": len(result.steps)
            }
            await redis_client.setex(
                f"task_result:{task_id}",
                3600,
                json.dumps(result_data)
            )
        
        logger.info(f"Task API status updated: {task_id} -> {status}")
        
    except Exception as e:
        logger.error(f"Failed to update Task API status for {task_id}: {e}")

async def start_runtime_service(runtime, redis_manager=None):
    """å¯åŠ¨ç»™å®šè¿è¡Œæ—¶çš„ä»»åŠ¡æ¶ˆè´¹æœåŠ¡"""
    import os
    import asyncio
    import json
    from .interfaces import TaskSpec, ErrorType, TrajectoryResult

    async def _run_service():
        # ä½¿ç”¨ä¼ å…¥çš„redis_managerï¼Œå¦åˆ™fallbackåˆ°å†…å­˜æ¨¡å¼
        if redis_manager and not redis_manager.is_fallback_mode():
            import redis.asyncio as redis_client
            redis_url = redis_manager.get_redis_url()
            r = redis_client.from_url(redis_url)
        else:
            # å†…å­˜æ¨¡å¼ - åˆ›å»ºè™šæ‹Ÿé˜Ÿåˆ—æœåŠ¡
            logger.warning(f"Runtime {runtime.runtime_id} è¿è¡Œåœ¨å†…å­˜æ¨¡å¼ï¼Œä»»åŠ¡é˜Ÿåˆ—åŠŸèƒ½å—é™")
            
            # åœ¨å†…å­˜æ¨¡å¼ä¸‹ï¼Œæ¨¡æ‹Ÿä»»åŠ¡å¤„ç†
            while True:
                try:
                    # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦æœ‰æ¨¡æ‹Ÿä»»åŠ¡
                    await asyncio.sleep(30)
                    logger.debug(f"Runtime {runtime.runtime_id} åœ¨å†…å­˜æ¨¡å¼ä¸‹ç­‰å¾…ä»»åŠ¡...")
                except KeyboardInterrupt:
                    break
                except Exception as e:
                    logger.error(f"Runtime {runtime.runtime_id} å†…å­˜æ¨¡å¼å‡ºé”™: {e}")
                    await asyncio.sleep(5)
            return
        
        # æ ¹æ®runtimeçš„èƒ½åŠ›ç¡®å®šé˜Ÿåˆ—å
        queue_name = None


        
        # if hasattr(runtime, 'capabilities'):
        #     try:
        #         # å¦‚æœcapabilitiesæ˜¯åç¨‹ï¼Œéœ€è¦await
        #         if asyncio.iscoroutine(runtime.capabilities):
        #             capabilities = await runtime.capabilities
        #         elif callable(runtime.capabilities):
        #             capabilities = await runtime.capabilities()
        #         else:
        #             capabilities = runtime.capabilities
                
        #         if capabilities:
        #             # æ¨ç†runtimeå¤„ç†reasoningé˜Ÿåˆ—
        #             if 'browser' in capabilities and 'python_executor' in capabilities:
        #                 queue_name = "tasks:reasoning"
        #             # ä»£ç æ‰§è¡Œruntimeå¤„ç†codeé˜Ÿåˆ—
        #             elif 'python_code_execution' in capabilities:
        #                 queue_name = "tasks:code"
        #             # Webå¯¼èˆªruntimeå¤„ç†webé˜Ÿåˆ—
        #             elif 'browser_navigation' in capabilities:
        #                 queue_name = "tasks:web"
        #     except Exception as e:
        #         logger.warning(f"Error getting runtime capabilities: {e}")
        #         capabilities = None
        
        # å¦‚æœæ— æ³•ä»capabilitiesç¡®å®šï¼Œå°è¯•ä»runtimeç±»åæ¨æ–­
        if not queue_name:
            runtime_class = runtime.__class__.__name__.lower()
            if 'reasoning' in runtime_class:
                queue_name = "tasks:reasoning"
            elif 'sandbox' in runtime_class or 'code' in runtime_class:
                queue_name = "tasks:code"
            elif 'web' in runtime_class or 'navigator' in runtime_class:
                queue_name = "tasks:web"
            else:
                # é»˜è®¤ä¸ºreasoningé˜Ÿåˆ—
                queue_name = "tasks:reasoning"
        
        logger.info(f"Runtime {runtime.runtime_id} starting to consume from queue: {queue_name}")
        
        group = "workers"
        consumer_id = runtime.runtime_id
        # åˆ›å»ºæ¶ˆè´¹è€…ç»„
        try:
            await r.xgroup_create(queue_name, group, id="0", mkstream=True)
        except Exception:
            pass
        # æ¶ˆè´¹å¾ªç¯
        logger.info(f"Runtime {runtime.runtime_id} starting consumer loop for queue {queue_name}")
        while True:
            try:
                logger.debug(f"Runtime {runtime.runtime_id} reading from queue {queue_name}...")
                msgs = await r.xreadgroup(group, consumer_id, {queue_name: ">"}, count=1, block=5000)
                if not msgs:
                    logger.debug(f"Runtime {runtime.runtime_id} no new messages after 5s timeout, continuing...")
                    continue
            except asyncio.CancelledError:
                # æ­£å¸¸å–æ¶ˆï¼Œé€€å‡ºå¾ªç¯
                logger.info(f"Runtime {runtime.runtime_id} consumer cancelled")
                break
            except Exception as e:
                logger.error(f"Error reading from queue {queue_name}: {e}")
                await asyncio.sleep(1)
                continue
            logger.info(f"Runtime {runtime.runtime_id} received {len(msgs)} message(s) from queue {queue_name}")
            for _, entries in msgs:
                for msg_id, fields in entries:
                    try:
                        logger.info(f"Runtime {runtime.runtime_id} processing message {msg_id}")
                        
                        # ğŸš€ æ£€æŸ¥æ¶ˆæ¯çš„äº¤ä»˜æ¬¡æ•° - æ¯’ä¸¸æ£€æµ‹
                        delivery_count = 1  # Redis Streamé»˜è®¤ä»1å¼€å§‹
                        
                        # ä»Redis Streamä¿¡æ¯ä¸­è·å–å®é™…çš„delivery count
                        try:
                            pending_info = await r.xpending(queue_name, group, msg_id, msg_id, 1)
                            if pending_info:
                                delivery_count = pending_info[0][3]  # delivery_countæ˜¯ç¬¬4ä¸ªå­—æ®µ
                        except Exception as e:
                            logger.debug(f"æ— æ³•è·å–æ¶ˆæ¯delivery count: {e}")
                        
                        MAX_RETRIES = 3
                        if delivery_count > MAX_RETRIES:
                            # ğŸš¨ æ¯’ä¸¸å¤„ç†ï¼šå°†æ¶ˆæ¯ç§»åˆ°æ­»ä¿¡é˜Ÿåˆ—
                            logger.warning(f"Poison pill detected: message {msg_id} has been delivered {delivery_count} times (>{MAX_RETRIES})")
                            
                            try:
                                # å°†æ¶ˆæ¯æ·»åŠ åˆ°æ­»ä¿¡é˜Ÿåˆ—
                                dead_letter_data = {
                                    'original_message_id': msg_id,
                                    'original_queue': queue_name,
                                    'delivery_count': delivery_count,
                                    'poison_detected_at': time.time(),
                                    'original_data': fields[b'task'].decode()
                                }
                                await r.xadd('tasks:dead_letter', dead_letter_data)
                                logger.info(f"Moved poison pill {msg_id} to dead letter queue")
                            except Exception as e:
                                logger.error(f"Failed to move poison pill to dead letter queue: {e}")
                            
                            # ç«‹å³ç¡®è®¤æ¶ˆæ¯ä»¥ä»ä¸»é˜Ÿåˆ—ç§»é™¤
                            await r.xack(queue_name, group, msg_id)
                            logger.info(f"Acknowledged and removed poison pill {msg_id} from main queue")
                            continue  # è·³è¿‡å¤„ç†ï¼Œç»§ç»­ä¸‹ä¸€æ¡æ¶ˆæ¯
                        
                        data = json.loads(fields[b'task'].decode())
                        task = TaskSpec.from_dict(data)
                        logger.info(f"Processing task {task.task_id} from queue {queue_name} (delivery #{delivery_count})")
                        
                        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
                        await _update_task_api_status(r, task.task_id, "running")
                        
                        result = await runtime.execute(task)
                        
                        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå·²å®Œæˆ
                        final_status = "completed" if result.success else "failed"
                        await _update_task_api_status(r, task.task_id, final_status, result)
                        
                        # è·¯å¾„ç”± runtime è‡ªè¡Œä¿å­˜è½¨è¿¹
                        logger.info(f"Task {task.task_id} executed successfully: {result.success}")
                    except Exception as e:
                        # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                        logger.error(f"Error executing task {data.get('task_id', 'unknown')}: {e}", exc_info=True)
                        
                        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
                        task_id = data.get('task_id', 'unknown')
                        await _update_task_api_status(r, task_id, "failed")
                        
                        # åˆ›å»ºé”™è¯¯è½¨è¿¹ç»“æœ
                        try:
                            error_result = TrajectoryResult(
                                task_name=data.get('task_id', 'unknown'),
                                task_id=data.get('task_id', 'unknown'),
                                task_description=data.get('description', ''),
                                runtime_id=getattr(runtime, 'runtime_id', 'unknown'),
                                success=False,
                                steps=[],
                                final_result="",
                                error_message=str(e),
                                error_type=ErrorType.SYSTEM_ERROR,
                                total_duration=0,
                                metadata={"execution_error": True, "error_details": str(e)}
                            )
                            
                            # å°è¯•ä¿å­˜é”™è¯¯è½¨è¿¹
                            if hasattr(runtime, '_save_trajectory'):
                                await runtime._save_trajectory(error_result)
                        except Exception as save_error:
                            logger.error(f"Failed to save error trajectory: {save_error}")
                        
                        # è®°å½•æŒ‡æ ‡
                        if hasattr(runtime, 'metrics'):
                            runtime.metrics.record_task_failure(
                                data.get('task_id', 'unknown'), 
                                getattr(runtime, 'runtime_id', 'unknown'),
                                "system_error"
                            )
                    finally:
                        await r.xack(queue_name, group, msg_id)

    # === æ”¹è¿›ï¼šè‡ªåŠ¨é‡å¯æ¶ˆè´¹åç¨‹ï¼Œé˜²æ­¢å¼‚å¸¸é€€å‡ºå¯¼è‡´ä»»åŠ¡å †ç§¯ ===
    while True:
        try:
            await _run_service()
        except (asyncio.CancelledError, GeneratorExit):
            # æ­£å¸¸å–æ¶ˆæˆ–ç”Ÿæˆå™¨é€€å‡ºæ—¶ç›´æ¥é€€å‡º
            logger.info(f"Runtime {getattr(runtime, 'runtime_id', 'unknown')} service stopped normally")
            break
        except Exception as fatal_err:
            logger.exception(f"âŒ Runtime {getattr(runtime, 'runtime_id', 'unknown')} crashed: {fatal_err}")
            # ç•™å‡ºçŸ­æš‚å†·å´æ—¶é—´åè‡ªåŠ¨é‡å¯
            await asyncio.sleep(3)