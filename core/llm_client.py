#!/usr/bin/env python3
"""
LLMå®¢æˆ·ç«¯ç»Ÿä¸€æ¥å£
æ”¯æŒå¤šç§APIæä¾›å•†ï¼švLLMæœ¬åœ°æœåŠ¡ã€OpenAIã€Google Geminiã€DeepSeekç­‰
"""

import os
import logging
from typing import Dict, Any, Optional, List
from enum import Enum
import time
import httpx # é‡æ–°å¼•å…¥httpxï¼Œå› ä¸º_call_apiä¸­çš„å¼‚å¸¸å¤„ç†éœ€è¦å®ƒ

from core.llm_providers.interfaces import ILLMProvider
from core.llm_providers.openai_provider import OpenAIProvider
from core.llm_providers.gemini_provider import GeminiProvider
from core.llm_providers.deepseek_provider import DeepSeekProvider
from core.llm_providers.vllm_provider import VLLMProvider

# å¯¼å…¥æç¤ºæ„å»ºå™¨
from core.llm.prompt_builders.interfaces import IPromptBuilder
from core.llm.prompt_builders.code_prompt_builder import CodePromptBuilder
from core.llm.prompt_builders.web_prompt_builder import WebPromptBuilder
from core.llm.prompt_builders.reasoning_prompt_builder import ReasoningPromptBuilder
from core.llm.prompt_builders.summary_prompt_builder import SummaryPromptBuilder
from core.llm.prompt_builders.completion_check_prompt_builder import CompletionCheckPromptBuilder
from core.llm.prompt_builders.task_analysis_prompt_builder import TaskAnalysisPromptBuilder

# å¯¼å…¥å“åº”è§£æå™¨
from core.llm.response_parsers.interfaces import IResponseParser
from core.llm.response_parsers.reasoning_response_parser import ReasoningResponseParser
from core.llm.response_parsers.code_response_parser import CodeResponseParser
from core.llm.response_parsers.web_actions_response_parser import WebActionsResponseParser
from core.llm.response_parsers.completion_check_response_parser import CompletionCheckResponseParser
from core.llm.response_parsers.task_analysis_response_parser import TaskAnalysisResponseParser


logger = logging.getLogger(__name__)

class LLMProvider(Enum):
    """LLMæä¾›å•†æšä¸¾"""
    VLLM = "vllm"
    OPENAI = "openai"
    GEMINI = "gemini"
    DEEPSEEK = "deepseek"

class LLMClient:
    """ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.provider_instance: Optional[ILLMProvider] = None # åˆå§‹åŒ–ä¸ºNone
        
        # ç¡®ä¿ç¯å¢ƒå˜é‡ä¼ é€’åˆ°é…ç½®ä¸­
        self._enrich_config_with_env_vars()
        
        # å®ä¾‹åŒ–æç¤ºæ„å»ºå™¨
        self.code_prompt_builder: IPromptBuilder = CodePromptBuilder()
        self.web_prompt_builder: IPromptBuilder = WebPromptBuilder()
        self.reasoning_prompt_builder: IPromptBuilder = ReasoningPromptBuilder()
        self.summary_prompt_builder: IPromptBuilder = SummaryPromptBuilder()
        self.completion_check_prompt_builder: IPromptBuilder = CompletionCheckPromptBuilder()
        self.task_analysis_prompt_builder: IPromptBuilder = TaskAnalysisPromptBuilder()

        # å®ä¾‹åŒ–å“åº”è§£æå™¨
        self.reasoning_response_parser: IResponseParser = ReasoningResponseParser()
        self.code_response_parser: IResponseParser = CodeResponseParser()
        self.web_actions_response_parser: IResponseParser = WebActionsResponseParser()
        self.completion_check_response_parser: IResponseParser = CompletionCheckResponseParser()
        self.task_analysis_response_parser: IResponseParser = TaskAnalysisResponseParser()

        # ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­æŒ‡å®šçš„æä¾›å•†ï¼Œæ²¡æœ‰åˆ™è¿›è¡Œè‡ªåŠ¨æ£€æµ‹
        provider_name = config.get('provider') or config.get('default_provider')
        if provider_name:
            provider_name = provider_name.lower()
            if provider_name == 'vllm':
                self.provider = LLMProvider.VLLM
                self.provider_instance = VLLMProvider(config)
            elif provider_name == 'openai':
                self.provider = LLMProvider.OPENAI
                self.provider_instance = OpenAIProvider(config)
            elif provider_name == 'gemini':
                self.provider = LLMProvider.GEMINI
                # ä»åµŒå¥—é…ç½®ä¸­æå– Gemini ç‰¹å®šé…ç½®å¹¶åˆå¹¶åˆ°æ ¹çº§åˆ«
                gemini_config = config.copy()
                if 'providers' in config and 'gemini' in config['providers']:
                    gemini_provider_config = config['providers']['gemini']
                    gemini_config.update(gemini_provider_config)
                self.provider_instance = GeminiProvider(gemini_config)
            elif provider_name == 'deepseek':
                self.provider = LLMProvider.DEEPSEEK
                self.provider_instance = DeepSeekProvider(config)
            else:
                logger.warning(f"Unknown provider in config: {provider_name}, falling back to auto-detection")
                self.provider = self._detect_provider()
                self._initialize_provider_instance()
        else:
            self.provider = self._detect_provider()
            self._initialize_provider_instance()
            
        logger.info(f"Initialized LLM client with provider: {self.provider.value}")

    def _enrich_config_with_env_vars(self):
        """å°†ç¯å¢ƒå˜é‡æ·»åŠ åˆ°é…ç½®ä¸­ä»¥ç¡®ä¿providersèƒ½æ­£ç¡®è®¿é—®"""
        env_vars = {
            'gemini_api_key': os.getenv('GEMINI_API_KEY'),
            'gemini_api_url': os.getenv('GEMINI_API_URL'),
            'openai_api_key': os.getenv('OPENAI_API_KEY'),
            'openai_base_url': os.getenv('OPENAI_BASE_URL'),
            'deepseek_api_key': os.getenv('DEEPSEEK_API_KEY'),
            'deepseek_base_url': os.getenv('DEEPSEEK_BASE_URL'),
            'vllm_base_url': os.getenv('VLLM_BASE_URL')
        }
        
        # åªæ·»åŠ éç©ºçš„ç¯å¢ƒå˜é‡
        for key, value in env_vars.items():
            if value:
                self.config[key] = value
        
        logger.debug(f"Enriched config with environment variables: {list(self.config.keys())}")

    def _initialize_provider_instance(self):
        """æ ¹æ®æ£€æµ‹åˆ°çš„æä¾›å•†åˆå§‹åŒ–å…·ä½“çš„LLMæä¾›å•†å®ä¾‹"""
        if self.provider == LLMProvider.VLLM:
            self.provider_instance = VLLMProvider(self.config)
        elif self.provider == LLMProvider.OPENAI:
            self.provider_instance = OpenAIProvider(self.config)
        elif self.provider == LLMProvider.GEMINI:
            # ä»åµŒå¥—é…ç½®ä¸­æå– Gemini ç‰¹å®šé…ç½®å¹¶åˆå¹¶åˆ°æ ¹çº§åˆ«
            gemini_config = self.config.copy()
            if 'providers' in self.config and 'gemini' in self.config['providers']:
                gemini_provider_config = self.config['providers']['gemini']
                gemini_config.update(gemini_provider_config)
            self.provider_instance = GeminiProvider(gemini_config)
        elif self.provider == LLMProvider.DEEPSEEK:
            self.provider_instance = DeepSeekProvider(self.config)
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")
    
    def _detect_provider(self) -> LLMProvider:
        """è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨çš„LLMæä¾›å•†"""
        # ä¼˜å…ˆçº§ï¼šGemini > DeepSeek > OpenAI > vLLM
        if os.getenv('GEMINI_API_KEY'):
            return LLMProvider.GEMINI
        elif os.getenv('DEEPSEEK_API_KEY'):
            return LLMProvider.DEEPSEEK
        elif os.getenv('OPENAI_API_KEY'):
            return LLMProvider.OPENAI
        else:
            return LLMProvider.VLLM
    
    async def generate_code(self, description: str, language: str = "python") -> Dict[str, Any]:
        """ç”Ÿæˆä»£ç ï¼Œå¹¶è¿”å›æ€è€ƒè¿‡ç¨‹å’Œä»£ç """
        disable_cache = os.getenv("DISABLE_CACHE") or self.config.get("disable_cache", False)
        logger.debug(f"LLMClient.generate_code called: disable_cache={disable_cache}, description={description[:50]}")
        messages = self.code_prompt_builder.build_prompt(description=description, language=language)
        
        try:
            response = await self._call_api(messages)
            return self.code_response_parser.parse_response(response, language=language)
        except Exception as e:
            logger.error(f"Failed to generate code: {e}")
            raise RuntimeError(f"æ— æ³•ç”Ÿæˆä»£ç : {e}") from e
    
    async def generate_web_actions(self, description: str, page_content: str = "") -> Dict[str, Any]:
        """ç”ŸæˆWebæ“ä½œæ­¥éª¤"""
        messages = self.web_prompt_builder.build_prompt(description=description, page_content=page_content)
        
        try:
            response = await self._call_api(messages)
            return self.web_actions_response_parser.parse_response(response, description=description)
        except Exception as e:
            logger.error(f"Failed to generate web actions: {e}")
            # å¤‡ç”¨é€»è¾‘ç°åœ¨ç”±è§£æå™¨å†…éƒ¨å¤„ç†
            return self.web_actions_response_parser.parse_response("", description=description) # ä¼ å…¥ç©ºå­—ç¬¦ä¸²è§¦å‘å¤‡ç”¨
    
    async def generate_reasoning(self, task_description: str, available_tools: List[str],
                                previous_steps: Optional[List[Dict[str, Any]]] = None,
                                browser_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨ç†æ­¥éª¤å’Œå·¥å…·è°ƒç”¨"""
        messages = self.reasoning_prompt_builder.build_prompt(
            task_description=task_description,
            available_tools=available_tools,
            previous_steps=previous_steps,
            browser_context=browser_context
        )
        
        try:
            response = await self._call_api(messages)
            return self.reasoning_response_parser.parse_response(response)
        except Exception as e:
            logger.error(f"Failed to generate reasoning: {e}")
            return {
                "thinking": f"Error occurred while processing: {e}",
                "action": "error",
                "tool": None,
                "parameters": {},
                "confidence": 0.0
            }
    
    async def generate_enhanced_reasoning(self, task_description: str, available_tools: List[str],
                                         tool_descriptions: str,
                                         previous_steps: Optional[List[Dict[str, Any]]] = None,
                                         execution_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ç”Ÿæˆå¢å¼ºæ¨ç†æ­¥éª¤å’Œå·¥å…·è°ƒç”¨ - ä½¿ç”¨ä¸°å¯Œçš„å·¥å…·æè¿°å’Œæ‰§è¡Œä¸Šä¸‹æ–‡"""
        messages = self.reasoning_prompt_builder.build_prompt(
            task_description=task_description,
            available_tools=available_tools,
            tool_descriptions=tool_descriptions,
            previous_steps=previous_steps,
            execution_context=execution_context
        )
        
        try:
            response = await self._call_api(messages)
            return self.reasoning_response_parser.parse_response(response)
        except Exception as e:
            logger.error(f"Failed to generate enhanced reasoning: {e}")
            return {
                "thinking": f"Error occurred while processing: {e}",
                "action": "error",
                "tool": None,
                "parameters": {},
                "confidence": 0.0
            }
    
    async def generate_task_summary(self, task_description: str, steps: List[Dict],
                                   final_outputs: List[str]) -> str:
        """ç”Ÿæˆä»»åŠ¡æ‰§è¡Œæ€»ç»“"""
        messages = self.summary_prompt_builder.build_prompt(
            task_description=task_description,
            steps=steps,
            final_outputs=final_outputs
        )
        
        try:
            response = await self._call_api(messages)
            return response.strip() # æ€»ç»“é€šå¸¸æ˜¯çº¯æ–‡æœ¬ï¼Œç›´æ¥è¿”å›
        except Exception as e:
            logger.error(f"Failed to generate summary: {e}")
            return f"Task completed with {len(steps)} steps. Final outputs: {'; '.join(final_outputs[:3])}"
    
    async def check_task_completion(self, task_description: str, steps: List[Dict],
                                   current_outputs: List[str]) -> Dict[str, Any]:
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å®Œæˆ"""
        messages = self.completion_check_prompt_builder.build_prompt(
            task_description=task_description,
            steps=steps,
            current_outputs=current_outputs
        )
        
        try:
            response = await self._call_api(messages)
            return self.completion_check_response_parser.parse_response(response)
        except Exception as e:
            logger.error(f"Failed to check completion: {e}")
            return {"completed": False, "confidence": 0.0, "reason": f"Error: {e}"}

    async def analyze_task_requirements(self, task_description: str) -> Dict[str, Any]:
        """åˆ†æä»»åŠ¡æè¿°ï¼Œæ€»ç»“éœ€è¦çš„åŠŸèƒ½å’Œèƒ½åŠ› - å¸®åŠ©LLMæ›´å¥½åœ°åœ¨mcp_tools.jsonä¸­æ‰¾åˆ°åˆé€‚å·¥å…·"""
        messages = self.task_analysis_prompt_builder.build_prompt(task_description=task_description)
        
        try:
            response = await self._call_api(messages)
            return self.task_analysis_response_parser.parse_response(response)
        except Exception as e:
            logger.error(f"Failed to analyze task requirements: {e}")
            return {
                "task_type": "unknown",
                "required_capabilities": [],
                "tools_needed": [],
                "reasoning": f"åˆ†æå¤±è´¥: {str(e)}",
                "confidence": 0.0
            }

    async def get_next_action(self, task_description: str, available_tools: List[str],
                                tool_descriptions: str, previous_steps: Optional[List[Dict[str, Any]]] = None, # ä¿®æ”¹ç±»å‹æç¤º
                                execution_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        è·å–LLMçš„ä¸‹ä¸€ä¸ªè¡ŒåŠ¨å†³ç­–ã€‚
        è¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¥å£ï¼Œç”¨äºåœ¨å¢å¼ºæ¨ç†æ¨¡å¼ä¸‹è·å–LLMçš„å†³ç­–ã€‚
        å®ƒå°†ç›´æ¥è°ƒç”¨ generate_enhanced_reasoning æ–¹æ³•ã€‚
        """
        logger.info("Calling get_next_action (unified LLM decision interface)")
        return await self.generate_enhanced_reasoning(
            task_description=task_description,
            available_tools=available_tools,
            tool_descriptions=tool_descriptions,
            previous_steps=previous_steps,
            execution_context=execution_context
        )

    async def _call_api(self, messages: List[Dict[str, Any]]) -> str: # ä¿®æ”¹ç­¾å
        """è°ƒç”¨ç›¸åº”çš„APIï¼Œå¹¶è®°å½•å®Œæ•´çš„äº¤äº’ä¿¡æ¯"""
        # ğŸ” æ–°å¢ï¼šè®°å½•APIè°ƒç”¨ä¿¡æ¯
        logger.info("ğŸš€ LLM APIè°ƒç”¨å¼€å§‹")
        logger.info(f"   æä¾›å•†: {self.provider.value}")
        # logger.info(f"   Prompté•¿åº¦: {len(prompt)} å­—ç¬¦") # ç§»é™¤ï¼Œå› ä¸ºç°åœ¨æ˜¯æ¶ˆæ¯åˆ—è¡¨
        
        # è®°å½•promptå†…å®¹ï¼ˆè°ƒè¯•æ¨¡å¼ä¸‹è®°å½•æ›´å¤šè¯¦æƒ…ï¼‰
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"   å®Œæ•´Messageså†…å®¹:\n{messages}") # ä¿®æ”¹ä¸ºMessages
        else:
            # ç”Ÿäº§æ¨¡å¼ä¸‹åªè®°å½•å‰åç‰‡æ®µ
            # prompt_preview = prompt[:200] + "..." + prompt[-100:] if len(prompt) > 300 else prompt
            # logger.info(f"   Prompté¢„è§ˆ: {prompt_preview}")
            pass # æš‚æ—¶ä¸è®°å½•é¢„è§ˆï¼Œå› ä¸ºæ¶ˆæ¯åˆ—è¡¨é¢„è§ˆå¤æ‚
        
        start_time = time.time()
        
        try:
            if self.provider_instance is None:
                raise ValueError("LLM provider instance is not initialized.")
            
            # è·å–é»˜è®¤æ¨¡å‹å¹¶ä¼ é€’ç»™ generate_response
            model_name = self.provider_instance.get_default_model()
            response = await self.provider_instance.generate_response(messages=messages, model=model_name) # ç›´æ¥ä¼ é€’messages
            
            # ğŸ” æ–°å¢ï¼šè®°å½•APIå“åº”ä¿¡æ¯
            duration = time.time() - start_time
            logger.info("âœ… LLM APIè°ƒç”¨æˆåŠŸ")
            logger.info(f"   å“åº”æ—¶é—´: {duration:.2f}ç§’")
            logger.info(f"   å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            
            # è®°å½•å“åº”å†…å®¹
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"   å®Œæ•´å“åº”å†…å®¹:\n{response}")
            else:
                # ç”Ÿäº§æ¨¡å¼ä¸‹åªè®°å½•å‰åç‰‡æ®µ
                response_preview = response[:200] + "..." + response[-100:] if len(response) > 300 else response
                logger.info(f"   å“åº”é¢„è§ˆ: {response_preview}")
            
            return response
            
        except httpx.HTTPStatusError as e: # æ•è·æ›´å…·ä½“çš„HTTPé”™è¯¯
            # ğŸ” æ–°å¢ï¼šè®°å½•APIé”™è¯¯ä¿¡æ¯
            duration = time.time() - start_time
            logger.error("âŒ LLM APIè°ƒç”¨å¤±è´¥")
            logger.error(f"   å¤±è´¥æ—¶é—´: {duration:.2f}ç§’")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.error(f"   HTTPçŠ¶æ€ç : {e.response.status_code}")
            logger.error(f"   å“åº”å†…å®¹: {e.response.text}")
            raise
        except Exception as e:
            # ğŸ” æ–°å¢ï¼šè®°å½•APIé”™è¯¯ä¿¡æ¯
            duration = time.time() - start_time
            logger.error("âŒ LLM APIè°ƒç”¨å¤±è´¥")
            logger.error(f"   å¤±è´¥æ—¶é—´: {duration:.2f}ç§’")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
            # å¯¹äºéHTTPStatusErrorï¼Œå¯èƒ½æ²¡æœ‰responseå±æ€§
            raise
