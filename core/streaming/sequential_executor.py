"""
Sequential Streaming Executor - Sequentialæµå¼æ‰§è¡Œå¼•æ“
æ ¸å¿ƒç»„ä»¶ï¼Œå®ç°å¤šæ­¥éª¤XMLå·¥å…·è°ƒç”¨åºåˆ—çš„è‡ªåŠ¨åŒ–æ‰§è¡Œ
"""

import asyncio
import logging
import re
import uuid
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

from .state_manager import StreamingStateManager, ExecutionStep, ExecutionContext
from .result_injector import ResultInjector

logger = logging.getLogger(__name__)

class SequentialStreamingExecutor:
    """Sequentialæµå¼æ‰§è¡Œå¼•æ“"""
    
    def __init__(self, llm_client=None, tool_executor=None):
        """
        åˆå§‹åŒ–Sequentialæ‰§è¡Œå¼•æ“
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼Œç”¨äºç»§ç»­æ¨ç†
            tool_executor: å·¥å…·æ‰§è¡Œå™¨ï¼Œç”¨äºæ‰§è¡Œå…·ä½“å·¥å…·è°ƒç”¨
        """
        self.llm_client = llm_client
        self.tool_executor = tool_executor
        self.result_injector = ResultInjector()
        self.execution_id = str(uuid.uuid4())[:8]
        
        logger.info(f"ğŸš€ SequentialStreamingExecutoråˆå§‹åŒ– - ID: {self.execution_id}")
    
    async def execute_streaming_task(self, initial_response: str, 
                                   task_description: str = "",
                                   max_steps: int = 10,
                                   timeout_per_step: int = 300,
                                   total_timeout: int = 1800) -> Dict[str, Any]:
        """
        æ‰§è¡ŒSequentialæµå¼ä»»åŠ¡
        
        Args:
            initial_response: åˆå§‹XMLå“åº”
            task_description: ä»»åŠ¡æè¿°
            max_steps: æœ€å¤§æ­¥éª¤æ•°
            timeout_per_step: å•æ­¥è¶…æ—¶ï¼ˆç§’ï¼‰
            total_timeout: æ€»è¶…æ—¶ï¼ˆç§’ï¼‰
            
        Returns:
            æ‰§è¡Œç»“æœå­—å…¸
        """
        logger.info(f"ğŸ¯ å¼€å§‹Sequentialæ‰§è¡Œ - ä»»åŠ¡: {task_description[:100]}...")
        
        # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
        context = ExecutionContext(
            task_description=task_description,
            initial_response=initial_response,
            current_response=initial_response,
            max_steps=max_steps,
            timeout_per_step=timeout_per_step,
            total_timeout=total_timeout
        )
        
        # åˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨
        state_manager = StreamingStateManager(context)
        
        try:
            # é˜¶æ®µ1: è§£ææ­¥éª¤åºåˆ—
            steps = self._parse_sequential_steps(initial_response)
            logger.info(f"ğŸ“‹ è§£æåˆ° {len(steps)} ä¸ªæ‰§è¡Œæ­¥éª¤")
            
            # æ·»åŠ æ­¥éª¤åˆ°çŠ¶æ€ç®¡ç†å™¨
            for step in steps:
                state_manager.add_step(step)
            
            # é˜¶æ®µ2: Sequentialæ‰§è¡Œå¾ªç¯
            current_response = initial_response
            step_execution_count = 0
            
            while state_manager.has_pending_executions():
                # æ£€æŸ¥åœæ­¢æ¡ä»¶
                should_stop, stop_reason = state_manager.should_stop_execution()
                if should_stop:
                    logger.warning(f"â¹ï¸ Sequentialæ‰§è¡Œåœæ­¢: {stop_reason}")
                    break
                
                # è·å–ä¸‹ä¸€ä¸ªå¯æ‰§è¡Œæ­¥éª¤
                next_step = state_manager.get_next_executable_step()
                if not next_step:
                    logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯æ‰§è¡Œçš„æ­¥éª¤ï¼Œç»“æŸæ‰§è¡Œ")
                    break
                
                logger.info(f"ğŸ”„ æ‰§è¡Œæ­¥éª¤ {step_execution_count + 1}: {next_step.step_type} - {next_step.step_id}")
                
                # æ ‡è®°æ­¥éª¤å¼€å§‹æ‰§è¡Œ
                state_manager.mark_step_executing(next_step.step_id)
                
                # æ‰§è¡Œæ­¥éª¤
                try:
                    result = await self._execute_step(next_step, state_manager)
                    
                    # ä¿å­˜æ­¥éª¤ç»“æœ
                    state_manager.add_step_result(next_step.step_id, result)
                    
                    # æ³¨å…¥ç»“æœåˆ°å“åº”æµ
                    if result.get('success', True):
                        current_response = self._inject_step_result(
                            current_response, next_step, result
                        )
                        state_manager.update_response(current_response)
                    
                    step_execution_count += 1
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­æ¨ç†ï¼ˆç”Ÿæˆæ–°æ­¥éª¤ï¼‰
                    if self._should_continue_reasoning(next_step, result):
                        logger.info("ğŸ§  è§¦å‘LLMç»§ç»­æ¨ç†...")
                        new_response = await self._continue_llm_reasoning(
                            state_manager, current_response
                        )
                        
                        if new_response and new_response != current_response:
                            # è§£ææ–°çš„æ­¥éª¤
                            new_steps = self._parse_sequential_steps(new_response)
                            logger.info(f"ğŸ†• LLMç”Ÿæˆäº† {len(new_steps)} ä¸ªæ–°æ­¥éª¤")
                            
                            # æ·»åŠ æ–°æ­¥éª¤
                            for step in new_steps:
                                if not any(s.step_id == step.step_id for s in state_manager.steps):
                                    state_manager.add_step(step)
                            
                            current_response = new_response
                            state_manager.update_response(current_response)
                    
                except Exception as e:
                    logger.error(f"âŒ æ­¥éª¤æ‰§è¡Œå¤±è´¥: {next_step.step_id} - {e}")
                    error_result = {
                        'success': False,
                        'error': str(e),
                        'step_id': next_step.step_id
                    }
                    state_manager.add_step_result(next_step.step_id, error_result)
                    
                    # å†³å®šæ˜¯å¦ç»§ç»­æ‰§è¡Œ
                    if not self._should_continue_after_error(e, state_manager):
                        logger.error("ğŸ’¥ é”™è¯¯è¿‡å¤šï¼Œåœæ­¢Sequentialæ‰§è¡Œ")
                        break
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç­”æ¡ˆå®Œæˆæ ‡å¿—
            if '<answer>' in current_response:
                state_manager.mark_completed()
            
            # é˜¶æ®µ3: æ„å»ºæœ€ç»ˆç»“æœ
            final_result = self._build_final_result(state_manager, current_response)
            
            logger.info(f"ğŸ‰ Sequentialæ‰§è¡Œå®Œæˆ - æ‰§è¡Œäº† {step_execution_count} ä¸ªæ­¥éª¤")
            return final_result
            
        except Exception as e:
            logger.error(f"ğŸ’¥ Sequentialæ‰§è¡Œå¼‚å¸¸: {e}")
            return {
                'success': False,
                'error': str(e),
                'execution_id': self.execution_id,
                'partial_results': state_manager.step_results,
                'execution_stats': state_manager.get_execution_stats()
            }
    
    def _parse_sequential_steps(self, xml_response: str) -> List[ExecutionStep]:
        """
        è§£æSequential XMLæ­¥éª¤åºåˆ—
        
        Args:
            xml_response: XMLå“åº”
            
        Returns:
            æ‰§è¡Œæ­¥éª¤åˆ—è¡¨
        """
        steps = []
        
        # æ­£åˆ™åŒ¹é…æ‰€æœ‰XMLæ ‡ç­¾
        xml_pattern = r'<(think|microsandbox|deepsearch|browser|search|answer)>(.*?)</\\1>'
        
        for match in re.finditer(xml_pattern, xml_response, re.DOTALL):
            tag_name = match.group(1)
            content = match.group(2).strip()
            position = match.span()
            
            # ç”Ÿæˆæ­¥éª¤ID
            step_id = f"{tag_name}_{len(steps)}_{uuid.uuid4().hex[:6]}"
            
            step = ExecutionStep(
                step_id=step_id,
                step_type=tag_name,
                content=content,
                position=position,
                needs_execution=tag_name in ['microsandbox', 'deepsearch', 'browser', 'search']
            )
            
            steps.append(step)
            logger.debug(f"ğŸ“ è§£ææ­¥éª¤: {step_id} ({tag_name}) - {content[:50]}...")
        
        return steps
    
    async def _execute_step(self, step: ExecutionStep, 
                          state_manager: StreamingStateManager) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªæ­¥éª¤
        
        Args:
            step: æ‰§è¡Œæ­¥éª¤
            state_manager: çŠ¶æ€ç®¡ç†å™¨
            
        Returns:
            æ‰§è¡Œç»“æœ
        """
        if not self.tool_executor:
            logger.warning("âš ï¸ æ²¡æœ‰å·¥å…·æ‰§è¡Œå™¨ï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ")
            return {
                'success': True,
                'output': f"[æ¨¡æ‹Ÿæ‰§è¡Œ] {step.step_type}: {step.content[:100]}",
                'step_id': step.step_id,
                'execution_time': 0.1
            }
        
        try:
            start_time = datetime.now()
            
            # æ ¹æ®æ­¥éª¤ç±»å‹æ‰§è¡Œç›¸åº”çš„å·¥å…·
            if step.step_type == 'microsandbox':
                result = await self._execute_microsandbox(step, state_manager)
            elif step.step_type == 'deepsearch':
                result = await self._execute_deepsearch(step, state_manager)
            elif step.step_type == 'browser':
                result = await self._execute_browser(step, state_manager)
            elif step.step_type == 'search':
                result = await self._execute_search(step, state_manager)
            else:
                result = {
                    'success': False,
                    'error': f"æœªçŸ¥çš„æ­¥éª¤ç±»å‹: {step.step_type}"
                }
            
            execution_time = (datetime.now() - start_time).total_seconds()
            result['execution_time'] = execution_time
            result['step_id'] = step.step_id
            
            logger.info(f"âœ… æ­¥éª¤æ‰§è¡Œå®Œæˆ: {step.step_id} - è€—æ—¶: {execution_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ­¥éª¤æ‰§è¡Œå¼‚å¸¸: {step.step_id} - {e}")
            return {
                'success': False,
                'error': str(e),
                'step_id': step.step_id,
                'execution_time': 0
            }
    
    async def _execute_microsandbox(self, step: ExecutionStep, 
                                  state_manager: StreamingStateManager) -> Dict[str, Any]:
        """æ‰§è¡Œmicrosandboxæ­¥éª¤"""
        # æ„å»ºtool_idå’Œaction
        tool_id = "microsandbox"
        action = "auto_select"  # ä½¿ç”¨auto_selectè®©ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©
        parameters = {"instruction": step.content}
        
        # è°ƒç”¨å·¥å…·æ‰§è¡Œå™¨
        if hasattr(self.tool_executor, '_execute_tool_action'):
            return await self.tool_executor._execute_tool_action({
                'tool_id': tool_id,
                'action': action,
                'parameters': parameters,
                'thinking': f"Sequentialæ‰§è¡Œæ­¥éª¤: {step.step_id}"
            })
        else:
            # é™çº§å¤„ç†
            return {
                'success': True,
                'output': f"[microsandbox] æ‰§è¡Œä»£ç : {step.content[:100]}...",
                'tool_id': tool_id,
                'action': action
            }
    
    async def _execute_deepsearch(self, step: ExecutionStep, 
                                state_manager: StreamingStateManager) -> Dict[str, Any]:
        """æ‰§è¡Œdeepsearchæ­¥éª¤"""
        tool_id = "mcp-deepsearch"
        action = "auto_select"
        parameters = {"instruction": step.content}
        
        if hasattr(self.tool_executor, '_execute_tool_action'):
            return await self.tool_executor._execute_tool_action({
                'tool_id': tool_id,
                'action': action,
                'parameters': parameters,
                'thinking': f"Sequentialæ‰§è¡Œæ­¥éª¤: {step.step_id}"
            })
        else:
            return {
                'success': True,
                'output': f"[deepsearch] ç ”ç©¶ç»“æœ: {step.content[:100]}...",
                'tool_id': tool_id,
                'action': action
            }
    
    async def _execute_browser(self, step: ExecutionStep, 
                             state_manager: StreamingStateManager) -> Dict[str, Any]:
        """æ‰§è¡Œbrowseræ­¥éª¤"""
        tool_id = "browser_use"
        action = "auto_select"
        parameters = {"instruction": step.content}
        
        if hasattr(self.tool_executor, '_execute_tool_action'):
            return await self.tool_executor._execute_tool_action({
                'tool_id': tool_id,
                'action': action,
                'parameters': parameters,
                'thinking': f"Sequentialæ‰§è¡Œæ­¥éª¤: {step.step_id}"
            })
        else:
            return {
                'success': True,
                'output': f"[browser] æµè§ˆå™¨æ“ä½œ: {step.content[:100]}...",
                'tool_id': tool_id,
                'action': action
            }
    
    async def _execute_search(self, step: ExecutionStep, 
                            state_manager: StreamingStateManager) -> Dict[str, Any]:
        """æ‰§è¡Œsearchæ­¥éª¤"""
        tool_id = "mcp-search-tool"
        action = "auto_select"
        parameters = {"instruction": step.content}
        
        if hasattr(self.tool_executor, '_execute_tool_action'):
            return await self.tool_executor._execute_tool_action({
                'tool_id': tool_id,
                'action': action,
                'parameters': parameters,
                'thinking': f"Sequentialæ‰§è¡Œæ­¥éª¤: {step.step_id}"
            })
        else:
            return {
                'success': True,
                'output': f"[search] æœç´¢ç»“æœ: {step.content[:100]}...",
                'tool_id': tool_id,
                'action': action
            }
    
    def _inject_step_result(self, current_response: str, step: ExecutionStep, 
                          result: Dict[str, Any]) -> str:
        """æ³¨å…¥æ­¥éª¤ç»“æœåˆ°å“åº”æµ"""
        try:
            return self.result_injector.inject_result(
                current_response, step.position, result, step.step_id
            )
        except Exception as e:
            logger.error(f"âŒ ç»“æœæ³¨å…¥å¤±è´¥: {step.step_id} - {e}")
            return current_response
    
    def _should_continue_reasoning(self, step: ExecutionStep, 
                                 result: Dict[str, Any]) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­æ¨ç†
        
        Args:
            step: å½“å‰æ­¥éª¤
            result: æ‰§è¡Œç»“æœ
            
        Returns:
            æ˜¯å¦éœ€è¦ç»§ç»­æ¨ç†
        """
        # å¦‚æœæ˜¯æœ€åä¸€ä¸ªå·¥å…·è°ƒç”¨ï¼Œä¸”æ²¡æœ‰answeræ ‡ç­¾ï¼Œéœ€è¦ç»§ç»­æ¨ç†
        if step.step_type in ['microsandbox', 'deepsearch', 'browser', 'search']:
            if result.get('success', True):
                # æ£€æŸ¥å½“å‰å“åº”ä¸­æ˜¯å¦å·²æœ‰answeræ ‡ç­¾
                current_response = result.get('current_response', '')
                if '<answer>' not in current_response:
                    return True
        
        return False
    
    async def _continue_llm_reasoning(self, state_manager: StreamingStateManager, 
                                    current_response: str) -> Optional[str]:
        """
        è§¦å‘LLMç»§ç»­æ¨ç†
        
        Args:
            state_manager: çŠ¶æ€ç®¡ç†å™¨
            current_response: å½“å‰å“åº”
            
        Returns:
            æ–°çš„å“åº”æˆ–None
        """
        if not self.llm_client:
            logger.warning("âš ï¸ æ²¡æœ‰LLMå®¢æˆ·ç«¯ï¼Œæ— æ³•ç»§ç»­æ¨ç†")
            return None
        
        try:
            # æ„å»ºç»§ç»­æ¨ç†çš„prompt
            continue_prompt = self._build_continue_prompt(state_manager, current_response)
            
            # è°ƒç”¨LLMç»§ç»­æ¨ç†
            if hasattr(self.llm_client, 'generate_response'):
                new_response = await self.llm_client.generate_response(continue_prompt)
                return new_response
            else:
                logger.warning("âš ï¸ LLMå®¢æˆ·ç«¯æ²¡æœ‰generate_responseæ–¹æ³•")
                return None
                
        except Exception as e:
            logger.error(f"âŒ LLMç»§ç»­æ¨ç†å¤±è´¥: {e}")
            return None
    
    def _build_continue_prompt(self, state_manager: StreamingStateManager, 
                             current_response: str) -> str:
        """æ„å»ºç»§ç»­æ¨ç†çš„prompt"""
        stats = state_manager.get_execution_stats()
        
        prompt = f"""Continue your reasoning based on the tool execution results:

Current progress:
{current_response}

Execution status:
- Completed steps: {stats['completed_steps']}
- Failed steps: {stats['failed_steps']}
- Success rate: {stats['success_rate']:.2f}

Please continue with your analysis and next steps. Use the same XML format:
- <think>your reasoning</think> for analysis
- <microsandbox>code</microsandbox> for code execution
- <deepsearch>question</deepsearch> for research
- <browser>task</browser> for web browsing  
- <search>query</search> for file searching
- <answer>final result</answer> when complete

Continue:"""
        
        return prompt
    
    def _should_continue_after_error(self, error: Exception, 
                                   state_manager: StreamingStateManager) -> bool:
        """åˆ¤æ–­é”™è¯¯åæ˜¯å¦ç»§ç»­æ‰§è¡Œ"""
        stats = state_manager.get_execution_stats()
        
        # å¦‚æœé”™è¯¯ç‡è¿‡é«˜ï¼Œåœæ­¢æ‰§è¡Œ
        if stats['error_count'] >= 3:
            return False
        
        # å¦‚æœæ˜¯è¶…æ—¶é”™è¯¯ï¼Œåœæ­¢æ‰§è¡Œ
        if isinstance(error, asyncio.TimeoutError):
            return False
        
        # å…¶ä»–é”™è¯¯ç»§ç»­æ‰§è¡Œ
        return True
    
    def _build_final_result(self, state_manager: StreamingStateManager, 
                          final_response: str) -> Dict[str, Any]:
        """æ„å»ºæœ€ç»ˆæ‰§è¡Œç»“æœ"""
        stats = state_manager.get_execution_stats()
        
        # æå–æœ€ç»ˆç­”æ¡ˆ
        final_answer = ""
        answer_match = re.search(r'<answer>(.*?)</answer>', final_response, re.DOTALL)
        if answer_match:
            final_answer = answer_match.group(1).strip()
        
        # æå–æ‰€æœ‰thinkingå†…å®¹
        thinking_segments = re.findall(r'<think>(.*?)</think>', final_response, re.DOTALL)
        complete_thinking = '\n\n'.join(segment.strip() for segment in thinking_segments)
        
        return {
            'success': stats['failed_steps'] == 0,
            'execution_id': self.execution_id,
            'final_answer': final_answer,
            'complete_thinking': complete_thinking,
            'final_response': final_response,
            'execution_stats': stats,
            'step_results': state_manager.step_results,
            'steps_executed': [
                {
                    'step_id': step.step_id,
                    'step_type': step.step_type,
                    'content': step.content[:100],
                    'status': step.status,
                    'execution_time': step.execution_time
                }
                for step in state_manager.steps if step.needs_execution
            ],
            'state_summary': state_manager.get_state_summary()
        }