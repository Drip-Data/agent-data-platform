

"""
Sequential Streaming Executor - Sequentialæµå¼æ‰§è¡Œå¼•æ“
æ ¸å¿ƒç»„ä»¶ï¼Œå®ç°å¤šæ­¥éª¤XMLå·¥å…·è°ƒç”¨åºåˆ—çš„è‡ªåŠ¨åŒ–æ‰§è¡Œ
"""

import asyncio
import logging
import re
import uuid
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

from .state_manager import StreamingStateManager, ExecutionStep, ExecutionContext
from .result_injector import ResultInjector

logger = logging.getLogger(__name__)

class SequentialStreamingExecutor:
    """Sequentialæµå¼æ‰§è¡Œå¼•æ“"""
    
    def __init__(self, llm_client=None, tool_executor=None, memory_manager=None):
        """
        åˆå§‹åŒ–Sequentialæ‰§è¡Œå¼•æ“
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼Œç”¨äºç»§ç»­æ¨ç†
            tool_executor: å·¥å…·æ‰§è¡Œå™¨ï¼Œç”¨äºæ‰§è¡Œå…·ä½“å·¥å…·è°ƒç”¨
            memory_manager: è®°å¿†ç®¡ç†å™¨ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢ä¸Šä¸‹æ–‡
        """
        self.llm_client = llm_client
        self.tool_executor = tool_executor
        self.memory_manager = memory_manager
        self.result_injector = ResultInjector()
        self.execution_id = str(uuid.uuid4())[:8]
        
        logger.info(f"ğŸš€ SequentialStreamingExecutoråˆå§‹åŒ– - ID: {self.execution_id}")
    
    async def execute_streaming_task(self, initial_response: str, 
                                   task_description: str = "",
                                   max_steps: int = 10,
                                   timeout_per_step: int = 300,
                                   total_timeout: int = 1800,
                                   session_id: Optional[str] = None) -> Dict[str, Any]:
        """
        æ‰§è¡ŒSequentialæµå¼ä»»åŠ¡
        
        Args:
            initial_response: åˆå§‹XMLå“åº”
            task_description: ä»»åŠ¡æè¿°
            max_steps: æœ€å¤§æ­¥éª¤æ•°
            timeout_per_step: å•æ­¥è¶…æ—¶ï¼ˆç§’ï¼‰
            total_timeout: æ€»è¶…æ—¶ï¼ˆç§’ï¼‰
            session_id: å½“å‰ä»»åŠ¡çš„ä¼šè¯ID
        """
        logger.info(f"ğŸ¯ å¼€å§‹Sequentialæ‰§è¡Œ - ä»»åŠ¡: {task_description[:100]}...")
        
        if not session_id:
            session_id = f"session_{self.execution_id}"

        context = ExecutionContext(
            task_description=task_description,
            initial_response=initial_response,
            current_response=initial_response,
            max_steps=max_steps,
            timeout_per_step=timeout_per_step,
            total_timeout=total_timeout
        )
        
        state_manager = StreamingStateManager(context)
        
        try:
            steps = self._parse_sequential_steps(initial_response)
            logger.info(f"ğŸ“‹ è§£æåˆ° {len(steps)} ä¸ªæ‰§è¡Œæ­¥éª¤")
            
            for step in steps:
                state_manager.add_step(step)
            
            current_response = initial_response
            step_execution_count = 0
            
            while state_manager.has_pending_executions():
                should_stop, stop_reason = state_manager.should_stop_execution()
                if should_stop:
                    logger.warning(f"â¹ï¸ Sequentialæ‰§è¡Œåœæ­¢: {stop_reason}")
                    break
                
                next_step = state_manager.get_next_executable_step()
                if not next_step:
                    logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯æ‰§è¡Œçš„æ­¥éª¤ï¼Œç»“æŸæ‰§è¡Œ")
                    break
                
                logger.info(f"ğŸ”„ æ‰§è¡Œæ­¥éª¤ {step_execution_count + 1}: {next_step.step_type} - {next_step.step_id}")
                
                state_manager.mark_step_executing(next_step.step_id)
                
                try:
                    result = await self._execute_step(next_step, state_manager)
                    state_manager.add_step_result(next_step.step_id, result)
                    
                    if result.get('success', True):
                        current_response = self._inject_step_result(current_response, next_step, result)
                        state_manager.update_response(current_response)
                    
                    step_execution_count += 1
                    
                    if self._should_continue_reasoning(next_step, result, state_manager):
                        logger.info("ğŸ§  è§¦å‘LLMç»§ç»­æ¨ç†...")
                        new_response = await self._continue_llm_reasoning(state_manager, current_response)
                        
                        if new_response and new_response != current_response:
                            new_steps = self._parse_sequential_steps(new_response)
                            logger.info(f"ğŸ†• LLMç”Ÿæˆäº† {len(new_steps)} ä¸ªæ–°æ­¥éª¤")
                            for step in new_steps:
                                if not any(s.step_id == step.step_id for s in state_manager.steps):
                                    state_manager.add_step(step)
                            current_response = new_response
                            state_manager.update_response(current_response)
                    
                except Exception as e:
                    logger.error(f"âŒ æ­¥éª¤æ‰§è¡Œå¤±è´¥: {next_step.step_id} - {e}")
                    error_result = {'success': False, 'error': str(e), 'step_id': next_step.step_id}
                    state_manager.add_step_result(next_step.step_id, error_result)
                    if not self._should_continue_after_error(e, state_manager):
                        logger.error("ğŸ’¥ é”™è¯¯è¿‡å¤šï¼Œåœæ­¢Sequentialæ‰§è¡Œ")
                        break
            
            if '<answer>' in current_response:
                state_manager.mark_completed()
            
            final_result = self._build_final_result(state_manager, current_response)
            logger.info(f"ğŸ‰ Sequentialæ‰§è¡Œå®Œæˆ - æ‰§è¡Œäº† {step_execution_count} ä¸ªæ­¥éª¤")
            return final_result
            
        except Exception as e:
            logger.error(f"ğŸ’¥ Sequentialæ‰§è¡Œå¼‚å¸¸: {e}", exc_info=True)
            return {'success': False, 'error': str(e), 'execution_id': self.execution_id}
    
    def _parse_sequential_steps(self, xml_response: str) -> List[ExecutionStep]:
        steps = []
        xml_pattern = r'<(think|microsandbox|deepsearch|browser|search|answer|parallel)>(.*?)</\1>'
        for match in re.finditer(xml_pattern, xml_response, re.DOTALL):
            tag_name, content, position = match.group(1), match.group(2).strip(), match.span()
            step_id = f"{tag_name}_{len(steps)}_{uuid.uuid4().hex[:6]}"
            
            if tag_name == 'parallel':
                step = ExecutionStep(step_id=step_id, step_type='parallel', content=self._parse_parallel_block(content, position), position=position, needs_execution=True)
            else:
                step = ExecutionStep(step_id=step_id, step_type=tag_name, content=content, position=position, needs_execution=tag_name in ['microsandbox', 'deepsearch', 'browser', 'search'])
            
            steps.append(step)
            logger.debug(f"ğŸ“ è§£ææ­¥éª¤: {step_id} ({tag_name})")
        return steps

    def _parse_parallel_block(self, block_content: str, parent_position: Tuple[int, int]) -> List[ExecutionStep]:
        sub_steps = []
        tool_pattern = r'<(microsandbox|deepsearch|browser|search)>(.*?)</\1>'
        for match in re.finditer(tool_pattern, block_content, re.DOTALL):
            tag_name, content = match.group(1), match.group(2).strip()
            position = (parent_position[0] + match.start(), parent_position[0] + match.end())
            step_id = f"{tag_name}_{len(sub_steps)}_{uuid.uuid4().hex[:6]}"
            sub_steps.append(ExecutionStep(step_id=step_id, step_type=tag_name, content=content, position=position, needs_execution=True))
        return sub_steps

    async def _execute_step(self, step: ExecutionStep, state_manager: StreamingStateManager) -> Dict[str, Any]:
        if not self.tool_executor:
            return {'success': True, 'output': f"[æ¨¡æ‹Ÿæ‰§è¡Œ] {step.step_type}", 'step_id': step.step_id}

        result = {}
        try:
            start_time = datetime.now()
            
            if step.step_type == 'parallel':
                result = await self._execute_parallel_block(step, state_manager)
            else:
                result = await self._execute_single_tool(step, state_manager)

            result['execution_time'] = (datetime.now() - start_time).total_seconds()
            result['step_id'] = step.step_id
            
            if not result.get('success', True):
                await self._handle_execution_error(step, result, state_manager)

            logger.info(f"âœ… æ­¥éª¤æ‰§è¡Œå®Œæˆ: {step.step_id} - è€—æ—¶: {result.get('execution_time', 0):.2f}s")
            
        except Exception as e:
            logger.error(f"âŒ æ­¥éª¤æ‰§è¡Œå¼‚å¸¸: {step.step_id} - {e}", exc_info=True)
            result = {'success': False, 'error': str(e), 'step_id': step.step_id}
            await self._handle_execution_error(step, result, state_manager)
        
        if self.memory_manager and state_manager.context.session_id:
            try:
                await self.memory_manager.store_conversation_step(
                    task_id=state_manager.context.session_id.split('_', 1)[1],
                    session_id=state_manager.context.session_id,
                    user_input=f"Execute step: {step.step_type}",
                    agent_output=str(result.get('output', result.get('error', ''))),
                    thinking_summary=f"Executing step {step.step_id}",
                    tools_used=[step.step_type],
                    success=result.get('success', True),
                    error_message=result.get('error'),
                    metadata={'step_id': step.step_id, 'content': step.content}
                )
                logger.debug(f"ğŸ’¾ æ­¥éª¤ {step.step_id} å·²å­˜å‚¨åˆ°è®°å¿†")
            except Exception as mem_e:
                logger.warning(f"å­˜å‚¨æ­¥éª¤ {step.step_id} åˆ°è®°å¿†æ—¶å¤±è´¥: {mem_e}")

        return result

    async def _handle_execution_error(self, failed_step: ExecutionStep, error_result: Dict[str, Any], state_manager: StreamingStateManager):
        if not self.llm_client:
            logger.error("LLMå®¢æˆ·ç«¯æœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œé”™è¯¯åæ€")
            return

        try:
            current_xml_with_error = self.result_injector.inject_result(state_manager.context.current_response, failed_step.position, error_result, failed_step.step_id)
            prompt = f"""An error occurred. Analyze the error and provide a corrected plan.
**Task**: {state_manager.context.task_description}
**History & Error**:
{current_xml_with_error}
**Guideline**: Analyze the error and respond with new `<think>` and tool call XML tags to continue.
"""
            logger.info("ğŸ§  è¯·æ±‚LLMå¯¹é”™è¯¯è¿›è¡Œåæ€å’Œçº æ­£...")
            correction_response = await self.llm_client._call_api([{"role": "user", "content": prompt}])
            
            if correction_response:
                logger.info(f"ğŸ†• LLMæä¾›äº†çº æ­£è®¡åˆ’:\n{correction_response}")
                new_response_with_correction = current_xml_with_error + "\n" + correction_response
                state_manager.update_response(new_response_with_correction)
                new_steps = self._parse_sequential_steps(correction_response)
                if new_steps:
                    logger.info(f"â• å°† {len(new_steps)} ä¸ªçº æ­£æ­¥éª¤æ·»åŠ åˆ°æ‰§è¡Œé˜Ÿåˆ—")
                    for step in new_steps:
                        if not any(s.step_id == step.step_id for s in state_manager.steps):
                            state_manager.add_step(step)
            else:
                logger.warning("LLMæœªèƒ½æä¾›çº æ­£è®¡åˆ’ã€‚")
        except Exception as e:
            logger.error(f"å¤„ç†æ‰§è¡Œé”™è¯¯æ—¶å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)

    async def _execute_single_tool(self, step: ExecutionStep, state_manager: StreamingStateManager) -> Dict[str, Any]:
        tool_id, original_content = step.step_type, step.content
        action, parameters = None, {}
        
        try:
            is_nested_call = re.match(r'<(\w+)>.*</\1>', original_content, re.DOTALL)

            if not original_content or not is_nested_call:
                action, parameters = await self._request_action_selection(tool_id, original_content, state_manager)
                if not action:
                    return {'success': False, 'error': f"æœªèƒ½ä¸º {tool_id} é€‰æ‹©å…·ä½“åŠ¨ä½œ"}
                step.content = f"<{action}>{parameters.get('instruction', '')}</{action}>"
                logger.debug(f"äºŒçº§é€‰æ‹©åï¼Œæ›´æ–°æ­¥éª¤å†…å®¹ä¸º: {step.content}")
            else:
                action_match = re.match(r'<(\w+)>(.*)</\1>', original_content, re.DOTALL)
                if not action_match: raise ValueError("æ— æ•ˆçš„åµŒå¥—å·¥å…·è°ƒç”¨æ ¼å¼")
                action, parameters = action_match.group(1), {"instruction": action_match.group(2).strip()}

            logger.info(f"ğŸ”§ å‡†å¤‡ï¿½ï¿½è¡Œ: tool_id='{tool_id}', action='{action}'")
            
            execution_result = await self.tool_executor.execute_tool(
                tool_id=tool_id,
                action=action,
                parameters=parameters
            )

            if not isinstance(execution_result, dict):
                logger.warning(f"å·¥å…· '{tool_id}' è¿”å›äº†éå­—å…¸ç±»å‹: {type(execution_result)}. å°†å…¶è§†ä¸ºé”™è¯¯ã€‚")
                return {'success': False, 'error': f"å·¥å…·è¿”å›äº†æ— æ•ˆçš„å“åº”ç±»å‹: {str(execution_result)}"}

            return execution_result

        except Exception as e:
            logger.error(f"æ‰§è¡Œå·¥å…· '{tool_id}' æ—¶å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
            return {'success': False, 'error': f"æ‰§è¡Œå·¥å…·æ—¶å‘ç”Ÿå†…éƒ¨å¼‚å¸¸: {e}"}

    async def _request_action_selection(self, tool_id: str, instruction: str, state_manager: StreamingStateManager) -> Tuple[Optional[str], Dict[str, Any]]:
        if not self.llm_client:
            logger.error("LLMå®¢æˆ·ç«¯æœªé…ç½®ï¼Œæ— æ³•è¿›è¡ŒäºŒçº§åŠ¨ä½œé€‰æ‹©")
            return None, {}

        logger.debug(f"ä¸ºæœåŠ¡ '{tool_id}' è¯·æ±‚äºŒçº§åŠ¨ä½œé€‰æ‹©ï¼ŒåŸå§‹æŒ‡ä»¤: '{instruction}'")
        try:
            tool_schema = await self.tool_executor.get_tool_schema(tool_id)
            if not tool_schema or 'actions' not in tool_schema:
                logger.error(f"æœªèƒ½è·å– {tool_id} çš„å·¥å…·schemaæˆ–actions")
                return None, {}
            
            actions_description = "\n".join([f"- <{action['name']}>: {action.get('description', 'No description')}" for action in tool_schema['actions']])
            logger.debug(f"ä» Schema è·å–åˆ° '{tool_id}' çš„å¯ç”¨åŠ¨ä½œ:\n{actions_description}")

            prompt = f"""You have selected the '{tool_id}' service. Your instruction is: '{instruction}'.
Available actions:
{actions_description}
Choose the most appropriate action and respond with a single XML tag. Example: <chosen_action>parameters</chosen_action>"""
            logger.debug(f"å‘é€ç»™ LLM çš„äºŒçº§é€‰æ‹© Prompt:\n{prompt}")

            response = await self.llm_client._call_api([{"role": "user", "content": prompt}])
            logger.debug(f"LLM è¿”å›çš„äºŒçº§é€‰æ‹©ç»“æœ: {response.strip()}")

            action_match = re.match(r'<(\w+)>(.*)</\1>', response.strip(), re.DOTALL)
            if not action_match:
                logger.error(f"äºŒçº§åŠ¨ä½œé€‰æ‹©çš„LLMå“åº”æ ¼å¼ä¸æ­£ç¡®: {response}")
                return None, {}
            
            action, params_str = action_match.group(1), action_match.group(2).strip()
            logger.info(f"âœ… äºŒçº§åŠ¨ä½œé€‰æ‹©æˆåŠŸ: {action}")
            return action, {"instruction": params_str}
        except Exception as e:
            logger.error(f"è¯·æ±‚äºŒçº§åŠ¨ä½œé€‰æ‹©æ—¶å‡ºé”™: {e}", exc_info=True)
            return None, {}

    async def _execute_parallel_block(self, parallel_step: ExecutionStep, state_manager: StreamingStateManager) -> Dict[str, Any]:
        sub_steps = parallel_step.content
        if not isinstance(sub_steps, list): return {'success': False, 'error': 'æ— æ•ˆçš„å¹¶è¡Œæ­¥éª¤æ ¼å¼'}
        logger.info(f"âš¡ï¸ å¹¶å‘æ‰§è¡Œ {len(sub_steps)} ä¸ªå­æ­¥éª¤...")
        tasks = [self._execute_single_tool(sub_step, state_manager) for sub_step in sub_steps]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        final_results, all_success = [], True
        for i, res in enumerate(results):
            sub_step = sub_steps[i]
            if isinstance(res, Exception):
                all_success = False
                final_results.append({'success': False, 'error': str(res), 'step_id': sub_step.step_id, 'position': sub_step.position})
            else:
                if not res.get('success', True): all_success = False
                final_results.append({**res, 'position': sub_step.position})
        return {'success': all_success, 'results': final_results, 'step_id': parallel_step.step_id}
    
    def _inject_step_result(self, current_response: str, step: ExecutionStep, result: Dict[str, Any]) -> str:
        try:
            if step.step_type == 'parallel' and 'results' in result:
                injection_data = [{'tool_call_pos': sub_res['position'], 'result': sub_res, 'step_id': sub_res['step_id']} for sub_res in result['results']]
                return self.result_injector.inject_multiple_results(current_response, injection_data)
            else:
                return self.result_injector.inject_result(current_response, step.position, result, step.step_id)
        except Exception as e:
            logger.error(f"âŒ ç»“æœæ³¨å…¥å¤±è´¥: {step.step_id} - {e}")
            return current_response
    
    def _should_continue_reasoning(self, step: ExecutionStep, result: Dict[str, Any], state_manager: StreamingStateManager) -> bool:
        # åªæœ‰åœ¨æˆåŠŸæ‰§è¡Œä¸”ä»»åŠ¡æœªå®Œæˆæ—¶æ‰ç»§ç»­
        if result.get('success', True) and not state_manager.is_completed:
             # å¦‚æœå“åº”ä¸­æ²¡æœ‰answeræ ‡ç­¾ï¼Œåˆ™éœ€è¦ç»§ç»­
            return '<answer>' not in state_manager.context.current_response
        return False
    
    async def _continue_llm_reasoning(self, state_manager: StreamingStateManager, current_response: str) -> Optional[str]:
        if not self.llm_client: return None
        try:
            prompt = self._build_continue_prompt(state_manager, current_response)
            return await self.llm_client._call_api([{"role": "user", "content": prompt}])
        except Exception as e:
            logger.error(f"âŒ LLMç»§ç»­æ¨ç†å¤±è´¥: {e}")
            return None
    
    def _build_continue_prompt(self, state_manager: StreamingStateManager, current_response: str) -> str:
        stats = state_manager.get_execution_stats()
        return f"""Continue your reasoning based on the tool execution results:
Current progress:
{current_response}
Execution status:
- Completed steps: {stats['completed_steps']}
- Failed steps: {stats['failed_steps']}
Please continue with your analysis and next steps in XML format.
Continue:"""
    
    def _should_continue_after_error(self, error: Exception, state_manager: StreamingStateManager) -> bool:
        stats = state_manager.get_execution_stats()
        return not (stats['error_count'] >= 3 or isinstance(error, asyncio.TimeoutError))
    
    def _build_final_result(self, state_manager: StreamingStateManager, final_response: str) -> Dict[str, Any]:
        stats = state_manager.get_execution_stats()
        answer_match = re.search(r'<answer>(.*?)</answer>', final_response, re.DOTALL)
        final_answer = answer_match.group(1).strip() if answer_match else ""
        thinking_segments = re.findall(r'<think>(.*?)</think>', final_response, re.DOTALL)
        complete_thinking = '\n\n'.join(segment.strip() for segment in thinking_segments)
        
        return {
            'success': stats['failed_steps'] == 0,
            'execution_id': self.execution_id,
            'final_answer': final_answer,
            'complete_thinking': complete_thinking,
            'final_response': final_response,
            'execution_stats': stats,
            'step_results': state_manager.step_results,
            'steps_executed': [{'step_id': s.step_id, 'step_type': s.step_type, 'content': s.content if isinstance(s.content, str) else f"Parallel block with {len(s.content)} steps", 'status': s.status, 'execution_time': s.execution_time} for s in state_manager.steps if s.needs_execution],
            'state_summary': state_manager.get_state_summary()
        }
