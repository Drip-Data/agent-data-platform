#!/usr/bin/env python3
"""
æ­¥éª¤çº§è¯Šæ–­æ—¥å¿—è®°å½•å™¨

ç”¨äºè®°å½•Agentæ‰§è¡Œçš„æ¯ä¸€æ­¥è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬LLMåŸå§‹è¾“å‡ºã€å·¥å…·è°ƒç”¨åŸå§‹å“åº”ç­‰
"""

import json
import os
import aiofiles
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
import logging

logger = logging.getLogger(__name__)


class StepDiagnosticLogger:
    """æ­¥éª¤çº§è¯Šæ–­æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, base_path: str = "output/logs"):
        self.base_path = base_path
        self.current_task_data = None
        self.current_step_data = None
        
    def start_task(self, task_id: str, task_description: str):
        """å¼€å§‹ä»»åŠ¡è®°å½•"""
        self.current_task_data = {
            "task_id": task_id,
            "task_description": task_description,
            "task_start_time": datetime.now().isoformat(),
            "task_end_time": None,
            "task_duration_seconds": None,
            "task_final_status": None,
            "task_final_result": None,
            "steps": []
        }
        logger.info(f"ğŸ” å¼€å§‹è®°å½•ä»»åŠ¡æ­¥éª¤æ—¥å¿—: {task_id}")
        
    def start_step(self, step_index: int):
        """å¼€å§‹æ­¥éª¤è®°å½•"""
        if not self.current_task_data:
            logger.warning("âš ï¸ å°è¯•å¼€å§‹æ­¥éª¤è®°å½•ï¼Œä½†ä»»åŠ¡æœªåˆå§‹åŒ–")
            return
            
        self.current_step_data = {
            "step_index": step_index,
            "step_start_time": datetime.now().isoformat(),
            "step_end_time": None,
            "step_duration_seconds": None,
            "llm_interaction": {},
            "parsing_stage": {},
            "tool_executions": []
        }
        logger.debug(f"ğŸ“ å¼€å§‹è®°å½•æ­¥éª¤ {step_index}")
        
    def log_llm_call(self, prompt: List[Dict], raw_response: str, stop_sequence: str, 
                     start_time: float, end_time: float, token_usage: Optional[Dict] = None):
        """è®°å½•LLMè°ƒç”¨çš„åŸå§‹æ•°æ®ï¼ŒåŒ…å«å®Œæ•´çš„Tokenä½¿ç”¨ç»Ÿè®¡"""
        if not self.current_step_data:
            logger.warning("âš ï¸ å°è¯•è®°å½•LLMè°ƒç”¨ï¼Œä½†æ­¥éª¤æœªåˆå§‹åŒ–")
            return
            
        duration = end_time - start_time
        
        # ğŸ”§ å¢å¼ºTokenä½¿ç”¨ç»Ÿè®¡ - æä¾›è¯¦ç»†çš„æˆæœ¬åˆ†ææ•°æ®
        enhanced_token_usage = self._enhance_token_usage(token_usage, raw_response, prompt)
        
        self.current_step_data["llm_interaction"] = {
            "llm_call_start_time": datetime.fromtimestamp(start_time).isoformat(),
            "llm_call_end_time": datetime.fromtimestamp(end_time).isoformat(),
            "llm_call_duration_seconds": round(duration, 3),
            "prompt_sent_to_llm": prompt,
            "raw_llm_response": raw_response,
            "stop_sequence_triggered": stop_sequence,
            "token_usage": enhanced_token_usage,
            "total_cost_usd": self._calculate_simple_cost(enhanced_token_usage),
            "cost_analysis": self._calculate_enhanced_cost_analysis(enhanced_token_usage, duration)
        }
        logger.debug(f"ğŸ¤– è®°å½•LLMè°ƒç”¨ï¼Œå“åº”é•¿åº¦: {len(raw_response)} å­—ç¬¦")
        
    def log_parsing_result(self, think_content: Optional[str], execution_block: Optional[str],
                          answer_content: Optional[str], actions: List[Dict], 
                          parsing_errors: List[str], start_time: float, end_time: float):
        """è®°å½•è§£æç»“æœ"""
        if not self.current_step_data:
            logger.warning("âš ï¸ å°è¯•è®°å½•è§£æç»“æœï¼Œä½†æ­¥éª¤æœªåˆå§‹åŒ–")
            return
            
        duration = end_time - start_time
        self.current_step_data["parsing_stage"] = {
            "parsing_start_time": datetime.fromtimestamp(start_time).isoformat(),
            "parsing_duration_seconds": round(duration, 4),
            "extracted_think_content": think_content,
            "extracted_execution_block": execution_block,
            "extracted_answer_content": answer_content,
            "parsed_actions": actions,
            "parsing_errors": parsing_errors
        }
        logger.debug(f"ğŸ” è®°å½•è§£æç»“æœï¼Œè¯†åˆ«åˆ° {len(actions)} ä¸ªåŠ¨ä½œ")
        
    def log_step_error(self, step_index: int, error_type: str, error_message: str, 
                      recovery_attempted: bool = False):
        """
        è®°å½•æ­¥éª¤æ‰§è¡Œé”™è¯¯
        """
        if not self.current_step_data:
            logger.warning("âš ï¸ å°è¯•è®°å½•æ­¥éª¤é”™è¯¯ï¼Œä½†æ­¥éª¤æœªåˆå§‹åŒ–")
            return
        
        error_info = {
            "step_index": step_index,
            "error_type": error_type,
            "error_message": error_message,
            "error_time": datetime.now().isoformat(),
            "recovery_attempted": recovery_attempted
        }
        
        # å¦‚æœstep_dataä¸­æ²¡æœ‰errorså­—æ®µï¼Œåˆ›å»ºä¸€ä¸ª
        if "errors" not in self.current_step_data:
            self.current_step_data["errors"] = []
        
        self.current_step_data["errors"].append(error_info)
        logger.debug(f"âŒ è®°å½•æ­¥éª¤é”™è¯¯: {error_type} - {error_message}")
        
    def log_tool_execution(self, execution_index: int, action: Dict, 
                          toolscore_request: Dict, raw_response: Dict,
                          formatted_result: str, start_time: float, end_time: float,
                          execution_status: str, error_details: Optional[str] = None):
        """è®°å½•å·¥å…·æ‰§è¡Œçš„åŸå§‹æ•°æ®ï¼ŒåŒ…å«ç»“æ„åŒ–é”™è¯¯ä¿¡æ¯"""
        if not self.current_step_data:
            logger.warning("âš ï¸ å°è¯•è®°å½•å·¥å…·æ‰§è¡Œï¼Œä½†æ­¥éª¤æœªåˆå§‹åŒ–")
            return
            
        duration = end_time - start_time
        
        # ğŸ”§ ç»“æ„åŒ–é”™è¯¯ä¿¡æ¯å¤„ç†
        structured_error = self._structure_error_details(error_details, raw_response, action) if error_details else None
        
        execution_data = {
            "execution_index": execution_index,
            "action": action,
            "execution_timing": {
                "call_start_time": datetime.fromtimestamp(start_time).isoformat(),
                "call_end_time": datetime.fromtimestamp(end_time).isoformat(),
                "execution_duration_seconds": round(duration, 3)
            },
            "toolscore_request": toolscore_request,
            "toolscore_raw_response": raw_response,
            "formatted_result_for_llm": formatted_result,
            "execution_status": execution_status,
            "error_details": error_details,
            "structured_error": structured_error,
            "success_metrics": self._calculate_execution_metrics(execution_status, duration, raw_response)
        }
        
        self.current_step_data["tool_executions"].append(execution_data)
        logger.debug(f"ğŸ”§ è®°å½•å·¥å…·æ‰§è¡Œ: {action.get('service')}/{action.get('tool')}")
        
    def finish_step(self, step_conclusion: Optional[str] = None):
        """å®Œæˆæ­¥éª¤è®°å½•"""
        if not self.current_step_data or not self.current_task_data:
            logger.warning("âš ï¸ å°è¯•å®Œæˆæ­¥éª¤è®°å½•ï¼Œä½†æ•°æ®æœªåˆå§‹åŒ–")
            return
            
        self.current_step_data["step_end_time"] = datetime.now().isoformat()
        
        # è®¡ç®—æ­¥éª¤æŒç»­æ—¶é—´
        start_time = datetime.fromisoformat(self.current_step_data["step_start_time"])
        end_time = datetime.fromisoformat(self.current_step_data["step_end_time"])
        duration = (end_time - start_time).total_seconds()
        self.current_step_data["step_duration_seconds"] = round(duration, 3)
        
        # æ·»åŠ æ­¥éª¤ç»“è®º
        if step_conclusion:
            self.current_step_data["step_conclusion"] = step_conclusion
            
        # å°†æ­¥éª¤æ·»åŠ åˆ°ä»»åŠ¡æ•°æ®ä¸­
        self.current_task_data["steps"].append(self.current_step_data)
        logger.debug(f"âœ… å®Œæˆæ­¥éª¤ {self.current_step_data['step_index']} è®°å½•")
        
        # é‡ç½®å½“å‰æ­¥éª¤æ•°æ®
        self.current_step_data = None
        
    async def finalize_task(self, final_status: str, final_result: str):
        """å®Œæˆä»»åŠ¡å¹¶å†™å…¥æ—¥å¿—æ–‡ä»¶"""
        if not self.current_task_data:
            logger.warning("âš ï¸ å°è¯•å®Œæˆä»»åŠ¡è®°å½•ï¼Œä½†ä»»åŠ¡æ•°æ®æœªåˆå§‹åŒ–")
            return
            
        # å®Œæˆä»»åŠ¡å…ƒæ•°æ®
        self.current_task_data["task_end_time"] = datetime.now().isoformat()
        self.current_task_data["task_final_status"] = final_status
        self.current_task_data["task_final_result"] = final_result
        
        # è®¡ç®—ä»»åŠ¡æ€»æŒç»­æ—¶é—´
        start_time = datetime.fromisoformat(self.current_task_data["task_start_time"])
        end_time = datetime.fromisoformat(self.current_task_data["task_end_time"])
        duration = (end_time - start_time).total_seconds()
        self.current_task_data["task_duration_seconds"] = round(duration, 3)
        
        # å†™å…¥æ—¥å¿—æ–‡ä»¶
        await self._save_log_file()
        
        # é‡ç½®ä»»åŠ¡æ•°æ®
        self.current_task_data = None
    
    async def get_execution_steps(self) -> List[Dict]:
        """ğŸ”§ æ–°å¢ï¼šè·å–å½“å‰ä»»åŠ¡çš„æ‰§è¡Œæ­¥éª¤åˆ—è¡¨
        
        Returns:
            List[Dict]: åŒ…å«æ­¥éª¤ä¿¡æ¯çš„åˆ—è¡¨ï¼Œå…¼å®¹TrajectoryResult.stepsæ ¼å¼
        """
        if not self.current_task_data or not self.current_task_data.get("steps"):
            return []
        
        # è½¬æ¢å†…éƒ¨æ­¥éª¤æ ¼å¼ä¸ºExecutionStepå…¼å®¹æ ¼å¼
        execution_steps = []
        for step in self.current_task_data["steps"]:
            # åˆ›å»ºExecutionStepå…¼å®¹çš„å­—å…¸
            execution_step = {
                "step_id": f"step_{step.get('step_index', 0)}",
                "thinking": step.get("parsing_stage", {}).get("think_content", ""),
                "action_type": "tool_call" if step.get("tool_executions") else "reasoning",
                "action_params": {},
                "observation": "",
                "success": True,
                "duration": step.get("step_duration_seconds", 0.0)
            }
            
            # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
            tool_executions = step.get("tool_executions", [])
            if tool_executions:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªå·¥å…·è°ƒç”¨çš„ä¿¡æ¯
                first_tool = tool_executions[0]
                execution_step["action_params"] = {
                    "tool_id": first_tool.get("tool_id", "unknown"),
                    "input": first_tool.get("input_data", {}),
                }
                execution_step["observation"] = str(first_tool.get("output_data", ""))
                execution_step["success"] = first_tool.get("success", True)
            
            execution_steps.append(execution_step)
        
        return execution_steps
        
    async def _save_log_file(self):
        """ä¿å­˜æ—¥å¿—æ–‡ä»¶åˆ°æŒ‰æ—¥æœŸåˆ†ç»„çš„ç›®å½•"""
        try:
            # åˆ›å»ºæ—¥æœŸåˆ†ç»„ç›®å½•
            date_str = datetime.now().strftime("%Y-%m-%d")
            log_dir = os.path.join(self.base_path, "grouped", date_str)
            os.makedirs(log_dir, exist_ok=True)
            
            # æ—¥å¿—æ–‡ä»¶è·¯å¾„
            log_file = os.path.join(log_dir, f"step_logs_{date_str}.jsonl")
            
            # å¼‚æ­¥å†™å…¥JSONLæ–‡ä»¶ï¼ˆå•è¡Œæ ¼å¼ï¼‰
            async with aiofiles.open(log_file, 'a', encoding='utf-8') as f:
                json_line = json.dumps(self.current_task_data, ensure_ascii=False)
                await f.write(json_line + '\n')
                
            logger.info(f"ğŸ“Š æ­¥éª¤æ—¥å¿—å·²ä¿å­˜: {log_file}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ­¥éª¤æ—¥å¿—å¤±è´¥: {e}")
            
    def _extract_think_content(self, response_text: str) -> Optional[str]:
        """ä»LLMå“åº”ä¸­æå–æ€è€ƒå†…å®¹"""
        import re
        think_pattern = r'<think>(.*?)</think>'
        match = re.search(think_pattern, response_text, re.DOTALL)
        if match:
            return match.group(1).strip()
        return None
        
    def _extract_answer_content(self, response_text: str) -> Optional[str]:
        """ä»LLMå“åº”ä¸­æå–ç­”æ¡ˆå†…å®¹"""
        import re
        answer_pattern = r'<answer>(.*?)</answer>'
        match = re.search(answer_pattern, response_text, re.DOTALL)
        if match:
            return match.group(1).strip()
        return None
        
    def _extract_execution_block(self, response_text: str) -> Optional[str]:
        """ä»LLMå“åº”ä¸­æå–æ‰§è¡Œå—å†…å®¹"""
        import re
        # æŸ¥æ‰¾ <execute_tools> åˆ° </execute_tools> æˆ– <execute_tools/> ä¹‹é—´çš„å†…å®¹
        pattern1 = r'<execute_tools>(.*?)</execute_tools>'
        pattern2 = r'<execute_tools\s*/>'
        
        match1 = re.search(pattern1, response_text, re.DOTALL)
        if match1:
            return match1.group(1).strip()
            
        match2 = re.search(pattern2, response_text)
        if match2:
            return match2.group(0)
            
        return None
    
    def _enhance_token_usage(self, token_usage: Optional[Dict], raw_response: str, prompt: List[Dict]) -> Dict:
        """ğŸ”§ ç®€åŒ–çš„Tokenä½¿ç”¨ç»Ÿè®¡ - ä¼˜å…ˆä½¿ç”¨çœŸå®APIæ•°æ®"""
        
        # æ£€æŸ¥æ˜¯å¦æœ‰çœŸå®çš„APIæ•°æ® - æ‰©å±•æ£€æµ‹é€»è¾‘
        data_source = token_usage.get('data_source', 'unknown') if token_usage else 'unknown'
        
        # å¦‚æœæ¥æºåŒ…å«è¿™äº›å…³é”®è¯ï¼Œè®¤ä¸ºæ˜¯çœŸå®APIæ•°æ®
        is_real_api = any(keyword in str(data_source).lower() for keyword in ['real_api', 'api_response', 'gemini_api', 'api_provided']) if data_source != 'enhanced_estimation' else False
        
        if token_usage and is_real_api:
            # ä½¿ç”¨çœŸå®APIè¿”å›çš„tokenæ•°æ®
            enhanced = {
                "prompt_tokens": token_usage.get('prompt_tokens', 0),
                "completion_tokens": token_usage.get('completion_tokens', 0),
                "total_tokens": token_usage.get('total_tokens', 0),
                "model": token_usage.get('model', 'gemini-2.5-flash-lite-preview-06-17'),
                "data_source": "real_api"
            }
            # æ·»åŠ ä¸€äº›æœ‰ç”¨çš„æ€§èƒ½åˆ†æ
            enhanced.update({
                "tokens_per_second": enhanced.get('completion_tokens', 0) / max(0.1, token_usage.get('response_time', 1)),
                "efficiency_ratio": enhanced.get('completion_tokens', 0) / max(1, enhanced.get('prompt_tokens', 1))
            })
        elif token_usage and all(key in token_usage for key in ['prompt_tokens', 'completion_tokens']):
            # ä½¿ç”¨æä¾›çš„tokenæ•°æ®å¹¶ä¿ç•™æœ‰ç”¨åˆ†æ
            enhanced = token_usage.copy()  # ä¿ç•™åŸå§‹æ•°æ®
            
            # ç¡®ä¿åŸºæœ¬å­—æ®µå­˜åœ¨å¹¶æ ‡è®°æ•°æ®æº
            enhanced.update({
                "prompt_tokens": token_usage.get('prompt_tokens', 0),
                "completion_tokens": token_usage.get('completion_tokens', 0),
                "total_tokens": token_usage.get('total_tokens', token_usage.get('prompt_tokens', 0) + token_usage.get('completion_tokens', 0)),
                "model": token_usage.get('model', 'gemini-2.5-flash-lite-preview-06-17'),
                "data_source": data_source  # ä¿æŒåŸå§‹æ•°æ®æºæ ‡è®°
            })
        else:
            # è¿›è¡ŒåŸºç¡€ä¼°ç®—
            prompt_text = " ".join([msg.get('content', '') for msg in prompt if isinstance(msg, dict)])
            estimated_prompt_tokens = self._accurate_token_estimation(prompt_text)
            estimated_completion_tokens = self._accurate_token_estimation(raw_response)
            
            enhanced = {
                "prompt_tokens": estimated_prompt_tokens,
                "completion_tokens": estimated_completion_tokens,
                "total_tokens": estimated_prompt_tokens + estimated_completion_tokens,
                "model": "gemini-2.5-flash-lite-preview-06-17",
                "data_source": "estimation"
            }
        
        return enhanced
    
    def _estimate_tokens(self, text: str) -> int:
        """ç®€å•çš„tokenä¼°ç®—å‡½æ•°"""
        if not text:
            return 0
        
        # ç®€å•çš„tokenä¼°ç®—ï¼šä¸­æ–‡çº¦1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦/token
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        other_chars = len(text) - chinese_chars
        
        estimated_tokens = int(chinese_chars / 1.5 + other_chars / 4.0)
        return max(estimated_tokens, 1)
    
    def _accurate_token_estimation(self, text: str) -> int:
        """åŸºäºGeminiç‰¹æ€§çš„å‡†ç¡®tokenä¼°ç®—"""
        if not text:
            return 0
        
        # ä¸­æ–‡å­—ç¬¦ç»Ÿè®¡
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        japanese_chars = sum(1 for c in text if '\u3040' <= c <= '\u309f' or '\u30a0' <= c <= '\u30ff')
        korean_chars = sum(1 for c in text if '\uac00' <= c <= '\ud7af')
        
        # å…¶ä»–å­—ç¬¦
        other_chars = len(text) - chinese_chars - japanese_chars - korean_chars
        
        # åŸºäºGemini tokenizerçš„æ”¹è¿›ä¼°ç®—
        # ä¸­æ–‡: ~1.5 chars/token, æ—¥æ–‡: ~2 chars/token, éŸ©æ–‡: ~2 chars/token, è‹±æ–‡: ~4 chars/token
        estimated_tokens = int(
            chinese_chars / 1.5 + 
            japanese_chars / 2.0 + 
            korean_chars / 2.0 + 
            other_chars / 4.0
        )
        
        # è€ƒè™‘ç‰¹æ®Šæ ‡è®°å’Œæ ¼å¼
        special_tokens = text.count('<') + text.count('>') + text.count('{') + text.count('}')
        estimated_tokens += special_tokens * 0.5  # ç‰¹æ®Šæ ‡è®°é€šå¸¸å ç”¨é¢å¤–token
        
        return max(estimated_tokens, 1)
    
    def _analyze_content_type(self, content: str) -> Dict[str, Any]:
        """åˆ†æå†…å®¹ç±»å‹å’Œå¤æ‚åº¦"""
        import re
        
        analysis = {
            "has_code": bool(re.search(r'```|`[^`]+`|def |class |import |function', content)),
            "has_json": bool(re.search(r'\{[^}]*\}|\[[^\]]*\]', content)),
            "has_xml": bool(re.search(r'<[^>]+>', content)),
            "has_markdown": bool(re.search(r'#+|[*_]{1,2}[^*_]+[*_]{1,2}|\[.*\]\(.*\)', content)),
            "line_count": content.count('\n') + 1,
            "avg_line_length": len(content) / max(1, content.count('\n') + 1),
            "complexity_score": self._calculate_content_complexity(content)
        }
        
        return analysis
    
    def _analyze_prompt_complexity(self, prompt: List[Dict]) -> Dict[str, Any]:
        """åˆ†æpromptå¤æ‚åº¦"""
        total_length = sum(len(str(msg.get('content', ''))) for msg in prompt)
        message_count = len(prompt)
        
        # æ£€æµ‹ç‰¹æ®ŠæŒ‡ä»¤
        all_content = " ".join([str(msg.get('content', '')) for msg in prompt])
        
        complexity = {
            "message_count": message_count,
            "total_length": total_length,
            "avg_message_length": total_length / max(1, message_count),
            "has_system_prompt": any(msg.get('role') == 'system' for msg in prompt),
            "has_examples": 'example' in all_content.lower() or 'ç¤ºä¾‹' in all_content,
            "has_constraints": any(word in all_content.lower() for word in ['must', 'should', 'cannot', 'å¿…é¡»', 'ä¸èƒ½']),
            "instruction_density": self._calculate_instruction_density(all_content)
        }
        
        return complexity
    
    def _calculate_content_complexity(self, content: str) -> float:
        """è®¡ç®—å†…å®¹å¤æ‚åº¦åˆ†æ•° (0-10)"""
        score = 0.0
        
        # é•¿åº¦å› å­ (0-2åˆ†)
        score += min(2.0, len(content) / 1000)
        
        # ç»“æ„åŒ–å†…å®¹ (0-3åˆ†)
        if '{' in content or '[' in content:
            score += 1.0
        if '<' in content and '>' in content:
            score += 1.0
        if '```' in content:
            score += 1.0
        
        # ç‰¹æ®Šå­—ç¬¦å¯†åº¦ (0-2åˆ†)
        special_chars = sum(1 for c in content if c in '{}[]()<>*_`#|\\')
        score += min(2.0, special_chars / max(1, len(content)) * 100)
        
        # æ¢è¡Œå¯†åº¦ (0-2åˆ†)
        line_density = content.count('\n') / max(1, len(content)) * 100
        score += min(2.0, line_density * 10)
        
        # æ•°å­—å’Œæ ‡ç‚¹å¯†åº¦ (0-1åˆ†)
        numbers_punct = sum(1 for c in content if c.isdigit() or c in '.,;:!?')
        score += min(1.0, numbers_punct / max(1, len(content)) * 50)
        
        return min(10.0, score)
    
    def _calculate_instruction_density(self, content: str) -> float:
        """è®¡ç®—æŒ‡ä»¤å¯†åº¦"""
        instruction_words = [
            'please', 'must', 'should', 'need', 'require', 'ensure', 'make sure',
            'è¯·', 'å¿…é¡»', 'éœ€è¦', 'ç¡®ä¿', 'è¦æ±‚', 'åº”è¯¥', 'åŠ¡å¿…'
        ]
        
        content_lower = content.lower()
        instruction_count = sum(1 for word in instruction_words if word in content_lower)
        word_count = len(content.split())
        
        return instruction_count / max(1, word_count) * 100
    
    def _estimate_cache_savings(self, prompt_tokens: int) -> Dict[str, Any]:
        """ä¼°ç®—ç¼“å­˜èŠ‚çœï¼ˆåŸºäºGemini 2.5ç¼“å­˜æœºåˆ¶ï¼‰"""
        if prompt_tokens < 1024:  # ä¸æ»¡è¶³Gemini 2.5æœ€å°ç¼“å­˜è¦æ±‚
            return {"eligible": False, "reason": "Below minimum 1024 tokens"}
        
        # åŸºäºGemini 2.5 Flashå®šä»·
        normal_cost = (prompt_tokens / 1_000_000) * 0.30  # $0.30 per 1M input tokens
        cache_cost = (prompt_tokens / 1_000_000) * 0.075  # $0.075 per 1M cached tokens (25% of input cost)
        
        savings_per_reuse = normal_cost - cache_cost
        
        return {
            "eligible": True,
            "normal_cost_usd": round(normal_cost, 6),
            "cache_cost_usd": round(cache_cost, 6),
            "savings_per_reuse_usd": round(savings_per_reuse, 6),
            "break_even_uses": 2,  # ç¼“å­˜åç¬¬2æ¬¡ä½¿ç”¨å¼€å§‹èŠ‚çœ
            "potential_savings_5_uses": round(savings_per_reuse * 4, 6)  # ä½¿ç”¨5æ¬¡çš„èŠ‚çœ
        }
    
    def _calculate_simple_cost(self, token_usage: Dict) -> float:
        """æ ¹æ®ç³»ç»Ÿå®é™…ä½¿ç”¨çš„æ¨¡å‹è®¡ç®—ç®€å•æˆæœ¬"""
        prompt_tokens = token_usage.get('prompt_tokens', 0)
        completion_tokens = token_usage.get('completion_tokens', 0)
        model = token_usage.get('model', 'gemini-2.5-flash-lite-preview-06-17')
        
        # ç¡®ä¿tokenæ•°ä¸ºæ•°å­—ç±»å‹
        if isinstance(prompt_tokens, str):
            prompt_tokens = 0
        if isinstance(completion_tokens, str):
            completion_tokens = 0
        
        # Gemini 2.5ç³»åˆ—å®šä»·ï¼ˆç¾å…ƒæ¯100ä¸‡tokenï¼‰
        pricing_config = {
            "gemini-2.5-pro": {"input": 1.25, "output": 10.0},
            "gemini-2.5-flash": {"input": 0.30, "output": 2.50},
            "gemini-2.5-flash-lite": {"input": 0.10, "output": 0.40},
            "gemini-2.5-flash-lite-preview-06-17": {"input": 0.10, "output": 0.40}  # å½“å‰ç³»ç»Ÿä½¿ç”¨çš„æ¨¡å‹
        }
        
        # è·å–æ¨¡å‹å®šä»·ï¼Œé»˜è®¤ä½¿ç”¨flash-lite
        pricing = pricing_config.get(model, pricing_config["gemini-2.5-flash-lite"])
        
        # è®¡ç®—æ€»æˆæœ¬
        input_cost = (prompt_tokens / 1_000_000) * pricing["input"]
        output_cost = (completion_tokens / 1_000_000) * pricing["output"]
        total_cost = input_cost + output_cost
        
        return round(total_cost, 6)
    
    def _calculate_enhanced_cost_analysis(self, token_usage: Dict, duration: float) -> Dict:
        """è®¡ç®—å¢å¼ºçš„æˆæœ¬åˆ†æ - ä¿ç•™æœ‰ç”¨ä¿¡æ¯"""
        prompt_tokens = token_usage.get('prompt_tokens', 0)
        completion_tokens = token_usage.get('completion_tokens', 0)
        model = token_usage.get('model', 'gemini-2.5-flash-lite-preview-06-17')
        
        # ç¡®ä¿tokenæ•°ä¸ºæ•°å­—ç±»å‹
        if isinstance(prompt_tokens, str):
            prompt_tokens = 0
        if isinstance(completion_tokens, str):
            completion_tokens = 0
        
        # å®šä»·é…ç½®
        pricing_config = {
            "gemini-2.5-pro": {"input": 1.25, "output": 10.0},
            "gemini-2.5-flash": {"input": 0.30, "output": 2.50},
            "gemini-2.5-flash-lite": {"input": 0.10, "output": 0.40},
            "gemini-2.5-flash-lite-preview-06-17": {"input": 0.10, "output": 0.40}
        }
        
        pricing = pricing_config.get(model, pricing_config["gemini-2.5-flash-lite"])
        
        # è®¡ç®—æˆæœ¬åˆ†è§£
        input_cost = (prompt_tokens / 1_000_000) * pricing["input"]
        output_cost = (completion_tokens / 1_000_000) * pricing["output"]
        total_cost = input_cost + output_cost
        
        # æ€§èƒ½æŒ‡æ ‡
        tokens_per_second = completion_tokens / max(0.1, duration)
        cost_per_second = total_cost / max(0.1, duration)
        tokens_per_dollar = (prompt_tokens + completion_tokens) / max(0.000001, total_cost)
        efficiency_score = completion_tokens / max(0.1, duration)
        
        # ç¼“å­˜åˆ†æ
        cache_eligible = prompt_tokens >= 1024
        cache_savings_usd = 0.0
        cache_efficiency = 0.0
        
        if cache_eligible:
            # è®¡ç®—ç¼“å­˜èŠ‚çœï¼ˆä¼°ç®—75%èŠ‚çœï¼‰
            cache_cost = input_cost * 0.25  # 25%çš„åŸå§‹æˆæœ¬
            cache_savings_usd = input_cost - cache_cost
            cache_efficiency = cache_savings_usd / input_cost if input_cost > 0 else 0
        
        # ä¼˜åŒ–å»ºè®®
        optimization_suggestions = []
        if prompt_tokens >= 1024:
            optimization_suggestions.append("è¾“å…¥è¶…è¿‡1024 tokensï¼Œå»ºè®®å¯ç”¨ä¸Šä¸‹æ–‡ç¼“å­˜ä»¥èŠ‚çœæˆæœ¬")
        if cache_efficiency == 0 and cache_eligible:
            optimization_suggestions.append("ç¼“å­˜ä½¿ç”¨ç‡ä»…0.0%ï¼Œå¯ä¼˜åŒ–ç©ºé—´è¾ƒå¤§")
        
        return {
            "model": model,
            "estimated_cost_usd": round(total_cost, 6),
            "cost_per_second": round(cost_per_second, 6),
            "tokens_per_dollar": int(tokens_per_dollar),
            "efficiency_score": round(efficiency_score, 2),
            "cost_breakdown": {
                "input_cost": round(input_cost, 6),
                "output_cost": round(output_cost, 6),
                "total_cost": round(total_cost, 6)
            },
            "cache_analysis": {
                "cache_eligible": cache_eligible,
                "cache_savings_usd": round(cache_savings_usd, 6),
                "cache_efficiency": round(cache_efficiency, 3),
                "without_cache_cost": round(input_cost + output_cost, 6)
            },
            "performance_metrics": {
                "tokens_per_second": round(tokens_per_second, 1),
                "cost_per_input_token": round(pricing["input"] / 1_000_000, 6),
                "cost_per_output_token": round(pricing["output"] / 1_000_000, 6),
                "total_tokens": prompt_tokens + completion_tokens,
                "cost_efficiency_rating": self._get_efficiency_rating(efficiency_score)
            },
            "optimization_suggestions": optimization_suggestions
        }
    
    def _get_efficiency_rating(self, efficiency_score: float) -> str:
        """æ ¹æ®æ•ˆç‡åˆ†æ•°è·å–è¯„çº§"""
        if efficiency_score >= 200:
            return "Excellent"
        elif efficiency_score >= 100:
            return "Good"
        elif efficiency_score >= 50:
            return "Fair"
        else:
            return "Poor"
    
    def _calculate_cost_metrics(self, token_usage: Dict, duration: float) -> Dict:
        """ğŸ’° åŸºäºGemini 2.5å®é™…å®šä»·çš„ç²¾ç¡®æˆæœ¬è®¡ç®—"""
        
        prompt_tokens = token_usage.get('prompt_tokens', 0)
        completion_tokens = token_usage.get('completion_tokens', 0)
        cached_tokens = token_usage.get('cached_tokens', 0)
        model = token_usage.get('model', 'gemini-2.5-flash')
        
        # ç¡®ä¿tokenæ•°ä¸ºæ•°å­—ç±»å‹
        if isinstance(prompt_tokens, str) or isinstance(completion_tokens, str):
            prompt_tokens = 0
            completion_tokens = 0
        
        # Gemini 2.5ç³»åˆ—å®é™…å®šä»·ï¼ˆç¾å…ƒæ¯100ä¸‡tokenï¼‰
        pricing_config = {
            "gemini-2.5-pro": {
                "input": 1.25,      # $1.25 per 1M input tokens
                "output": 10.0,     # $10.0 per 1M output tokens  
                "cache": 0.3125,    # $0.3125 per 1M cached tokens (25% of input)
                "storage_per_hour": 4.50  # $4.50 per 1M tokens per hour storage
            },
            "gemini-2.5-flash": {
                "input": 0.30,      # $0.30 per 1M input tokens
                "output": 2.50,     # $2.50 per 1M output tokens
                "cache": 0.075,     # $0.075 per 1M cached tokens (25% of input)
                "storage_per_hour": 1.0   # $1.0 per 1M tokens per hour storage
            },
            "gemini-2.5-flash-lite": {
                "input": 0.10,      # $0.10 per 1M input tokens
                "output": 0.40,     # $0.40 per 1M output tokens
                "cache": 0.025,     # $0.025 per 1M cached tokens (25% of input)
                "storage_per_hour": 1.0   # Same as flash
            }
        }
        
        # è·å–æ¨¡å‹å®šä»·ï¼Œé»˜è®¤ä½¿ç”¨flash
        pricing = pricing_config.get(model, pricing_config["gemini-2.5-flash"])
        
        # è®¡ç®—å®é™…è¾“å…¥æˆæœ¬ï¼ˆæ’é™¤ç¼“å­˜éƒ¨åˆ†ï¼‰
        actual_input_tokens = max(0, prompt_tokens - cached_tokens)
        input_cost = (actual_input_tokens / 1_000_000) * pricing["input"]
        
        # ç¼“å­˜æˆæœ¬ï¼ˆå¦‚æœæœ‰ä½¿ç”¨ç¼“å­˜ï¼‰
        cache_cost = (cached_tokens / 1_000_000) * pricing["cache"]
        
        # è¾“å‡ºæˆæœ¬
        output_cost = (completion_tokens / 1_000_000) * pricing["output"]
        
        # æ€»æˆæœ¬
        total_cost = input_cost + cache_cost + output_cost
        
        # è®¡ç®—å¦‚æœæ²¡æœ‰ç¼“å­˜çš„æˆæœ¬ï¼ˆç”¨äºæ¯”è¾ƒèŠ‚çœï¼‰
        no_cache_cost = (prompt_tokens / 1_000_000) * pricing["input"] + output_cost
        cache_savings = no_cache_cost - total_cost
        
        return {
            "model": model,
            "estimated_cost_usd": round(total_cost, 6),
            "cost_per_second": round(total_cost / max(0.1, duration), 6),
            "tokens_per_dollar": int((prompt_tokens + completion_tokens) / max(0.000001, total_cost)),
            "efficiency_score": round(completion_tokens / max(0.1, duration), 2),
            
            # è¯¦ç»†æˆæœ¬åˆ†è§£
            "cost_breakdown": {
                "input_cost": round(input_cost, 6),
                "cache_cost": round(cache_cost, 6),
                "output_cost": round(output_cost, 6),
                "total_cost": round(total_cost, 6)
            },
            
            # ç¼“å­˜æ•ˆç›Šåˆ†æ
            "cache_analysis": {
                "cached_tokens": cached_tokens,
                "cache_savings_usd": round(cache_savings, 6),
                "cache_efficiency": round(cache_savings / max(0.000001, no_cache_cost) * 100, 2),
                "without_cache_cost": round(no_cache_cost, 6)
            },
            
            # æ€§èƒ½æŒ‡æ ‡
            "performance_metrics": {
                "cost_per_input_token": round(total_cost / max(1, prompt_tokens) * 1000, 6),  # æ¯1Kè¾“å…¥tokenæˆæœ¬
                "cost_per_output_token": round(output_cost / max(1, completion_tokens) * 1000, 6),  # æ¯1Kè¾“å‡ºtokenæˆæœ¬
                "total_tokens": prompt_tokens + completion_tokens,
                "cost_efficiency_rating": self._calculate_cost_efficiency_rating(total_cost, prompt_tokens + completion_tokens)
            },
            
            # ä¼˜åŒ–å»ºè®®
            "optimization_suggestions": self._generate_cost_optimization_suggestions(
                model, total_cost, prompt_tokens, completion_tokens, cached_tokens
            )
        }
    
    def _calculate_cost_efficiency_rating(self, cost: float, total_tokens: int) -> str:
        """è®¡ç®—æˆæœ¬æ•ˆç‡è¯„çº§"""
        if total_tokens == 0:
            return "N/A"
        
        cost_per_1k_tokens = (cost / total_tokens) * 1000
        
        if cost_per_1k_tokens <= 0.0001:
            return "Excellent"
        elif cost_per_1k_tokens <= 0.0005:
            return "Good"
        elif cost_per_1k_tokens <= 0.002:
            return "Fair"
        else:
            return "Expensive"
    
    def _generate_cost_optimization_suggestions(self, model: str, cost: float, 
                                              prompt_tokens: int, completion_tokens: int, 
                                              cached_tokens: int) -> List[str]:
        """ç”Ÿæˆæˆæœ¬ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # æ¨¡å‹é€‰æ‹©å»ºè®®
        if model == "gemini-2.5-pro" and completion_tokens < 1000:
            suggestions.append("å¯¹äºç®€çŸ­å›å¤ï¼Œè€ƒè™‘ä½¿ç”¨gemini-2.5-flashä»¥é™ä½æˆæœ¬")
        
        if model == "gemini-2.5-flash" and prompt_tokens > 50000:
            suggestions.append("å¤§é‡è¾“å…¥æ—¶ï¼Œgemini-2.5-flash-liteå¯èƒ½æ›´ç»æµ")
        
        # ç¼“å­˜å»ºè®®
        if prompt_tokens > 1024 and cached_tokens == 0:
            suggestions.append("è¾“å…¥è¶…è¿‡1024 tokensï¼Œå»ºè®®å¯ç”¨ä¸Šä¸‹æ–‡ç¼“å­˜ä»¥èŠ‚çœæˆæœ¬")
        
        if cached_tokens > 0:
            cache_ratio = cached_tokens / prompt_tokens
            if cache_ratio > 0.5:
                suggestions.append(f"ç¼“å­˜æ•ˆæœè‰¯å¥½ï¼ˆ{cache_ratio:.1%}ï¼‰ï¼Œç»§ç»­ä¿æŒ")
            else:
                suggestions.append("ç¼“å­˜ä½¿ç”¨ç‡è¾ƒä½ï¼Œå¯ä¼˜åŒ–é‡å¤å†…å®¹çš„è¯†åˆ«")
        
        # æˆæœ¬è­¦å‘Š
        if cost > 0.01:  # è¶…è¿‡1ç¾åˆ†
            suggestions.append("å•æ¬¡è¯·æ±‚æˆæœ¬è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥è¾“å…¥é•¿åº¦å’Œæ¨¡å‹é€‰æ‹©")
        
        # æ•ˆç‡å»ºè®®
        if completion_tokens > prompt_tokens * 2:
            suggestions.append("è¾“å‡ºè¿œè¶…è¾“å…¥ï¼Œè€ƒè™‘ä¼˜åŒ–promptä»¥è·å¾—æ›´ç®€æ´çš„å›å¤")
        
        return suggestions
    
    def _structure_error_details(self, error_details: str, raw_response: Dict, action: Dict) -> Dict:
        """ğŸ”§ ç»“æ„åŒ–é”™è¯¯ä¿¡æ¯ - ä¾¿äºè‡ªåŠ¨åŒ–é”™è¯¯åˆ†æ"""
        from core.interfaces import ErrorMessageConstants
        
        structured = {
            "error_message": error_details,
            "error_category": self._categorize_error(error_details),
            "error_source": self._identify_error_source(raw_response, action),
            "recoverable": self._assess_error_recoverability(error_details),
            "suggested_actions": self._suggest_error_recovery(error_details, action),
            "error_context": {
                "tool_service": action.get('service', 'unknown'),
                "tool_action": action.get('tool', 'unknown'),
                "parameters": action.get('parameters', {}),
                "response_status": raw_response.get('status', 'unknown') if isinstance(raw_response, dict) else 'no_response'
            }
        }
        
        return structured
    
    def _categorize_error(self, error_details: str) -> str:
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        error_lower = error_details.lower()
        
        if any(keyword in error_lower for keyword in ['timeout', 'è¶…æ—¶', 'timed out']):
            return 'timeout_error'
        elif any(keyword in error_lower for keyword in ['connection', 'è¿æ¥', 'network']):
            return 'network_error'
        elif any(keyword in error_lower for keyword in ['parameter', 'å‚æ•°', 'invalid']):
            return 'parameter_error'
        elif any(keyword in error_lower for keyword in ['permission', 'æƒé™', 'unauthorized']):
            return 'permission_error'
        elif any(keyword in error_lower for keyword in ['not found', 'æœªæ‰¾åˆ°', '404']):
            return 'resource_not_found'
        else:
            return 'general_error'
    
    def _identify_error_source(self, raw_response: Dict, action: Dict) -> str:
        """è¯†åˆ«é”™è¯¯æ¥æº"""
        if not isinstance(raw_response, dict):
            return 'response_format_error'
        
        if 'error' in raw_response:
            return 'tool_service_error'
        elif raw_response.get('status') == 'timeout':
            return 'execution_timeout'
        elif not raw_response.get('result'):
            return 'empty_response'
        else:
            return 'unknown_source'
    
    def _assess_error_recoverability(self, error_details: str) -> bool:
        """è¯„ä¼°é”™è¯¯æ˜¯å¦å¯æ¢å¤"""
        error_lower = error_details.lower()
        
        # ä¸å¯æ¢å¤çš„é”™è¯¯
        non_recoverable = ['permission denied', 'æƒé™ä¸è¶³', 'unauthorized', 'not found', 'æœªæ‰¾åˆ°']
        if any(keyword in error_lower for keyword in non_recoverable):
            return False
        
        # å¯èƒ½å¯æ¢å¤çš„é”™è¯¯
        recoverable = ['timeout', 'è¶…æ—¶', 'connection', 'è¿æ¥', 'parameter', 'å‚æ•°']
        if any(keyword in error_lower for keyword in recoverable):
            return True
        
        return False  # é»˜è®¤è®¤ä¸ºä¸å¯æ¢å¤
    
    def _suggest_error_recovery(self, error_details: str, action: Dict) -> List[str]:
        """å»ºè®®é”™è¯¯æ¢å¤æ–¹æ¡ˆ"""
        suggestions = []
        error_lower = error_details.lower()
        
        if 'timeout' in error_lower or 'è¶…æ—¶' in error_lower:
            suggestions.extend([
                "å¢åŠ å·¥å…·æ‰§è¡Œè¶…æ—¶æ—¶é—´",
                "é‡è¯•æ‰§è¡Œï¼Œä½¿ç”¨æ›´çŸ­çš„å‚æ•°",
                "æ£€æŸ¥ç½‘ç»œè¿æ¥çŠ¶æ€"
            ])
        elif 'parameter' in error_lower or 'å‚æ•°' in error_lower:
            suggestions.extend([
                "æ£€æŸ¥å‚æ•°æ ¼å¼æ˜¯å¦æ­£ç¡®",
                "éªŒè¯å¿…éœ€å‚æ•°æ˜¯å¦å®Œæ•´",
                "å‚è€ƒå·¥å…·æ–‡æ¡£ç¡®è®¤å‚æ•°åç§°"
            ])
        elif 'connection' in error_lower or 'è¿æ¥' in error_lower:
            suggestions.extend([
                "æ£€æŸ¥æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ",
                "éªŒè¯ç½‘ç»œè¿æ¥",
                "é‡å¯ç›¸å…³æœåŠ¡"
            ])
        else:
            suggestions.append("æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—ç¡®å®šå…·ä½“åŸå› ")
        
        return suggestions
    
    def _calculate_execution_metrics(self, execution_status: str, duration: float, raw_response: Dict) -> Dict:
        """è®¡ç®—æ‰§è¡ŒæˆåŠŸæŒ‡æ ‡"""
        is_success = execution_status == 'success'
        
        # å“åº”è´¨é‡è¯„ä¼°
        response_quality = 0.0
        if isinstance(raw_response, dict):
            if raw_response.get('result'):
                response_quality += 0.5
            if raw_response.get('status') == 'success':
                response_quality += 0.3
            if 'error' not in raw_response:
                response_quality += 0.2
        
        return {
            "success": is_success,
            "execution_duration_seconds": round(duration, 3),
            "performance_rating": "fast" if duration < 2.0 else "normal" if duration < 5.0 else "slow",
            "response_quality_score": round(response_quality, 2),
            "execution_efficiency": round(1.0 / max(0.1, duration), 3)  # executions per second potential
        }