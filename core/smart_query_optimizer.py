#!/usr/bin/env python3
"""
ğŸ” æ™ºèƒ½æŸ¥è¯¢ä¼˜åŒ–å™¨ (Smart Query Optimizer)
ä¼˜åŒ–å·¥å…·æŸ¥è¯¢çš„ç²¾ç¡®åº¦å’ŒæˆåŠŸç‡ï¼Œå‡å°‘æ— æ•ˆå°è¯•
"""

import logging
import re
import json
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


class QueryType(Enum):
    """æŸ¥è¯¢ç±»å‹"""
    FACTUAL = "factual"           # äº‹å®æŸ¥è¯¢
    PRICE = "price"               # ä»·æ ¼æŸ¥è¯¢  
    DEFINITION = "definition"     # å®šä¹‰æŸ¥è¯¢
    COMPARISON = "comparison"     # æ¯”è¾ƒæŸ¥è¯¢
    ANALYSIS = "analysis"         # åˆ†ææŸ¥è¯¢
    TECHNICAL = "technical"       # æŠ€æœ¯æŸ¥è¯¢
    CURRENT_EVENT = "current_event"  # æ—¶äº‹æŸ¥è¯¢


class QueryQuality(Enum):
    """æŸ¥è¯¢è´¨é‡"""
    EXCELLENT = "excellent"      # ä¼˜ç§€ - ç²¾ç¡®ã€å…·ä½“
    GOOD = "good"               # è‰¯å¥½ - è¾ƒä¸ºæ˜ç¡®  
    AVERAGE = "average"         # ä¸€èˆ¬ - æ¨¡ç³Šä½†å¯ç”¨
    POOR = "poor"               # è¾ƒå·® - è¿‡äºå®½æ³›
    VERY_POOR = "very_poor"     # å¾ˆå·® - æ— æ³•ä½¿ç”¨


@dataclass
class QueryAnalysis:
    """æŸ¥è¯¢åˆ†æç»“æœ"""
    original_query: str
    query_type: QueryType
    quality: QueryQuality
    specificity_score: float  # 0-1, è¶Šé«˜è¶Šå…·ä½“
    optimization_suggestions: List[str]
    optimized_queries: List[str]
    confidence: float


@dataclass  
class QueryPattern:
    """æŸ¥è¯¢æ¨¡å¼"""
    pattern: str
    query_type: QueryType
    quality_indicators: Dict[str, float]
    optimization_rules: List[str]


class SmartQueryOptimizer:
    """
    ğŸ” æ™ºèƒ½æŸ¥è¯¢ä¼˜åŒ–å™¨
    
    åŠŸèƒ½ï¼š
    1. åˆ†ææŸ¥è¯¢è´¨é‡å’Œç±»å‹
    2. æä¾›æŸ¥è¯¢ä¼˜åŒ–å»ºè®®
    3. å­¦ä¹ æˆåŠŸ/å¤±è´¥æ¨¡å¼
    4. ç”Ÿæˆæ›´ç²¾ç¡®çš„æŸ¥è¯¢å˜ä½“
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æŸ¥è¯¢ä¼˜åŒ–å™¨"""
        self.query_patterns = self._load_query_patterns()
        self.success_history: Dict[str, List[Dict]] = {}  # æˆåŠŸæŸ¥è¯¢å†å²
        self.failure_history: Dict[str, List[Dict]] = {}  # å¤±è´¥æŸ¥è¯¢å†å²
        self.optimization_rules = self._load_optimization_rules()
        
        logger.info("ğŸ” SmartQueryOptimizer initialized")
    
    def _load_query_patterns(self) -> List[QueryPattern]:
        """åŠ è½½æŸ¥è¯¢æ¨¡å¼"""
        return [
            # å®šä¹‰æŸ¥è¯¢æ¨¡å¼
            QueryPattern(
                pattern=r"(ä»€ä¹ˆæ˜¯|what is|define|definition of)\s+(.+)",
                query_type=QueryType.DEFINITION,
                quality_indicators={
                    "specific_term": 0.8,
                    "context_provided": 0.3,
                    "institution_mentioned": 0.6
                },
                optimization_rules=[
                    "add_context_keywords",
                    "include_institution_name", 
                    "use_official_terminology"
                ]
            ),
            
            # ä»·æ ¼æŸ¥è¯¢æ¨¡å¼
            QueryPattern(
                pattern=r"(price|cost|ä»·æ ¼|æˆæœ¬|è‚¡ä»·).*?(stock|share|è‚¡ç¥¨|å…¬å¸)",
                query_type=QueryType.PRICE,
                quality_indicators={
                    "company_name": 0.9,
                    "ticker_symbol": 0.8,
                    "time_specific": 0.6,
                    "market_specified": 0.4
                },
                optimization_rules=[
                    "add_ticker_symbol",
                    "specify_market",
                    "add_time_context",
                    "use_financial_terms"
                ]
            ),
            
            # äº‹å®æŸ¥è¯¢æ¨¡å¼
            QueryPattern(
                pattern=r"(who|what|when|where|why|how|è°|ä»€ä¹ˆ|ä½•æ—¶|å“ªé‡Œ|ä¸ºä»€ä¹ˆ|å¦‚ä½•)",
                query_type=QueryType.FACTUAL,
                quality_indicators={
                    "specific_entities": 0.7,
                    "clear_question": 0.8,
                    "context_provided": 0.5
                },
                optimization_rules=[
                    "add_entity_context",
                    "clarify_question_scope",
                    "include_relevant_keywords"
                ]
            ),
            
            # æŠ€æœ¯æŸ¥è¯¢æ¨¡å¼
            QueryPattern(
                pattern=r"(algorithm|method|technique|implementation|ç®—æ³•|æ–¹æ³•|æŠ€æœ¯|å®ç°)",
                query_type=QueryType.TECHNICAL,
                quality_indicators={
                    "technical_terms": 0.8,
                    "specific_domain": 0.7,
                    "clear_objective": 0.6
                },
                optimization_rules=[
                    "add_domain_context",
                    "specify_use_case",
                    "include_technical_keywords"
                ]
            ),
            
            # æ¯”è¾ƒæŸ¥è¯¢æ¨¡å¼
            QueryPattern(
                pattern=r"(compare|vs|versus|difference|æ¯”è¾ƒ|å¯¹æ¯”|åŒºåˆ«)",
                query_type=QueryType.COMPARISON,
                quality_indicators={
                    "clear_entities": 0.9,
                    "comparison_criteria": 0.7,
                    "specific_aspects": 0.6
                },
                optimization_rules=[
                    "clarify_comparison_criteria",
                    "specify_entities_clearly",
                    "add_context_domain"
                ]
            )
        ]
    
    def _load_optimization_rules(self) -> Dict[str, callable]:
        """åŠ è½½ä¼˜åŒ–è§„åˆ™"""
        return {
            "add_context_keywords": self._add_context_keywords,
            "include_institution_name": self._include_institution_name,
            "use_official_terminology": self._use_official_terminology,
            "add_ticker_symbol": self._add_ticker_symbol,
            "specify_market": self._specify_market,
            "add_time_context": self._add_time_context,
            "use_financial_terms": self._use_financial_terms,
            "add_entity_context": self._add_entity_context,
            "clarify_question_scope": self._clarify_question_scope,
            "include_relevant_keywords": self._include_relevant_keywords,
            "add_domain_context": self._add_domain_context,
            "specify_use_case": self._specify_use_case,
            "include_technical_keywords": self._include_technical_keywords,
            "clarify_comparison_criteria": self._clarify_comparison_criteria,
            "specify_entities_clearly": self._specify_entities_clearly,
            "add_context_domain": self._add_context_domain
        }
    
    def analyze_query(self, query: str, context: Optional[Dict[str, Any]] = None) -> QueryAnalysis:
        """
        ğŸ” åˆ†ææŸ¥è¯¢è´¨é‡å’Œç±»å‹
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            QueryAnalysis: æŸ¥è¯¢åˆ†æç»“æœ
        """
        # 1. ç¡®å®šæŸ¥è¯¢ç±»å‹
        query_type = self._identify_query_type(query)
        
        # 2. è¯„ä¼°æŸ¥è¯¢è´¨é‡
        quality, specificity_score = self._assess_query_quality(query, query_type)
        
        # 3. ç”Ÿæˆä¼˜åŒ–å»ºè®®
        suggestions = self._generate_optimization_suggestions(query, query_type, quality, context)
        
        # 4. åˆ›å»ºä¼˜åŒ–æŸ¥è¯¢
        optimized_queries = self._create_optimized_queries(query, query_type, suggestions, context)
        
        # 5. è®¡ç®—ç½®ä¿¡åº¦
        confidence = self._calculate_confidence(query, query_type, specificity_score)
        
        analysis = QueryAnalysis(
            original_query=query,
            query_type=query_type,
            quality=quality,
            specificity_score=specificity_score,
            optimization_suggestions=suggestions,
            optimized_queries=optimized_queries,
            confidence=confidence
        )
        
        logger.info(f"ğŸ” æŸ¥è¯¢åˆ†æå®Œæˆ: {query_type.value}, è´¨é‡: {quality.value}, ç½®ä¿¡åº¦: {confidence:.2f}")
        return analysis
    
    def _identify_query_type(self, query: str) -> QueryType:
        """è¯†åˆ«æŸ¥è¯¢ç±»å‹"""
        query_lower = query.lower()
        
        # ä¼˜å…ˆè¿›è¡Œå…³é”®è¯åŒ¹é…ï¼Œæ›´å‡†ç¡®
        if any(keyword in query_lower for keyword in ["ä»€ä¹ˆæ˜¯", "æ˜¯ä»€ä¹ˆ", "what is", "define", "å®šä¹‰"]):
            return QueryType.DEFINITION
        elif any(keyword in query_lower for keyword in ["price", "cost", "è‚¡ä»·", "ä»·æ ¼", "$"]):
            return QueryType.PRICE
        elif any(keyword in query_lower for keyword in ["compare", "vs", "versus", "æ¯”è¾ƒ", "å¯¹æ¯”"]):
            return QueryType.COMPARISON
        elif any(keyword in query_lower for keyword in ["algorithm", "method", "implement", "ç®—æ³•", "æ–¹æ³•", "å®ç°"]):
            return QueryType.TECHNICAL
        elif any(keyword in query_lower for keyword in ["analyze", "analysis", "calculate", "åˆ†æ", "è®¡ç®—"]):
            return QueryType.ANALYSIS
        elif any(keyword in query_lower for keyword in ["recent", "latest", "current", "æœ€æ–°", "å½“å‰", "news"]):
            return QueryType.CURRENT_EVENT
        
        # æŒ‰æ¨¡å¼åŒ¹é…
        for pattern in self.query_patterns:
            if re.search(pattern.pattern, query, re.IGNORECASE):
                return pattern.query_type
        
        # é»˜è®¤ä¸ºäº‹å®æŸ¥è¯¢
        return QueryType.FACTUAL
    
    def _assess_query_quality(self, query: str, query_type: QueryType) -> Tuple[QueryQuality, float]:
        """è¯„ä¼°æŸ¥è¯¢è´¨é‡"""
        specificity_score = 0.0
        quality_factors = []
        
        # 1. é•¿åº¦è¯„ä¼°
        word_count = len(query.split())
        if word_count >= 5:
            specificity_score += 0.2
            quality_factors.append("adequate_length")
        elif word_count <= 2:
            specificity_score -= 0.3
            quality_factors.append("too_short")
        
        # 2. ä¸“æœ‰åè¯æ£€æµ‹ï¼ˆåŒ…æ‹¬ä¸­æ–‡æœºæ„åï¼‰
        proper_nouns = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', query)
        chinese_institutions = re.findall(r'(å¤§å­¦|å­¦é™¢|ç ”ç©¶æ‰€|å…¬å¸|æœºæ„)', query)
        acronyms = re.findall(r'\b[A-Z]{2,}\b', query)  # ç¼©å†™è¯å¦‚IORA
        
        total_specificity_indicators = len(proper_nouns) + len(chinese_institutions) + len(acronyms)
        if total_specificity_indicators > 0:
            specificity_score += 0.2 * total_specificity_indicators
            quality_factors.append("contains_specific_entities")
        
        # 3. å…·ä½“æ€§æŒ‡æ ‡
        specific_indicators = [
            r'\b\d{4}\b',  # å¹´ä»½
            r'\$\d+',      # ä»·æ ¼
            r'\b[A-Z]{2,5}\b',  # è‚¡ç¥¨ä»£ç ç­‰
            r'\b(latest|current|recent|æœ€æ–°|å½“å‰|æœ€è¿‘)\b',  # æ—¶é—´é™å®š
        ]
        
        for indicator in specific_indicators:
            if re.search(indicator, query, re.IGNORECASE):
                specificity_score += 0.1
                quality_factors.append("specific_indicator")
        
        # 4. æ¨¡ç³Šæ€§æ£€æµ‹
        vague_terms = ["thing", "stuff", "something", "anything", "ä¸œè¥¿", "äº‹æƒ…", "ä»€ä¹ˆçš„"]
        if any(term in query.lower() for term in vague_terms):
            specificity_score -= 0.2
            quality_factors.append("contains_vague_terms")
        
        # 5. ç±»å‹ç‰¹å®šè¯„ä¼°
        specificity_score += self._type_specific_assessment(query, query_type)
        
        # 6. ç¡®å®šè´¨é‡ç­‰çº§
        specificity_score = max(0.0, min(1.0, specificity_score))
        
        if specificity_score >= 0.8:
            quality = QueryQuality.EXCELLENT
        elif specificity_score >= 0.6:
            quality = QueryQuality.GOOD
        elif specificity_score >= 0.4:
            quality = QueryQuality.AVERAGE
        elif specificity_score >= 0.2:
            quality = QueryQuality.POOR
        else:
            quality = QueryQuality.VERY_POOR
        
        return quality, specificity_score
    
    def _type_specific_assessment(self, query: str, query_type: QueryType) -> float:
        """ç±»å‹ç‰¹å®šçš„è´¨é‡è¯„ä¼°"""
        score_adjustment = 0.0
        
        if query_type == QueryType.DEFINITION:
            # å®šä¹‰æŸ¥è¯¢åº”è¯¥åŒ…å«å…·ä½“æœ¯è¯­
            if len(re.findall(r'\b[A-Z]+\b', query)) > 0:  # ç¼©å†™è¯
                score_adjustment += 0.2
            if any(org in query.lower() for org in ["university", "college", "institute", "å¤§å­¦", "å­¦é™¢", "ç ”ç©¶æ‰€"]):
                score_adjustment += 0.2
        
        elif query_type == QueryType.PRICE:
            # ä»·æ ¼æŸ¥è¯¢åº”è¯¥åŒ…å«å…¬å¸åæˆ–ä»£ç 
            if re.search(r'\b[A-Z]{1,5}\b', query):  # å¯èƒ½çš„è‚¡ç¥¨ä»£ç 
                score_adjustment += 0.3
            if any(term in query.lower() for term in ["stock", "share", "equity", "è‚¡ç¥¨", "è‚¡ä»½"]):
                score_adjustment += 0.1
        
        elif query_type == QueryType.TECHNICAL:
            # æŠ€æœ¯æŸ¥è¯¢åº”è¯¥åŒ…å«ä¸“ä¸šæœ¯è¯­
            technical_terms = ["algorithm", "method", "implementation", "framework", "ç®—æ³•", "æ–¹æ³•", "å®ç°", "æ¡†æ¶"]
            if any(term in query.lower() for term in technical_terms):
                score_adjustment += 0.2
        
        return score_adjustment
    
    def _generate_optimization_suggestions(self, query: str, query_type: QueryType, 
                                        quality: QueryQuality, context: Optional[Dict[str, Any]]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # 1. åŸºäºè´¨é‡çš„é€šç”¨å»ºè®®
        if quality in [QueryQuality.POOR, QueryQuality.VERY_POOR]:
            suggestions.append("æŸ¥è¯¢è¿‡äºå®½æ³›ï¼Œå»ºè®®æ·»åŠ æ›´å…·ä½“çš„å…³é”®è¯")
            suggestions.append("è€ƒè™‘æ·»åŠ æ—¶é—´ã€åœ°ç‚¹æˆ–å…¶ä»–é™å®šæ¡ä»¶")
        
        # 2. åŸºäºç±»å‹çš„ç‰¹å®šå»ºè®®
        pattern = next((p for p in self.query_patterns if p.query_type == query_type), None)
        if pattern:
            suggestions.extend(self._apply_pattern_suggestions(query, pattern))
        
        # 3. åŸºäºä¸Šä¸‹æ–‡çš„å»ºè®®
        if context:
            suggestions.extend(self._generate_context_based_suggestions(query, context))
        
        # 4. åŸºäºå†å²çš„å»ºè®®
        historical_suggestions = self._get_historical_suggestions(query, query_type)
        suggestions.extend(historical_suggestions)
        
        return list(set(suggestions))  # å»é‡
    
    def _apply_pattern_suggestions(self, query: str, pattern: QueryPattern) -> List[str]:
        """åº”ç”¨æ¨¡å¼ç‰¹å®šçš„å»ºè®®"""
        suggestions = []
        
        for rule_name in pattern.optimization_rules:
            if rule_name in self.optimization_rules:
                try:
                    rule_func = self.optimization_rules[rule_name]
                    suggestion = rule_func(query, pattern)
                    if suggestion:
                        suggestions.append(suggestion)
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¼˜åŒ–è§„åˆ™ {rule_name} æ‰§è¡Œå¤±è´¥: {e}")
        
        return suggestions
    
    def _create_optimized_queries(self, query: str, query_type: QueryType, 
                                suggestions: List[str], context: Optional[Dict[str, Any]]) -> List[str]:
        """åˆ›å»ºä¼˜åŒ–åçš„æŸ¥è¯¢"""
        optimized_queries = []
        
        # 1. åŸºäºå»ºè®®åˆ›å»ºä¼˜åŒ–æŸ¥è¯¢
        for suggestion in suggestions[:3]:  # æœ€å¤š3ä¸ªå»ºè®®
            optimized_query = self._apply_suggestion_to_query(query, suggestion, context)
            if optimized_query and optimized_query != query:
                optimized_queries.append(optimized_query)
        
        # 2. åŸºäºç±»å‹åˆ›å»ºç‰¹å®šä¼˜åŒ–
        type_optimized = self._create_type_specific_optimizations(query, query_type, context)
        optimized_queries.extend(type_optimized)
        
        # 3. åŸºäºæˆåŠŸå†å²åˆ›å»ºå˜ä½“
        historical_optimized = self._create_historical_variations(query, query_type)
        optimized_queries.extend(historical_optimized)
        
        # 4. å»é‡å¹¶æ’åº
        unique_queries = list(set(optimized_queries))
        return unique_queries[:5]  # æœ€å¤šè¿”å›5ä¸ªä¼˜åŒ–æŸ¥è¯¢
    
    def _calculate_confidence(self, query: str, query_type: QueryType, specificity_score: float) -> float:
        """è®¡ç®—ä¼˜åŒ–ä¿¡å¿ƒåº¦"""
        base_confidence = specificity_score
        
        # åŸºäºå†å²æˆåŠŸç‡è°ƒæ•´
        historical_success_rate = self._get_historical_success_rate(query_type)
        
        # åŸºäºæŸ¥è¯¢å¤æ‚åº¦è°ƒæ•´
        complexity_factor = len(query.split()) / 10  # ç®€å•çš„å¤æ‚åº¦è¯„ä¼°
        
        # ç»¼åˆè®¡ç®—
        confidence = (base_confidence * 0.6 + 
                     historical_success_rate * 0.3 + 
                     complexity_factor * 0.1)
        
        return min(1.0, max(0.0, confidence))
    
    def record_query_result(self, query: str, query_type: QueryType, 
                          success: bool, result_content: Optional[str] = None):
        """
        ğŸ“Š è®°å½•æŸ¥è¯¢ç»“æœç”¨äºå­¦ä¹ 
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            query_type: æŸ¥è¯¢ç±»å‹
            success: æ˜¯å¦æˆåŠŸ
            result_content: ç»“æœå†…å®¹
        """
        record = {
            "query": query,
            "timestamp": datetime.now().isoformat(),
            "result_content": result_content,
            "query_length": len(query.split()),
            "specificity_score": self._assess_query_quality(query, query_type)[1]
        }
        
        type_key = query_type.value
        
        if success:
            if type_key not in self.success_history:
                self.success_history[type_key] = []
            self.success_history[type_key].append(record)
            logger.info(f"âœ… è®°å½•æˆåŠŸæŸ¥è¯¢: {query[:50]}...")
        else:
            if type_key not in self.failure_history:
                self.failure_history[type_key] = []
            self.failure_history[type_key].append(record)
            logger.info(f"âŒ è®°å½•å¤±è´¥æŸ¥è¯¢: {query[:50]}...")
        
        # é™åˆ¶å†å²è®°å½•å¤§å°
        self._trim_history()
    
    def _trim_history(self, max_records: int = 100):
        """ä¿®å‰ªå†å²è®°å½•"""
        for type_key in self.success_history:
            if len(self.success_history[type_key]) > max_records:
                self.success_history[type_key] = self.success_history[type_key][-max_records:]
        
        for type_key in self.failure_history:
            if len(self.failure_history[type_key]) > max_records:
                self.failure_history[type_key] = self.failure_history[type_key][-max_records:]
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_success_queries": sum(len(records) for records in self.success_history.values()),
            "total_failure_queries": sum(len(records) for records in self.failure_history.values()),
            "success_by_type": {},
            "failure_by_type": {},
            "success_rate_by_type": {}
        }
        
        for query_type in QueryType:
            type_key = query_type.value
            success_count = len(self.success_history.get(type_key, []))
            failure_count = len(self.failure_history.get(type_key, []))
            total_count = success_count + failure_count
            
            stats["success_by_type"][type_key] = success_count
            stats["failure_by_type"][type_key] = failure_count
            
            if total_count > 0:
                stats["success_rate_by_type"][type_key] = success_count / total_count
            else:
                stats["success_rate_by_type"][type_key] = 0.0
        
        return stats
    
    # ä¼˜åŒ–è§„åˆ™å®ç°
    def _add_context_keywords(self, query: str, pattern: QueryPattern) -> str:
        """æ·»åŠ ä¸Šä¸‹æ–‡å…³é”®è¯"""
        if "å¤§å­¦" in query or "university" in query.lower():
            return "å»ºè®®æ·»åŠ å…·ä½“çš„å­¦é™¢ã€ç³»åˆ«æˆ–é¡¹ç›®åç§°æ¥æé«˜æŸ¥è¯¢ç²¾ç¡®åº¦"
        return "å»ºè®®æ·»åŠ ç›¸å…³çš„ä¸Šä¸‹æ–‡å…³é”®è¯"
    
    def _include_institution_name(self, query: str, pattern: QueryPattern) -> str:
        """åŒ…å«æœºæ„åç§°"""
        return "å»ºè®®åœ¨æŸ¥è¯¢ä¸­åŒ…å«å®Œæ•´çš„æœºæ„æˆ–ç»„ç»‡åç§°"
    
    def _use_official_terminology(self, query: str, pattern: QueryPattern) -> str:
        """ä½¿ç”¨å®˜æ–¹æœ¯è¯­"""
        return "å»ºè®®ä½¿ç”¨å®˜æ–¹æˆ–æ­£å¼çš„æœ¯è¯­æ›¿ä»£å£è¯­åŒ–è¡¨è¾¾"
    
    def _add_ticker_symbol(self, query: str, pattern: QueryPattern) -> str:
        """æ·»åŠ è‚¡ç¥¨ä»£ç """
        return "å»ºè®®æ·»åŠ å…¬å¸çš„è‚¡ç¥¨ä»£ç ï¼ˆå¦‚AAPL, GOOGLï¼‰ä»¥æé«˜æŸ¥è¯¢ç²¾ç¡®åº¦"
    
    def _specify_market(self, query: str, pattern: QueryPattern) -> str:
        """æŒ‡å®šå¸‚åœº"""
        return "å»ºè®®æŒ‡å®šå…·ä½“çš„è‚¡ç¥¨å¸‚åœºï¼ˆå¦‚NASDAQ, NYSE, ä¸Šäº¤æ‰€ï¼‰"
    
    def _add_time_context(self, query: str, pattern: QueryPattern) -> str:
        """æ·»åŠ æ—¶é—´ä¸Šä¸‹æ–‡"""
        return "å»ºè®®æ·»åŠ æ—¶é—´é™å®šï¼ˆå¦‚'æœ€æ–°'ã€'å½“å‰'ã€'2024å¹´'ï¼‰"
    
    def _use_financial_terms(self, query: str, pattern: QueryPattern) -> str:
        """ä½¿ç”¨é‡‘èæœ¯è¯­"""
        return "å»ºè®®ä½¿ç”¨ä¸“ä¸šçš„é‡‘èæœ¯è¯­ï¼ˆå¦‚'å¸‚å€¼'ã€'è‚¡ä»·'ã€'äº¤æ˜“ä»·æ ¼'ï¼‰"
    
    def _add_entity_context(self, query: str, pattern: QueryPattern) -> str:
        """æ·»åŠ å®ä½“ä¸Šä¸‹æ–‡"""
        return "å»ºè®®ä¸ºæåˆ°çš„å®ä½“æ·»åŠ æ›´å¤šæè¿°æ€§ä¿¡æ¯"
    
    def _clarify_question_scope(self, query: str, pattern: QueryPattern) -> str:
        """æ˜ç¡®é—®é¢˜èŒƒå›´"""
        return "å»ºè®®æ˜ç¡®é—®é¢˜çš„å…·ä½“èŒƒå›´å’Œè¾¹ç•Œ"
    
    def _include_relevant_keywords(self, query: str, pattern: QueryPattern) -> str:
        """åŒ…å«ç›¸å…³å…³é”®è¯"""
        return "å»ºè®®æ·»åŠ ä¸ä¸»é¢˜ç›¸å…³çš„ä¸“ä¸šå…³é”®è¯"
    
    def _add_domain_context(self, query: str, pattern: QueryPattern) -> str:
        """æ·»åŠ é¢†åŸŸä¸Šä¸‹æ–‡"""
        return "å»ºè®®æŒ‡å®šå…·ä½“çš„æŠ€æœ¯é¢†åŸŸæˆ–åº”ç”¨åœºæ™¯"
    
    def _specify_use_case(self, query: str, pattern: QueryPattern) -> str:
        """æŒ‡å®šç”¨ä¾‹"""
        return "å»ºè®®æ˜ç¡®æŠ€æœ¯çš„å…·ä½“åº”ç”¨åœºæ™¯æˆ–ç”¨ä¾‹"
    
    def _include_technical_keywords(self, query: str, pattern: QueryPattern) -> str:
        """åŒ…å«æŠ€æœ¯å…³é”®è¯"""
        return "å»ºè®®æ·»åŠ ç›¸å…³çš„æŠ€æœ¯æœ¯è¯­å’Œä¸“ä¸šè¯æ±‡"
    
    def _clarify_comparison_criteria(self, query: str, pattern: QueryPattern) -> str:
        """æ˜ç¡®æ¯”è¾ƒæ ‡å‡†"""
        return "å»ºè®®æ˜ç¡®æ¯”è¾ƒçš„å…·ä½“æ ‡å‡†å’Œç»´åº¦"
    
    def _specify_entities_clearly(self, query: str, pattern: QueryPattern) -> str:
        """æ˜ç¡®æŒ‡å®šå®ä½“"""
        return "å»ºè®®æ¸…æ¥šåœ°æŒ‡å®šè¦æ¯”è¾ƒçš„å¯¹è±¡æˆ–å®ä½“"
    
    def _add_context_domain(self, query: str, pattern: QueryPattern) -> str:
        """æ·»åŠ ä¸Šä¸‹æ–‡é¢†åŸŸ"""
        return "å»ºè®®æŒ‡å®šæ¯”è¾ƒçš„å…·ä½“é¢†åŸŸæˆ–ä¸Šä¸‹æ–‡"
    
    def _apply_suggestion_to_query(self, query: str, suggestion: str, 
                                 context: Optional[Dict[str, Any]]) -> Optional[str]:
        """å°†å»ºè®®åº”ç”¨åˆ°æŸ¥è¯¢ä¸­"""
        # åŸºäºå»ºè®®ç±»å‹å®é™…ä¿®æ”¹æŸ¥è¯¢
        query_lower = query.lower()
        
        # æ·»åŠ æ—¶é—´é™å®š
        if "æ—¶é—´é™å®š" in suggestion or "æœ€æ–°" in suggestion:
            if "æœ€æ–°" not in query_lower and "current" not in query_lower:
                return f"{query} æœ€æ–°"
        
        # æ·»åŠ è‚¡ç¥¨ä»£ç 
        if "è‚¡ç¥¨ä»£ç " in suggestion or "ticker" in suggestion:
            if context and "companies" in context:
                companies = context["companies"]
                if "apple" in query_lower and "AAPL" not in query:
                    return query.replace("Apple", "Apple (AAPL)")
        
        # æ·»åŠ å…·ä½“å…³é”®è¯
        if "å…·ä½“" in suggestion and "å…³é”®è¯" in suggestion:
            if "iora" in query_lower and "ç ”ç©¶æ‰€" not in query_lower:
                return query.replace("iora", "IORAç ”ç©¶æ‰€")
        
        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        if "ä¸Šä¸‹æ–‡" in suggestion and context:
            if "companies" in context:
                companies = context["companies"][:1]  # å–ç¬¬ä¸€ä¸ªå…¬å¸
                return f"{query} ({', '.join(companies)})"
        
        return None  # æ— æ³•åº”ç”¨å»ºè®®æ—¶è¿”å›None
    
    def _create_type_specific_optimizations(self, query: str, query_type: QueryType, 
                                          context: Optional[Dict[str, Any]]) -> List[str]:
        """åˆ›å»ºç±»å‹ç‰¹å®šçš„ä¼˜åŒ–"""
        optimizations = []
        
        if query_type == QueryType.DEFINITION and "iora" in query.lower():
            optimizations.extend([
                "National University of Singapore IORA meaning",
                "NUS IORA institute operations research analytics", 
                "Singapore university IORA department"
            ])
        elif query_type == QueryType.PRICE and "apple" in query.lower():
            optimizations.extend([
                "AAPL stock price current",
                "Apple Inc AAPL share price latest",
                "Apple company stock market price today"
            ])
        
        return optimizations
    
    def _create_historical_variations(self, query: str, query_type: QueryType) -> List[str]:
        """åŸºäºå†å²åˆ›å»ºæŸ¥è¯¢å˜ä½“"""
        variations = []
        
        type_key = query_type.value
        if type_key in self.success_history:
            # åˆ†ææˆåŠŸæŸ¥è¯¢çš„æ¨¡å¼
            successful_queries = self.success_history[type_key][-10:]  # æœ€è¿‘10ä¸ªæˆåŠŸæŸ¥è¯¢
            
            # æå–å¸¸è§çš„æˆåŠŸæ¨¡å¼
            for record in successful_queries:
                successful_query = record["query"]
                if self._queries_similar(query, successful_query):
                    variations.append(successful_query)
        
        return variations[:2]  # æœ€å¤š2ä¸ªå†å²å˜ä½“
    
    def _queries_similar(self, query1: str, query2: str) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªæŸ¥è¯¢æ˜¯å¦ç›¸ä¼¼"""
        words1 = set(query1.lower().split())
        words2 = set(query2.lower().split())
        
        # ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        if len(union) == 0:
            return False
        
        similarity = len(intersection) / len(union)
        return similarity > 0.3  # 30%ä»¥ä¸Šç›¸ä¼¼åº¦
    
    def _generate_context_based_suggestions(self, query: str, context: Dict[str, Any]) -> List[str]:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå»ºè®®"""
        suggestions = []
        
        # å¦‚æœä¸Šä¸‹æ–‡ä¸­æœ‰ä»·æ ¼ä¿¡æ¯ï¼Œå»ºè®®ä½¿ç”¨
        if "price" in str(context).lower() or "cost" in str(context).lower():
            suggestions.append("å¯ä»¥å¼•ç”¨ä¹‹å‰è·å–çš„ä»·æ ¼ä¿¡æ¯è¿›è¡Œæ›´ç²¾ç¡®çš„æŸ¥è¯¢")
        
        # å¦‚æœä¸Šä¸‹æ–‡ä¸­æœ‰å…¬å¸ä¿¡æ¯ï¼Œå»ºè®®ä½¿ç”¨
        if any(term in str(context).lower() for term in ["company", "corp", "inc", "ltd"]):
            suggestions.append("å»ºè®®ä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­æåˆ°çš„å…·ä½“å…¬å¸ä¿¡æ¯")
        
        return suggestions
    
    def _get_historical_suggestions(self, query: str, query_type: QueryType) -> List[str]:
        """è·å–åŸºäºå†å²çš„å»ºè®®"""
        suggestions = []
        
        type_key = query_type.value
        
        # åˆ†æå¤±è´¥æŸ¥è¯¢çš„æ¨¡å¼
        if type_key in self.failure_history:
            failure_patterns = self._analyze_failure_patterns(self.failure_history[type_key])
            for pattern in failure_patterns:
                suggestions.append(f"é¿å…ä½¿ç”¨è¿‡äº{pattern}çš„è¡¨è¾¾æ–¹å¼")
        
        return suggestions
    
    def _analyze_failure_patterns(self, failure_records: List[Dict]) -> List[str]:
        """åˆ†æå¤±è´¥æ¨¡å¼"""
        patterns = []
        
        # åˆ†æå¤±è´¥æŸ¥è¯¢çš„å…±åŒç‰¹å¾
        short_queries = sum(1 for record in failure_records if record["query_length"] <= 3)
        total_queries = len(failure_records)
        
        if total_queries > 0:
            if short_queries / total_queries > 0.5:
                patterns.append("ç®€çŸ­")
        
        return patterns
    
    def _get_historical_success_rate(self, query_type: QueryType) -> float:
        """è·å–å†å²æˆåŠŸç‡"""
        type_key = query_type.value
        
        success_count = len(self.success_history.get(type_key, []))
        failure_count = len(self.failure_history.get(type_key, []))
        total_count = success_count + failure_count
        
        if total_count == 0:
            return 0.5  # é»˜è®¤æˆåŠŸç‡
        
        return success_count / total_count