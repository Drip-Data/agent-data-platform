"""
å¢å¼ºLLMå®¢æˆ·ç«¯ - æ”¶é›†è¯¦ç»†çš„ä»¤ç‰Œä½¿ç”¨å’Œæˆæœ¬ä¿¡æ¯
ä¸ºè½¨è¿¹è®°å½•ç³»ç»Ÿæä¾›æ›´ä¸°å¯Œçš„LLMäº¤äº’å…ƒæ•°æ®
"""

import time
import logging
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from core.llm_client import LLMClient
from core.interfaces import LLMInteraction

logger = logging.getLogger(__name__)

@dataclass
class LLMResponse:
    """å¢å¼ºçš„LLMå“åº”ï¼ŒåŒ…å«è¯¦ç»†å…ƒæ•°æ®"""
    content: str
    provider: str
    model: str
    token_usage: Dict[str, Any]
    cost_info: Dict[str, Any]
    response_time: float
    metadata: Dict[str, Any]

class EnhancedLLMClient:
    """å¢å¼ºLLMå®¢æˆ·ç«¯åŒ…è£…å™¨"""
    
    def __init__(self, base_client: LLMClient):
        self.base_client = base_client
        self.interaction_history: List[LLMInteraction] = []
        
        # ğŸ”§ ç§»é™¤å†—ä½™çš„æˆæœ¬è®¡ç®—é…ç½® - ç»Ÿä¸€ç”±CostAnalyzerå¤„ç†
        # ä¿ç•™interaction_historyç”¨äºè½¨è¿¹è®°å½•
    
    def estimate_token_count(self, text: str) -> int:
        """ä¼°ç®—ä»¤ç‰Œæ•°é‡ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # ç®€åŒ–çš„ä»¤ç‰Œä¼°ç®—ï¼šè‹±æ–‡çº¦4å­—ç¬¦/ä»¤ç‰Œï¼Œä¸­æ–‡çº¦1.5å­—ç¬¦/ä»¤ç‰Œ
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        other_chars = len(text) - chinese_chars
        
        estimated_tokens = int(chinese_chars / 1.5 + other_chars / 4)
        return max(estimated_tokens, 1)  # è‡³å°‘1ä¸ªä»¤ç‰Œ
    
    # ğŸ”§ ç§»é™¤å†—ä½™çš„æˆæœ¬è®¡ç®—æ–¹æ³• - ç»Ÿä¸€ç”±CostAnalyzerå¤„ç†
    # calculate_costæ–¹æ³•å·²åˆ é™¤ï¼Œé¿å…ä¸CostAnalyzeré‡å¤
    
    async def enhanced_call_api(self, messages: List[Dict[str, Any]], context: str = "unknown") -> LLMResponse:
        """å¢å¼ºçš„APIè°ƒç”¨ï¼Œæ”¶é›†è¯¦ç»†å…ƒæ•°æ®"""
        start_time = time.time()
        
        # ä¼°ç®—è¾“å…¥ä»¤ç‰Œæ•°
        input_text = " ".join([msg.get('content', '') for msg in messages])
        estimated_input_tokens = self.estimate_token_count(input_text)
        
        # è·å–æä¾›å•†å’Œæ¨¡å‹ä¿¡æ¯
        provider = self.base_client.provider.value if hasattr(self.base_client.provider, 'value') else str(self.base_client.provider)
        model = getattr(self.base_client.provider_instance, 'get_default_model', lambda: 'unknown')()
        
        try:
            # è°ƒç”¨åŸºç¡€å®¢æˆ·ç«¯ï¼ˆæ–°æ ¼å¼ï¼šè¿”å›å­—å…¸åŒ…å«contentå’Œusageï¼‰
            response_data = await self.base_client._call_api(messages)
            
            response_time = time.time() - start_time
            response_content = response_data.get('content', '')
            real_usage = response_data.get('usage')
            
            # ğŸ”§ ä¼˜å…ˆä½¿ç”¨çœŸå®tokenæ•°æ®ï¼Œå›é€€åˆ°ä¼°ç®—
            if real_usage and real_usage.get('data_source') == 'real_api':
                # ä½¿ç”¨çœŸå®APIè¿”å›çš„tokenæ•°æ®
                token_usage = real_usage
                logger.info(f"âœ… ä½¿ç”¨çœŸå®tokenæ•°æ®: prompt={real_usage['prompt_tokens']}, completion={real_usage['completion_tokens']}")
                
                # ğŸ”§ æˆæœ¬è®¡ç®—ç”±CostAnalyzerç»Ÿä¸€å¤„ç†ï¼Œæ­¤å¤„ä¸å†è®¡ç®—
                cost_info = {
                    'note': 'Cost calculation delegated to CostAnalyzer',
                    'data_source': 'real_api',
                    'model': model
                }
                
            else:
                # å›é€€åˆ°ä¼°ç®—æ¨¡å¼
                logger.warning("âš ï¸ æœªè·å–åˆ°çœŸå®tokenæ•°æ®ï¼Œä½¿ç”¨ä¼°ç®—æ¨¡å¼")
                estimated_output_tokens = self.estimate_token_count(response_content)
                
                # æ„å»ºä¼°ç®—çš„ä»¤ç‰Œä½¿ç”¨ä¿¡æ¯
                token_usage = {
                    'prompt_tokens': estimated_input_tokens,
                    'completion_tokens': estimated_output_tokens,
                    'total_tokens': estimated_input_tokens + estimated_output_tokens,
                    'data_source': 'estimation',
                    'model': model,
                    'estimation_method': 'character_based'
                }
                
                # ğŸ”§ æˆæœ¬è®¡ç®—ç”±CostAnalyzerç»Ÿä¸€å¤„ç†
                cost_info = {
                    'note': 'Cost calculation delegated to CostAnalyzer',
                    'data_source': 'estimation',
                    'model': model
                }
            
            # åˆ›å»ºLLMäº¤äº’è®°å½•
            interaction = LLMInteraction(
                provider=provider,
                model=model,
                context=context,
                prompt=input_text[:1000] + "..." if len(input_text) > 1000 else input_text,
                prompt_length=len(input_text),
                response=response_content[:1000] + "..." if len(response_content) > 1000 else response_content,
                response_length=len(response_content),
                response_time=response_time,
                token_usage=token_usage,
                cost_info=cost_info,
                success=True
            )
            
            self.interaction_history.append(interaction)
            
            return LLMResponse(
                content=response_content,
                provider=provider,
                model=model,
                token_usage=token_usage,
                cost_info=cost_info,
                response_time=response_time,
                metadata={
                    'context': context,
                    'interaction_id': interaction.interaction_id,
                    'timestamp': start_time
                }
            )
            
        except Exception as e:
            response_time = time.time() - start_time
            
            # è®°å½•å¤±è´¥çš„äº¤äº’
            interaction = LLMInteraction(
                provider=provider,
                model=model,
                context=context,
                prompt=input_text[:1000] + "..." if len(input_text) > 1000 else input_text,
                prompt_length=len(input_text),
                response_time=response_time,
                success=False,
                error_message=str(e)
            )
            
            self.interaction_history.append(interaction)
            
            raise e
    
    def get_accumulated_metrics(self) -> Dict[str, Any]:
        """è·å–ç´¯ç§¯çš„LLMä½¿ç”¨æŒ‡æ ‡"""
        total_interactions = len(self.interaction_history)
        total_cost = sum(interaction.cost_info.get('total_cost', 0) for interaction in self.interaction_history)
        total_tokens = sum(interaction.token_usage.get('total_tokens', 0) for interaction in self.interaction_history)
        total_response_time = sum(interaction.response_time for interaction in self.interaction_history)
        
        successful_interactions = sum(1 for interaction in self.interaction_history if interaction.success)
        
        return {
            'total_interactions': total_interactions,
            'successful_interactions': successful_interactions,
            'success_rate': successful_interactions / max(total_interactions, 1),
            'total_cost': round(total_cost, 6),
            'total_tokens': total_tokens,
            'total_response_time': round(total_response_time, 3),
            'average_response_time': round(total_response_time / max(total_interactions, 1), 3)
        }
    
    def clear_history(self):
        """æ¸…é™¤äº¤äº’å†å²"""
        self.interaction_history.clear()
    
    # ä»£ç†æ‰€æœ‰å…¶ä»–æ–¹æ³•åˆ°åŸºç¡€å®¢æˆ·ç«¯
    def __getattr__(self, name):
        return getattr(self.base_client, name)