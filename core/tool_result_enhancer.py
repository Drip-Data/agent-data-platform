#!/usr/bin/env python3
"""
ğŸ”§ å·¥å…·ç»“æœå¢å¼ºå™¨ (Tool Result Enhancer)
ä¼˜åŒ–å·¥å…·æ‰§è¡Œç»“æœçš„ç»“æ„åŒ–å­˜å‚¨å’Œå¤„ç†ï¼Œç¡®ä¿ä¿¡æ¯æœ‰æ•ˆä¼ é€’
"""

import logging
import json
import re
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum

logger = logging.getLogger(__name__)


class ResultType(Enum):
    """ç»“æœç±»å‹"""
    SUCCESS = "success"
    PARTIAL_SUCCESS = "partial_success"
    FAILURE = "failure"
    ERROR = "error"
    EMPTY = "empty"


class DataFormat(Enum):
    """æ•°æ®æ ¼å¼"""
    TEXT = "text"
    JSON = "json"
    TABLE = "table"
    LIST = "list"
    NUMBER = "number"
    URL = "url"
    CODE = "code"
    MIXED = "mixed"


@dataclass
class EnhancedResult:
    """å¢å¼ºçš„å·¥å…·æ‰§è¡Œç»“æœ"""
    tool_name: str
    original_result: Any
    result_type: ResultType
    data_format: DataFormat
    extracted_data: Dict[str, Any]
    confidence_score: float
    processing_timestamp: datetime
    metadata: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        result = asdict(self)
        result['processing_timestamp'] = self.processing_timestamp.isoformat()
        result['result_type'] = self.result_type.value
        result['data_format'] = self.data_format.value
        return result


@dataclass
class ExtractionRule:
    """æ•°æ®æå–è§„åˆ™"""
    name: str
    pattern: str
    target_field: str
    data_type: str
    priority: int
    post_process: Optional[str] = None


class ToolResultEnhancer:
    """
    ğŸ”§ å·¥å…·ç»“æœå¢å¼ºå™¨
    
    åŠŸèƒ½ï¼š
    1. æ™ºèƒ½åˆ†æå·¥å…·æ‰§è¡Œç»“æœ
    2. æå–å…³é”®æ•°æ®å’Œä¿¡æ¯
    3. ç»“æ„åŒ–å­˜å‚¨å¤„ç†ç»“æœ
    4. æä¾›ä¸‹æ¸¸å·¥å…·å¯ç”¨çš„æ•°æ®æ ¼å¼
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å·¥å…·ç»“æœå¢å¼ºå™¨"""
        self.extraction_rules = self._load_extraction_rules()
        self.tool_specific_processors = self._initialize_tool_processors()
        
        logger.info("ğŸ”§ ToolResultEnhancer initialized")
    
    def _load_extraction_rules(self) -> Dict[str, List[ExtractionRule]]:
        """åŠ è½½æ•°æ®æå–è§„åˆ™"""
        return {
            "deepsearch": [
                ExtractionRule(
                    name="price_extraction",
                    pattern=r'\$?(\d+\.?\d*)\s*(?:per\s+share|each|USD|dollars?|ç¾å…ƒ)',
                    target_field="prices",
                    data_type="number",
                    priority=10,
                    post_process="parse_float"
                ),
                ExtractionRule(
                    name="url_extraction", 
                    pattern=r'https?://[^\s<>"{}|\\^`[\]]+',
                    target_field="urls",
                    data_type="list",
                    priority=8
                ),
                ExtractionRule(
                    name="company_name",
                    pattern=r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*(?:\s+(?:Inc|Corp|Ltd|LLC|Company|Co)\.?))\b',
                    target_field="companies",
                    data_type="list",
                    priority=7
                ),
                ExtractionRule(
                    name="key_facts",
                    pattern=r'(?:ç»“æœ|æ˜¾ç¤º|è¡¨æ˜|indicates?|shows?|reveals?)[:ï¼š]\s*([^.!?]*[.!?])',
                    target_field="key_facts",
                    data_type="list",
                    priority=6
                )
            ],
            
            "browser_use": [
                ExtractionRule(
                    name="page_title",
                    pattern=r'<title[^>]*>(.*?)</title>',
                    target_field="page_title",
                    data_type="text",
                    priority=9
                ),
                ExtractionRule(
                    name="stock_price",
                    pattern=r'(?:Price|è‚¡ä»·|Current|å½“å‰)[:ï¼š]?\s*\$?(\d+\.?\d*)',
                    target_field="stock_prices",
                    data_type="number",
                    priority=10,
                    post_process="parse_float"
                ),
                ExtractionRule(
                    name="links",
                    pattern=r'href=["\'](https?://[^"\']+)["\']',
                    target_field="links",
                    data_type="list",
                    priority=5
                ),
                ExtractionRule(
                    name="error_messages",
                    pattern=r'(?:Error|Failed|é”™è¯¯|å¤±è´¥)[:ï¼š]\s*([^\n]+)',
                    target_field="errors",
                    data_type="list",
                    priority=8
                )
            ],
            
            "microsandbox": [
                ExtractionRule(
                    name="calculation_result",
                    pattern=r'(?:Result|ç»“æœ|Answer|ç­”æ¡ˆ)[:ï¼š]?\s*([^\n]+)',
                    target_field="calculations",
                    data_type="text",
                    priority=10
                ),
                ExtractionRule(
                    name="numeric_output",
                    pattern=r'\b(\d+\.?\d*(?:[eE][+-]?\d+)?)\b',
                    target_field="numbers",
                    data_type="number",
                    priority=8,
                    post_process="parse_float"
                ),
                ExtractionRule(
                    name="python_errors",
                    pattern=r'((?:Traceback|Error|Exception)[^\n]*(?:\n[^\n]*)*)',
                    target_field="python_errors",
                    data_type="list",
                    priority=9
                ),
                ExtractionRule(
                    name="variable_assignments",
                    pattern=r'(\w+)\s*=\s*([^\n]+)',
                    target_field="variables",
                    data_type="dict",
                    priority=6
                )
            ],
            
            "search_tool": [
                ExtractionRule(
                    name="file_paths",
                    pattern=r'([^\s]+\.[a-zA-Z]{2,4})',
                    target_field="file_paths",
                    data_type="list",
                    priority=8
                ),
                ExtractionRule(
                    name="search_matches",
                    pattern=r'Found:\s*([^\n]+)',
                    target_field="matches",
                    data_type="list",
                    priority=7
                )
            ]
        }
    
    def _initialize_tool_processors(self) -> Dict[str, callable]:
        """åˆå§‹åŒ–å·¥å…·ç‰¹å®šçš„å¤„ç†å™¨"""
        return {
            "deepsearch": self._process_deepsearch_result,
            "browser_use": self._process_browser_result,
            "microsandbox": self._process_microsandbox_result,
            "search_tool": self._process_search_result
        }
    
    def enhance_tool_result(self, tool_name: str, raw_result: Any, 
                          execution_context: Optional[Dict[str, Any]] = None) -> EnhancedResult:
        """
        ğŸ”§ å¢å¼ºå·¥å…·æ‰§è¡Œç»“æœ
        
        Args:
            tool_name: å·¥å…·åç§°
            raw_result: åŸå§‹ç»“æœ
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            EnhancedResult: å¢å¼ºåçš„ç»“æœ
        """
        processing_start = datetime.now()
        
        # 1. ç¡®å®šç»“æœç±»å‹
        result_type = self._determine_result_type(raw_result)
        
        # 2. ç¡®å®šæ•°æ®æ ¼å¼
        data_format = self._determine_data_format(raw_result)
        
        # 3. æå–ç»“æ„åŒ–æ•°æ®
        extracted_data = self._extract_structured_data(tool_name, raw_result)
        
        # 4. åº”ç”¨å·¥å…·ç‰¹å®šå¤„ç†
        if tool_name in self.tool_specific_processors:
            try:
                enhanced_data = self.tool_specific_processors[tool_name](raw_result, extracted_data)
                extracted_data.update(enhanced_data)
            except Exception as e:
                logger.warning(f"âš ï¸ å·¥å…·ç‰¹å®šå¤„ç†å¤±è´¥ ({tool_name}): {e}")
        
        # 5. è®¡ç®—ç½®ä¿¡åº¦
        confidence_score = self._calculate_confidence(result_type, extracted_data, raw_result)
        
        # 6. ç”Ÿæˆå…ƒæ•°æ®
        metadata = self._generate_metadata(tool_name, raw_result, execution_context)
        
        enhanced_result = EnhancedResult(
            tool_name=tool_name,
            original_result=raw_result,
            result_type=result_type,
            data_format=data_format,
            extracted_data=extracted_data,
            confidence_score=confidence_score,
            processing_timestamp=processing_start,
            metadata=metadata
        )
        
        logger.info(f"ğŸ”§ ç»“æœå¢å¼ºå®Œæˆ: {tool_name}, ç±»å‹: {result_type.value}, ç½®ä¿¡åº¦: {confidence_score:.2f}")
        return enhanced_result
    
    def _determine_result_type(self, raw_result: Any) -> ResultType:
        """ç¡®å®šç»“æœç±»å‹"""
        if raw_result is None:
            return ResultType.EMPTY
        
        result_str = str(raw_result).lower()
        
        # æ£€æŸ¥é”™è¯¯æŒ‡ç¤º
        error_indicators = ["error", "failed", "exception", "timeout", "é”™è¯¯", "å¤±è´¥", "å¼‚å¸¸"]
        if any(indicator in result_str for indicator in error_indicators):
            return ResultType.ERROR
        
        # æ£€æŸ¥ç©ºç»“æœ
        if not result_str.strip() or result_str.strip() in ["none", "null", ""]:
            return ResultType.EMPTY
        
        # ç‰¹æ®Šæƒ…å†µï¼šæ˜ç¡®æŒ‡ç¤ºå¤±è´¥ï¼ˆä¼˜å…ˆæ£€æŸ¥ï¼‰
        failure_indicators = ["no results", "not found", "no data", "æ— ç»“æœ", "æœªæ‰¾åˆ°"]
        if any(indicator in result_str for indicator in failure_indicators):
            return ResultType.FAILURE
        
        # æ£€æŸ¥éƒ¨åˆ†æˆåŠŸ
        partial_indicators = ["partial", "incomplete", "some", "éƒ¨åˆ†", "ä¸å®Œæ•´"]
        if any(indicator in result_str for indicator in partial_indicators):
            return ResultType.PARTIAL_SUCCESS
        
        # æ£€æŸ¥æˆåŠŸæŒ‡ç¤º
        success_indicators = ["success", "completed", "found", "result", "æˆåŠŸ", "å®Œæˆ", "æ‰¾åˆ°", "ç»“æœ"]
        if any(indicator in result_str for indicator in success_indicators):
            return ResultType.SUCCESS
        
        # å¦‚æœæœ‰å®è´¨å†…å®¹ï¼Œè®¤ä¸ºæ˜¯æˆåŠŸ
        if len(result_str.strip()) > 10:
            return ResultType.SUCCESS
        
        return ResultType.SUCCESS  # é»˜è®¤ä¸ºæˆåŠŸï¼Œé™¤éæ˜ç¡®æŒ‡ç¤ºå¤±è´¥
    
    def _determine_data_format(self, raw_result: Any) -> DataFormat:
        """ç¡®å®šæ•°æ®æ ¼å¼"""
        if raw_result is None:
            return DataFormat.TEXT
        
        # æ£€æŸ¥JSONæ ¼å¼
        if isinstance(raw_result, (dict, list)):
            return DataFormat.JSON
        
        result_str = str(raw_result)
        
        # å°è¯•è§£æJSONï¼ˆä½†æ’é™¤çº¯æ•°å­—ï¼‰
        try:
            parsed = json.loads(result_str)
            # å¦‚æœè§£ææˆåŠŸä½†æ˜¯çº¯æ•°å­—ï¼Œä¸ç®—JSONæ ¼å¼
            if isinstance(parsed, (int, float)) and result_str.replace('.', '').replace('-', '').isdigit():
                pass  # ç»§ç»­å…¶ä»–æ£€æŸ¥
            else:
                return DataFormat.JSON
        except (json.JSONDecodeError, TypeError):
            pass
        
        # æ£€æŸ¥è¡¨æ ¼æ ¼å¼
        if self._is_table_format(result_str):
            return DataFormat.TABLE
        
        # æ£€æŸ¥åˆ—è¡¨æ ¼å¼
        if self._is_list_format(result_str):
            return DataFormat.LIST
        
        # æ£€æŸ¥æ•°å­—æ ¼å¼
        if self._is_numeric_format(result_str):
            return DataFormat.NUMBER
        
        # æ£€æŸ¥URLæ ¼å¼
        if self._is_url_format(result_str):
            return DataFormat.URL
        
        # æ£€æŸ¥ä»£ç æ ¼å¼
        if self._is_code_format(result_str):
            return DataFormat.CODE
        
        # æ£€æŸ¥æ··åˆæ ¼å¼ - å¢å¼ºæ£€æµ‹
        format_indicators = [
            self._contains_numbers(result_str),
            self._contains_urls(result_str), 
            self._contains_lists(result_str),
            self._contains_json(result_str)
        ]
        
        # ç‰¹æ®Šæƒ…å†µï¼šçŸ­æ–‡æœ¬ä¸­åŒ…å«æ•°å­—å’Œå…¶ä»–å†…å®¹
        if (self._contains_numbers(result_str) and 
            len(result_str) < 50 and 
            any([self._contains_urls(result_str), ":" in result_str, "$" in result_str])):
            return DataFormat.MIXED
        
        # ä¸€èˆ¬æƒ…å†µï¼šå¤šç§æ ¼å¼å¹¶å­˜
        format_count = sum(format_indicators)
        if format_count >= 2:
            return DataFormat.MIXED
        
        return DataFormat.TEXT
    
    def _extract_structured_data(self, tool_name: str, raw_result: Any) -> Dict[str, Any]:
        """æå–ç»“æ„åŒ–æ•°æ®"""
        extracted_data = {}
        
        if tool_name not in self.extraction_rules:
            return extracted_data
        
        result_str = str(raw_result)
        rules = self.extraction_rules[tool_name]
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºè§„åˆ™
        sorted_rules = sorted(rules, key=lambda x: x.priority, reverse=True)
        
        for rule in sorted_rules:
            try:
                matches = re.finditer(rule.pattern, result_str, re.IGNORECASE | re.MULTILINE)
                values = []
                
                for match in matches:
                    if match.groups():
                        # æœ‰æ•è·ç»„
                        value = match.group(1).strip()
                    else:
                        # æ— æ•è·ç»„ï¼Œä½¿ç”¨æ•´ä¸ªåŒ¹é…
                        value = match.group(0).strip()
                    
                    # åº”ç”¨åå¤„ç†
                    if rule.post_process:
                        value = self._apply_post_processing(value, rule.post_process)
                    
                    values.append(value)
                
                if values:
                    if rule.data_type == "number":
                        # æ•°å­—ç±»å‹å§‹ç»ˆä½œä¸ºåˆ—è¡¨å­˜å‚¨ï¼Œä¾¿äºåç»­å¤„ç†
                        extracted_data[rule.target_field] = values
                    elif rule.data_type == "text" and len(values) == 1:
                        extracted_data[rule.target_field] = values[0]
                    else:
                        extracted_data[rule.target_field] = values
                        
            except Exception as e:
                logger.warning(f"âš ï¸ æå–è§„åˆ™æ‰§è¡Œå¤±è´¥ ({rule.name}): {e}")
        
        return extracted_data
    
    def _apply_post_processing(self, value: str, post_process: str) -> Any:
        """åº”ç”¨åå¤„ç†"""
        try:
            if post_process == "parse_float":
                # æ¸…ç†æ•°å­—å­—ç¬¦ä¸²å¹¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                clean_value = re.sub(r'[^\d.-]', '', value)
                return float(clean_value) if clean_value else 0.0
            elif post_process == "parse_int":
                clean_value = re.sub(r'[^\d-]', '', value)
                return int(clean_value) if clean_value else 0
            elif post_process == "to_lower":
                return value.lower()
            elif post_process == "strip_html":
                return re.sub(r'<[^>]+>', '', value)
            elif post_process == "extract_domain":
                match = re.search(r'https?://(?:www\.)?([^/]+)', value)
                return match.group(1) if match else value
        except Exception as e:
            logger.warning(f"âš ï¸ åå¤„ç†å¤±è´¥ ({post_process}): {e}")
            return value
        
        return value
    
    def _calculate_confidence(self, result_type: ResultType, extracted_data: Dict[str, Any], 
                            raw_result: Any) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        base_confidence = 0.0
        
        # åŸºäºç»“æœç±»å‹
        if result_type == ResultType.SUCCESS:
            base_confidence = 0.8
        elif result_type == ResultType.PARTIAL_SUCCESS:
            base_confidence = 0.6
        elif result_type == ResultType.FAILURE:
            base_confidence = 0.3
        elif result_type == ResultType.ERROR:
            base_confidence = 0.1
        else:  # EMPTY
            base_confidence = 0.0
        
        # åŸºäºæå–æ•°æ®çš„ä¸°å¯Œåº¦
        data_richness = len(extracted_data) / 10  # æ¯ä¸ªæå–å­—æ®µè´¡çŒ®0.1
        data_richness = min(0.2, data_richness)  # æœ€å¤šè´¡çŒ®0.2
        
        # åŸºäºå†…å®¹é•¿åº¦
        content_length = len(str(raw_result))
        if content_length > 100:
            length_bonus = 0.1
        elif content_length > 50:
            length_bonus = 0.05
        else:
            length_bonus = 0.0
        
        # ç»¼åˆè®¡ç®—
        total_confidence = base_confidence + data_richness + length_bonus
        return min(1.0, max(0.0, total_confidence))
    
    def _generate_metadata(self, tool_name: str, raw_result: Any, 
                         execution_context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆå…ƒæ•°æ®"""
        metadata = {
            "result_length": len(str(raw_result)),
            "has_numbers": self._contains_numbers(str(raw_result)),
            "has_urls": self._contains_urls(str(raw_result)),
            "has_errors": "error" in str(raw_result).lower(),
            "processing_version": "1.0"
        }
        
        if execution_context:
            metadata["execution_context"] = execution_context
            
        return metadata
    
    # å·¥å…·ç‰¹å®šå¤„ç†å™¨
    def _process_deepsearch_result(self, raw_result: Any, extracted_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†DeepSearchç»“æœ"""
        enhanced_data = {}
        
        result_str = str(raw_result)
        
        # æå–æœç´¢æ‘˜è¦
        summary_match = re.search(r'(?:Summary|æ‘˜è¦)[:ï¼š]\s*([^\n]+)', result_str, re.IGNORECASE)
        if summary_match:
            enhanced_data["search_summary"] = summary_match.group(1).strip()
        
        # æå–ç›¸å…³æ€§åˆ†æ•°
        relevance_matches = re.findall(r'relevance[:ï¼š]?\s*(\d+\.?\d*)%?', result_str, re.IGNORECASE)
        if relevance_matches:
            enhanced_data["relevance_scores"] = [float(score) for score in relevance_matches]
        
        # æå–æ¥æºä¿¡æ¯
        source_matches = re.findall(r'source[:ï¼š]?\s*([^\n]+)', result_str, re.IGNORECASE)
        if source_matches:
            enhanced_data["sources"] = source_matches
        
        return enhanced_data
    
    def _process_browser_result(self, raw_result: Any, extracted_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†Browserç»“æœ"""
        enhanced_data = {}
        
        result_str = str(raw_result)
        
        # æå–é¡µé¢çŠ¶æ€
        if "200" in result_str:
            enhanced_data["page_status"] = "success"
        elif any(code in result_str for code in ["404", "500", "403"]):
            enhanced_data["page_status"] = "error"
        else:
            enhanced_data["page_status"] = "unknown"
        
        # æå–é¡µé¢ç±»å‹
        if "stock" in result_str.lower() or "finance" in result_str.lower():
            enhanced_data["page_type"] = "financial"
        elif "news" in result_str.lower():
            enhanced_data["page_type"] = "news"
        else:
            enhanced_data["page_type"] = "general"
        
        # æå–æ—¶é—´æˆ³
        timestamp_match = re.search(r'(\d{4}-\d{2}-\d{2}|\d{1,2}/\d{1,2}/\d{4})', result_str)
        if timestamp_match:
            enhanced_data["page_timestamp"] = timestamp_match.group(1)
        
        return enhanced_data
    
    def _process_microsandbox_result(self, raw_result: Any, extracted_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†MicroSandboxç»“æœ"""
        enhanced_data = {}
        
        result_str = str(raw_result)
        
        # åˆ†æä»£ç æ‰§è¡ŒçŠ¶æ€
        if "error" in result_str.lower() or "exception" in result_str.lower():
            enhanced_data["execution_status"] = "error"
        elif "warning" in result_str.lower():
            enhanced_data["execution_status"] = "warning"
        else:
            enhanced_data["execution_status"] = "success"
        
        # æå–æ•°æ®ç±»å‹ä¿¡æ¯
        data_types = []
        if re.search(r'\bDataFrame\b', result_str):
            data_types.append("pandas_dataframe")
        if re.search(r'\blist\b', result_str):
            data_types.append("list")
        if re.search(r'\bdict\b', result_str):
            data_types.append("dictionary")
        if re.search(r'\bnumpy\b', result_str):
            data_types.append("numpy_array")
        
        enhanced_data["detected_data_types"] = data_types
        
        # æå–å¯¼å…¥çš„æ¨¡å—
        import_matches = re.findall(r'import\s+(\w+)', result_str)
        if import_matches:
            enhanced_data["imported_modules"] = list(set(import_matches))
        
        return enhanced_data
    
    def _process_search_result(self, raw_result: Any, extracted_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†Searchå·¥å…·ç»“æœ"""
        enhanced_data = {}
        
        result_str = str(raw_result)
        
        # æå–åŒ¹é…ç»Ÿè®¡
        match_count = len(re.findall(r'found|match|åŒ¹é…', result_str, re.IGNORECASE))
        enhanced_data["match_count"] = match_count
        
        # æå–æ–‡ä»¶ç±»å‹
        file_extensions = re.findall(r'\.(\w+)', result_str)
        if file_extensions:
            enhanced_data["file_types"] = list(set(file_extensions))
        
        return enhanced_data
    
    # æ ¼å¼æ£€æµ‹è¾…åŠ©æ–¹æ³•
    def _is_table_format(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºè¡¨æ ¼æ ¼å¼"""
        lines = text.split('\n')
        if len(lines) < 2:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¡¨æ ¼åˆ†éš”ç¬¦
        separators = ['|', '\t', '  +  ']
        return any(sep in text for sep in separators)
    
    def _is_list_format(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºåˆ—è¡¨æ ¼å¼"""
        list_indicators = [r'^\s*[\d\w]\.\s', r'^\s*[-*]\s', r'^\s*\d+\)\s']
        lines = text.split('\n')
        list_lines = 0
        
        for line in lines:
            if any(re.match(pattern, line) for pattern in list_indicators):
                list_lines += 1
        
        return list_lines >= 2
    
    def _is_numeric_format(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å­—æ ¼å¼"""
        # ç§»é™¤ç©ºç™½åæ£€æŸ¥æ˜¯å¦ä¸»è¦æ˜¯æ•°å­—
        clean_text = re.sub(r'\s', '', text)
        numeric_chars = len(re.findall(r'[\d.-]', clean_text))
        total_chars = len(clean_text)
        
        return total_chars > 0 and numeric_chars / total_chars > 0.7
    
    def _is_url_format(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºURLæ ¼å¼"""
        url_pattern = r'https?://[^\s<>"{}|\\^`[\]]+'
        urls = re.findall(url_pattern, text)
        return len(urls) > 0 and len(' '.join(urls)) / len(text) > 0.5
    
    def _is_code_format(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºä»£ç æ ¼å¼"""
        code_indicators = [
            r'def\s+\w+\(',
            r'class\s+\w+',
            r'import\s+\w+',
            r'from\s+\w+\s+import',
            r'```',
            r'\w+\s*=\s*\w+',
        ]
        
        return any(re.search(pattern, text) for pattern in code_indicators)
    
    def _contains_numbers(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­—"""
        return bool(re.search(r'\d+', text))
    
    def _contains_urls(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«URL"""
        return bool(re.search(r'https?://', text))
    
    def _contains_lists(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«åˆ—è¡¨"""
        return bool(re.search(r'^\s*[-*]\s', text, re.MULTILINE))
    
    def _contains_json(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«JSON"""
        return bool(re.search(r'[{\[].*[}\]]', text, re.DOTALL))
    
    def get_extractable_data_summary(self, enhanced_result: EnhancedResult) -> Dict[str, Any]:
        """
        ğŸ“Š è·å–å¯æå–æ•°æ®çš„æ‘˜è¦
        
        Args:
            enhanced_result: å¢å¼ºç»“æœ
            
        Returns:
            Dict[str, Any]: æ•°æ®æ‘˜è¦
        """
        summary = {
            "tool_name": enhanced_result.tool_name,
            "result_type": enhanced_result.result_type.value,
            "data_format": enhanced_result.data_format.value,
            "confidence_score": enhanced_result.confidence_score,
            "extracted_fields": list(enhanced_result.extracted_data.keys()),
            "key_data": {}
        }
        
        # æå–å…³é”®æ•°æ®
        for field, value in enhanced_result.extracted_data.items():
            if isinstance(value, list) and len(value) > 0:
                summary["key_data"][field] = value[:3]  # æœ€å¤šæ˜¾ç¤º3ä¸ª
            elif isinstance(value, (int, float, str)) and value:
                summary["key_data"][field] = value
        
        return summary
    
    def prepare_for_next_tool(self, enhanced_result: EnhancedResult, 
                            target_tool: str) -> Dict[str, Any]:
        """
        ğŸ¯ ä¸ºä¸‹ä¸€ä¸ªå·¥å…·å‡†å¤‡æ•°æ®
        
        Args:
            enhanced_result: å¢å¼ºç»“æœ
            target_tool: ç›®æ ‡å·¥å…·
            
        Returns:
            Dict[str, Any]: ä¸ºç›®æ ‡å·¥å…·å‡†å¤‡çš„æ•°æ®
        """
        prepared_data = {
            "source_tool": enhanced_result.tool_name,
            "confidence": enhanced_result.confidence_score,
            "available_data": {}
        }
        
        # æ ¹æ®ç›®æ ‡å·¥å…·é€‰æ‹©ç›¸å…³æ•°æ®
        if target_tool == "microsandbox":
            # ä¸ºä»£ç æ‰§è¡Œå‡†å¤‡æ•°æ®
            if "prices" in enhanced_result.extracted_data:
                prepared_data["available_data"]["price_data"] = enhanced_result.extracted_data["prices"]
            if "numbers" in enhanced_result.extracted_data:
                prepared_data["available_data"]["numeric_data"] = enhanced_result.extracted_data["numbers"]
            if "companies" in enhanced_result.extracted_data:
                prepared_data["available_data"]["company_names"] = enhanced_result.extracted_data["companies"]
                
        elif target_tool == "browser_use":
            # ä¸ºæµè§ˆå™¨å‡†å¤‡æ•°æ®
            if "urls" in enhanced_result.extracted_data:
                prepared_data["available_data"]["target_urls"] = enhanced_result.extracted_data["urls"]
            if "companies" in enhanced_result.extracted_data:
                prepared_data["available_data"]["search_terms"] = enhanced_result.extracted_data["companies"]
                
        elif target_tool == "deepsearch":
            # ä¸ºæœç´¢å‡†å¤‡æ•°æ®
            if "key_facts" in enhanced_result.extracted_data:
                prepared_data["available_data"]["context_facts"] = enhanced_result.extracted_data["key_facts"]
            if "companies" in enhanced_result.extracted_data:
                prepared_data["available_data"]["related_entities"] = enhanced_result.extracted_data["companies"]
        
        # æ·»åŠ åŸå§‹ç»“æœçš„ç²¾ç®€ç‰ˆæœ¬
        original_text = str(enhanced_result.original_result)
        if len(original_text) > 200:
            prepared_data["result_summary"] = original_text[:200] + "..."
        else:
            prepared_data["result_summary"] = original_text
        
        return prepared_data