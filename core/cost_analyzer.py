#!/usr/bin/env python3
"""
æˆæœ¬åˆ†æå™¨ - ä¸ºè½¨è¿¹å’Œç§å­ä»»åŠ¡æä¾›æˆæœ¬ä¿¡æ¯æ³¨å…¥åŠŸèƒ½
"""

import logging
import os
import json
import yaml
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime

logger = logging.getLogger(__name__)

@dataclass
class CostAnalysis:
    """æˆæœ¬åˆ†æç»“æœæ•°æ®ç»“æ„"""
    total_tokens: int
    total_cost_usd: float
    cost_breakdown: Dict[str, Any]

@dataclass
class SynthesisCostAnalysis:
    """ç§å­ä»»åŠ¡åˆæˆæˆæœ¬åˆ†æç»“æœ"""
    total_synthesis_tokens: int
    total_synthesis_cost_usd: float
    synthesis_breakdown: Dict[str, float]
    source_trajectory_cost_usd: float

class CostAnalyzer:
    """æˆæœ¬åˆ†æå™¨ - æä¾›ç»Ÿä¸€çš„æˆæœ¬è®¡ç®—å’Œåˆ†æåŠŸèƒ½"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # ğŸ†• ä»é…ç½®æ–‡ä»¶åŠ è½½LLMé…ç½®
        self.llm_config = self._load_llm_config()
        self.default_provider = self.llm_config.get('default_provider', 'gemini')
        
        # ğŸ†• å¤šæ¨¡å‹åŠ¨æ€å®šä»·é…ç½®ï¼ˆåŸºäºå®é™…APIå®šä»·ï¼‰
        self.model_pricing = {
            # Geminiæ¨¡å‹å®šä»·
            'gemini-2.5-flash': {
                'input_cost_per_million': 0.30,
                'output_cost_per_million': 2.50
            },
            'gemini-2.5-flash-lite-preview-06-17': {
                'input_cost_per_million': 0.075,  # Flash Liteæ›´ä¾¿å®œ
                'output_cost_per_million': 0.30
            },
            'gemini-2.5-pro': {
                'input_cost_per_million': 3.50,
                'output_cost_per_million': 15.00
            },
            # OpenAIæ¨¡å‹å®šä»·
            'gpt-4o': {
                'input_cost_per_million': 2.50,
                'output_cost_per_million': 10.00
            },
            'gpt-4o-mini': {
                'input_cost_per_million': 0.15,
                'output_cost_per_million': 0.60
            },
            # vLLM/æœ¬åœ°æ¨¡å‹ï¼ˆå‡ ä¹å…è´¹ï¼‰
            'default-model': {
                'input_cost_per_million': 0.001,
                'output_cost_per_million': 0.001
            }
        }
        
        # step_logsæ–‡ä»¶è·¯å¾„é…ç½®
        self.step_logs_base_path = "output/logs/grouped"
        
        # å·¥å…·Tokenä½¿ç”¨æ¨¡å¼ä¼°ç®—ï¼ˆåŸºäºå®é™…è§‚å¯Ÿï¼‰
        self.tool_token_patterns = {
            "microsandbox": {
                "base_input_tokens": 50,     # åŸºç¡€æŒ‡ä»¤token
                "output_tokens_per_call": 100,  # æ¯æ¬¡è°ƒç”¨è¾“å‡º
                "tokens_per_code_line": 10,    # æ¯è¡Œä»£ç é¢å¤–token
                "description": "MicroSandboxä»£ç æ‰§è¡Œ"
            },
            "browser_use": {
                "base_input_tokens": 150,    # æµè§ˆå™¨æŒ‡ä»¤ç›¸å¯¹å¤æ‚
                "output_tokens_per_call": 200,  # è¿”å›HTML/çŠ¶æ€ä¿¡æ¯
                "tokens_per_action": 50,      # æ¯ä¸ªæ“ä½œé¢å¤–token
                "description": "æµè§ˆå™¨è‡ªåŠ¨åŒ–"
            },
            "deepsearch": {
                "base_input_tokens": 100,    # æœç´¢æŸ¥è¯¢
                "output_tokens_per_call": 500,  # æœç´¢ç»“æœé€šå¸¸è¾ƒé•¿
                "tokens_per_result": 100,     # æ¯ä¸ªç»“æœé¢å¤–token
                "description": "æ·±åº¦æœç´¢"
            },
            "search_tool": {
                "base_input_tokens": 30,     # ç®€å•æœç´¢
                "output_tokens_per_call": 50,   # åŸºç¡€æœç´¢ç»“æœ
                "tokens_per_match": 20,       # æ¯ä¸ªåŒ¹é…é¢å¤–token
                "description": "åŸºç¡€æœç´¢"
            },
            "default": {
                "base_input_tokens": 50,     # é»˜è®¤å·¥å…·
                "output_tokens_per_call": 100,  # é»˜è®¤è¾“å‡º
                "tokens_per_operation": 25,   # æ¯ä¸ªæ“ä½œé¢å¤–token
                "description": "å…¶ä»–å·¥å…·é»˜è®¤æˆæœ¬"
            }
        }
    
    def _load_llm_config(self) -> Dict[str, Any]:
        """
        åŠ è½½LLMé…ç½®æ–‡ä»¶
        
        Returns:
            Dict[str, Any]: LLMé…ç½®ä¿¡æ¯
        """
        try:
            config_path = "config/llm_config.yaml"
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                    self.logger.debug(f"âœ… åŠ è½½LLMé…ç½®: {config_path}")
                    return config
            else:
                self.logger.warning(f"âš ï¸ LLMé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
                return {}
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½LLMé…ç½®å¤±è´¥: {e}")
            return {}
    
    def _get_model_pricing(self, model_name: str) -> Dict[str, float]:
        """
        æ ¹æ®æ¨¡å‹åç§°è·å–å®šä»·ä¿¡æ¯
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            Dict[str, float]: åŒ…å«input_cost_per_millionå’Œoutput_cost_per_millionçš„å®šä»·ä¿¡æ¯
        """
        # é¦–å…ˆæ£€æŸ¥ç²¾ç¡®åŒ¹é…
        if model_name in self.model_pricing:
            return self.model_pricing[model_name]
        
        # æ¨¡ç³ŠåŒ¹é…å¸¸è§æ¨¡å‹åç§°å˜ä½“
        model_lower = model_name.lower()
        
        # Geminiæ¨¡å‹åŒ¹é…
        if 'gemini-2.5-flash-lite' in model_lower:
            return self.model_pricing['gemini-2.5-flash-lite-preview-06-17']
        elif 'gemini-2.5-flash' in model_lower:
            return self.model_pricing['gemini-2.5-flash']
        elif 'gemini-2.5-pro' in model_lower:
            return self.model_pricing['gemini-2.5-pro']
        
        # OpenAIæ¨¡å‹åŒ¹é…
        elif 'gpt-4o-mini' in model_lower:
            return self.model_pricing['gpt-4o-mini']
        elif 'gpt-4o' in model_lower:
            return self.model_pricing['gpt-4o']
        
        # æœ¬åœ°/vLLMæ¨¡å‹åŒ¹é…
        elif any(keyword in model_lower for keyword in ['vllm', 'local', 'default']):
            return self.model_pricing['default-model']
        
        # é»˜è®¤å›é€€åˆ°é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤æ¨¡å‹å®šä»·
        default_model = self.llm_config.get('llm_providers', {}).get(self.default_provider, {}).get('model', 'gemini-2.5-flash-lite-preview-06-17')
        if default_model in self.model_pricing:
            self.logger.warning(f"âš ï¸ æœªçŸ¥æ¨¡å‹ {model_name}ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹ {default_model} çš„å®šä»·")
            return self.model_pricing[default_model]
        
        # æœ€ç»ˆå›é€€
        self.logger.warning(f"âš ï¸ æœªçŸ¥æ¨¡å‹ {model_name}ï¼Œä½¿ç”¨Gemini Flash Liteé»˜è®¤å®šä»·")
        return self.model_pricing['gemini-2.5-flash-lite-preview-06-17']

    def _load_step_logs_for_task(self, task_id: str, date_str: str = None) -> List[Dict[str, Any]]:
        """
        æ ¹æ®ä»»åŠ¡IDå’Œæ—¥æœŸåŠ è½½ç›¸åº”çš„step_logsæ•°æ®
        
        Args:
            task_id: ä»»åŠ¡ID
            date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨ä»Šå¤©
            
        Returns:
            List[Dict[str, Any]]: åŒ¹é…çš„step_logsæ•°æ®
        """
        try:
            if not date_str:
                date_str = datetime.now().strftime("%Y-%m-%d")
            
            step_logs_file = os.path.join(
                self.step_logs_base_path, 
                date_str, 
                f"step_logs_{date_str}.jsonl"
            )
            
            if not os.path.exists(step_logs_file):
                self.logger.warning(f"âš ï¸ step_logsæ–‡ä»¶ä¸å­˜åœ¨: {step_logs_file}")
                return []
            
            step_logs = []
            with open(step_logs_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        log_entry = json.loads(line.strip())
                        if log_entry.get('task_id') == task_id:
                            # æå–stepsæ•°æ®
                            steps = log_entry.get('steps', [])
                            step_logs.extend(steps)
                    except json.JSONDecodeError:
                        continue
            
            self.logger.debug(f"âœ… åŠ è½½step_logsæ•°æ®: {len(step_logs)}ä¸ªæ­¥éª¤ï¼Œä»»åŠ¡ID: {task_id}")
            return step_logs
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½step_logså¤±è´¥: {e}")
            return []

    def analyze_trajectory_cost(self, trajectory_data: Dict[str, Any], 
                              step_logs: List[Dict[str, Any]] = None) -> CostAnalysis:
        """
        åˆ†æè½¨è¿¹çš„å®Œæ•´æˆæœ¬ä¿¡æ¯
        
        Args:
            trajectory_data: è½¨è¿¹æ•°æ®
            step_logs: æ­¥éª¤æ—¥å¿—åˆ—è¡¨ï¼ˆåŒ…å«tokenä½¿ç”¨ä¿¡æ¯ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åŠ è½½
            
        Returns:
            CostAnalysis: æˆæœ¬åˆ†æç»“æœ
        """
        try:
            # ğŸ†• å¦‚æœæ²¡æœ‰æä¾›step_logsï¼Œåˆ™å°è¯•è‡ªåŠ¨åŠ è½½
            if step_logs is None:
                task_id = trajectory_data.get('task_id')
                if task_id:
                    # å°è¯•ä»ä»Šå¤©çš„æ—¥æœŸåŠ è½½
                    step_logs = self._load_step_logs_for_task(task_id)
                    
                    # å¦‚æœä»Šå¤©æ²¡æœ‰ï¼Œå°è¯•ä»è½¨è¿¹æ—¶é—´æˆ³æ¨æ–­æ—¥æœŸ
                    if not step_logs and 'timestamp' in trajectory_data:
                        try:
                            timestamp = trajectory_data['timestamp']
                            if isinstance(timestamp, str):
                                date_str = timestamp[:10]  # æå–YYYY-MM-DDéƒ¨åˆ†
                                step_logs = self._load_step_logs_for_task(task_id, date_str)
                        except Exception:
                            pass
            
            # 1. ä»step_logsæ”¶é›†LLMæˆæœ¬ä¿¡æ¯
            llm_cost_usd = 0.0
            total_tokens = 0
            cache_savings_usd = 0.0
            
            if step_logs:
                for step_log in step_logs:
                    # ğŸ†• ä»æ­¥éª¤æ—¥å¿—ä¸­æå–çœŸå®tokenä½¿ç”¨ä¿¡æ¯
                    token_usage = None
                    
                    # æ£€æŸ¥å¤šç§å¯èƒ½çš„token_usageä½ç½®
                    if 'token_usage' in step_log:
                        token_usage = step_log['token_usage']
                    elif 'llm_interaction' in step_log and 'token_usage' in step_log['llm_interaction']:
                        token_usage = step_log['llm_interaction']['token_usage']
                    
                    if token_usage:
                        prompt_tokens = token_usage.get('prompt_tokens', 0)
                        completion_tokens = token_usage.get('completion_tokens', 0)
                        cached_tokens = token_usage.get('cached_tokens', 0)
                        data_source = token_usage.get('data_source', 'unknown')
                        model_name = token_usage.get('model', 'unknown')
                        
                        total_tokens += prompt_tokens + completion_tokens
                        
                        # ğŸ†• åŸºäºå®é™…æ¨¡å‹çš„åŠ¨æ€å®šä»·è®¡ç®—çœŸå®æˆæœ¬
                        pricing = self._get_model_pricing(model_name)
                        input_cost = (prompt_tokens / 1_000_000) * pricing['input_cost_per_million']
                        output_cost = (completion_tokens / 1_000_000) * pricing['output_cost_per_million']
                        
                        llm_cost_usd += input_cost + output_cost
                        
                        # ç¼“å­˜èŠ‚çœï¼ˆå¦‚æœæœ‰ç¼“å­˜tokenï¼‰
                        if cached_tokens > 0:
                            cache_savings_usd += (cached_tokens / 1_000_000) * pricing['input_cost_per_million']
                        
                        self.logger.debug(f"âœ… ä½¿ç”¨çœŸå®tokenæ•°æ®: {prompt_tokens}+{completion_tokens}, æ¨¡å‹: {model_name}, æ¥æº: {data_source}")
                    
                    # å¤‡ç”¨ï¼šä»cost_infoæå–æˆæœ¬ä¿¡æ¯
                    elif 'cost_info' in step_log:
                        cost_info = step_log['cost_info']
                        llm_cost_usd += cost_info.get('estimated_cost_usd', 0.0)
                        cache_savings_usd += cost_info.get('cache_savings_usd', 0.0)
                
                if total_tokens > 0:
                    self.logger.info(f"âœ… æˆåŠŸåŠ è½½çœŸå®tokenæ•°æ®: {total_tokens} tokens, LLMæˆæœ¬: ${llm_cost_usd:.6f}")
            
            # 2. åˆ†æå·¥å…·ä½¿ç”¨æˆæœ¬
            tool_costs = self._analyze_tool_costs(trajectory_data, step_logs)
            
            # 3. è®¡ç®—å­˜å‚¨æˆæœ¬ï¼ˆåŸºäºè½¨è¿¹å¤§å°çš„ç®€å•ä¼°ç®—ï¼‰
            storage_cost_usd = self._estimate_storage_cost(trajectory_data)
            
            # 4. æ„å»ºæˆæœ¬åˆ†è§£
            cost_breakdown = {
                "llm_cost_usd": llm_cost_usd,
                "tool_costs": tool_costs,
                "storage_cost_usd": storage_cost_usd,
                "cache_savings_usd": cache_savings_usd
            }
            
            # 5. è®¡ç®—æ€»æˆæœ¬
            total_cost_usd = llm_cost_usd + sum(tool_costs.values()) + storage_cost_usd
            
            return CostAnalysis(
                total_tokens=total_tokens,
                total_cost_usd=total_cost_usd,
                cost_breakdown=cost_breakdown
            )
            
        except Exception as e:
            self.logger.error(f"è½¨è¿¹æˆæœ¬åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤çš„æˆæœ¬åˆ†æç»“æœ
            return CostAnalysis(
                total_tokens=0,
                total_cost_usd=0.0,
                cost_breakdown={
                    "llm_cost_usd": 0.0,
                    "tool_costs": {
                        "microsandbox_cost_usd": 0.0,
                        "browser_use_cost_usd": 0.0,
                        "deepsearch_cost_usd": 0.0,
                        "other_tools_cost_usd": 0.0
                    },
                    "storage_cost_usd": 0.0,
                    "cache_savings_usd": 0.0
                }
            )
    
    def analyze_synthesis_cost(self, synthesis_phases: List[Dict[str, Any]], 
                             source_trajectory_cost: float = 0.0) -> SynthesisCostAnalysis:
        """
        åˆ†æç§å­ä»»åŠ¡åˆæˆè¿‡ç¨‹çš„æˆæœ¬
        
        Args:
            synthesis_phases: åˆæˆé˜¶æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªé˜¶æ®µåŒ…å«tokenä½¿ç”¨å’Œæˆæœ¬ä¿¡æ¯
            source_trajectory_cost: æºè½¨è¿¹çš„æˆæœ¬
            
        Returns:
            SynthesisCostAnalysis: åˆæˆæˆæœ¬åˆ†æç»“æœ
        """
        try:
            total_tokens = 0
            total_cost = 0.0
            breakdown = {}
            
            # é¢„å®šä¹‰çš„åˆæˆé˜¶æ®µ
            phase_names = {
                "seed_extraction": "seed_extraction_cost_usd",
                "task_expansion": "task_expansion_cost_usd", 
                "quality_validation": "quality_validation_cost_usd"
            }
            
            # åˆ†ææ¯ä¸ªåˆæˆé˜¶æ®µçš„æˆæœ¬
            for phase in synthesis_phases:
                phase_name = phase.get('phase', 'unknown')
                phase_tokens = phase.get('tokens', 0)
                phase_cost = phase.get('cost_usd', 0.0)
                
                total_tokens += phase_tokens
                total_cost += phase_cost
                
                # æ˜ å°„åˆ°æ ‡å‡†é˜¶æ®µåç§°
                if phase_name in phase_names:
                    breakdown[phase_names[phase_name]] = phase_cost
                else:
                    # å¤„ç†æœªçŸ¥é˜¶æ®µ
                    breakdown[f"{phase_name}_cost_usd"] = phase_cost
            
            # ç¡®ä¿æ‰€æœ‰é¢„æœŸé˜¶æ®µéƒ½æœ‰æ•°å€¼
            for standard_phase in phase_names.values():
                if standard_phase not in breakdown:
                    breakdown[standard_phase] = 0.0
            
            return SynthesisCostAnalysis(
                total_synthesis_tokens=total_tokens,
                total_synthesis_cost_usd=total_cost,
                synthesis_breakdown=breakdown,
                source_trajectory_cost_usd=source_trajectory_cost
            )
            
        except Exception as e:
            self.logger.error(f"åˆæˆæˆæœ¬åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å€¼
            return SynthesisCostAnalysis(
                total_synthesis_tokens=0,
                total_synthesis_cost_usd=0.0,
                synthesis_breakdown={
                    "seed_extraction_cost_usd": 0.0,
                    "task_expansion_cost_usd": 0.0,
                    "quality_validation_cost_usd": 0.0
                },
                source_trajectory_cost_usd=source_trajectory_cost
            )
    
    def _analyze_tool_costs(self, trajectory_data: Dict[str, Any], 
                           step_logs: List[Dict[str, Any]] = None) -> Dict[str, float]:
        """åˆ†æå·¥å…·ä½¿ç”¨æˆæœ¬ï¼ˆåŸºäºçœŸå®tokenä½¿ç”¨ï¼‰"""
        tool_costs = {
            "microsandbox_cost_usd": 0.0,
            "browser_use_cost_usd": 0.0, 
            "deepsearch_cost_usd": 0.0,
            "other_tools_cost_usd": 0.0
        }
        
        try:
            # ğŸ†• ä»step_logsä¸­æå–çœŸå®å·¥å…·ä½¿ç”¨å’Œtokenç»Ÿè®¡
            if step_logs:
                for step_log in step_logs:
                    # ğŸ†• ä»tool_executionsä¸­æå–çœŸå®å·¥å…·ä½¿ç”¨æ•°æ®
                    tool_executions = step_log.get('tool_executions', [])
                    
                    # è·å–è¯¥æ­¥éª¤çš„LLMäº¤äº’tokenæ•°æ®
                    llm_token_usage = step_log.get('llm_interaction', {}).get('token_usage', {})
                    
                    # å¤„ç†æ¯ä¸ªå·¥å…·æ‰§è¡Œ
                    for tool_execution in tool_executions:
                        # ä»tool_executionä¸­æå–å·¥å…·ä¿¡æ¯
                        action = tool_execution.get('action', {})
                        service = action.get('service', 'unknown')
                        tool_name = action.get('tool', 'unknown')
                        
                        # æ„é€ tool_info
                        tool_info = {
                            'service': service,
                            'tool': tool_name,
                            'input': action.get('input', ''),
                            'execution_status': tool_execution.get('execution_status', 'unknown'),
                            'execution_duration': tool_execution.get('execution_timing', {}).get('execution_duration_seconds', 0)
                        }
                        
                        # è®¡ç®—åŸºäºçœŸå®tokenä½¿ç”¨çš„æˆæœ¬
                        # ä½¿ç”¨è¯¥æ­¥éª¤çš„LLMäº¤äº’tokenæ•°æ®
                        tool_cost = self._calculate_tool_cost_from_tokens(
                            service,  # ä½¿ç”¨serviceä½œä¸ºå·¥å…·åç§°
                            tool_info,
                            llm_token_usage  # ä¼ é€’çœŸå®LLM tokenä½¿ç”¨æ•°æ®
                        )
                        
                        # åˆ†ç±»ç´¯è®¡æˆæœ¬
                        if service == "microsandbox":
                            tool_costs["microsandbox_cost_usd"] += tool_cost
                        elif service == "browser_use":
                            tool_costs["browser_use_cost_usd"] += tool_cost
                        elif service == "deepsearch":
                            tool_costs["deepsearch_cost_usd"] += tool_cost
                        else:
                            tool_costs["other_tools_cost_usd"] += tool_cost
                        
                        self.logger.debug(f"âœ… å¤„ç†å·¥å…·æ‰§è¡Œ: {service}.{tool_name}, æˆæœ¬: ${tool_cost:.6f}")
                            
                self.logger.info(f"âœ… å·¥å…·æˆæœ¬åˆ†æå®Œæˆï¼ˆåŸºäºçœŸå®tokenæ•°æ®ï¼‰: "
                               f"MicroSandbox=${tool_costs['microsandbox_cost_usd']:.6f}, "
                               f"Browser=${tool_costs['browser_use_cost_usd']:.6f}, "
                               f"DeepSearch=${tool_costs['deepsearch_cost_usd']:.6f}, "
                               f"Other=${tool_costs['other_tools_cost_usd']:.6f}")
            
            # ğŸ†• ä»è½¨è¿¹æ•°æ®ä¸­æ¨æ–­å·¥å…·ä½¿ç”¨ï¼ˆæ”¹è¿›çš„æ™ºèƒ½ä¼°ç®—ï¼‰
            has_tool_executions = any(
                step_log.get('tool_executions') for step_log in (step_logs or [])
            )
            
            if trajectory_data and not any(tool_costs.values()):
                # åªæœ‰åœ¨æ²¡æœ‰ä»step_logsè·å–çœŸå®æ•°æ®æ—¶æ‰ä½¿ç”¨æ™ºèƒ½ä¼°ç®—
                estimated_costs = self._estimate_tool_costs_from_trajectory(trajectory_data)
                for key, value in estimated_costs.items():
                    tool_costs[key] = max(tool_costs[key], value)
                
                # æ›´ç²¾ç¡®çš„è­¦å‘Šä¿¡æ¯
                if has_tool_executions:
                    self.logger.warning(f"âš ï¸ ä½¿ç”¨è½¨è¿¹æ•°æ®ä¼°ç®—å·¥å…·æˆæœ¬ï¼ˆå·¥å…·æ‰§è¡Œç¼ºå°‘çœŸå®tokenæ•°æ®ï¼‰")
                else:
                    self.logger.debug(f"ğŸ“Š ä»»åŠ¡æ— å·¥å…·è°ƒç”¨ï¼Œè·³è¿‡å·¥å…·æˆæœ¬åˆ†æ")
            
        except Exception as e:
            self.logger.error(f"å·¥å…·æˆæœ¬åˆ†æå¤±è´¥: {e}")
        
        return tool_costs
    
    def _calculate_tool_cost_from_tokens(self, tool_name: str, tool_info: Dict[str, Any], 
                                       token_usage: Dict[str, Any]) -> float:
        """åŸºäºçœŸå®tokenä½¿ç”¨è®¡ç®—å·¥å…·æˆæœ¬ã€‚ä¼˜å…ˆä½¿ç”¨çœŸå®APIæ•°æ®ï¼Œå¤‡ç”¨æ™ºèƒ½ä¼°ç®—ã€‚"""
        try:
            # ğŸ†• ä¼˜å…ˆä½¿ç”¨çœŸå®tokenæ•°æ®ï¼ˆæ¥è‡ªå·¥å…·è°ƒç”¨æ—¶çš„LLMäº¤äº’ï¼‰
            actual_input = token_usage.get('prompt_tokens', 0)
            actual_output = token_usage.get('completion_tokens', 0)
            data_source = token_usage.get('data_source', 'unknown')
            
            # æ£€æµ‹æ˜¯å¦ä¸ºçœŸå®APIæ•°æ®
            is_real_api = (
                data_source == 'real_api' or 
                'api' in str(data_source).lower() or
                (actual_input > 0 and actual_output > 0)
            )
            
            if is_real_api and (actual_input > 0 or actual_output > 0):
                # ä½¿ç”¨çœŸå®APIè¿”å›çš„tokenæ•°æ®
                input_tokens = actual_input
                output_tokens = actual_output
                
                self.logger.info(f"âœ… ä½¿ç”¨çœŸå®API tokenæ•°æ®è®¡ç®—å·¥å…·æˆæœ¬ {tool_name}: "
                               f"input={input_tokens}, output={output_tokens}, "
                               f"source={data_source}")
                
            else:
                # ä½¿ç”¨æ™ºèƒ½ä¼°ç®—çš„tokenæ•°æ®
                input_tokens, output_tokens = self._estimate_tool_tokens(tool_name, tool_info)
                
                self.logger.debug(f"âš ï¸ ä½¿ç”¨ä¼°ç®—tokenæ•°æ®è®¡ç®—å·¥å…·æˆæœ¬ {tool_name}: "
                                f"input={input_tokens}, output={output_tokens}")
            
            # ğŸ†• è®¡ç®—æˆæœ¬ï¼ˆä½¿ç”¨åŠ¨æ€æ¨¡å‹å®šä»·ï¼‰
            model_name = token_usage.get('model', 'unknown')
            if model_name == 'unknown':
                # å¦‚æœæ²¡æœ‰æ¨¡å‹ä¿¡æ¯ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤æ¨¡å‹
                model_name = self.llm_config.get('llm_providers', {}).get(self.default_provider, {}).get('model', 'gemini-2.5-flash-lite-preview-06-17')
            
            pricing = self._get_model_pricing(model_name)
            input_cost = (input_tokens / 1_000_000) * pricing['input_cost_per_million']
            output_cost = (output_tokens / 1_000_000) * pricing['output_cost_per_million']
            
            total_cost = input_cost + output_cost
            
            self.logger.debug(f"ğŸ’° å·¥å…·æˆæœ¬è®¡ç®—å®Œæˆ {tool_name}: "
                            f"input={input_tokens} tokens (${input_cost:.6f}), "
                            f"output={output_tokens} tokens (${output_cost:.6f}), "
                            f"model={model_name}, total=${total_cost:.6f}")
            
            return total_cost
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—å·¥å…·æˆæœ¬å¤±è´¥ {tool_name}: {e}")
            # è¿”å›åŸºç¡€é»˜è®¤æˆæœ¬
            return 0.001
    
    def _estimate_tool_tokens(self, tool_name: str, tool_info: Dict[str, Any]) -> Tuple[int, int]:
        """åŸºäºå·¥å…·ä¿¡æ¯ä¼°ç®—tokenä½¿ç”¨é‡"""
        try:
            # è·å–å·¥å…·çš„tokenä½¿ç”¨æ¨¡å¼
            pattern = self.tool_token_patterns.get(tool_name, self.tool_token_patterns["default"])
            
            # ä»å·¥å…·ä¿¡æ¯ä¸­æå–å…·ä½“å‚æ•°
            call_count = tool_info.get('call_count', 1)
            code_lines = tool_info.get('code_lines', 0)  # å¯¹äºmicrosandbox
            actions_count = tool_info.get('actions_count', 0)  # å¯¹äºbrowser_use
            results_count = tool_info.get('results_count', 0)  # å¯¹äºdeepsearch
            matches_count = tool_info.get('matches_count', 0)  # å¯¹äºsearch_tool
            
            # è®¡ç®—è¾“å…¥tokenæ•°é‡
            input_tokens = pattern["base_input_tokens"] * call_count
            
            # æ ¹æ®å·¥å…·ç±»å‹æ·»åŠ é¢å¤–token
            if tool_name == "microsandbox":
                input_tokens += code_lines * pattern["tokens_per_code_line"]
            elif tool_name == "browser_use":
                input_tokens += actions_count * pattern["tokens_per_action"]
            elif tool_name == "deepsearch":
                input_tokens += results_count * pattern["tokens_per_result"]
            elif tool_name == "search_tool":
                input_tokens += matches_count * pattern["tokens_per_match"]
            else:
                # é»˜è®¤å·¥å…·
                operations = tool_info.get('operations_count', 1)
                input_tokens += operations * pattern["tokens_per_operation"]
            
            # è®¡ç®—è¾“å‡ºtokenæ•°é‡
            output_tokens = pattern["output_tokens_per_call"] * call_count
            
            return input_tokens, output_tokens
            
        except Exception as e:
            self.logger.error(f"ä¼°ç®—å·¥å…·tokenå¤±è´¥ {tool_name}: {e}")
            return 50, 100  # é»˜è®¤åŸºç¡€ä¼°ç®—
    
    def _estimate_tool_costs_from_trajectory(self, trajectory_data: Dict[str, Any]) -> Dict[str, float]:
        """ä»è½¨è¿¹æ•°æ®æ™ºèƒ½ä¼°ç®—å·¥å…·æˆæœ¬ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        estimated_costs = {
            "microsandbox_cost_usd": 0.0,
            "browser_use_cost_usd": 0.0, 
            "deepsearch_cost_usd": 0.0,
            "other_tools_cost_usd": 0.0
        }
        
        try:
            task_type = trajectory_data.get('task_type', '')
            task_content = trajectory_data.get('task_content', '')
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹å’Œå†…å®¹æ™ºèƒ½ä¼°ç®—
            if task_type == 'code' or 'python' in task_content.lower():
                # ä»£ç ä»»åŠ¡ä¼°ç®—
                code_lines = len(task_content.split('\n'))
                estimated_costs["microsandbox_cost_usd"] = self._calculate_tool_cost_from_tokens(
                    "microsandbox", 
                    {'call_count': 1, 'code_lines': code_lines}, 
                    {}
                )
            elif task_type == 'web' or any(word in task_content.lower() for word in ['browser', 'website', 'scrape']):
                # Webä»»åŠ¡ä¼°ç®—
                actions_count = max(1, task_content.count('click') + task_content.count('fill'))
                estimated_costs["browser_use_cost_usd"] = self._calculate_tool_cost_from_tokens(
                    "browser_use", 
                    {'call_count': 1, 'actions_count': actions_count}, 
                    {}
                )
            elif task_type == 'research' or any(word in task_content.lower() for word in ['search', 'research', 'find']):
                # ç ”ç©¶ä»»åŠ¡ä¼°ç®—
                query_count = max(1, task_content.count('?'))
                estimated_costs["deepsearch_cost_usd"] = self._calculate_tool_cost_from_tokens(
                    "deepsearch", 
                    {'call_count': 1, 'results_count': query_count * 3}, 
                    {}
                )
            else:
                # å…¶ä»–ä»»åŠ¡ä½¿ç”¨é»˜è®¤å·¥å…·æˆæœ¬
                estimated_costs["other_tools_cost_usd"] = self._calculate_tool_cost_from_tokens(
                    "default", 
                    {'call_count': 1, 'operations_count': 1}, 
                    {}
                )
            
        except Exception as e:
            self.logger.error(f"æ™ºèƒ½ä¼°ç®—å·¥å…·æˆæœ¬å¤±è´¥: {e}")
        
        return estimated_costs
    
    def _estimate_storage_cost(self, trajectory_data: Dict[str, Any]) -> float:
        """ä¼°ç®—å­˜å‚¨æˆæœ¬"""
        try:
            # ç®€å•çš„å­˜å‚¨æˆæœ¬ä¼°ç®—ï¼šåŸºäºæ•°æ®å¤§å°
            import json
            data_size = len(json.dumps(trajectory_data, ensure_ascii=False))
            # å‡è®¾æ¯KBå­˜å‚¨æˆæœ¬ä¸º0.000001ç¾å…ƒ
            storage_cost = (data_size / 1024) * 0.000001
            return round(storage_cost, 6)
        except:
            return 0.0
    
    def inject_trajectory_cost(self, trajectory_data: Dict[str, Any], 
                             step_logs: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ä¸ºè½¨è¿¹æ•°æ®æ³¨å…¥æˆæœ¬ä¿¡æ¯
        
        Args:
            trajectory_data: åŸå§‹è½¨è¿¹æ•°æ®
            step_logs: æ­¥éª¤æ—¥å¿—åˆ—è¡¨
            
        Returns:
            Dict[str, Any]: æ³¨å…¥æˆæœ¬ä¿¡æ¯åçš„è½¨è¿¹æ•°æ®
        """
        try:
            # åˆ†ææˆæœ¬
            cost_analysis = self.analyze_trajectory_cost(trajectory_data, step_logs)
            
            # åˆ›å»ºå‰¯æœ¬å¹¶æ³¨å…¥æˆæœ¬ä¿¡æ¯
            enhanced_trajectory = trajectory_data.copy()
            enhanced_trajectory['cost_analysis'] = asdict(cost_analysis)
            
            self.logger.info(f"è½¨è¿¹æˆæœ¬æ³¨å…¥å®Œæˆ - Token: {cost_analysis.total_tokens}, "
                           f"æˆæœ¬: ${cost_analysis.total_cost_usd:.4f}")
            
            return enhanced_trajectory
            
        except Exception as e:
            self.logger.error(f"è½¨è¿¹æˆæœ¬æ³¨å…¥å¤±è´¥: {e}")
            return trajectory_data
    
    def inject_synthesis_cost(self, seed_task_data: Dict[str, Any],
                            synthesis_phases: List[Dict[str, Any]] = None,
                            source_trajectory_cost: float = 0.0) -> Dict[str, Any]:
        """
        ä¸ºç§å­ä»»åŠ¡æ•°æ®æ³¨å…¥åˆæˆæˆæœ¬ä¿¡æ¯
        
        Args:
            seed_task_data: åŸå§‹ç§å­ä»»åŠ¡æ•°æ®
            synthesis_phases: åˆæˆé˜¶æ®µåˆ—è¡¨
            source_trajectory_cost: æºè½¨è¿¹æˆæœ¬
            
        Returns:
            Dict[str, Any]: æ³¨å…¥æˆæœ¬ä¿¡æ¯åçš„ç§å­ä»»åŠ¡æ•°æ®
        """
        try:
            # åˆ†æåˆæˆæˆæœ¬
            synthesis_cost = self.analyze_synthesis_cost(synthesis_phases or [], source_trajectory_cost)
            
            # åˆ›å»ºå‰¯æœ¬å¹¶æ³¨å…¥æˆæœ¬ä¿¡æ¯
            enhanced_seed_task = seed_task_data.copy()
            cost_analysis_dict = asdict(synthesis_cost)
            
            # ğŸ†• æ·»åŠ åŸå§‹synthesis_phasesåˆ°æˆæœ¬åˆ†æä¸­
            cost_analysis_dict['synthesis_phases'] = synthesis_phases or []
            
            enhanced_seed_task['synthesis_cost_analysis'] = cost_analysis_dict
            
            self.logger.info(f"ç§å­ä»»åŠ¡åˆæˆæˆæœ¬æ³¨å…¥å®Œæˆ - Token: {synthesis_cost.total_synthesis_tokens}, "
                           f"æˆæœ¬: ${synthesis_cost.total_synthesis_cost_usd:.4f}, "
                           f"é˜¶æ®µæ•°: {len(synthesis_phases or [])}")
            
            return enhanced_seed_task
            
        except Exception as e:
            self.logger.error(f"ç§å­ä»»åŠ¡æˆæœ¬æ³¨å…¥å¤±è´¥: {e}")
            return seed_task_data


# å…¨å±€æˆæœ¬åˆ†æå™¨å®ä¾‹
_cost_analyzer = None

def get_cost_analyzer() -> CostAnalyzer:
    """è·å–å…¨å±€æˆæœ¬åˆ†æå™¨å®ä¾‹"""
    global _cost_analyzer
    if _cost_analyzer is None:
        _cost_analyzer = CostAnalyzer()
    return _cost_analyzer