# SynthesisCore Ã— TaskCraft å¢å¼ºè®¾è®¡æ–‡æ¡£

**ç‰ˆæœ¬**: 2.0  
**æ—¥æœŸ**: 2025å¹´6æœˆ20æ—¥  
**ç›®æ ‡**: åŸºäºTaskCraftç®—æ³•ï¼Œå…¨é¢å¢å¼ºagent-data-platformçš„synthesiscoreæ¨¡å—ï¼Œå®ç°åŸå­ä»»åŠ¡ç”Ÿæˆå’Œä»»åŠ¡æ‰©å±•èƒ½åŠ›

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æ–‡æ¡£åŸºäºå¯¹TaskCraftå‚è€ƒåº“çš„æ·±åº¦åˆ†æï¼Œè®¾è®¡äº†å¯¹agent-data-platformæ ¸å¿ƒsynthesiscoreæ¨¡å—çš„å…¨é¢å¢å¼ºæ–¹æ¡ˆã€‚é€šè¿‡å¼•å…¥**åŸå­ä»»åŠ¡æå–**ã€**æ·±åº¦ä¼˜å…ˆæ‰©å±•**å’Œ**å®½åº¦ä¼˜å…ˆæ‰©å±•**ç®—æ³•ï¼Œå°†å½“å‰çš„"è¢«åŠ¨è½¨è¿¹åˆ†æ"æ¨¡å¼å‡çº§ä¸º"ä¸»åŠ¨ä»»åŠ¡ç”Ÿæˆå¼•æ“"ï¼Œå®ç°ä»»åŠ¡-èƒ½åŠ›è‡ªè¿›åŒ–çš„å®Œæ•´é—­ç¯ã€‚

### ğŸ¯ æ ¸å¿ƒæˆæœé¢„æœŸ
- **åŸå­ä»»åŠ¡ç”Ÿæˆ**: ä»è½¨è¿¹ä¸­è‡ªåŠ¨æ‹†è§£å‡ºæœ€å°ç²’åº¦çš„å¯æ‰§è¡Œä»»åŠ¡
- **æ·±åº¦æ‰©å±•**: å°†ç®€å•ä»»åŠ¡é€’å½’æ‰©å±•ä¸ºå¤šæ­¥éª¤å¤æ‚ä»»åŠ¡
- **å®½åº¦æ‰©å±•**: å°†å¤šä¸ªåŸå­ä»»åŠ¡èåˆä¸ºå¤åˆå¹¶è¡Œä»»åŠ¡
- **è‡ªéªŒè¯ç³»ç»Ÿ**: ç¡®ä¿ç”Ÿæˆä»»åŠ¡çš„è´¨é‡å’Œå¯æ‰§è¡Œæ€§
- **å®Œæ•´æ•°æ®æµ**: ä»è½¨è¿¹â†’åŸå­ä»»åŠ¡â†’æ‰©å±•ä»»åŠ¡â†’éªŒè¯â†’æ–°ç§å­ä»»åŠ¡çš„å®Œæ•´ç®¡é“

---

## ğŸ” å½“å‰ç³»ç»Ÿç°çŠ¶åˆ†æ

### ç°æœ‰SynthesisCoreæ¶æ„ä¼˜åŠ¿
1. **å®Œæ•´çš„è½¨è¿¹è®°å½•ç³»ç»Ÿ**: Enhanced Runtimeå·²èƒ½è®°å½•è¯¦ç»†çš„æ‰§è¡Œè½¨è¿¹
2. **LLMé›†æˆåŸºç¡€**: ç»Ÿä¸€çš„LLMClientæ”¯æŒå¤šç§æ¨¡å‹è°ƒç”¨
3. **å·¥å…·ç”Ÿæ€ç³»ç»Ÿ**: ToolScore+MCPæ¶æ„æä¾›ä¸°å¯Œçš„å·¥å…·èƒ½åŠ›
4. **ä»»åŠ¡æœ¬è´¨æå–**: å·²å®ç°åŸºç¡€çš„TaskEssenceæå–é€»è¾‘
5. **JSONæ–‡ä»¶å­˜å‚¨**: è½»é‡åŒ–ã€å¯æ‰©å±•çš„æ•°æ®å­˜å‚¨æ–¹æ¡ˆ

### å…³é”®ç¼ºå¤±èƒ½åŠ›
1. **ä¸»åŠ¨è¯­æ–™é‡‡æ ·**: ç›®å‰åªæ¶ˆè´¹å·²å®Œæˆè½¨è¿¹ï¼Œæ— ä¸»åŠ¨é‡‡æ ·è¯­æ–™ä¸é€ é¢˜é€»è¾‘
2. **åŸå­ä»»åŠ¡æ‹†è§£**: ç¼ºä¹ä»å®Œæ•´è½¨è¿¹ä¸­æ‹†è§£å‡ºåŸå­ä»»åŠ¡çš„èƒ½åŠ›
3. **ä»»åŠ¡æ‰©å±•ç®—æ³•**: æ— æ·±åº¦/å®½åº¦ä¼˜å…ˆçš„ä»»åŠ¡å¤æ‚åŒ–æœºåˆ¶
4. **ä¸¥æ ¼éªŒè¯æµç¨‹**: ç¼ºä¹å¯¹ç”Ÿæˆä»»åŠ¡çš„è‡ªåŠ¨åŒ–è´¨é‡éªŒè¯
5. **è‡ªé€‚åº”Prompt**: æ— æ³•æ ¹æ®éªŒè¯ç»“æœåŠ¨æ€ä¼˜åŒ–ç”Ÿæˆç­–ç•¥

---

## ğŸ—ï¸ TaskCraftæ ¸å¿ƒç®—æ³•è§£æ

åŸºäºå¯¹TaskCraftæºç çš„æ·±åº¦åˆ†æï¼Œæ ¸å¿ƒç®—æ³•åŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®ç»„ä»¶ï¼š

### 1. åŸå­ä»»åŠ¡ç”Ÿæˆç®—æ³• (Atomic Task Generation)
```python
# æ ¸å¿ƒæµç¨‹: i_T â†’ æ‰§è¡Œå·¥å…·å–å¾—ä¸Šä¸‹æ–‡ C â†’ LLM æŠ½å–ç­”æ¡ˆ a å¹¶æ¨æ–­å…³ç³» R â†’ ç»„è£…é—®å¥ q
def generate_atomic_task(content_source):
    # 1. å†…å®¹è¯†åˆ«å’Œè¯­æ–™æå–
    content_identifier, readed_context = extract_content(content_source)
    
    # 2. ç»“è®ºæå– (Conclusion Extraction)
    conclusions = extract_conclusions(readed_context)  # æ¯ä¸ªç»“è®ºåŒ…å« {conclusion, R}
    
    # 3. é—®é¢˜ç”Ÿæˆ (Question Generation) 
    initial_questions = generate_questions(conclusions, content_identifier)
    
    # 4. åŸå­æ€§éªŒè¯ (Atomicity Verification)
    atomic_tasks = filter_atomic_questions(initial_questions)
    
    return atomic_tasks
```

**å…³é”®ç‰¹æ€§**:
- **åŸå­æ€§**: æ¯ä¸ªä»»åŠ¡å¿…é¡»æ˜¯ä¸å¯å†åˆ†çš„åŸºæœ¬äº‹å®
- **å¯éªŒè¯æ€§**: åŒ…å«æ˜ç¡®çš„æ•°å€¼ã€æ—¶é—´æˆ–å”¯ä¸€æ ‡è¯†ç¬¦
- **åŒé‡éªŒè¯**: Agentèƒ½è§£ä½†çº¯LLMè§£ä¸äº†ï¼Œç¡®ä¿éœ€è¦å·¥å…·è°ƒç”¨

### 2. æ·±åº¦ä¼˜å…ˆæ‰©å±•ç®—æ³• (Depth-based Extension)
```python
# æ ¸å¿ƒæ€æƒ³: æ‰¾åˆ°å½“å‰ä»»åŠ¡è¾“å…¥çš„"è¶…é›†"ï¼Œæ„å»ºä¸­é—´ä»»åŠ¡å±‚çº§
def depth_extend(atomic_task, max_hops=2):
    current_element = atomic_task.answer
    extended_tasks = [atomic_task]
    
    for hop in range(max_hops):
        # 1. åå‘æœç´¢: å¯»æ‰¾åŒ…å«å½“å‰å…ƒç´ çš„è¶…é›†
        superset_info = backward_search(current_element)
        
        # 2. è¶…é›†éªŒè¯: ç¡®ä¿è¶…é›†â†’å…ƒç´ çš„å”¯ä¸€æ˜ å°„å…³ç³»
        if not validate_superset(superset_info, current_element):
            break
            
        # 3. ä¸­é—´ä»»åŠ¡ç”Ÿæˆ: åŸºäºè¶…é›†ç”Ÿæˆæ–°é—®é¢˜
        intermediate_task = generate_intermediate_task(superset_info)
        
        # 4. ä»»åŠ¡åˆå¹¶: å°†ä¸­é—´ä»»åŠ¡åˆå¹¶åˆ°åŸé—®é¢˜ä¸­
        extended_task = merge_tasks(extended_tasks[-1], intermediate_task)
        
        # 5. éªŒè¯æ‰©å±•ä»»åŠ¡çš„æœ‰æ•ˆæ€§
        if verify_extended_task(extended_task):
            extended_tasks.append(extended_task)
            current_element = superset_info.identifier
        else:
            break
    
    return extended_tasks
```

**å…³é”®æœºåˆ¶**:
- **è¶…é›†æœç´¢**: é€šè¿‡æœç´¢å·¥å…·æ‰¾åˆ°åŒ…å«å…³ç³» (song â†’ album â†’ artist discography)
- **å…³ç³»éªŒè¯**: LLMåˆ¤æ–­è¶…é›†ä¸åŸå…ƒç´ çš„é€»è¾‘å…³ç³»
- **æ¸è¿›å¤æ‚åŒ–**: æ¯ä¸€è·³å¢åŠ ä¸€ä¸ªæ¨ç†æ­¥éª¤

### 3. å®½åº¦ä¼˜å…ˆæ‰©å±•ç®—æ³• (Width-based Extension)
```python
# æ ¸å¿ƒæ€æƒ³: å°†å¤šä¸ªç‹¬ç«‹åŸå­ä»»åŠ¡èåˆä¸ºå¤åˆä»»åŠ¡
def width_extend(atomic_tasks_batch):
    # 1. ä»»åŠ¡åˆ†ç»„: åŸºäºå†…å®¹ç›¸å…³æ€§è¿›è¡Œæ™ºèƒ½åˆ†ç»„
    grouped_tasks = group_related_tasks(atomic_tasks_batch)
    
    merged_tasks = []
    for group in grouped_tasks:
        # 2. é—®é¢˜èåˆ: LLMé‡å†™å¤šä¸ªé—®é¢˜ä¸ºä¸€ä¸ªè¿è´¯é—®é¢˜
        merged_question = merge_questions(group)
        
        # 3. åŒé‡éªŒè¯
        # 3a. åˆ†è§£éªŒè¯: å¤åˆé—®é¢˜èƒ½å¦åˆ†è§£å›åŸé—®é¢˜
        if not validate_decomposition(merged_question, group):
            continue
            
        # 3b. LLMéªŒè¯: çº¯LLMä¸èƒ½ç›´æ¥è§£å†³
        if not validate_complexity(merged_question):
            continue
            
        merged_tasks.append(merged_question)
    
    return merged_tasks
```

**å…³é”®ç‰¹æ€§**:
- **æ™ºèƒ½åˆ†ç»„**: åŸºäºä¸»é¢˜ç›¸å…³æ€§å°†2-3ä¸ªåŸå­ä»»åŠ¡ç»„åˆ
- **è¯­ä¹‰èåˆ**: ä¿æŒæ‰€æœ‰åŸå§‹ä¿¡æ¯çš„åŒæ—¶ç”Ÿæˆè‡ªç„¶è¯­è¨€é—®é¢˜
- **ä¸¥æ ¼éªŒè¯**: ç¡®ä¿èåˆåçš„é—®é¢˜é€»è¾‘æ¸…æ™°ä¸”å…·æœ‰æŒ‘æˆ˜æ€§

---

## ğŸš€ å¢å¼ºæ–¹æ¡ˆè®¾è®¡

### æ ¸å¿ƒæ¶æ„å‡çº§

```
Enhanced SynthesisCore v2.0
â”œâ”€â”€ 1. Corpus Ingestor (æ–°å¢)
â”‚   â”œâ”€â”€ TrajectoryCorpusExtractor  # ä»è½¨è¿¹æå–è¯­æ–™
â”‚   â”œâ”€â”€ ExternalCorpusLoader       # å¤–éƒ¨è¯­æ–™å¯¼å…¥
â”‚   â””â”€â”€ ContentProcessor           # å†…å®¹é¢„å¤„ç†å™¨
â”œâ”€â”€ 2. Atomic Task Generator (æ–°å¢)
â”‚   â”œâ”€â”€ ConclusionExtractor        # ç»“è®ºæå–å™¨
â”‚   â”œâ”€â”€ QuestionGenerator          # é—®é¢˜ç”Ÿæˆå™¨
â”‚   â”œâ”€â”€ AtomicityVerifier         # åŸå­æ€§éªŒè¯å™¨
â”‚   â””â”€â”€ TaskQualityFilter         # ä»»åŠ¡è´¨é‡è¿‡æ»¤å™¨
â”œâ”€â”€ 3. Task Extender (æ–°å¢)
â”‚   â”œâ”€â”€ DepthExtender             # æ·±åº¦ä¼˜å…ˆæ‰©å±•
â”‚   â”œâ”€â”€ WidthExtender             # å®½åº¦ä¼˜å…ˆæ‰©å±•
â”‚   â”œâ”€â”€ SupersetSearchAgent      # è¶…é›†æœç´¢ä»£ç†
â”‚   â””â”€â”€ TaskMerger               # ä»»åŠ¡èåˆå™¨
â”œâ”€â”€ 4. Verification Engine (å¢å¼º)
â”‚   â”œâ”€â”€ TaskExecutabilityVerifier # ä»»åŠ¡å¯æ‰§è¡Œæ€§éªŒè¯
â”‚   â”œâ”€â”€ QualityScorer            # è´¨é‡è¯„åˆ†å™¨
â”‚   â”œâ”€â”€ DifficultyEstimator      # éš¾åº¦ä¼°ç®—å™¨
â”‚   â””â”€â”€ ValidationReporter       # éªŒè¯æŠ¥å‘Šå™¨
â”œâ”€â”€ 5. Adaptive Prompt Manager (æ–°å¢)
â”‚   â”œâ”€â”€ PromptOptimizer          # æç¤ºè¯ä¼˜åŒ–å™¨
â”‚   â”œâ”€â”€ FewShotExampleManager   # å°‘æ ·æœ¬ç¤ºä¾‹ç®¡ç†
â”‚   â”œâ”€â”€ SuccessRateTracker       # æˆåŠŸç‡è·Ÿè¸ªå™¨
â”‚   â””â”€â”€ PromptVersionControl     # æç¤ºè¯ç‰ˆæœ¬æ§åˆ¶
â””â”€â”€ 6. Enhanced Export Pipeline (å‡çº§)
    â”œâ”€â”€ TaskDatasetExporter      # ä»»åŠ¡æ•°æ®é›†å¯¼å‡ºå™¨
    â”œâ”€â”€ TrainingDataFormatter   # è®­ç»ƒæ•°æ®æ ¼å¼åŒ–
    â”œâ”€â”€ QualityMetricsReporter  # è´¨é‡æŒ‡æ ‡æŠ¥å‘Š
    â””â”€â”€ FineTuningPipelineConnector # å¾®è°ƒç®¡é“è¿æ¥å™¨
```

### 1. Corpus Ingestor: ä¸»åŠ¨è¯­æ–™é‡‡æ ·

**ç›®æ ‡**: å°†è¢«åŠ¨è½¨è¿¹æ¶ˆè´¹å‡çº§ä¸ºä¸»åŠ¨è¯­æ–™é‡‡æ ·å’Œç”Ÿæˆ

```python
class CorpusIngestor:
    """è¯­æ–™å¯¼å…¥å™¨ - ä¸»åŠ¨é‡‡æ ·å’Œè½¨è¿¹å¤„ç†"""
    
    async def extract_from_trajectories(self, trajectories: List[TrajectoryResult]) -> List[CorpusContent]:
        """ä»è½¨è¿¹ä¸­æå–åŸå­è¯­æ–™"""
        corpus_contents = []
        
        for trajectory in trajectories:
            # 1. æ­¥éª¤çº§åˆ«çš„è¯­æ–™æå–
            for step in trajectory.steps:
                if step.action_type == ActionType.TOOL_CALL:
                    if 'browser_navigator' in step.action_params.get('tool_id', ''):
                        # ä»æµè§ˆå™¨å·¥å…·çš„è¾“å‡ºä¸­æå–ç½‘é¡µå†…å®¹
                        content = self._extract_web_content(step.observation)
                        corpus_contents.append(CorpusContent(
                            source=f"trajectory_{trajectory.task_id}_step_{step.step_id}",
                            content_type="web",
                            text_content=content,
                            metadata={
                                "url": step.action_params.get('url'),
                                "task_description": trajectory.task_description
                            }
                        ))
                    elif 'python_executor' in step.action_params.get('tool_id', ''):
                        # ä»ä»£ç æ‰§è¡Œç»“æœä¸­æå–æ•°æ®
                        result = self._extract_code_results(step.observation)
                        corpus_contents.append(CorpusContent(
                            source=f"trajectory_{trajectory.task_id}_step_{step.step_id}",
                            content_type="code_output",
                            text_content=result,
                            metadata={
                                "code": step.action_params.get('code'),
                                "execution_success": step.success
                            }
                        ))
        
        return corpus_contents
    
    async def active_corpus_sampling(self, domains: List[str]) -> List[CorpusContent]:
        """ä¸»åŠ¨è¯­æ–™é‡‡æ ·"""
        corpus_contents = []
        
        for domain in domains:
            # ä½¿ç”¨æœç´¢å·¥å…·ä¸»åŠ¨é‡‡æ ·ç›¸å…³è¯­æ–™
            search_queries = await self._generate_domain_queries(domain)
            for query in search_queries:
                # è°ƒç”¨æœç´¢MCPæœåŠ¡è·å–å†…å®¹
                search_results = await self.mcp_client.call_tool("deepsearch", {
                    "query": query,
                    "max_results": 5
                })
                
                for result in search_results.get('results', []):
                    # ä½¿ç”¨æµè§ˆå™¨å·¥å…·è·å–å®Œæ•´å†…å®¹
                    content = await self.mcp_client.call_tool("browser_navigator", {
                        "action": "navigate",
                        "url": result['url']
                    })
                    
                    corpus_contents.append(CorpusContent(
                        source=f"active_sampling_{domain}",
                        content_type="web",
                        text_content=content.get('page_text', ''),
                        metadata={
                            "domain": domain,
                            "search_query": query,
                            "url": result['url']
                        }
                    ))
        
        return corpus_contents
```

### 2. Atomic Task Generator: åŸå­ä»»åŠ¡ç”Ÿæˆå¼•æ“

**ç›®æ ‡**: å®ç°ä»è¯­æ–™åˆ°åŸå­ä»»åŠ¡çš„å®Œæ•´ç”Ÿæˆç®¡é“

```python
class AtomicTaskGenerator:
    """åŸå­ä»»åŠ¡ç”Ÿæˆå™¨ - æ ¸å¿ƒç®—æ³•å®ç°"""
    
    def __init__(self, llm_client: LLMClient, verification_agent: VerificationAgent):
        self.llm_client = llm_client
        self.verification_agent = verification_agent
        self.atomic_prompts = self._load_atomic_prompts()
    
    async def generate_atomic_tasks(self, corpus_content: CorpusContent) -> List[AtomicTask]:
        """ä»è¯­æ–™ç”ŸæˆåŸå­ä»»åŠ¡"""
        
        # 1. å†…å®¹è¯†åˆ«å’Œé¢„å¤„ç†
        content_identifier = await self._identify_content(corpus_content)
        processed_content = await self._preprocess_content(corpus_content)
        
        # 2. ç»“è®ºæå– (åŸºäºTaskCraftç®—æ³•)
        conclusions = await self._extract_conclusions(processed_content, content_identifier)
        
        # 3. åˆå§‹é—®é¢˜ç”Ÿæˆ
        candidate_questions = []
        for conclusion in conclusions:
            question = await self._generate_question_from_conclusion(conclusion, content_identifier)
            if question:
                candidate_questions.append(question)
        
        # 4. åŸå­æ€§éªŒè¯å’Œè¿‡æ»¤
        atomic_tasks = []
        for question in candidate_questions:
            verification_result = await self._verify_atomicity(question)
            if verification_result['is_atomic'] and verification_result['requires_tools']:
                atomic_tasks.append(AtomicTask(
                    task_id=f"atomic_{self._generate_id()}",
                    question=question['question'],
                    golden_answer=question['answer'],
                    content_identifier=content_identifier,
                    source_corpus=corpus_content.source,
                    verification_score=verification_result['score'],
                    required_tools=verification_result['tools_needed']
                ))
        
        return atomic_tasks
    
    async def _extract_conclusions(self, content: str, identifier: str) -> List[Dict]:
        """æå–åŸå­ç»“è®º - å¤ç”¨TaskCraftçš„æç¤ºè¯ç­–ç•¥"""
        prompt = self.atomic_prompts['extract_conclusions'].format(
            content=content,
            identifier=identifier
        )
        
        response = await self.llm_client.generate_reasoning(
            task_description=prompt,
            available_tools=[],  # çº¯LLMæ¨ç†ä»»åŠ¡
            execution_context={"mode": "conclusion_extraction"}
        )
        
        # è§£æç»“è®ºï¼Œæ¯ä¸ªç»“è®ºåŒ…å« {conclusion, R(relationship)}
        return self._parse_conclusions_response(response['thinking'])
    
    async def _verify_atomicity(self, question: Dict) -> Dict:
        """éªŒè¯ä»»åŠ¡åŸå­æ€§ - åŒé‡éªŒè¯æœºåˆ¶"""
        
        # éªŒè¯1: Agentèƒ½è§£ä½†çº¯LLMè§£ä¸äº†
        agent_result = await self.verification_agent.solve_with_tools(
            question['question'], expected_answer=question['answer']
        )
        
        llm_result = await self.verification_agent.solve_without_tools(
            question['question'], expected_answer=question['answer']
        )
        
        # éªŒè¯2: ç¡®ä¿ä»»åŠ¡æ˜¯ä¸å¯å†åˆ†çš„
        atomicity_check = await self._check_task_atomicity(question['question'])
        
        return {
            'is_atomic': atomicity_check['is_atomic'],
            'requires_tools': agent_result['success'] and not llm_result['success'],
            'score': agent_result['confidence'],
            'tools_needed': agent_result['tools_used']
        }
```

### 3. Task Extender: ä»»åŠ¡æ‰©å±•å¼•æ“

#### 3.1 æ·±åº¦ä¼˜å…ˆæ‰©å±• (Depth-based Extension)

```python
class DepthExtender:
    """æ·±åº¦ä¼˜å…ˆä»»åŠ¡æ‰©å±•å™¨"""
    
    async def extend_task_depth(self, atomic_task: AtomicTask, max_hops: int = 2) -> List[ExtendedTask]:
        """æ·±åº¦æ‰©å±•: åŸå­ä»»åŠ¡ â†’ å¤šæ­¥æ¨ç†ä»»åŠ¡"""
        extended_tasks = [atomic_task]
        current_element = atomic_task.golden_answer
        
        for hop in range(max_hops):
            logger.info(f"ğŸ”„ å¼€å§‹ç¬¬ {hop + 1} è·³æ·±åº¦æ‰©å±•")
            
            # 1. åå‘æœç´¢è¶…é›†
            superset_info = await self._backward_search(current_element)
            if not superset_info:
                logger.warning(f"âŒ ç¬¬ {hop + 1} è·³: æœªæ‰¾åˆ°æœ‰æ•ˆè¶…é›†")
                break
            
            # 2. éªŒè¯è¶…é›†-å­é›†å…³ç³»
            if not await self._validate_superset_relation(superset_info, current_element):
                logger.warning(f"âŒ ç¬¬ {hop + 1} è·³: è¶…é›†å…³ç³»éªŒè¯å¤±è´¥")
                break
            
            # 3. ç”Ÿæˆä¸­é—´ä»»åŠ¡
            intermediate_task = await self._generate_intermediate_task(superset_info, current_element)
            
            # 4. ä»»åŠ¡åˆå¹¶
            base_task = extended_tasks[-1]
            merged_task = await self._merge_tasks(base_task, intermediate_task, current_element)
            
            # 5. éªŒè¯æ‰©å±•ä»»åŠ¡
            if await self._verify_extended_task(merged_task):
                extended_tasks.append(merged_task)
                current_element = superset_info['identifier']
                logger.info(f"âœ… ç¬¬ {hop + 1} è·³æ‰©å±•æˆåŠŸ")
            else:
                logger.warning(f"âŒ ç¬¬ {hop + 1} è·³: æ‰©å±•ä»»åŠ¡éªŒè¯å¤±è´¥")
                break
        
        return extended_tasks
    
    async def _backward_search(self, element: str) -> Optional[Dict]:
        """åå‘æœç´¢: å¯»æ‰¾åŒ…å«å½“å‰å…ƒç´ çš„è¶…é›†"""
        
        # ä½¿ç”¨æœç´¢MCPå·¥å…·å¯»æ‰¾è¶…é›†
        search_queries = await self._generate_superset_queries(element)
        
        for query in search_queries:
            search_results = await self.mcp_client.call_tool("deepsearch", {
                "query": query,
                "max_results": 10
            })
            
            # ä½¿ç”¨LLMåˆ†ææœç´¢ç»“æœï¼Œæ‰¾åˆ°æœ€ä½³è¶…é›†
            superset_analysis = await self.llm_client.generate_reasoning(
                task_description=f"ä»æœç´¢ç»“æœä¸­è¯†åˆ« '{element}' çš„è¶…é›†",
                available_tools=[],
                execution_context={
                    "search_results": search_results,
                    "element": element,
                    "mode": "superset_identification"
                }
            )
            
            if superset_analysis.get('confidence', 0) > 0.7:
                return superset_analysis.get('superset_info')
        
        return None
    
    async def _merge_tasks(self, base_task: TaskBase, intermediate_task: Dict, element: str) -> ExtendedTask:
        """ä»»åŠ¡åˆå¹¶: å°†ä¸­é—´ä»»åŠ¡åˆå¹¶åˆ°åŸºç¡€ä»»åŠ¡ä¸­"""
        
        merge_prompt = f"""
        åŸºç¡€ä»»åŠ¡: {base_task.question}
        ä¸­é—´ä»»åŠ¡: {intermediate_task['question']}
        è¦æ›¿æ¢çš„å…ƒç´ : {element}
        
        è¯·å°†ä¸­é—´ä»»åŠ¡åˆå¹¶åˆ°åŸºç¡€ä»»åŠ¡ä¸­ï¼Œç”Ÿæˆä¸€ä¸ªæ›´å¤æ‚ä½†é€»è¾‘æ¸…æ™°çš„é—®é¢˜ã€‚
        è¦æ±‚:
        1. æ–°é—®é¢˜åº”è¯¥æ¯”åŸé—®é¢˜å¤æ‚ä¸€ä¸ªæ¨ç†æ­¥éª¤
        2. ç­”æ¡ˆä»ç„¶æŒ‡å‘: {base_task.golden_answer}
        3. ä¸èƒ½é€éœ²ç­”æ¡ˆä¿¡æ¯
        4. ä¿æŒè¯­è¨€è‡ªç„¶æµç•…
        """
        
        merge_result = await self.llm_client.generate_reasoning(
            task_description=merge_prompt,
            available_tools=[],
            execution_context={"mode": "task_merging"}
        )
        
        return ExtendedTask(
            task_id=f"depth_extended_{self._generate_id()}",
            question=merge_result['merged_question'],
            golden_answer=base_task.golden_answer,
            hop_level=getattr(base_task, 'hop_level', 1) + 1,
            source_atomic_task=base_task.task_id,
            intermediate_steps=[intermediate_task],
            expected_tools=base_task.required_tools + intermediate_task.get('tools_needed', [])
        )
```

#### 3.2 å®½åº¦ä¼˜å…ˆæ‰©å±• (Width-based Extension)

```python
class WidthExtender:
    """å®½åº¦ä¼˜å…ˆä»»åŠ¡æ‰©å±•å™¨"""
    
    async def extend_task_width(self, atomic_tasks_batch: List[AtomicTask]) -> List[CompositeTask]:
        """å®½åº¦æ‰©å±•: å¤šä¸ªåŸå­ä»»åŠ¡ â†’ å¤åˆå¹¶è¡Œä»»åŠ¡"""
        
        # 1. æ™ºèƒ½åˆ†ç»„: åŸºäºå†…å®¹ç›¸å…³æ€§
        task_groups = await self._group_related_tasks(atomic_tasks_batch)
        
        composite_tasks = []
        for group in task_groups:
            if len(group) < 2:  # éœ€è¦è‡³å°‘2ä¸ªä»»åŠ¡æ‰èƒ½åˆå¹¶
                continue
            
            # 2. ä»»åŠ¡èåˆ
            merged_task = await self._merge_atomic_tasks(group)
            if not merged_task:
                continue
            
            # 3. åŒé‡éªŒè¯
            if await self._validate_composite_task(merged_task, group):
                composite_tasks.append(merged_task)
                logger.info(f"âœ… æˆåŠŸåˆ›å»ºå¤åˆä»»åŠ¡: {len(group)}ä¸ªåŸå­ä»»åŠ¡ â†’ 1ä¸ªå¤åˆä»»åŠ¡")
        
        return composite_tasks
    
    async def _group_related_tasks(self, tasks: List[AtomicTask]) -> List[List[AtomicTask]]:
        """æ™ºèƒ½åˆ†ç»„: åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§å’Œå†…å®¹ç›¸å…³æ€§"""
        
        # ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰åˆ†ç»„
        grouping_prompt = f"""
        è¯·å°†ä»¥ä¸‹åŸå­ä»»åŠ¡æŒ‰ç…§ä¸»é¢˜ç›¸å…³æ€§è¿›è¡Œåˆ†ç»„ï¼Œæ¯ç»„2-3ä¸ªä»»åŠ¡:
        
        ä»»åŠ¡åˆ—è¡¨:
        {json.dumps([{'id': t.task_id, 'question': t.question, 'identifier': t.content_identifier} for t in tasks], ensure_ascii=False, indent=2)}
        
        åˆ†ç»„è¦æ±‚:
        1. åŒç»„ä»»åŠ¡å¿…é¡»æœ‰æ˜ç¡®çš„ä¸»é¢˜å…³è”
        2. æ¯ç»„åŒ…å«2-3ä¸ªä»»åŠ¡
        3. ç¡®ä¿æ‰€æœ‰ä»»åŠ¡éƒ½è¢«åˆ†ç»„
        """
        
        grouping_result = await self.llm_client.generate_reasoning(
            task_description=grouping_prompt,
            available_tools=[],
            execution_context={"mode": "task_grouping"}
        )
        
        # è§£æåˆ†ç»„ç»“æœï¼Œè¿”å›åˆ†ç»„åçš„ä»»åŠ¡åˆ—è¡¨
        return self._parse_grouping_result(grouping_result, tasks)
    
    async def _merge_atomic_tasks(self, task_group: List[AtomicTask]) -> Optional[CompositeTask]:
        """å°†å¤šä¸ªåŸå­ä»»åŠ¡èåˆä¸ºä¸€ä¸ªå¤åˆä»»åŠ¡"""
        
        merge_prompt = f"""
        è¯·å°†ä»¥ä¸‹ç›¸å…³çš„åŸå­ä»»åŠ¡èåˆä¸ºä¸€ä¸ªè¿è´¯çš„å¤åˆé—®é¢˜:
        
        åŸå­ä»»åŠ¡:
        {json.dumps([{'question': t.question, 'answer': t.golden_answer} for t in task_group], ensure_ascii=False, indent=2)}
        
        èåˆè¦æ±‚:
        1. ä¿ç•™æ‰€æœ‰åŸå§‹ä»»åŠ¡çš„ä¿¡æ¯è¦ç´ 
        2. å½¢æˆé€»è¾‘è¿è´¯çš„å¤åˆé—®é¢˜
        3. ä¸æ·»åŠ åŸä»»åŠ¡ä¸­æ²¡æœ‰çš„æ–°ä¿¡æ¯
        4. ç¡®ä¿é—®é¢˜å¯ä»¥åˆ†è§£å›åŸä»»åŠ¡
        """
        
        merge_result = await self.llm_client.generate_reasoning(
            task_description=merge_prompt,
            available_tools=[],
            execution_context={"mode": "width_merging"}
        )
        
        if merge_result.get('confidence', 0) < 0.8:
            return None
        
        return CompositeTask(
            task_id=f"width_extended_{self._generate_id()}",
            question=merge_result['merged_question'],
            golden_answers=[task.golden_answer for task in task_group],
            source_atomic_tasks=[task.task_id for task in task_group],
            original_questions=[task.question for task in task_group],
            content_identifier=task_group[0].content_identifier,  # ä½¿ç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡çš„æ ‡è¯†ç¬¦
            expected_tools=list(set().union(*[task.required_tools for task in task_group]))
        )
    
    async def _validate_composite_task(self, composite_task: CompositeTask, original_tasks: List[AtomicTask]) -> bool:
        """éªŒè¯å¤åˆä»»åŠ¡çš„è´¨é‡"""
        
        # éªŒè¯1: åˆ†è§£éªŒè¯ - å¤åˆé—®é¢˜èƒ½å¦åˆ†è§£å›åŸé—®é¢˜
        decomposition_result = await self._validate_decomposition(composite_task, original_tasks)
        
        # éªŒè¯2: å¤æ‚æ€§éªŒè¯ - ç¡®ä¿LLMä¸èƒ½ç›´æ¥è§£å†³
        complexity_result = await self._validate_complexity(composite_task)
        
        return decomposition_result and complexity_result
```

### 4. Verification Engine: éªŒè¯å¼•æ“å‡çº§

```python
class EnhancedVerificationEngine:
    """å¢å¼ºéªŒè¯å¼•æ“ - ç¡®ä¿ç”Ÿæˆä»»åŠ¡è´¨é‡"""
    
    async def comprehensive_task_verification(self, task: Union[AtomicTask, ExtendedTask, CompositeTask]) -> VerificationResult:
        """ç»¼åˆä»»åŠ¡éªŒè¯"""
        
        verification_results = {}
        
        # 1. å¯æ‰§è¡Œæ€§éªŒè¯
        executability = await self._verify_executability(task)
        verification_results['executability'] = executability
        
        # 2. éš¾åº¦é€‚ä¸­æ€§éªŒè¯  
        difficulty = await self._assess_difficulty(task)
        verification_results['difficulty'] = difficulty
        
        # 3. ç­”æ¡ˆå”¯ä¸€æ€§éªŒè¯
        answer_uniqueness = await self._verify_answer_uniqueness(task)
        verification_results['answer_uniqueness'] = answer_uniqueness
        
        # 4. å·¥å…·éœ€æ±‚éªŒè¯
        tool_requirements = await self._verify_tool_requirements(task)
        verification_results['tool_requirements'] = tool_requirements
        
        # 5. è¯­è¨€è´¨é‡éªŒè¯
        language_quality = await self._assess_language_quality(task)
        verification_results['language_quality'] = language_quality
        
        # ç»¼åˆè¯„åˆ†
        overall_score = self._calculate_overall_score(verification_results)
        
        return VerificationResult(
            task_id=task.task_id,
            overall_score=overall_score,
            details=verification_results,
            recommendation=self._get_recommendation(overall_score),
            suggested_improvements=self._suggest_improvements(verification_results)
        )
    
    async def _verify_executability(self, task) -> Dict:
        """éªŒè¯ä»»åŠ¡å¯æ‰§è¡Œæ€§"""
        try:
            # ä½¿ç”¨éªŒè¯ä»£ç†å®é™…æ‰§è¡Œä»»åŠ¡
            execution_result = await self.verification_agent.execute_task(
                task.question,
                expected_answer=task.golden_answer if hasattr(task, 'golden_answer') else task.golden_answers,
                timeout=60
            )
            
            return {
                'executable': execution_result.success,
                'execution_time': execution_result.duration,
                'tools_used': execution_result.tools_used,
                'error_message': execution_result.error_message if not execution_result.success else None
            }
        except Exception as e:
            return {
                'executable': False,
                'execution_time': 0,
                'tools_used': [],
                'error_message': str(e)
            }
    
    async def _assess_difficulty(self, task) -> Dict:
        """è¯„ä¼°ä»»åŠ¡éš¾åº¦"""
        difficulty_indicators = {
            'steps_required': 1,  # é»˜è®¤1æ­¥
            'tools_required': len(getattr(task, 'expected_tools', [])),
            'reasoning_complexity': 'simple'
        }
        
        # å¯¹äºæ‰©å±•ä»»åŠ¡ï¼Œè¯„ä¼°å…¶å¤æ‚åº¦
        if isinstance(task, ExtendedTask):
            difficulty_indicators['steps_required'] = task.hop_level
            difficulty_indicators['reasoning_complexity'] = 'complex' if task.hop_level > 2 else 'medium'
        elif isinstance(task, CompositeTask):
            difficulty_indicators['steps_required'] = len(task.source_atomic_tasks)
            difficulty_indicators['reasoning_complexity'] = 'complex'
        
        # ä½¿ç”¨LLMè¯„ä¼°è®¤çŸ¥éš¾åº¦
        cognitive_assessment = await self._llm_assess_cognitive_difficulty(task)
        difficulty_indicators.update(cognitive_assessment)
        
        return difficulty_indicators
```

### 5. Adaptive Prompt Manager: è‡ªé€‚åº”æç¤ºè¯ç®¡ç†

```python
class AdaptivePromptManager:
    """è‡ªé€‚åº”æç¤ºè¯ç®¡ç†å™¨ - æ ¹æ®éªŒè¯ç»“æœåŠ¨æ€ä¼˜åŒ–"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client
        self.prompt_versions = {}  # æç¤ºè¯ç‰ˆæœ¬ç®¡ç†
        self.success_rates = {}    # æˆåŠŸç‡è·Ÿè¸ª
        self.few_shot_examples = {} # å°‘æ ·æœ¬ç¤ºä¾‹æ± 
    
    async def optimize_prompts_based_on_feedback(self, task_type: str, verification_results: List[VerificationResult]):
        """åŸºäºéªŒè¯åé¦ˆä¼˜åŒ–æç¤ºè¯"""
        
        # 1. åˆ†æéªŒè¯ç»“æœæ¨¡å¼
        failure_patterns = self._analyze_failure_patterns(verification_results)
        
        # 2. æ›´æ–°å°‘æ ·æœ¬ç¤ºä¾‹æ± 
        await self._update_few_shot_examples(task_type, verification_results)
        
        # 3. ç”Ÿæˆä¼˜åŒ–æç¤ºè¯
        if failure_patterns['needs_optimization']:
            optimized_prompt = await self._generate_optimized_prompt(task_type, failure_patterns)
            
            # 4. A/Bæµ‹è¯•æ–°æç¤ºè¯
            test_results = await self._ab_test_prompt(task_type, optimized_prompt)
            
            # 5. æ›´æ–°æœ€ä½³æç¤ºè¯
            if test_results['improvement'] > 0.1:  # 10%æ”¹è¿›é˜ˆå€¼
                self._update_prompt_version(task_type, optimized_prompt)
                logger.info(f"âœ… æç¤ºè¯ä¼˜åŒ–æˆåŠŸ: {task_type} æˆåŠŸç‡æå‡ {test_results['improvement']:.2%}")
    
    async def _update_few_shot_examples(self, task_type: str, verification_results: List[VerificationResult]):
        """æ›´æ–°å°‘æ ·æœ¬ç¤ºä¾‹æ± """
        
        if task_type not in self.few_shot_examples:
            self.few_shot_examples[task_type] = {'positive': [], 'negative': []}
        
        for result in verification_results:
            if result.overall_score > 0.8:  # é«˜è´¨é‡ç¤ºä¾‹
                self.few_shot_examples[task_type]['positive'].append({
                    'task': result.task_data,
                    'score': result.overall_score,
                    'timestamp': datetime.now()
                })
            elif result.overall_score < 0.4:  # å¤±è´¥ç¤ºä¾‹
                self.few_shot_examples[task_type]['negative'].append({
                    'task': result.task_data,
                    'issues': result.details,
                    'timestamp': datetime.now()
                })
        
        # ä¿æŒç¤ºä¾‹æ± å¤§å°
        for category in ['positive', 'negative']:
            self.few_shot_examples[task_type][category] = sorted(
                self.few_shot_examples[task_type][category],
                key=lambda x: x.get('score', x.get('timestamp')),
                reverse=True
            )[:20]  # ä¿ç•™æœ€å¥½çš„20ä¸ªç¤ºä¾‹
```

### 6. Enhanced Export Pipeline: å¢å¼ºå¯¼å‡ºç®¡é“

```python
class EnhancedExportPipeline:
    """å¢å¼ºå¯¼å‡ºç®¡é“ - ç”Ÿæˆè®­ç»ƒæ•°æ®å’Œè¯„ä¼°æŠ¥å‘Š"""
    
    async def export_training_dataset(self, tasks: List[Union[AtomicTask, ExtendedTask, CompositeTask]], format_type: str = "sft") -> str:
        """å¯¼å‡ºè®­ç»ƒæ•°æ®é›†"""
        
        if format_type == "sft":  # Supervised Fine-Tuningæ ¼å¼
            return await self._export_sft_format(tasks)
        elif format_type == "rl":  # Reinforcement Learningæ ¼å¼
            return await self._export_rl_format(tasks)
        elif format_type == "evaluation":  # è¯„ä¼°æ•°æ®é›†æ ¼å¼
            return await self._export_evaluation_format(tasks)
        else:
            raise ValueError(f"Unsupported format: {format_type}")
    
    async def _export_sft_format(self, tasks: List) -> str:
        """å¯¼å‡ºSFTè®­ç»ƒæ ¼å¼"""
        sft_data = []
        
        for task in tasks:
            # ä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆå®Œæ•´çš„å¯¹è¯æ ¼å¼
            conversation = await self._generate_training_conversation(task)
            sft_data.append(conversation)
        
        # ä¿å­˜ä¸ºJSONLæ ¼å¼
        export_path = get_output_dir() / f"sft_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
        
        async with aiofiles.open(export_path, 'w', encoding='utf-8') as f:
            for item in sft_data:
                await f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        logger.info(f"âœ… SFTæ•°æ®é›†å¯¼å‡ºå®Œæˆ: {export_path} ({len(sft_data)} æ¡è®°å½•)")
        return str(export_path)
    
    async def generate_quality_report(self, verification_results: List[VerificationResult]) -> str:
        """ç”Ÿæˆè´¨é‡è¯„ä¼°æŠ¥å‘Š"""
        
        report = {
            "summary": {
                "total_tasks": len(verification_results),
                "high_quality": len([r for r in verification_results if r.overall_score > 0.8]),
                "medium_quality": len([r for r in verification_results if 0.5 <= r.overall_score <= 0.8]),
                "low_quality": len([r for r in verification_results if r.overall_score < 0.5]),
                "average_score": sum(r.overall_score for r in verification_results) / len(verification_results) if verification_results else 0
            },
            "detailed_analysis": {
                "difficulty_distribution": self._analyze_difficulty_distribution(verification_results),
                "tool_usage_patterns": self._analyze_tool_usage(verification_results),
                "common_failure_modes": self._analyze_failure_modes(verification_results)
            },
            "recommendations": self._generate_quality_recommendations(verification_results)
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = get_output_dir() / f"quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        async with aiofiles.open(report_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(report, ensure_ascii=False, indent=2))
        
        logger.info(f"ğŸ“Š è´¨é‡æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {report_path}")
        return str(report_path)
```

---

## ğŸ“Š å®æ–½è®¡åˆ’å’Œé‡Œç¨‹ç¢‘

### Phase 1: åŸºç¡€è®¾æ–½å»ºè®¾ (2å‘¨)
- [x] å®ŒæˆTaskCraftç®—æ³•åˆ†æ
- [ ] å®ç°Corpus Ingestoræ¨¡å—
- [ ] åˆ›å»ºåŸºç¡€æ•°æ®ç»“æ„å®šä¹‰
- [ ] å»ºç«‹éªŒè¯ä»£ç†æ¡†æ¶
- [ ] é…ç½®å¢å¼ºçš„Redisé˜Ÿåˆ—ç³»ç»Ÿ

### Phase 2: æ ¸å¿ƒç®—æ³•å®ç° (3å‘¨)
- [ ] å®ç°Atomic Task Generator
- [ ] å®ç°Depth Extenderç®—æ³•
- [ ] å®ç°Width Extenderç®—æ³•
- [ ] å»ºç«‹Enhanced Verification Engine
- [ ] é›†æˆLLMå®¢æˆ·ç«¯å¢å¼ºåŠŸèƒ½

### Phase 3: éªŒè¯å’Œä¼˜åŒ– (2å‘¨)
- [ ] å®ç°Adaptive Prompt Manager
- [ ] å»ºç«‹è´¨é‡è¯„ä¼°ä½“ç³»
- [ ] å®ŒæˆA/Bæµ‹è¯•æ¡†æ¶
- [ ] ä¼˜åŒ–æ€§èƒ½å’Œå¹¶å‘å¤„ç†
- [ ] å»ºç«‹ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ

### Phase 4: å¯¼å‡ºå’Œé›†æˆ (1å‘¨)
- [ ] å®ç°Enhanced Export Pipeline
- [ ] å»ºç«‹è®­ç»ƒæ•°æ®ç”Ÿæˆç®¡é“
- [ ] é›†æˆç°æœ‰SynthesisCore API
- [ ] å®Œæˆç«¯åˆ°ç«¯æµ‹è¯•
- [ ] ç¼–å†™ä½¿ç”¨æ–‡æ¡£

### Phase 5: ç”Ÿäº§éƒ¨ç½² (1å‘¨)
- [ ] æ€§èƒ½å‹åŠ›æµ‹è¯•
- [ ] éƒ¨ç½²é…ç½®ä¼˜åŒ–
- [ ] ç›‘æ§ä»ªè¡¨ç›˜å»ºè®¾
- [ ] ç”¨æˆ·åŸ¹è®­å’Œæ–‡æ¡£
- [ ] æ­£å¼å‘å¸ƒv2.0

---

## ğŸ”§ æŠ€æœ¯å®ç°ç»†èŠ‚

### æ ¸å¿ƒæ•°æ®ç»“æ„å®šä¹‰

```python
@dataclass
class CorpusContent:
    """è¯­æ–™å†…å®¹æ•°æ®ç»“æ„"""
    source: str                    # è¯­æ–™æ¥æº
    content_type: str             # å†…å®¹ç±»å‹: web/pdf/code_output
    text_content: str             # æ–‡æœ¬å†…å®¹
    metadata: Dict[str, Any]      # å…ƒæ•°æ®
    extracted_at: str             # æå–æ—¶é—´
    processing_status: str = "pending"  # å¤„ç†çŠ¶æ€

@dataclass  
class AtomicTask:
    """åŸå­ä»»åŠ¡æ•°æ®ç»“æ„"""
    task_id: str                  # ä»»åŠ¡ID
    question: str                 # é—®é¢˜
    golden_answer: str            # æ ‡å‡†ç­”æ¡ˆ
    content_identifier: str       # å†…å®¹æ ‡è¯†ç¬¦
    source_corpus: str            # æºè¯­æ–™
    verification_score: float     # éªŒè¯åˆ†æ•°
    required_tools: List[str]     # æ‰€éœ€å·¥å…·
    difficulty_level: str = "simple"  # éš¾åº¦çº§åˆ«
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())

@dataclass
class ExtendedTask:
    """æ‰©å±•ä»»åŠ¡æ•°æ®ç»“æ„"""
    task_id: str                  # ä»»åŠ¡ID
    question: str                 # æ‰©å±•åé—®é¢˜
    golden_answer: str            # æ ‡å‡†ç­”æ¡ˆ
    hop_level: int                # è·³è·ƒçº§åˆ«
    source_atomic_task: str       # æºåŸå­ä»»åŠ¡ID
    intermediate_steps: List[Dict] # ä¸­é—´æ­¥éª¤
    expected_tools: List[str]     # é¢„æœŸå·¥å…·
    complexity_score: float = 0.0 # å¤æ‚åº¦åˆ†æ•°
    
@dataclass
class CompositeTask:
    """å¤åˆä»»åŠ¡æ•°æ®ç»“æ„"""
    task_id: str                  # ä»»åŠ¡ID
    question: str                 # èåˆåé—®é¢˜
    golden_answers: List[str]     # å¤šä¸ªæ ‡å‡†ç­”æ¡ˆ
    source_atomic_tasks: List[str] # æºåŸå­ä»»åŠ¡IDåˆ—è¡¨
    original_questions: List[str] # åŸå§‹é—®é¢˜åˆ—è¡¨
    content_identifier: str       # å†…å®¹æ ‡è¯†ç¬¦
    expected_tools: List[str]     # é¢„æœŸå·¥å…·
    merge_strategy: str = "width" # åˆå¹¶ç­–ç•¥

@dataclass
class VerificationResult:
    """éªŒè¯ç»“æœæ•°æ®ç»“æ„"""
    task_id: str                  # ä»»åŠ¡ID
    overall_score: float          # æ€»ä½“åˆ†æ•°
    details: Dict[str, Any]       # è¯¦ç»†éªŒè¯ç»“æœ
    recommendation: str           # å»ºè®®
    suggested_improvements: List[str] # æ”¹è¿›å»ºè®®
    verified_at: str = field(default_factory=lambda: datetime.now().isoformat())
```

### Redisé˜Ÿåˆ—è®¾è®¡

```python
# Redis Streamè®¾è®¡
REDIS_STREAMS = {
    "synthesis:corpus_queue": "å¾…å¤„ç†è¯­æ–™é˜Ÿåˆ—",
    "synthesis:atomic_tasks": "åŸå­ä»»åŠ¡é˜Ÿåˆ—", 
    "synthesis:extended_tasks": "æ‰©å±•ä»»åŠ¡é˜Ÿåˆ—",
    "synthesis:verification_queue": "éªŒè¯é˜Ÿåˆ—",
    "synthesis:training_data": "è®­ç»ƒæ•°æ®é˜Ÿåˆ—",
    "synthesis:quality_reports": "è´¨é‡æŠ¥å‘Šé˜Ÿåˆ—"
}

# Redis Keysè®¾è®¡
REDIS_KEYS = {
    "synthesis:config": "é…ç½®ä¿¡æ¯",
    "synthesis:prompt_versions": "æç¤ºè¯ç‰ˆæœ¬",
    "synthesis:success_rates": "æˆåŠŸç‡ç»Ÿè®¡",
    "synthesis:few_shot_examples": "å°‘æ ·æœ¬ç¤ºä¾‹æ± "
}
```

### APIæ¥å£è®¾è®¡

```python
# FastAPIè·¯ç”±æ‰©å±•
@app.post("/synthesis/v2/generate-atomic-tasks")
async def generate_atomic_tasks(corpus_data: CorpusData):
    """ç”ŸæˆåŸå­ä»»åŠ¡"""
    pass

@app.post("/synthesis/v2/extend-tasks-depth")
async def extend_tasks_depth(tasks: List[AtomicTask], max_hops: int = 2):
    """æ·±åº¦æ‰©å±•ä»»åŠ¡"""
    pass

@app.post("/synthesis/v2/extend-tasks-width") 
async def extend_tasks_width(tasks: List[AtomicTask]):
    """å®½åº¦æ‰©å±•ä»»åŠ¡"""
    pass

@app.post("/synthesis/v2/verify-tasks")
async def verify_tasks(tasks: List[Union[AtomicTask, ExtendedTask, CompositeTask]]):
    """éªŒè¯ä»»åŠ¡è´¨é‡"""
    pass

@app.get("/synthesis/v2/export-dataset/{format_type}")
async def export_dataset(format_type: str, task_filters: Optional[Dict] = None):
    """å¯¼å‡ºè®­ç»ƒæ•°æ®é›†"""
    pass

@app.get("/synthesis/v2/quality-report")
async def get_quality_report(date_range: Optional[str] = None):
    """è·å–è´¨é‡æŠ¥å‘Š"""
    pass
```

---

## ğŸ“ˆ é¢„æœŸæˆæœå’ŒKPI

### å®šé‡æŒ‡æ ‡
- **åŸå­ä»»åŠ¡ç”Ÿæˆç‡**: > 80% (ä»è½¨è¿¹ä¸­æˆåŠŸæå–åŸå­ä»»åŠ¡çš„æ¯”ä¾‹)
- **æ‰©å±•æˆåŠŸç‡**: > 70% (æ·±åº¦/å®½åº¦æ‰©å±•çš„æˆåŠŸç‡)
- **éªŒè¯é€šè¿‡ç‡**: > 85% (ç”Ÿæˆä»»åŠ¡é€šè¿‡è´¨é‡éªŒè¯çš„æ¯”ä¾‹)
- **ä»»åŠ¡æ‰§è¡ŒæˆåŠŸç‡**: > 90% (ç”Ÿæˆä»»åŠ¡èƒ½è¢«AgentæˆåŠŸæ‰§è¡Œçš„æ¯”ä¾‹)
- **æ•°æ®é›†è§„æ¨¡**: 10,000+ é«˜è´¨é‡ç§å­ä»»åŠ¡/æœˆ
- **å¤„ç†æ€§èƒ½**: < 5ç§’/ä»»åŠ¡ (å¹³å‡ç”Ÿæˆæ—¶é—´)

### å®šæ€§æŒ‡æ ‡
- **ä»»åŠ¡å¤šæ ·æ€§**: è¦†ç›–reasoning/web/codeä¸‰ç§ä¸»è¦ä»»åŠ¡ç±»å‹
- **éš¾åº¦æ¢¯åº¦**: ä»ç®€å•(1æ­¥)åˆ°å¤æ‚(4+æ­¥)çš„å®Œæ•´éš¾åº¦è°±
- **å·¥å…·é›†æˆåº¦**: å……åˆ†åˆ©ç”¨ç°æœ‰MCPç”Ÿæ€ç³»ç»Ÿ
- **è´¨é‡ç¨³å®šæ€§**: ç”Ÿæˆä»»åŠ¡è´¨é‡çš„ä¸€è‡´æ€§å’Œå¯é¢„æµ‹æ€§

### ä¸šåŠ¡ä»·å€¼
- **è‡ªåŠ¨åŒ–ç¨‹åº¦**: å‡å°‘90%çš„æ‰‹å·¥ä»»åŠ¡åˆ›å»ºå·¥ä½œ
- **æ¨¡å‹è®­ç»ƒæ•ˆæœ**: æä¾›é«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼Œæå‡Agentèƒ½åŠ›
- **ç³»ç»Ÿè¿›åŒ–èƒ½åŠ›**: çœŸæ­£å®ç°"ä»»åŠ¡-èƒ½åŠ›è‡ªè¿›åŒ–"é—­ç¯
- **å¯æ‰©å±•æ€§**: æ”¯æŒæ–°é¢†åŸŸå’Œæ–°å·¥å…·çš„å¿«é€Ÿé›†æˆ

---

## ğŸš¨ é£é™©è¯„ä¼°å’Œç¼“è§£ç­–ç•¥

### æŠ€æœ¯é£é™©
1. **LLMç”Ÿæˆè´¨é‡ä¸ç¨³å®š**
   - ç¼“è§£ç­–ç•¥: å¤šè½®éªŒè¯ã€è‡ªé€‚åº”æç¤ºè¯ä¼˜åŒ–ã€äººå·¥è´¨æ£€æŠ½æ ·
   
2. **ä»»åŠ¡éªŒè¯å¤æ‚åº¦é«˜**
   - ç¼“è§£ç­–ç•¥: åˆ†å±‚éªŒè¯ã€å¹¶è¡Œå¤„ç†ã€ç¼“å­˜éªŒè¯ç»“æœ

3. **ç³»ç»Ÿæ€§èƒ½ç“¶é¢ˆ**
   - ç¼“è§£ç­–ç•¥: å¼‚æ­¥å¤„ç†ã€Redisé˜Ÿåˆ—åˆ†æµã€ç›‘æ§å’Œå‘Šè­¦

### ä¸šåŠ¡é£é™©
1. **ç”Ÿæˆä»»åŠ¡ä¸å®é™…éœ€æ±‚ä¸åŒ¹é…**
   - ç¼“è§£ç­–ç•¥: é¢†åŸŸä¸“å®¶è¯„å®¡ã€ç”¨æˆ·åé¦ˆå¾ªç¯ã€æŒç»­ä¼˜åŒ–

2. **æ•°æ®è´¨é‡ä¸‹é™**
   - ç¼“è§£ç­–ç•¥: è´¨é‡ç›‘æ§ä»ªè¡¨ç›˜ã€è‡ªåŠ¨åŒ–è´¨æ£€ã€é™çº§å¤„ç†

### è¿ç»´é£é™©
1. **èµ„æºæ¶ˆè€—è¿‡é«˜**
   - ç¼“è§£ç­–ç•¥: èµ„æºç›‘æ§ã€å¼¹æ€§æ‰©å®¹ã€æˆæœ¬ä¼˜åŒ–

2. **æœåŠ¡ç¨³å®šæ€§é—®é¢˜**
   - ç¼“è§£ç­–ç•¥: æ•…éšœè½¬ç§»ã€é™çº§æœåŠ¡ã€å¤‡ä»½æ¢å¤

---

## ğŸ“š å‚è€ƒæ–‡çŒ®å’Œä¾èµ–

### æ ¸å¿ƒå‚è€ƒ
- **TaskCraftè®ºæ–‡**: "TaskCraft: Automated Generation of Agentic Tasks" (arXiv:2506.10055)
- **TaskCraftä»£ç åº“**: `/Users/zhaoxiang/Documents/Datapresso/TaskCraft`
- **ç°æœ‰SynthesisCore**: `agent-data-platform/core/synthesiscore/`

### æŠ€æœ¯ä¾èµ–
- **LLMå®¢æˆ·ç«¯**: `core/llm_client.py`
- **å·¥å…·ç”Ÿæ€**: ToolScore + MCPæ¶æ„
- **æ•°æ®å­˜å‚¨**: Redis + JSONæ–‡ä»¶
- **éªŒè¯ä»£ç†**: OAgentsæ¡†æ¶ (ä»TaskCraftç§»æ¤)

### å¤–éƒ¨ä¾èµ–
- **æœç´¢æœåŠ¡**: SerpAPI, DeepSearch MCP
- **å†…å®¹å¤„ç†**: Jina API, æµè§ˆå™¨MCP
- **è®¡ç®—èµ„æº**: LLM APIè°ƒç”¨é…é¢

---

## ğŸ’¡ æ€»ç»“

é€šè¿‡å¼•å…¥TaskCraftçš„æ ¸å¿ƒç®—æ³•ï¼Œagent-data-platformçš„synthesiscoreæ¨¡å—å°†å®ç°è´¨çš„é£è·ƒï¼Œä»ç®€å•çš„"è½¨è¿¹åˆ†æå™¨"è¿›åŒ–ä¸ºå®Œæ•´çš„"ä»»åŠ¡ç”Ÿæˆå¼•æ“"ã€‚è¿™ä¸€å‡çº§å°†å»ºç«‹çœŸæ­£çš„ä»»åŠ¡-èƒ½åŠ›è‡ªè¿›åŒ–é—­ç¯ï¼Œä¸ºAgentç³»ç»Ÿçš„æŒç»­å­¦ä¹ å’Œèƒ½åŠ›æå‡æä¾›å¼ºå¤§çš„æ•°æ®åŸºç¡€ã€‚

å…³é”®æˆåŠŸå› ç´ :
1. **ç®—æ³•ç§»æ¤çš„å‡†ç¡®æ€§**: ç¡®ä¿TaskCraftæ ¸å¿ƒç®—æ³•çš„æ­£ç¡®å®ç°
2. **ç³»ç»Ÿé›†æˆçš„æ— ç¼æ€§**: ä¸ç°æœ‰æ¶æ„çš„å¹³æ»‘é›†æˆ
3. **è´¨é‡æ§åˆ¶çš„ä¸¥æ ¼æ€§**: å»ºç«‹å®Œå–„çš„éªŒè¯å’Œç›‘æ§ä½“ç³»
4. **æ€§èƒ½ä¼˜åŒ–çš„æŒç»­æ€§**: ä¿æŒç³»ç»Ÿçš„é«˜æ•ˆè¿è¡Œ
5. **ç”¨æˆ·ä½“éªŒçš„å‹å¥½æ€§**: æä¾›ç›´è§‚æ˜“ç”¨çš„æ“ä½œç•Œé¢

é€šè¿‡åˆ†é˜¶æ®µå®æ–½è¿™ä¸€å¢å¼ºæ–¹æ¡ˆï¼Œagent-data-platformå°†åœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–å’Œæ™ºèƒ½ä½“èƒ½åŠ›è¿›åŒ–æ–¹é¢å–å¾—é‡å¤§çªç ´ã€‚

åŸºäºè®¾è®¡æ–‡æ¡£åˆ†æï¼Œå¢å¼ºåçš„SynthesisCoreæ¨¡å—ç›¸æ¯”å½“å‰ç‰ˆæœ¬æœ‰ä»¥ä¸‹æ˜¾è‘—æå‡ï¼š

  ğŸš€ æ ¸å¿ƒèƒ½åŠ›çš„è´¨å˜æå‡

  1. ä»»åŠ¡ç”Ÿæˆæ•°é‡çš„çˆ†ç‚¸æ€§å¢é•¿

  å½“å‰æ¨¡å¼: 1ä¸ªè½¨è¿¹ â†’ 1ä¸ªæœ¬è´¨ â†’ 1ä¸ªç§å­ä»»åŠ¡ (1:1:1)
  è½¨è¿¹A â†’ TaskEssence_A â†’ SeedTask_A

  å¢å¼ºåæ¨¡å¼: 1ä¸ªè½¨è¿¹ â†’ NÃ—MÃ—Kä¸ªç§å­ä»»åŠ¡ (1:NÃ—MÃ—K)
  è½¨è¿¹A â†’
  â”œâ”€ åŸå­ä»»åŠ¡1 â†’ æ·±åº¦æ‰©å±•(3çº§) â†’ 3ä¸ªç§å­ä»»åŠ¡
  â”œâ”€ åŸå­ä»»åŠ¡2 â†’ æ·±åº¦æ‰©å±•(2çº§) â†’ 2ä¸ªç§å­ä»»åŠ¡
  â”œâ”€ åŸå­ä»»åŠ¡3 â†’ æ·±åº¦æ‰©å±•(3çº§) â†’ 3ä¸ªç§å­ä»»åŠ¡
  â””â”€ å®½åº¦æ‰©å±•(1+2, 2+3) â†’ 2ä¸ªå¤åˆç§å­ä»»åŠ¡
  æ€»è®¡: 1ä¸ªè½¨è¿¹ â†’ 10ä¸ªé«˜è´¨é‡ç§å­ä»»åŠ¡

  å…·ä½“ç¤ºä¾‹:
  ä»ä¸€ä¸ª"ChatGPTä¸Claudeå¯¹æ¯”åˆ†æ"çš„è½¨è¿¹ä¸­ï¼š

  å½“å‰ç‰ˆæœ¬è¾“å‡º:
  {
    "task_id": "seed_reasoning_abc123",
    "description": "ä½¿ç”¨æµè§ˆå™¨æœç´¢å¹¶åˆ†æChatGPTå’ŒClaudeçš„ä¼˜ç¼ºç‚¹å¯¹æ¯”",
    "expected_tools": ["browser_navigator", "python_executor"]
  }

  å¢å¼ºç‰ˆæœ¬è¾“å‡º:
  [
    // åŸå­ä»»åŠ¡
    {
      "task_id": "atomic_001",
      "description": "æŸ¥æ‰¾ChatGPTçš„å‡†ç¡®ç‡æ•°æ®",
      "difficulty": "simple"
    },
    // æ·±åº¦æ‰©å±• - 1è·³
    {
      "task_id": "depth_001_1hop",
      "description": "åœ¨å¤§è¯­è¨€æ¨¡å‹è¯„æµ‹åŸºå‡†ä¸­ï¼Œæ‰¾åˆ°ChatGPTçš„å‡†ç¡®ç‡æ•°æ®",
      "difficulty": "medium"
    },
    // æ·±åº¦æ‰©å±• - 2è·³  
    {
      "task_id": "depth_001_2hop",
      "description": "åœ¨AIæ¨¡å‹æ€§èƒ½æ’è¡Œæ¦œä¸­ï¼Œé€šè¿‡MMLUç­‰æ ‡å‡†åŒ–è¯„æµ‹åŸºå‡†ï¼Œæ‰¾åˆ°ChatGPTçš„å‡†ç¡®ç‡æ•°æ®",
      "difficulty": "complex"
    },
    // å®½åº¦æ‰©å±• - å¤åˆä»»åŠ¡
    {
      "task_id": "width_001",
      "description": "åŒæ—¶è·å–ChatGPTå’ŒClaudeåœ¨å¤šä¸ªç»´åº¦çš„æ€§èƒ½å¯¹æ¯”æ•°æ®ï¼Œå¹¶åˆ†æå…¶ä¼˜ç¼ºç‚¹å·®å¼‚",
      "difficulty": "complex"
    }
  ]

  2. ä»»åŠ¡è´¨é‡çš„ç²¾ç»†åŒ–åˆ†çº§

  å½“å‰: ç²—ç³™çš„äºŒåˆ†ç±» (æˆåŠŸ/å¤±è´¥)
  å¢å¼ºå: ç²¾ç»†åŒ–7ç»´åº¦è¯„ä¼°
  - å¯æ‰§è¡Œæ€§è¯„åˆ† (0-1)
  - éš¾åº¦é€‚ä¸­æ€§ (simple/medium/complex)
  - ç­”æ¡ˆå”¯ä¸€æ€§éªŒè¯
  - å·¥å…·éœ€æ±‚å‡†ç¡®æ€§
  - è¯­è¨€è´¨é‡è¯„ä¼°
  - è®¤çŸ¥å¤æ‚åº¦åˆ†æ
  - ç»¼åˆè´¨é‡åˆ†æ•°

  ğŸ“Š ç”¨æˆ·ç›´è§‚æ„Ÿå—çš„æå‡

  1. ç§å­ä»»åŠ¡æ•°é‡æ¿€å¢

  # å½“å‰ç‰ˆæœ¬ (å¤„ç†100ä¸ªè½¨è¿¹)
  echo "ç§å­ä»»åŠ¡æ•°é‡: $(wc -l < output/seed_tasks.jsonl)"
  # è¾“å‡º: ç§å­ä»»åŠ¡æ•°é‡: 100

  # å¢å¼ºç‰ˆæœ¬ (å¤„ç†ç›¸åŒ100ä¸ªè½¨è¿¹)  
  echo "ç§å­ä»»åŠ¡æ•°é‡: $(wc -l < output/enhanced_seed_tasks.jsonl)"
  # è¾“å‡º: ç§å­ä»»åŠ¡æ•°é‡: 1000+

  2. ä»»åŠ¡éš¾åº¦çš„å®Œæ•´æ¢¯åº¦

  # æŸ¥çœ‹ä»»åŠ¡éš¾åº¦åˆ†å¸ƒ
  cat output/enhanced_seed_tasks.jsonl | jq -r '.difficulty' | sort | uniq -c
  # è¾“å‡º:
  #   300 simple     (1æ­¥ä»»åŠ¡ - é€‚åˆåˆå­¦è€…)
  #   500 medium     (2-3æ­¥ä»»åŠ¡ - é€‚åˆè¿›é˜¶ç»ƒä¹ )  
  #   200 complex    (4+æ­¥ä»»åŠ¡ - é€‚åˆä¸“å®¶æŒ‘æˆ˜)

  3. å·¥å…·ä½¿ç”¨çš„å¤šæ ·åŒ–è¦†ç›–

  # åˆ†æå·¥å…·ä½¿ç”¨æ¨¡å¼
  cat output/enhanced_seed_tasks.jsonl | jq -r '.expected_tools[]' | sort | uniq -c
  # è¾“å‡º:
  #   450 browser_navigator     (ç½‘é¡µæ“ä½œä»»åŠ¡)
  #   380 python_executor       (ä»£ç æ‰§è¡Œä»»åŠ¡)
  #   290 deepsearch           (æ·±åº¦æœç´¢ä»»åŠ¡)
  #   150 file_processor       (æ–‡ä»¶å¤„ç†ä»»åŠ¡)
  #   # å·¥å…·ç»„åˆä½¿ç”¨ç‡æå‡300%

  4. å®æ—¶è´¨é‡ç›‘æ§é¢æ¿

  å¢å¼ºç‰ˆæœ¬æä¾›ç›´è§‚çš„Webç›‘æ§ç•Œé¢:
  http://localhost:8081/synthesis/v2/dashboard

  ä»»åŠ¡ç”Ÿæˆæ¦‚è§ˆ:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ä»»åŠ¡ç±»å‹        â”‚ ä»Šæ—¥ç”Ÿæˆ â”‚ è´¨é‡åˆ†æ•° â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ åŸå­ä»»åŠ¡        â”‚ 245      â”‚ 0.89     â”‚
  â”‚ æ·±åº¦æ‰©å±•ä»»åŠ¡    â”‚ 180      â”‚ 0.85     â”‚
  â”‚ å®½åº¦æ‰©å±•ä»»åŠ¡    â”‚ 95       â”‚ 0.82     â”‚
  â”‚ æ€»è®¡           â”‚ 520      â”‚ 0.86     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  å¤„ç†ç®¡é“çŠ¶æ€:
  âœ… è¯­æ–™é‡‡æ ·å™¨: æ­£å¸¸ (23ä¸ªæ–°è¯­æ–™/å°æ—¶)
  âœ… åŸå­ç”Ÿæˆå™¨: æ­£å¸¸ (ç”ŸæˆæˆåŠŸç‡: 87%)
  âœ… æ‰©å±•å¼•æ“: æ­£å¸¸ (æ‰©å±•æˆåŠŸç‡: 74%)
  âœ… éªŒè¯å¼•æ“: æ­£å¸¸ (éªŒè¯é€šè¿‡ç‡: 91%)

  ğŸ¯ å¯¹ç‰¹å®šç”¨æˆ·ç¾¤ä½“çš„ä»·å€¼æå‡

  1. å¯¹AIç ”ç©¶è€…: æ•°æ®é›†è§„æ¨¡çˆ†ç‚¸æ€§å¢é•¿

  - ä¹‹å‰: æ‰‹å·¥åˆ›å»ºå°‘é‡ä»»åŠ¡ â†’ éœ€è¦æ•°æœˆæ”¶é›†1000ä¸ªä»»åŠ¡
  - ç°åœ¨: è‡ªåŠ¨ç”Ÿæˆæµ·é‡ä»»åŠ¡ â†’ ä¸€å‘¨ç”Ÿæˆ10000+ä¸ªé«˜è´¨é‡ä»»åŠ¡
  - ä»·å€¼: åŠ é€Ÿæ¨¡å‹è®­ç»ƒå’Œè¯„æµ‹å‘¨æœŸ

  2. å¯¹Agentå¼€å‘è€…: èƒ½åŠ›è¾¹ç•Œç³»ç»Ÿæ€§æ‰©å±•

  - ä¹‹å‰: éšæœºä»»åŠ¡æµ‹è¯• â†’ èƒ½åŠ›å‘ç°å…·æœ‰å¶ç„¶æ€§
  - ç°åœ¨: æ¢¯åº¦åŒ–ä»»åŠ¡åºåˆ— â†’ ç³»ç»Ÿæ€§å‘ç°å’Œæå‡Agentèƒ½åŠ›è¾¹ç•Œ
  - ä»·å€¼: ç²¾ç¡®å®šä½Agentçš„èƒ½åŠ›ç“¶é¢ˆ

  3. å¯¹äº§å“å›¢é˜Ÿ: åŠŸèƒ½éªŒè¯çš„å…¨é¢æ€§

  - ä¹‹å‰: æ‰‹å·¥è®¾è®¡æµ‹è¯•ç”¨ä¾‹ â†’ è¦†ç›–ç‡æœ‰é™
  - ç°åœ¨: è‡ªåŠ¨ç”Ÿæˆå¤šç»´åº¦æµ‹è¯•ä»»åŠ¡ â†’ å…¨é¢è¦†ç›–åŠŸèƒ½ç‚¹
  - ä»·å€¼: æé«˜äº§å“è´¨é‡å’Œç”¨æˆ·æ»¡æ„åº¦

  ğŸ“ˆ å®šé‡åŒ–çš„æ€§èƒ½æå‡æŒ‡æ ‡

  ä»»åŠ¡ç”Ÿæˆæ•ˆç‡æå‡

  # æ•ˆç‡å¯¹æ¯”ç¤ºä¾‹
  å½“å‰ç‰ˆæœ¬æ€§èƒ½æŒ‡æ ‡:
  {
      "è½¨è¿¹å¤„ç†é€Ÿåº¦": "1ä¸ªè½¨è¿¹/10ç§’",
      "ç§å­ä»»åŠ¡è¾“å‡º": "1ä¸ªä»»åŠ¡/è½¨è¿¹",
      "è´¨é‡è¯„ä¼°": "äººå·¥æŠ½æ ·æ£€æŸ¥",
      "ä»»åŠ¡å¤ç”¨ç‡": "ä½ (å•ä¸€éš¾åº¦)"
  }

  å¢å¼ºç‰ˆæœ¬æ€§èƒ½æŒ‡æ ‡:
  {
      "è½¨è¿¹å¤„ç†é€Ÿåº¦": "1ä¸ªè½¨è¿¹/5ç§’ (å¹¶è¡Œå¤„ç†)",
      "ç§å­ä»»åŠ¡è¾“å‡º": "10+ä¸ªä»»åŠ¡/è½¨è¿¹",
      "è´¨é‡è¯„ä¼°": "è‡ªåŠ¨åŒ–7ç»´åº¦éªŒè¯",
      "ä»»åŠ¡å¤ç”¨ç‡": "é«˜ (å¤šéš¾åº¦æ¢¯åº¦)"
  }

  æŠ•èµ„å›æŠ¥ç‡ (ROI):
  - å¼€å‘æŠ•å…¥: 6å‘¨å¼€å‘æ—¶é—´
  - äº§å‡ºæå‡: 10å€ä»»åŠ¡ç”Ÿæˆé‡ Ã— 3å€è´¨é‡ç¨³å®šæ€§
  - èŠ‚çœæˆæœ¬: å‡å°‘90%æ‰‹å·¥ä»»åŠ¡åˆ›å»ºå·¥ä½œ

  ç³»ç»Ÿæ™ºèƒ½åŒ–ç¨‹åº¦æå‡

  å½“å‰æ™ºèƒ½åŒ–æ°´å¹³: 30%
  â”œâ”€ è½¨è¿¹åˆ†æ: âœ… (è‡ªåŠ¨åŒ–)
  â”œâ”€ æœ¬è´¨æå–: âœ… (LLMè¾…åŠ©)
  â”œâ”€ ä»»åŠ¡ç”Ÿæˆ: âš ï¸ (å•ä¸€æ¨¡å¼)
  â”œâ”€ è´¨é‡éªŒè¯: âŒ (äººå·¥æ£€æŸ¥)
  â””â”€ æŒç»­ä¼˜åŒ–: âŒ (æ— åé¦ˆå¾ªç¯)

  å¢å¼ºåæ™ºèƒ½åŒ–æ°´å¹³: 95%
  â”œâ”€ è¯­æ–™é‡‡æ ·: âœ… (ä¸»åŠ¨+è¢«åŠ¨)
  â”œâ”€ åŸå­æå–: âœ… (æ™ºèƒ½æ‹†è§£)
  â”œâ”€ ä»»åŠ¡æ‰©å±•: âœ… (æ·±åº¦+å®½åº¦)
  â”œâ”€ è‡ªåŠ¨éªŒè¯: âœ… (å¤šç»´è¯„ä¼°)
  â”œâ”€ è‡ªé€‚åº”ä¼˜åŒ–: âœ… (æŒç»­å­¦ä¹ )
  â””â”€ è´¨é‡ç›‘æ§: âœ… (å®æ—¶ä»ªè¡¨ç›˜)

  ğŸš€ ç›´è§‚çš„ä½¿ç”¨ä½“éªŒæ”¹è¿›

  ç”¨æˆ·åœ¨ä½¿ç”¨æ—¶ä¼šæ˜æ˜¾æ„Ÿå—åˆ°ï¼š

  1. æ•°æ®ä¸°å¯Œåº¦: ä»ç¨€ç–çš„ç§å­ä»»åŠ¡å˜ä¸ºä¸°å¯Œçš„ä»»åŠ¡ç”Ÿæ€
  2. æ“ä½œç®€ä¾¿æ€§: ä»å¤æ‚é…ç½®å˜ä¸ºä¸€é”®å¯åŠ¨è‡ªåŠ¨ç”Ÿæˆ
  3. è´¨é‡å¯é æ€§: ä»ä¸ç¡®å®šè´¨é‡å˜ä¸ºç¨³å®šé«˜è´¨é‡è¾“å‡º
  4. æ‰©å±•çµæ´»æ€§: ä»å›ºå®šæ¨¡å¼å˜ä¸ºå¯é…ç½®çš„å¤šç§ç”Ÿæˆç­–ç•¥
  5. ç›‘æ§é€æ˜æ€§: ä»é»‘ç›’å¤„ç†å˜ä¸ºå…¨è¿‡ç¨‹å¯è§†åŒ–ç›‘æ§

  æ€»ç»“: å¢å¼ºåçš„SynthesisCoreå°†ä»"å•ä¸€è½¨è¿¹â†’å•ä¸€ä»»åŠ¡"çš„çº¿æ€§æ¨¡å¼ï¼Œå‡çº§ä¸º"å•ä¸€è½¨è¿¹â†’å¤šç»´åº¦ä»»åŠ¡çŸ©é˜µ"çš„æŒ‡æ•°çº§ç”Ÿæˆæ¨¡å¼ï¼ŒçœŸæ­£å®ç°äº†ä»»åŠ¡ç”Ÿæˆçš„å·¥ä¸šåŒ–å’Œæ™ºèƒ½åŒ–ã€‚