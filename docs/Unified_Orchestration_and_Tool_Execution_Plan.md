# Unified Orchestration and Tool Execution Plan

## 1. Overview & Core Philosophy

This document outlines a strategic plan to refactor the agent's core reasoning and execution engine. The primary goal is to move away from any hard-coded, rule-based logic and evolve towards a flexible, autonomous system inspired by the principles of advanced agents like `Tool-Star`, but tailored specifically for our MCP Server architecture.

The core philosophy is: **The LLM proposes, the System executes.**

Our system's role is not to decide *what* the LLM should do, but to be a powerful and reliable executor of the LLM's expressed intent. The LLM will communicate its intent through a structured XML format, and our Orchestrator will parse and execute this intent, providing the results back to the LLM for its next reasoning step. This creates a clean separation of concerns and paves the way for generating high-quality data for future Reinforcement Learning.

## 2. The Core Problem to Solve

The current system has limitations in dynamically handling complex tool interactions. The key challenge is to create a robust bridge between the LLM's abstract plan (e.g., "I need to run a command") and the concrete API calls to our various MCP Servers (e.g., `microsandbox`, `deepsearch`).

This plan directly addresses:
-   Eliminating hard-coded logic and string matching.
-   Enabling the LLM to call single, multiple parallel, or multiple sequential tools in a single reasoning step.
-   Establishing a clear, extensible "contract" (the XML schema) for how the LLM communicates its intent.

## 3. Proposed Architecture: The Interactive Orchestrator

We will implement a central `Orchestrator` class, likely within `runtimes/reasoning/enhanced_runtime.py`. This class will manage the primary control loop of the agent.

**The Control Loop:**

1.  **LLM Proposes**: The Orchestrator calls the LLM, which generates a plan containing `<think>` blocks and tool calls, ending with a special `<execute_tools />` signal.
2.  **System Stops**: The Orchestrator uses `stop_sequences` to pause the LLM immediately after it generates `<execute_tools />` or `Final Answer:`.
3.  **System Parses**: The Orchestrator parses the entire XML block generated by the LLM. It identifies if the tools are meant to be run in sequence, in parallel, or as a single call.
4.  **System Executes**: The Orchestrator dispatches the parsed actions to the appropriate MCP Servers. It handles sequential dependencies and parallel execution.
5.  **System Injects Result**: The results from the MCP Servers are collected, formatted into a `<result>` tag, and appended to the history as a new entry.
6.  **(Loop)**: Control is returned to the LLM for the next reasoning step, using the new result as context.

## 4. The XML "Contract": A Language for Intent

This schema is the formal "API" between the LLM and our system. It must be clearly defined in the system prompt. The fundamental unit of action is a hierarchical XML tag where the outer tag represents the **service (MCP Server)** and the inner tag represents the **tool** to be called on that service.

| Tag | Purpose | Example |
| :--- | :--- | :--- |
| **`<service><tool>`** | **Hierarchical Tool Call**. The core action primitive. | `<microsandbox><run_command>ls</run_command></microsandbox>` |
| **`<think>`** | Contains the LLM's reasoning, analysis, and planning. Not executed. | `<think>I need to find the file first, then read it.</think>` |
| **`<execute_tools />`** | **Temporary Stop Signal**. Tells the system "I'm done planning for this turn, please execute the tools I've listed." | Placed after all tool-call blocks. |
| **`Final Answer:`** | **Final Stop Signal**. Indicates the task is complete and the following text is the answer for the user. | `Final Answer: The file content is...` |
| **`<parallel>`** | Wraps multiple hierarchical tool calls that are independent and can be run concurrently. | `<parallel><microsandbox>...</microsandbox><deepsearch>...</deepsearch></parallel>` |
| **`<sequential>`** | Wraps multiple hierarchical tool calls that are dependent and must be run in order. | `<sequential><microsandbox>...</microsandbox><deepsearch>...</deepsearch></sequential>` |
| **`$result_of_step_N`** | **Placeholder**. Used within a `<sequential>` block to refer to the output of a previous step. | `<tool_b>$result_of_step_1</tool_b>` |
| **`<result>`** | **Sibling Tag for Results**. Contains the raw output of a tool call. It is a sibling to `<think>` and other tool tags, not a child. | `<result>File content here...</result>` |

## 5. Implementation Strategy

This will be a phased implementation to ensure stability. All work will primarily be in `runtimes/reasoning/enhanced_runtime.py`.

### Phase 1: Foundation - The Tool Bridge
*   **Objective**: Create the core, testable functions for parsing and execution.
*   **Actions**:
    1.  **[COMPLETED]** Create the `Orchestrator` class in `enhanced_runtime.py` (achieved by refactoring the existing class).
    2.  **[COMPLETED]** Implement `_load_mcp_config()` in the `__init__` to load server URLs from `config/mcp_servers.json`.
    3.  **[COMPLETED]** Implement `_parse_execution_block()` using `xml.etree.ElementTree` to robustly parse the LLM's XML output.
    4.  **[COMPLETED]** Implement `_execute_tool()` using `httpx` to make a single, asynchronous API call to the correct MCP Server.

### Phase 2: Execution Logic
*   **Objective**: Build the handlers for parallel and sequential execution flows.
*   **Actions**:
    1.  **[COMPLETED]** Implement `_execute_parallel()` which takes a list of actions and runs them concurrently using `asyncio.gather`.
    2.  **[COMPLETED]** Implement `_execute_sequential()` which takes a list of actions and runs them one-by-one, handling the `$result_of_step_N` placeholder replacement.

### Phase 3: The Main Control Loop
*   **Objective**: Tie all components together into the main interactive loop.
*   **Actions**:
    1.  **[COMPLETED]** Implement the main `run_task(user_query)` method (`_execute_xml_streaming`) containing the `while` loop.
    2.  **[COMPLETED]** Integrate the real LLM client, passing the correct `stop_sequences`.
    3.  **[COMPLETED]** Implement the dispatch logic that calls the appropriate execution function based on the parsed block type.
    4.  **[COMPLETED]** Implement `_format_result()` to correctly wrap a tool's output in a `<result>` tag.

## 6. Path to Reinforcement Learning

This architecture is designed with RL in mind. The control loop naturally generates the data needed for training:
-   **[COMPLETED]** **State (S)**: The `history` list before calling the LLM.
-   **[COMPLETED]** **Action (A)**: The raw XML string generated by the LLM.
-   **[COMPLETED]** **Observation (O) / Reward (R)**: The content of the `<result>` block. A reward function can be designed later based on the success, efficiency, or content of this result.

Each iteration of the `while` loop produces a high-quality `(S, A, O)` tuple, which can be logged and later used to fine-tune the model.

## 7. Guiding Principles for Implementation

-   **[PARTIALLY COMPLETED]** **Minimalism**: Only add code that is essential for this new architecture. Avoid bloating `enhanced_runtime.py`. (New code is in, but old code cleanup is pending).
-   **[COMPLETED]** **No Hard-Coding**: The Orchestrator is agnostic to the *specific* tools. It only needs to know the MCP Server's address. The LLM has full autonomy to choose any tool advertised in the prompt.
-   **[COMPLETED]** **Configuration Driven**: All external details (like server addresses) are loaded from configuration files.

## 8. Reliability and Integration Notes

Based on further analysis, two critical points must be addressed in the implementation.

### 8.1. Stop Sequence Reliability

Relying on a single, perfectly formatted stop string from an LLM is brittle. To mitigate this, we will implement a more robust stop mechanism:

-   **[COMPLETED]** **Expanded Stop Sequences**: The `stop_sequences` list passed to the LLM includes common variations to increase the chance of a successful stop.
-   **[COMPLETED]** **Implicit Stop Fallback**: The Orchestrator's main loop now handles cases where the LLM omits the `<execute_tools />` signal.

### 8.2. Integration with `toolscore_client`

The implementation must not bypass existing architectural components. Direct HTTP calls to MCP servers will be avoided.

-   **[COMPLETED]** **Use Existing Client**: All tool execution is routed through the `self.toolscore_client` instance.
-   **[COMPLETED]** **Correct Parameter Mapping**: The `_execute_tool` method translates the parsed XML action into the arguments required by `self.toolscore_client.execute_tool`.
