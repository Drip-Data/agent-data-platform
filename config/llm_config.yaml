# LLM 配置
streaming_mode: true  # 启用XML流式模式

llm_providers:
  vllm:
    enabled: true
    api_base: "http://localhost:8000/v1"
    model: "default-model" # 替换为你的vLLM模型名称
    api_key: "YOUR_VLLM_API_KEY" # 如果vLLM需要API Key，请填写
  openai:
    enabled: false
    api_key: "YOUR_OPENAI_API_KEY"
    model: "gpt-4o"
  gemini:
    enabled: true
    api_key: "AIzaSyCc-dEWgtklPtqe19JI1cOjWUjp8PLMhic"
    model: "gemini-2.5-flash-lite-preview-06-17" 
    temperature: 0.5
    max_tokens: 32768  # Significantly increased for complex tasks with extensive code generation
    #gemini-2.5-flash-lite-preview-06-17
    #gemini-2.5-flash-preview-05-20
    #gemini-2.5-pro

default_provider: "gemini"