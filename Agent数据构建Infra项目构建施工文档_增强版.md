# Agentæ•°æ®æ„å»ºå¹³å°MVPå®Œæ•´å·¥ç¨‹è“å›¾

> **ä¸€é”®éƒ¨ç½²ç›®æ ‡**ï¼šä»»ä½•å¼€å‘è€… `git clone` åï¼Œæ‰§è¡Œ3æ¡å‘½ä»¤å³å¯è·‘é€šå®Œæ•´çš„Agentæ•°æ®ç”Ÿäº§Pipeline

## ğŸ¯ æ¶æ„ä¼˜åŒ–ç‰ˆ

åŸºäºåé¦ˆï¼Œæˆ‘ä»¬å¯¹åŸæ–¹æ¡ˆè¿›è¡Œäº†å…³é”®ä¼˜åŒ–ï¼š

1. **è½»é‡æ²™ç›’**ï¼šSandboxRuntimeæ”¹ç”¨`nsjail`æ›¿ä»£Docker-in-Dockerï¼Œé•œåƒä»500MBé™è‡³80MB
2. **åˆ†æµé˜Ÿåˆ—**ï¼šDispatcheræŒ‰ç±»å‹å†™å…¥`tasks:code`/`tasks:web`ï¼Œé¿å…ç©ºè½®è¯¢
3. **å†…å­˜æ§åˆ¶**ï¼šWebRuntimeé™åˆ¶å¹¶å‘åº¦ï¼Œé¿å…Chromeå†…å­˜çˆ†ç‚¸
4. **å¤±è´¥åˆ†ç±»**ï¼šmetricså¢åŠ `error_type`æ ‡ç­¾ï¼Œä¾¿äºæ•…éšœå®šä½
5. **æ¨¡æ¿ç¼“å­˜**ï¼šé‡å¤ä»»åŠ¡å…ˆæŸ¥Redisç¼“å­˜ï¼Œå‡å°‘LLMè°ƒç”¨

---

## ğŸ“ å®Œæ•´é¡¹ç›®ç»“æ„

```bash
# ä¸€é”®åˆ›å»ºé¡¹ç›®ç»“æ„
mkdir -p agent-data-platform/{core,runtimes/{sandbox,web_navigator},config/{prometheus,grafana/dashboards},scripts,tests,output/{trajectories,logs}}

cd agent-data-platform

# é¡¹ç›®æœ€ç»ˆç»“æ„
agent-data-platform/
â”œâ”€â”€ README.md                           # æœ¬æ–‡æ¡£
â”œâ”€â”€ docker-compose.yml                  # å®Œæ•´ç”Ÿäº§é…ç½®
â”œâ”€â”€ docker-compose.minimal.yml          # å¿«é€ŸéªŒè¯é…ç½®
â”œâ”€â”€ requirements.txt                     # Pythonä¾èµ–
â”œâ”€â”€ .env.example                         # ç¯å¢ƒå˜é‡æ¨¡æ¿
â”œâ”€â”€ Dockerfile                           # ä¸»é¡¹ç›®é•œåƒ
â”œâ”€â”€ tasks.jsonl                          # ä»»åŠ¡è¾“å…¥æ–‡ä»¶
â”œâ”€â”€ core/                               # æ ¸å¿ƒæ¡†æ¶ä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ interfaces.py                   # æ ‡å‡†æ¥å£å®šä¹‰
â”‚   â”œâ”€â”€ dispatcher.py                   # ä»»åŠ¡åˆ†å‘å™¨
â”‚   â”œâ”€â”€ router.py                       # æ™ºèƒ½è·¯ç”±å™¨
â”‚   â”œâ”€â”€ task_manager.py                 # ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†
â”‚   â”œâ”€â”€ metrics.py                      # PrometheusæŒ‡æ ‡
â”‚   â”œâ”€â”€ cache.py                        # æ¨¡æ¿ç¼“å­˜
â”‚   â””â”€â”€ utils.py                        # å·¥å…·å‡½æ•°
â”œâ”€â”€ runtimes/                           # è¿è¡Œæ—¶å®ç°
â”‚   â”œâ”€â”€ sandbox/                        # è½»é‡ä»£ç æ‰§è¡Œ
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ runtime.py
â”‚   â”‚   â”œâ”€â”€ nsjail_executor.py
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â””â”€â”€ web_navigator/                  # Webå¯¼èˆª
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ runtime.py
â”‚       â”œâ”€â”€ browser_manager.py
â”‚       â””â”€â”€ requirements.txt
â”œâ”€â”€ config/                             # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â””â”€â”€ grafana/
â”‚       â””â”€â”€ dashboards/
â”‚           â””â”€â”€ agent-pipeline.json
â”œâ”€â”€ scripts/                            # éƒ¨ç½²å’Œæµ‹è¯•è„šæœ¬
â”‚   â”œâ”€â”€ build.sh
â”‚   â”œâ”€â”€ deploy.sh
â”‚   â”œâ”€â”€ smoke_test.sh
â”‚   â”œâ”€â”€ load_test.sh
â”‚   â””â”€â”€ integration_test.sh
â””â”€â”€ output/                             # è¾“å‡ºç›®å½•
    â”œâ”€â”€ trajectories/                   # è½¨è¿¹JSONæ–‡ä»¶
    â””â”€â”€ logs/                           # è¿è¡Œæ—¥å¿—
```

---

## ğŸ”§ æ ¸å¿ƒä»£ç å®ç°

### 1. æ ‡å‡†æ¥å£å®šä¹‰

```python
# core/interfaces.py
from abc import ABC, abstractmethod
from dataclasses import dataclass, asdict, field
from typing import List, Dict, Any, Optional
from enum import Enum
import time
import json
import uuid

class TaskType(Enum):
    CODE = "code"
    WEB = "web"
    REASONING = "reasoning"

class ActionType(Enum):
    CODE_EXECUTION = "code_execution"
    BROWSER_ACTION = "browser_action"
    TOOL_CALL = "tool_call"

class ErrorType(Enum):
    TIMEOUT = "timeout"
    NETWORK_ERROR = "network_error"
    COMPILE_ERROR = "compile_error"
    RUNTIME_ERROR = "runtime_error"
    BROWSER_ERROR = "browser_error"
    SYSTEM_ERROR = "system_error"

@dataclass
class TaskSpec:
    """æ ‡å‡†ä»»åŠ¡è§„èŒƒ"""
    task_id: str
    task_type: TaskType
    description: str
    expected_tools: List[str]
    constraints: Dict[str, Any] = field(default_factory=dict)
    max_steps: int = 10
    timeout: int = 300
    priority: int = 1
    
    def __post_init__(self):
        if not self.task_id:
            self.task_id = str(uuid.uuid4())
        if isinstance(self.task_type, str):
            self.task_type = TaskType(self.task_type)
    
    def to_dict(self) -> Dict:
        data = asdict(self)
        data['task_type'] = self.task_type.value
        return data
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'TaskSpec':
        if isinstance(data.get('task_type'), str):
            data['task_type'] = TaskType(data['task_type'])
        return cls(**data)
    
    def json(self) -> str:
        return json.dumps(self.to_dict())

@dataclass
class ExecutionStep:
    """æ‰§è¡Œæ­¥éª¤"""
    step_id: int
    action_type: ActionType
    action_params: Dict[str, Any]
    observation: str
    success: bool
    error_type: Optional[ErrorType] = None
    error_message: Optional[str] = None
    timestamp: float = field(default_factory=time.time)
    duration: float = 0.0

@dataclass
class TrajectoryResult:
    """è½¨è¿¹ç»“æœ"""
    task_id: str
    runtime_id: str
    success: bool
    steps: List[ExecutionStep]
    final_result: str
    error_type: Optional[ErrorType] = None
    error_message: Optional[str] = None
    total_duration: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: float = field(default_factory=time.time)
    
    def to_dict(self) -> Dict:
        return {
            'task_id': self.task_id,
            'runtime_id': self.runtime_id,
            'success': self.success,
            'steps': [
                {
                    'step_id': s.step_id,
                    'action_type': s.action_type.value,
                    'action_params': s.action_params,
                    'observation': s.observation,
                    'success': s.success,
                    'error_type': s.error_type.value if s.error_type else None,
                    'error_message': s.error_message,
                    'timestamp': s.timestamp,
                    'duration': s.duration
                } for s in self.steps
            ],
            'final_result': self.final_result,
            'error_type': self.error_type.value if self.error_type else None,
            'error_message': self.error_message,
            'total_duration': self.total_duration,
            'metadata': self.metadata,
            'created_at': self.created_at
        }
    
    def json(self) -> str:
        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)

class RuntimeInterface(ABC):
    """è¿è¡Œæ—¶æ ‡å‡†æ¥å£"""
    
    @property
    @abstractmethod
    def runtime_id(self) -> str:
        pass
    
    @property
    @abstractmethod
    def capabilities(self) -> List[str]:
        pass
    
    @abstractmethod
    async def execute(self, task: TaskSpec) -> TrajectoryResult:
        pass
    
    @abstractmethod
    async def health_check(self) -> bool:
        pass
    
    @abstractmethod
    async def cleanup(self):
        pass
```

### 2. æ¨¡æ¿ç¼“å­˜ç³»ç»Ÿ

```python
# core/cache.py
import hashlib
import json
import logging
from typing import Optional, Dict, Any
import redis.asyncio as redis

logger = logging.getLogger(__name__)

class TemplateCache:
    """ä»£ç /åŠ¨ä½œæ¨¡æ¿ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, redis_client: redis.Redis, ttl: int = 3600):
        self.redis = redis_client
        self.ttl = ttl
        self._hit_count = 0
        self._miss_count = 0
    
    def _make_key(self, task_type: str, description: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{task_type}:{description.lower().strip()}"
        hash_key = hashlib.md5(content.encode()).hexdigest()
        return f"template_cache:{hash_key}"
    
    async def get(self, task_type: str, description: str) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜çš„æ¨¡æ¿"""
        key = self._make_key(task_type, description)
        
        try:
            cached = await self.redis.get(key)
            if cached:
                self._hit_count += 1
                result = json.loads(cached.decode())
                logger.info(f"Cache hit for {task_type}: {description[:50]}...")
                return result
            else:
                self._miss_count += 1
                return None
        except Exception as e:
            logger.error(f"Cache get error: {e}")
            return None
    
    async def set(self, task_type: str, description: str, template: Dict[str, Any]):
        """è®¾ç½®ç¼“å­˜æ¨¡æ¿"""
        key = self._make_key(task_type, description)
        
        try:
            await self.redis.setex(
                key, 
                self.ttl, 
                json.dumps(template, ensure_ascii=False)
            )
            logger.info(f"Cache set for {task_type}: {description[:50]}...")
        except Exception as e:
            logger.error(f"Cache set error: {e}")
    
    async def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total = self._hit_count + self._miss_count
        hit_rate = self._hit_count / total if total > 0 else 0
        
        return {
            "hit_count": self._hit_count,
            "miss_count": self._miss_count,
            "hit_rate": hit_rate,
            "total_requests": total
        }
```

### 3. ä¼˜åŒ–åçš„æŒ‡æ ‡ç³»ç»Ÿ

```python
# core/metrics.py
import time
from typing import Dict
from prometheus_client import Counter, Histogram, Gauge, start_http_server, CollectorRegistry

class EnhancedMetrics:
    """å¢å¼ºç‰ˆPrometheusæŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, port: int = 8000):
        self.port = port
        self.registry = CollectorRegistry()
        
        # ä»»åŠ¡æŒ‡æ ‡
        self.tasks_submitted = Counter(
            'tasks_submitted_total',
            'Total submitted tasks',
            ['task_type', 'runtime'],
            registry=self.registry
        )
        
        self.tasks_completed = Counter(
            'tasks_completed_total',
            'Total completed tasks', 
            ['runtime', 'status'],
            registry=self.registry
        )
        
        self.tasks_failed = Counter(
            'tasks_failed_total',
            'Total failed tasks',
            ['runtime', 'error_type'],
            registry=self.registry
        )
        
        self.task_duration = Histogram(
            'task_duration_seconds',
            'Task execution duration',
            ['runtime'],
            registry=self.registry
        )
        
        self.active_tasks = Gauge(
            'active_tasks',
            'Currently active tasks',
            ['runtime'],
            registry=self.registry
        )
        
        # é˜Ÿåˆ—æŒ‡æ ‡
        self.queue_size = Gauge(
            'queue_size',
            'Task queue size',
            ['queue_name'],
            registry=self.registry
        )
        
        self.pending_lag_seconds = Gauge(
            'pending_lag_seconds',
            'Lag time for pending tasks',
            ['runtime'],
            registry=self.registry
        )
        
        # ç¼“å­˜æŒ‡æ ‡
        self.cache_hits = Counter(
            'cache_hits_total',
            'Template cache hits',
            ['cache_type'],
            registry=self.registry
        )
        
        self.cache_misses = Counter(
            'cache_misses_total', 
            'Template cache misses',
            ['cache_type'],
            registry=self.registry
        )
        
        # ç³»ç»ŸæŒ‡æ ‡
        self.runtime_health = Gauge(
            'runtime_health',
            'Runtime health status (1=healthy, 0=unhealthy)',
            ['runtime'],
            registry=self.registry
        )
        
        # ä»»åŠ¡è®¡æ—¶å™¨
        self._task_timers: Dict[str, float] = {}
        
    def start_server(self):
        """å¯åŠ¨metricsæœåŠ¡å™¨"""
        start_http_server(self.port, registry=self.registry)
    
    def record_task_submitted(self, task_type: str, runtime: str):
        """è®°å½•ä»»åŠ¡æäº¤"""
        self.tasks_submitted.labels(task_type=task_type, runtime=runtime).inc()
    
    def record_task_started(self, task_id: str, runtime: str):
        """è®°å½•ä»»åŠ¡å¼€å§‹"""
        self.active_tasks.labels(runtime=runtime).inc()
        self._task_timers[task_id] = time.time()
    
    def record_task_completed(self, task_id: str, runtime: str, success: bool, error_type: str = None):
        """è®°å½•ä»»åŠ¡å®Œæˆ"""
        self.active_tasks.labels(runtime=runtime).dec()
        
        # è®°å½•å®ŒæˆçŠ¶æ€
        status = 'success' if success else 'failure'
        self.tasks_completed.labels(runtime=runtime, status=status).inc()
        
        # è®°å½•å¤±è´¥åŸå› 
        if not success and error_type:
            self.tasks_failed.labels(runtime=runtime, error_type=error_type).inc()
        
        # è®°å½•æ‰§è¡Œæ—¶é—´
        if task_id in self._task_timers:
            duration = time.time() - self._task_timers[task_id]
            self.task_duration.labels(runtime=runtime).observe(duration)
            del self._task_timers[task_id]
    
    def record_cache_hit(self, cache_type: str = "template"):
        """è®°å½•ç¼“å­˜å‘½ä¸­"""
        self.cache_hits.labels(cache_type=cache_type).inc()
    
    def record_cache_miss(self, cache_type: str = "template"):
        """è®°å½•ç¼“å­˜æœªå‘½ä¸­"""
        self.cache_misses.labels(cache_type=cache_type).inc()
    
    def update_queue_size(self, queue_name: str, size: int):
        """æ›´æ–°é˜Ÿåˆ—å¤§å°"""
        self.queue_size.labels(queue_name=queue_name).set(size)
    
    def update_pending_lag(self, runtime: str, lag_seconds: float):
        """æ›´æ–°ä»»åŠ¡å»¶è¿Ÿ"""
        self.pending_lag_seconds.labels(runtime=runtime).set(lag_seconds)
    
    def update_runtime_health(self, runtime: str, healthy: bool):
        """æ›´æ–°è¿è¡Œæ—¶å¥åº·çŠ¶æ€"""
        self.runtime_health.labels(runtime=runtime).set(1 if healthy else 0)

# å…¨å±€metricså®ä¾‹
metrics = EnhancedMetrics()
```

### 4. ä¼˜åŒ–åçš„ä»»åŠ¡åˆ†å‘å™¨

```python
# core/dispatcher.py
import asyncio
import json
import logging
import os
import time
from typing import Dict, Set
import redis.asyncio as redis
from .interfaces import TaskSpec, TaskType
from .cache import TemplateCache
from .metrics import metrics

logger = logging.getLogger(__name__)

class TaskDispatcher:
    """ä¼˜åŒ–åçš„ä»»åŠ¡åˆ†å‘å™¨ - æ”¯æŒåˆ†æµå’Œç¼“å­˜"""
    
    def __init__(self, redis_url: str, task_file: str = "tasks.jsonl"):
        self.redis = redis.from_url(redis_url)
        self.task_file = task_file
        self.cache = TemplateCache(self.redis)
        
        # é˜Ÿåˆ—æ˜ å°„è¡¨
        self.queue_mapping = {
            TaskType.CODE: "tasks:code",
            TaskType.WEB: "tasks:web",
            TaskType.REASONING: "tasks:reasoning"
        }
        
    async def start(self):
        """å¯åŠ¨åˆ†å‘å™¨"""
        logger.info("Starting optimized task dispatcher...")
        
        # å¯åŠ¨metricsæœåŠ¡å™¨
        metrics.start_server()
        
        # å¹¶è¡Œå¯åŠ¨å„ä¸ªç»„ä»¶
        await asyncio.gather(
            self._load_and_dispatch_tasks(),
            self._monitor_queues(),
            self._monitor_pending_tasks()
        )
    
    async def _load_and_dispatch_tasks(self):
        """åŠ è½½å¹¶æŒ‰ç±»å‹åˆ†å‘ä»»åŠ¡"""
        processed_tasks: Set[str] = set()
        last_position = 0
        
        while True:
            try:
                if not os.path.exists(self.task_file):
                    await asyncio.sleep(5)
                    continue
                
                # è¯»å–æ–°ä»»åŠ¡
                with open(self.task_file, 'r', encoding='utf-8') as f:
                    for line_number, line in enumerate(f):
                        if line_number <= last_position:
                            continue
                        
                        line = line.strip()
                        if not line:
                            continue
                        
                        try:
                            task_data = json.loads(line)
                            task = TaskSpec.from_dict(task_data)
                            
                            if task.task_id in processed_tasks:
                                continue
                            
                            # ç›´æ¥åˆ†å‘åˆ°å¯¹åº”é˜Ÿåˆ—
                            queue_name = self.queue_mapping.get(task.task_type)
                            if queue_name:
                                await self.redis.xadd(
                                    queue_name,
                                    {
                                        "task": task.json(),
                                        "submitted_at": time.time(),
                                        "priority": task.priority
                                    }
                                )
                                
                                metrics.record_task_submitted(
                                    task.task_type.value,
                                    queue_name.split(":")[1]
                                )
                                processed_tasks.add(task.task_id)
                                
                                logger.info(f"Dispatched task {task.task_id} to {queue_name}")
                            else:
                                logger.error(f"No queue for task type {task.task_type}")
                                
                        except (json.JSONDecodeError, ValueError) as e:
                            logger.error(f"Invalid task at line {line_number}: {e}")
                        
                        last_position = line_number
                
                await asyncio.sleep(5)  # æ£€æŸ¥æ–°ä»»åŠ¡é—´éš”
                
            except Exception as e:
                logger.error(f"Error in task loading: {e}")
                await asyncio.sleep(10)
    
    async def _monitor_queues(self):
        """ç›‘æ§é˜Ÿåˆ—çŠ¶æ€"""
        while True:
            try:
                for task_type, queue_name in self.queue_mapping.items():
                    # è·å–é˜Ÿåˆ—é•¿åº¦
                    queue_length = await self.redis.xlen(queue_name)
                    metrics.update_queue_size(queue_name, queue_length)
                
                await asyncio.sleep(30)  # æ¯30ç§’æ›´æ–°
                
            except Exception as e:
                logger.error(f"Error monitoring queues: {e}")
                await asyncio.sleep(10)
    
    async def _monitor_pending_tasks(self):
        """ç›‘æ§æŒ‚èµ·ä»»åŠ¡å»¶è¿Ÿ"""
        while True:
            try:
                for task_type, queue_name in self.queue_mapping.items():
                    runtime = queue_name.split(":")[1]
                    
                    # æ£€æŸ¥æŒ‚èµ·ä»»åŠ¡
                    pending_info = await self.redis.xpending_range(
                        queue_name, "workers", count=100
                    )
                    
                    if pending_info:
                        # è®¡ç®—æœ€å¤§å»¶è¿Ÿ
                        current_time = time.time() * 1000  # Redisä½¿ç”¨æ¯«ç§’æ—¶é—´æˆ³
                        max_lag = 0
                        
                        for entry in pending_info:
                            idle_time = current_time - entry['time_since_delivered']
                            max_lag = max(max_lag, idle_time / 1000)  # è½¬æ¢ä¸ºç§’
                        
                        metrics.update_pending_lag(runtime, max_lag)
                
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥
                
            except Exception as e:
                logger.error(f"Error monitoring pending tasks: {e}")
                await asyncio.sleep(30)

async def main():
    """åˆ†å‘å™¨ä¸»ç¨‹åº"""
    redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
    task_file = os.getenv("TASK_FILE", "tasks.jsonl")
    
    dispatcher = TaskDispatcher(redis_url, task_file)
    
    try:
        await dispatcher.start()
    except KeyboardInterrupt:
        logger.info("Received interrupt signal")
    finally:
        await dispatcher.redis.close()

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    asyncio.run(main())
```

---

## ğŸƒâ€â™‚ï¸ è½»é‡è¿è¡Œæ—¶å®ç°

### 1. è½»é‡Sandboxè¿è¡Œæ—¶

```python
# runtimes/sandbox/runtime.py
import asyncio
import json
import logging
import os
import time
import subprocess
import tempfile
import uuid
from typing import Dict, List, Optional
import redis.asyncio as redis
from core.interfaces import (
    RuntimeInterface, TaskSpec, TrajectoryResult, 
    ExecutionStep, ActionType, ErrorType
)
from core.metrics import metrics
from core.cache import TemplateCache
from .nsjail_executor import NSJailExecutor

logger = logging.getLogger(__name__)

class LightweightSandboxRuntime(RuntimeInterface):
    """è½»é‡çº§æ²™ç›’è¿è¡Œæ—¶ - ä½¿ç”¨nsjailæ›¿ä»£Docker"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.redis = redis.from_url(config["redis_url"])
        self.vllm_url = config.get("vllm_url", "http://localhost:8000")
        self._runtime_id = "sandbox-runtime"
        self._capabilities = ["python_executor", "shell_executor", "file_operations"]
        self.executor = NSJailExecutor()
        self.cache = TemplateCache(self.redis)
        
    @property
    def runtime_id(self) -> str:
        return self._runtime_id
    
    @property
    def capabilities(self) -> List[str]:
        return self._capabilities
    
    async def start(self):
        """å¯åŠ¨è¿è¡Œæ—¶"""
        # åˆ›å»ºæ¶ˆè´¹è€…ç»„
        queue_name = "tasks:code"
        try:
            await self.redis.xgroup_create(queue_name, "workers", id="0", mkstream=True)
        except redis.ResponseError:
            pass
        
        # å¯åŠ¨metricsæœåŠ¡å™¨
        metrics.start_server(8001)
        
        # æ³¨å†Œå¥åº·çŠ¶æ€
        await self._register_health()
        
        # å¼€å§‹æ¶ˆè´¹ä»»åŠ¡
        await self._consume_tasks()
    
    async def execute(self, task: TaskSpec) -> TrajectoryResult:
        """æ‰§è¡Œä»£ç ä»»åŠ¡"""
        start_time = time.time()
        metrics.record_task_started(task.task_id, self.runtime_id)
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cached_result = await self.cache.get("code", task.description)
            if cached_result:
                metrics.record_cache_hit("template")
                logger.info(f"Using cached result for task {task.task_id}")
                
                # ä»ç¼“å­˜æ„é€ ç»“æœ
                result = TrajectoryResult(
                    task_id=task.task_id,
                    runtime_id=self.runtime_id,
                    success=cached_result["success"],
                    steps=[
                        ExecutionStep(
                            step_id=0,
                            action_type=ActionType.CODE_EXECUTION,
                            action_params={"code": cached_result["code"], "cached": True},
                            observation=cached_result["output"],
                            success=cached_result["success"],
                            duration=0.1  # ç¼“å­˜æ‰§è¡Œæ—¶é—´å¾ˆçŸ­
                        )
                    ],
                    final_result=cached_result["output"],
                    total_duration=time.time() - start_time,
                    metadata={"cache_hit": True}
                )
                
                metrics.record_task_completed(task.task_id, self.runtime_id, result.success)
                return result
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œç”Ÿæˆæ–°ä»£ç 
            metrics.record_cache_miss("template")
            code = await self._generate_code(task.description)
            
            # æ‰§è¡Œä»£ç 
            execution_result = await self._execute_code_safely(code)
            
            # ç¼“å­˜ç»“æœ
            cache_data = {
                "code": code,
                "output": execution_result["output"],
                "success": execution_result["success"]
            }
            await self.cache.set("code", task.description, cache_data)
            
            # æ„é€ è½¨è¿¹
            step = ExecutionStep(
                step_id=0,
                action_type=ActionType.CODE_EXECUTION,
                action_params={"code": code},
                observation=execution_result["output"],
                success=execution_result["success"],
                error_type=execution_result.get("error_type"),
                error_message=execution_result.get("error"),
                duration=execution_result["duration"]
            )
            
            result = TrajectoryResult(
                task_id=task.task_id,
                runtime_id=self.runtime_id,
                success=execution_result["success"],
                steps=[step],
                final_result=execution_result["output"],
                error_type=execution_result.get("error_type"),
                error_message=execution_result.get("error"),
                total_duration=time.time() - start_time,
                metadata={
                    "code_length": len(code),
                    "cache_hit": False
                }
            )
            
            metrics.record_task_completed(
                task.task_id, 
                self.runtime_id, 
                result.success,
                result.error_type.value if result.error_type else None
            )
            return result
            
        except Exception as e:
            logger.error(f"Error executing task {task.task_id}: {e}")
            error_type = ErrorType.SYSTEM_ERROR
            metrics.record_task_completed(task.task_id, self.runtime_id, False, error_type.value)
            
            return TrajectoryResult(
                task_id=task.task_id,
                runtime_id=self.runtime_id,
                success=False,
                steps=[],
                final_result="",
                error_type=error_type,
                error_message=str(e),
                total_duration=time.time() - start_time
            )
    
    async def _generate_code(self, description: str) -> str:
        """ç”ŸæˆPythonä»£ç  - ç®€åŒ–ç‰ˆLLMè°ƒç”¨"""
        # TODO: é›†æˆå®é™…çš„LLM APIè°ƒç”¨
        # è¿™é‡Œæä¾›ä¸€ä¸ªåŸºäºè§„åˆ™çš„ç®€å•å®ç°
        
        if "fibonacci" in description.lower():
            return """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# è®¡ç®—ç¬¬10é¡¹
result = fibonacci(10)
print(f"fibonacci(10) = {result}")
"""
        elif "factorial" in description.lower():
            return """
def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n-1)

# è®¡ç®—5!
result = factorial(5)
print(f"factorial(5) = {result}")
"""
        elif "prime" in description.lower():
            return """
def is_prime(n):
    if n < 2:
        return False
    for i in range(2, int(n**0.5) + 1):
        if n % i == 0:
            return False
    return True

def find_primes(limit):
    return [i for i in range(2, limit) if is_prime(i)]

# æ‰¾å‡º100ä»¥å†…çš„è´¨æ•°
primes = find_primes(100)
print(f"Primes up to 100: {primes}")
print(f"Count: {len(primes)}")
"""
        else:
            # é€šç”¨æ¨¡æ¿
            return f"""
# ç”Ÿæˆçš„ä»£ç ç”¨äº: {description}
def solution():
    # TODO: æ ¹æ®æè¿°å®ç°å…·ä½“é€»è¾‘
    return "This is a placeholder solution for: {description}"

result = solution()
print(result)
"""
    
    async def _execute_code_safely(self, code: str) -> Dict:
        """ä½¿ç”¨nsjailå®‰å…¨æ‰§è¡Œä»£ç """
        start_time = time.time()
        
        try:
            # ä½¿ç”¨NSJailæ‰§è¡Œå™¨
            result = await self.executor.execute(code, timeout=30)
            
            return {
                "success": result["exit_code"] == 0,
                "output": result["stdout"],
                "error": result["stderr"] if result["exit_code"] != 0 else None,
                "error_type": self._classify_error(result) if result["exit_code"] != 0 else None,
                "duration": time.time() - start_time,
                "exit_code": result["exit_code"]
            }
            
        except asyncio.TimeoutError:
            return {
                "success": False,
                "output": "",
                "error": "Execution timeout",
                "error_type": ErrorType.TIMEOUT,
                "duration": time.time() - start_time,
                "exit_code": -1
            }
        except Exception as e:
            return {
                "success": False,
                "output": "",
                "error": str(e),
                "error_type": ErrorType.SYSTEM_ERROR,
                "duration": time.time() - start_time,
                "exit_code": -1
            }
    
    def _classify_error(self, result: Dict) -> ErrorType:
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        stderr = result.get("stderr", "").lower()
        
        if "syntaxerror" in stderr:
            return ErrorType.COMPILE_ERROR
        elif "timeout" in stderr:
            return ErrorType.TIMEOUT
        elif "permission" in stderr or "access" in stderr:
            return ErrorType.SYSTEM_ERROR
        else:
            return ErrorType.RUNTIME_ERROR
    
    async def _consume_tasks(self):
        """æ¶ˆè´¹ä»»åŠ¡é˜Ÿåˆ—"""
        consumer_id = f"sandbox-{os.getpid()}"
        queue_name = "tasks:code"
        
        logger.info(f"Starting task consumer {consumer_id}")
        
        while True:
            try:
                # è¯»å–ä»»åŠ¡
                messages = await self.redis.xreadgroup(
                    "workers", consumer_id,
                    {queue_name: ">"},
                    count=1, block=1000
                )
                
                if not messages:
                    continue
                
                for stream, msgs in messages:
                    for msg_id, fields in msgs:
                        try:
                            # è§£æä»»åŠ¡
                            task_data = json.loads(fields[b'task'].decode())
                            task = TaskSpec.from_dict(task_data)
                            
                            # æ‰§è¡Œä»»åŠ¡
                            result = await self.execute(task)
                            
                            # ä¿å­˜è½¨è¿¹
                            await self._save_trajectory(result)
                            
                            # ç¡®è®¤æ¶ˆæ¯
                            await self.redis.xack(queue_name, "workers", msg_id)
                            
                        except Exception as e:
                            logger.error(f"Error processing message {msg_id}: {e}")
                            await self.redis.xack(queue_name, "workers", msg_id)
                            
            except Exception as e:
                logger.error(f"Error in consumer loop: {e}")
                await asyncio.sleep(5)
    
    async def _save_trajectory(self, result: TrajectoryResult):
        """ä¿å­˜è½¨è¿¹åˆ°æ–‡ä»¶"""
        output_dir = "/app/output/trajectories"
        os.makedirs(output_dir, exist_ok=True)
        
        file_path = os.path.join(output_dir, f"{result.task_id}.json")
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(result.json())
        
        logger.info(f"Trajectory saved: {file_path}")
    
    async def _register_health(self):
        """æ³¨å†Œå¥åº·çŠ¶æ€"""
        await self.redis.hset("runtime_health", self.runtime_id, "healthy")
        
        # å®šæœŸæ›´æ–°å¥åº·çŠ¶æ€
        asyncio.create_task(self._health_updater())
    
    async def _health_updater(self):
        """å®šæœŸæ›´æ–°å¥åº·çŠ¶æ€"""
        while True:
            try:
                healthy = await self.health_check()
                status = "healthy" if healthy else "unhealthy"
                await self.redis.hset("runtime_health", self.runtime_id, status)
                metrics.update_runtime_health(self.runtime_id, healthy)
                
                await asyncio.sleep(30)  # æ¯30ç§’æ›´æ–°
            except Exception as e:
                logger.error(f"Health update error: {e}")
                await asyncio.sleep(10)
    
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æ£€æŸ¥Redisè¿æ¥
            await self.redis.ping()
            
            # æ£€æŸ¥nsjailå¯ç”¨æ€§
            result = subprocess.run(
                ["nsjail", "--help"], 
                capture_output=True, 
                timeout=5
            )
            if result.returncode != 0:
                return False
            
            return True
        except Exception:
            return False
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        await self.redis.close()

async def main():
    """è¿è¡Œæ—¶å…¥å£"""
    config = {
        "redis_url": os.getenv("REDIS_URL", "redis://localhost:6379"),
        "vllm_url": os.getenv("VLLM_URL", "http://localhost:8000")
    }
    
    runtime = LightweightSandboxRuntime(config)
    
    try:
        await runtime.start()
    except KeyboardInterrupt:
        logger.info("Received interrupt signal")
    finally:
        await runtime.cleanup()

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    asyncio.run(main())
```

### 2. NSJailæ‰§è¡Œå™¨

```python
# runtimes/sandbox/nsjail_executor.py
import asyncio
import logging
import subprocess
import tempfile
import os
from typing import Dict

logger = logging.getLogger(__name__)

class NSJailExecutor:
    """åŸºäºNSJailçš„è½»é‡çº§ä»£ç æ‰§è¡Œå™¨"""
    
    def __init__(self):
        self.jail_config = {
            "memory_limit": "128M",
            "time_limit": 30,
            "cpu_limit": 1,
            "max_processes": 10
        }
    
    async def execute(self, code: str, timeout: int = 30) -> Dict:
        """å®‰å…¨æ‰§è¡ŒPythonä»£ç """
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(code)
            script_path = f.name
        
        try:
            # æ„å»ºnsjailå‘½ä»¤
            cmd = [
                "nsjail",
                "--mode", "o",  # one-shot mode
                "--hostname", "sandbox",
                "--user", "nobody",
                "--group", "nogroup",
                "--rlimit_as", self.jail_config["memory_limit"],
                "--rlimit_cpu", str(self.jail_config["time_limit"]),
                "--rlimit_nproc", str(self.jail_config["max_processes"]),
                "--time_limit", str(timeout),
                "--disable_proc",
                "--iface_no_lo",
                "--mount", "/usr/bin/python3:/usr/bin/python3:ro",
                "--mount", "/lib:/lib:ro",
                "--mount", "/lib64:/lib64:ro",
                "--mount", "/usr/lib:/usr/lib:ro",
                "--mount", f"{script_path}:/tmp/script.py:ro",
                "--cwd", "/tmp",
                "--",
                "/usr/bin/python3", "/tmp/script.py"
            ]
            
            # æ‰§è¡Œå‘½ä»¤
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            try:
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(), 
                    timeout=timeout + 5
                )
                
                return {
                    "exit_code": process.returncode,
                    "stdout": stdout.decode('utf-8', errors='replace'),
                    "stderr": stderr.decode('utf-8', errors='replace')
                }
                
            except asyncio.TimeoutError:
                process.kill()
                await process.wait()
                return {
                    "exit_code": -1,
                    "stdout": "",
                    "stderr": "Execution timeout"
                }
                
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(script_path)
            except:
                pass
```

### 3. å†…å­˜æ§åˆ¶çš„Webè¿è¡Œæ—¶

```python
# runtimes/web_navigator/runtime.py
import asyncio
import json
import logging
import os
import time
from typing import List, Dict, Optional
import redis.asyncio as redis
from playwright.async_api import async_playwright, Browser, Page, BrowserContext
from core.interfaces import (
    RuntimeInterface, TaskSpec, TrajectoryResult, 
    ExecutionStep, ActionType, ErrorType
)
from core.metrics import metrics
from core.cache import TemplateCache
from .browser_manager import BrowserManager

logger = logging.getLogger(__name__)

class MemoryControlledWebRuntime(RuntimeInterface):
    """å†…å­˜æ§åˆ¶çš„Webå¯¼èˆªè¿è¡Œæ—¶"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.redis = redis.from_url(config["redis_url"])
        self.vllm_url = config.get("vllm_url", "http://localhost:8000")
        self._runtime_id = "web-runtime"
        self._capabilities = ["web_search", "web_navigation", "form_filling"]
        
        # å¹¶å‘æ§åˆ¶
        self.max_concurrent = config.get("max_concurrent", 4)
        self.semaphore = asyncio.Semaphore(self.max_concurrent)
        
        self.browser_manager = BrowserManager()
        self.cache = TemplateCache(self.redis)
        
    @property
    def runtime_id(self) -> str:
        return self._runtime_id
    
    @property
    def capabilities(self) -> List[str]:
        return self._capabilities
    
    async def start(self):
        """å¯åŠ¨è¿è¡Œæ—¶"""
        # åˆå§‹åŒ–æµè§ˆå™¨ç®¡ç†å™¨
        await self.browser_manager.initialize()
        
        # åˆ›å»ºæ¶ˆè´¹è€…ç»„
        queue_name = "tasks:web"
        try:
            await self.redis.xgroup_create(queue_name, "workers", id="0", mkstream=True)
        except redis.ResponseError:
            pass
        
        # å¯åŠ¨metricsæœåŠ¡å™¨
        metrics.start_server(8002)
        
        # æ³¨å†Œå¥åº·çŠ¶æ€
        await self._register_health()
        
        # å¼€å§‹æ¶ˆè´¹ä»»åŠ¡
        await self._consume_tasks()
    
    async def execute(self, task: TaskSpec) -> TrajectoryResult:
        """æ‰§è¡ŒWebå¯¼èˆªä»»åŠ¡"""
        # å¹¶å‘æ§åˆ¶
        async with self.semaphore:
            return await self._execute_with_browser(task)
    
    async def _execute_with_browser(self, task: TaskSpec) -> TrajectoryResult:
        """åœ¨æµè§ˆå™¨ä¸­æ‰§è¡Œä»»åŠ¡"""
        start_time = time.time()
        metrics.record_task_started(task.task_id, self.runtime_id)
        
        page = None
        steps = []
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"web_{task.description}_{task.constraints.get('start_url', '')}"
            cached_result = await self.cache.get("web", cache_key)
            
            if cached_result:
                metrics.record_cache_hit("template")
                logger.info(f"Using cached web actions for task {task.task_id}")
                
                # ä»ç¼“å­˜å¿«é€Ÿæ‰§è¡Œ
                result = await self._execute_cached_actions(task, cached_result)
                metrics.record_task_completed(task.task_id, self.runtime_id, result.success)
                return result
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œæ–°å¯¼èˆª
            metrics.record_cache_miss("template")
            
            # è·å–æµè§ˆå™¨é¡µé¢
            page = await self.browser_manager.get_page()
            
            # è·å–èµ·å§‹URL
            start_url = task.constraints.get("start_url", "https://www.google.com")
            
            # å¯¼èˆªåˆ°èµ·å§‹é¡µé¢
            await page.goto(start_url, wait_until='networkidle', timeout=30000)
            initial_content = await self._extract_page_content(page)
            
            # ç¬¬ä¸€æ­¥ï¼šé¡µé¢å¯¼èˆª
            nav_step = ExecutionStep(
                step_id=0,
                action_type=ActionType.BROWSER_ACTION,
                action_params={"action": "goto", "url": start_url},
                observation=initial_content[:1000],  # é™åˆ¶è§‚å¯Ÿé•¿åº¦
                success=True,
                duration=1.0
            )
            steps.append(nav_step)
            
            # æ‰§è¡Œåç»­åŠ¨ä½œ
            current_step = 1
            actions_cache = []
            
            while current_step < task.max_steps:
                step_start = time.time()
                
                # ç”Ÿæˆä¸‹ä¸€æ­¥åŠ¨ä½œ
                action = await self._generate_next_action(
                    task.description, 
                    await self._extract_page_content(page),
                    page.url
                )
                
                if action.get("type") == "finish":
                    break
                
                # æ‰§è¡ŒåŠ¨ä½œ
                success, observation, error_type = await self._execute_browser_action(page, action)
                
                step = ExecutionStep(
                    step_id=current_step,
                    action_type=ActionType.BROWSER_ACTION,
                    action_params=action,
                    observation=observation[:1000],  # é™åˆ¶è§‚å¯Ÿé•¿åº¦
                    success=success,
                    error_type=error_type,
                    duration=time.time() - step_start
                )
                steps.append(step)
                
                # è®°å½•åŠ¨ä½œç”¨äºç¼“å­˜
                actions_cache.append({
                    "action": action,
                    "success": success
                })
                
                if not success:
                    break
                
                current_step += 1
                
                # å†…å­˜å‹åŠ›æ£€æŸ¥
                if current_step > 5:  # é™åˆ¶æœ€å¤§æ­¥æ•°ä»¥æ§åˆ¶å†…å­˜
                    break
            
            # è·å–æœ€ç»ˆç»“æœ
            final_content = await self._extract_page_content(page)
            
            # ç¼“å­˜æˆåŠŸçš„åŠ¨ä½œåºåˆ—
            if len(actions_cache) > 0:
                cache_data = {
                    "actions": actions_cache,
                    "final_result": final_content[:500]
                }
                await self.cache.set("web", cache_key, cache_data)
            
            result = TrajectoryResult(
                task_id=task.task_id,
                runtime_id=self.runtime_id,
                success=any(s.success for s in steps),
                steps=steps,
                final_result=final_content[:1000],
                total_duration=time.time() - start_time,
                metadata={
                    "final_url": page.url,
                    "total_steps": len(steps),
                    "cache_hit": False
                }
            )
            
            metrics.record_task_completed(
                task.task_id, 
                self.runtime_id, 
                result.success,
                result.error_type.value if result.error_type else None
            )
            return result
            
        except Exception as e:
            logger.error(f"Error executing web task {task.task_id}: {e}")
            error_type = ErrorType.BROWSER_ERROR
            metrics.record_task_completed(task.task_id, self.runtime_id, False, error_type.value)
            
            return TrajectoryResult(
                task_id=task.task_id,
                runtime_id=self.runtime_id,
                success=False,
                steps=steps,
                final_result="",
                error_type=error_type,
                error_message=str(e),
                total_duration=time.time() - start_time
            )
        finally:
            if page:
                await self.browser_manager.release_page(page)
    
    async def _execute_cached_actions(self, task: TaskSpec, cached_data: Dict) -> TrajectoryResult:
        """æ‰§è¡Œç¼“å­˜çš„åŠ¨ä½œåºåˆ—"""
        start_time = time.time()
        
        # æ„é€ ç¼“å­˜ç»“æœ
        steps = []
        for i, action_data in enumerate(cached_data.get("actions", [])):
            step = ExecutionStep(
                step_id=i,
                action_type=ActionType.BROWSER_ACTION,
                action_params=action_data["action"],
                observation="[Cached result]",
                success=action_data["success"],
                duration=0.1
            )
            steps.append(step)
        
        return TrajectoryResult(
            task_id=task.task_id,
            runtime_id=self.runtime_id,
            success=True,
            steps=steps,
            final_result=cached_data.get("final_result", ""),
            total_duration=time.time() - start_time,
            metadata={"cache_hit": True}
        )
    
    async def _extract_page_content(self, page: Page) -> str:
        """æå–é¡µé¢å†…å®¹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            # è·å–é¡µé¢æ ‡é¢˜å’Œä¸»è¦æ–‡æœ¬
            title = await page.title()
            text_content = await page.evaluate('''
                () => {
                    // ç§»é™¤è„šæœ¬å’Œæ ·å¼
                    const scripts = document.querySelectorAll('script, style, nav, footer');
                    scripts.forEach(el => el.remove());
                    
                    // è·å–ä¸»è¦å†…å®¹åŒºåŸŸ
                    const main = document.querySelector('main, #content, .content, [role="main"]');
                    const content = main ? main.innerText : document.body.innerText;
                    
                    return content.slice(0, 1500);  // é™åˆ¶é•¿åº¦
                }
            ''')
            
            return f"# {title}\n\n{text_content}"
            
        except Exception as e:
            logger.error(f"Error extracting page content: {e}")
            return f"Error extracting content: {str(e)}"
    
    async def _generate_next_action(self, task_description: str, page_content: str, current_url: str) -> Dict:
        """ç”Ÿæˆä¸‹ä¸€æ­¥åŠ¨ä½œï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # åŸºäºè§„åˆ™çš„ç®€å•åŠ¨ä½œç”Ÿæˆ
        if "search" in task_description.lower():
            if "google.com" in current_url:
                return {
                    "type": "fill_and_submit",
                    "selector": "input[name='q'], input[type='search']",
                    "text": self._extract_search_terms(task_description)
                }
            else:
                return {"type": "finish"}
        elif "click" in task_description.lower():
            return {
                "type": "click",
                "selector": "a:first-of-type, button:first-of-type"
            }
        else:
            return {"type": "finish"}
    
    def _extract_search_terms(self, description: str) -> str:
        """ä»æè¿°ä¸­æå–æœç´¢è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        words = description.split()
        keywords = [w for w in words if len(w) > 2 and w.lower() not in 
                   ['search', 'find', 'look', 'for', 'about', 'information']]
        return ' '.join(keywords[:3])
    
    async def _execute_browser_action(self, page: Page, action: Dict) -> tuple:
        """æ‰§è¡Œæµè§ˆå™¨åŠ¨ä½œ"""
        try:
            action_type = action.get("type")
            
            if action_type == "fill_and_submit":
                selector = action.get("selector")
                text = action.get("text")
                
                await page.fill(selector, text, timeout=5000)
                await page.press(selector, "Enter")
                await page.wait_for_load_state('networkidle', timeout=10000)
                
                observation = await self._extract_page_content(page)
                return True, observation, None
                
            elif action_type == "click":
                selector = action.get("selector")
                
                await page.click(selector, timeout=5000)
                await page.wait_for_load_state('networkidle', timeout=10000)
                
                observation = await self._extract_page_content(page)
                return True, observation, None
                
            else:
                return False, f"Unknown action type: {action_type}", ErrorType.BROWSER_ERROR
                
        except Exception as e:
            logger.error(f"Error executing browser action: {e}")
            return False, str(e), ErrorType.BROWSER_ERROR
    
    async def _consume_tasks(self):
        """æ¶ˆè´¹ä»»åŠ¡é˜Ÿåˆ—"""
        consumer_id = f"web-{os.getpid()}"
        queue_name = "tasks:web"
        
        logger.info(f"Starting task consumer {consumer_id}")
        
        while True:
            try:
                messages = await self.redis.xreadgroup(
                    "workers", consumer_id,
                    {queue_name: ">"},
                    count=1, block=1000
                )
                
                if not messages:
                    continue
                
                for stream, msgs in messages:
                    for msg_id, fields in msgs:
                        try:
                            task_data = json.loads(fields[b'task'].decode())
                            task = TaskSpec.from_dict(task_data)
                            
                            result = await self.execute(task)
                            await self._save_trajectory(result)
                            await self.redis.xack(queue_name, "workers", msg_id)
                            
                        except Exception as e:
                            logger.error(f"Error processing message {msg_id}: {e}")
                            await self.redis.xack(queue_name, "workers", msg_id)
                            
            except Exception as e:
                logger.error(f"Error in consumer loop: {e}")
                await asyncio.sleep(5)
    
    async def _save_trajectory(self, result: TrajectoryResult):
        """ä¿å­˜è½¨è¿¹åˆ°æ–‡ä»¶"""
        output_dir = "/app/output/trajectories"
        os.makedirs(output_dir, exist_ok=True)
        
        file_path = os.path.join(output_dir, f"{result.task_id}.json")
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(result.json())
        
        logger.info(f"Trajectory saved: {file_path}")
    
    async def _register_health(self):
        """æ³¨å†Œå¥åº·çŠ¶æ€"""
        await self.redis.hset("runtime_health", self.runtime_id, "healthy")
        asyncio.create_task(self._health_updater())
    
    async def _health_updater(self):
        """å®šæœŸæ›´æ–°å¥åº·çŠ¶æ€"""
        while True:
            try:
                healthy = await self.health_check()
                status = "healthy" if healthy else "unhealthy"
                await self.redis.hset("runtime_health", self.runtime_id, status)
                metrics.update_runtime_health(self.runtime_id, healthy)
                
                await asyncio.sleep(30)
            except Exception as e:
                logger.error(f"Health update error: {e}")
                await asyncio.sleep(10)
    
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            await self.redis.ping()
            return await self.browser_manager.health_check()
        except:
            return False
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        await self.browser_manager.cleanup()
        await self.redis.close()

async def main():
    """è¿è¡Œæ—¶å…¥å£"""
    config = {
        "redis_url": os.getenv("REDIS_URL", "redis://localhost:6379"),
        "vllm_url": os.getenv("VLLM_URL", "http://localhost:8000"),
        "max_concurrent": int(os.getenv("MAX_CONCURRENT", "4"))
    }
    
    runtime = MemoryControlledWebRuntime(config)
    
    try:
        await runtime.start()
    except KeyboardInterrupt:
        logger.info("Received interrupt signal")
    finally:
        await runtime.cleanup()

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    asyncio.run(main())
```

### 4. æµè§ˆå™¨ç®¡ç†å™¨

```python
# runtimes/web_navigator/browser_manager.py
import asyncio
import logging
from typing import Optional, List
from playwright.async_api import async_playwright, Browser, Page, BrowserContext

logger = logging.getLogger(__name__)

class BrowserManager:
    """æµè§ˆå™¨èµ„æºç®¡ç†å™¨ - æ§åˆ¶å†…å­˜ä½¿ç”¨"""
    
    def __init__(self, max_contexts: int = 4, max_pages_per_context: int = 2):
        self.max_contexts = max_contexts
        self.max_pages_per_context = max_pages_per_context
        self.browser: Optional[Browser] = None
        self.contexts: List[BrowserContext] = []
        self.available_pages: List[Page] = []
        self.busy_pages: List[Page] = []
        self._lock = asyncio.Lock()
        
    async def initialize(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        playwright = await async_playwright().start()
        
        self.browser = await playwright.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-background-networking',
                '--disable-background-timer-throttling',
                '--disable-renderer-backgrounding',
                '--disable-backgrounding-occluded-windows',
                '--disable-client-side-phishing-detection',
                '--disable-component-extensions-with-background-pages',
                '--disable-default-apps',
                '--disable-extensions',
                '--disable-features=TranslateUI',
                '--disable-hang-monitor',
                '--disable-ipc-flooding-protection',
                '--disable-popup-blocking',
                '--disable-prompt-on-repost',
                '--disable-sync',
                '--memory-pressure-off',
                '--max_old_space_size=512',  # é™åˆ¶V8å †å†…å­˜
                '--single-process',          # å•è¿›ç¨‹æ¨¡å¼å‡å°‘å†…å­˜
            ]
        )
        
        # é¢„åˆ›å»ºä¸€äº›ä¸Šä¸‹æ–‡
        for _ in range(min(2, self.max_contexts)):
            await self._create_context()
        
        logger.info(f"Browser manager initialized with {len(self.contexts)} contexts")
    
    async def _create_context(self) -> BrowserContext:
        """åˆ›å»ºæ–°çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡"""
        context = await self.browser.new_context(
            viewport={'width': 1280, 'height': 720},
            user_agent='Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
            java_script_enabled=True,
            images_enabled=False,  # ç¦ç”¨å›¾ç‰‡åŠ è½½èŠ‚çœå†…å­˜
            accept_downloads=False,
            bypass_csp=True,
        )
        
        self.contexts.append(context)
        return context
    
    async def get_page(self) -> Page:
        """è·å–å¯ç”¨é¡µé¢"""
        async with self._lock:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨é¡µé¢
            if self.available_pages:
                page = self.available_pages.pop()
                self.busy_pages.append(page)
                return page
            
            # å¦‚æœæ²¡æœ‰å¯ç”¨é¡µé¢ï¼Œåˆ›å»ºæ–°é¡µé¢
            if len(self.contexts) < self.max_contexts:
                context = await self._create_context()
            else:
                # ä½¿ç”¨ç°æœ‰ä¸Šä¸‹æ–‡
                context = min(self.contexts, key=lambda c: len([p for p in self.busy_pages if p.context == c]))
            
            # æ£€æŸ¥ä¸Šä¸‹æ–‡ä¸­çš„é¡µé¢æ•°é‡
            context_pages = [p for p in self.busy_pages if p.context == context]
            if len(context_pages) >= self.max_pages_per_context:
                # ä¸Šä¸‹æ–‡å·²æ»¡ï¼Œç­‰å¾…é¡µé¢é‡Šæ”¾
                # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥ç­‰å¾…æˆ–è€…æŠ›å‡ºå¼‚å¸¸
                logger.warning("All browser contexts are busy")
                context = self.contexts[0]  # æš‚æ—¶ä½¿ç”¨ç¬¬ä¸€ä¸ªä¸Šä¸‹æ–‡
            
            page = await context.new_page()
            self.busy_pages.append(page)
            
            logger.debug(f"Created new page, total busy pages: {len(self.busy_pages)}")
            return page
    
    async def release_page(self, page: Page):
        """é‡Šæ”¾é¡µé¢"""
        async with self._lock:
            if page in self.busy_pages:
                self.busy_pages.remove(page)
                
                # æ¸…ç†é¡µé¢çŠ¶æ€
                try:
                    await page.evaluate('() => { window.localStorage.clear(); window.sessionStorage.clear(); }')
                    await page.goto('about:blank')
                except:
                    pass
                
                # å¦‚æœé¡µé¢æ± æœªæ»¡ï¼Œå›æ”¶é¡µé¢
                if len(self.available_pages) < 2:
                    self.available_pages.append(page)
                    logger.debug(f"Page returned to pool, available: {len(self.available_pages)}")
                else:
                    # å…³é—­å¤šä½™é¡µé¢
                    await page.close()
                    logger.debug(f"Page closed, busy pages: {len(self.busy_pages)}")
    
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            if not self.browser:
                return False
            
            # æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦ä»ç„¶å¯ç”¨
            contexts = self.browser.contexts
            return len(contexts) >= 0  # ç®€å•æ£€æŸ¥
            
        except Exception as e:
            logger.error(f"Browser health check failed: {e}")
            return False
    
    async def cleanup(self):
        """æ¸…ç†æ‰€æœ‰èµ„æº"""
        if self.browser:
            await self.browser.close()
        
        self.contexts.clear()
        self.available_pages.clear()
        self.busy_pages.clear()
        
        logger.info("Browser manager cleaned up")
```

---

## ğŸ³ Dockeré…ç½®æ–‡ä»¶

### 1. ä¸»é¡¹ç›®Dockerfile

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    curl \
    procps \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶æºç 
COPY core/ ./core/
COPY *.py ./

# åˆ›å»ºè¾“å‡ºç›®å½•
RUN mkdir -p /app/output/{trajectories,logs}

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/metrics || exit 1

# è®¾ç½®å…¥å£ç‚¹
CMD ["python", "-m", "core.dispatcher"]
```

### 2. è½»é‡Sandbox Dockerfile

```dockerfile
# runtimes/sandbox/Dockerfile
FROM python:3.10-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–å’Œnsjail
RUN apt-get update && apt-get install -y \
    curl \
    git \
    build-essential \
    pkg-config \
    libnl-route-3-dev \
    libprotobuf-dev \
    protobuf-compiler \
    && git clone https://github.com/google/nsjail.git \
    && cd nsjail \
    && make \
    && cp nsjail /usr/local/bin/ \
    && cd .. \
    && rm -rf nsjail \
    && apt-get remove -y build-essential git pkg-config \
    && apt-get autoremove -y \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶è¿è¡Œæ—¶ä»£ç 
COPY runtime.py .
COPY nsjail_executor.py .
COPY ../core/ ./core/

# åˆ›å»ºè¾“å‡ºç›®å½•
RUN mkdir -p /app/output/{trajectories,logs}

# æš´éœ²metricsç«¯å£
EXPOSE 8001

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8001/metrics || exit 1

CMD ["python", "runtime.py"]
```

### 3. Webå¯¼èˆªDockerfile

```dockerfile
# runtimes/web_navigator/Dockerfile
FROM mcr.microsoft.com/playwright/python:v1.40.0-jammy

WORKDIR /app

# å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å®‰è£…Playwrightæµè§ˆå™¨ï¼ˆä»…Chromiumï¼‰
RUN playwright install chromium && \
    playwright install-deps chromium

# å¤åˆ¶è¿è¡Œæ—¶ä»£ç 
COPY runtime.py .
COPY browser_manager.py .
COPY ../core/ ./core/

# åˆ›å»ºè¾“å‡ºç›®å½•
RUN mkdir -p /app/output/{trajectories,logs}

# æš´éœ²metricsç«¯å£
EXPOSE 8002

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8002/metrics || exit 1

CMD ["python", "runtime.py"]
```

---

## ğŸ“¦ ä¾èµ–æ–‡ä»¶

### 1. ä¸»é¡¹ç›®ä¾èµ–

```txt
# requirements.txt
redis==5.0.1
prometheus-client==0.19.0
asyncio-timeout==4.0.3
aiohttp==3.9.1
pydantic==2.5.2
```

### 2. Sandboxè¿è¡Œæ—¶ä¾èµ–

```txt
# runtimes/sandbox/requirements.txt
redis==5.0.1
prometheus-client==0.19.0
asyncio-timeout==4.0.3
aiohttp==3.9.1
```

### 3. Webè¿è¡Œæ—¶ä¾èµ–

```txt
# runtimes/web_navigator/requirements.txt
redis==5.0.1
prometheus-client==0.19.0
playwright==1.40.0
asyncio-timeout==4.0.3
aiohttp==3.9.1
```

---

## ğŸ™ Docker Composeé…ç½®

### 1. å®Œæ•´ç”Ÿäº§é…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  # Redisé˜Ÿåˆ—æœåŠ¡
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: >
      redis-server 
      --appendonly yes 
      --maxmemory 4gb 
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    restart: unless-stopped

  # vLLMæ¨ç†æœåŠ¡ï¼ˆå¯é€‰ï¼‰
  vllm:
    image: vllm/vllm-openai:v0.2.7
    environment:
      - MODEL=Salesforce/codegen-350M-mono
      - TENSOR_PARALLEL_SIZE=1
      - GPU_MEMORY_UTILIZATION=0.8
      - MAX_MODEL_LEN=2048
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ä»»åŠ¡åˆ†å‘å™¨
  dispatcher:
    build: .
    environment:
      - REDIS_URL=redis://redis:6379
      - TASK_FILE=/app/tasks.jsonl
      - LOG_LEVEL=INFO
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - ./tasks.jsonl:/app/tasks.jsonl:ro
      - ./output:/app/output
    restart: unless-stopped
    command: python -m core.dispatcher
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # è½»é‡ä»£ç æ‰§è¡Œè¿è¡Œæ—¶
  sandbox-runtime:
    build: ./runtimes/sandbox
    environment:
      - REDIS_URL=redis://redis:6379
      - VLLM_URL=http://vllm:8000
      - LOG_LEVEL=INFO
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - ./output:/app/output
    ports:
      - "8001:8001"  # metrics
    restart: unless-stopped
    cap_add:
      - SYS_ADMIN
    security_opt:
      - apparmor:unconfined
    deploy:
      replicas: 2  # è¿è¡Œ2ä¸ªå®ä¾‹

  # Webå¯¼èˆªè¿è¡Œæ—¶
  web-runtime:
    build: ./runtimes/web_navigator
    environment:
      - REDIS_URL=redis://redis:6379
      - VLLM_URL=http://vllm:8000
      - LOG_LEVEL=INFO
      - MAX_CONCURRENT=4
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - ./output:/app/output
    ports:
      - "8002:8002"  # metrics
    restart: unless-stopped
    shm_size: 2gb
    deploy:
      replicas: 1

  # Prometheusç›‘æ§
  prometheus:
    image: prom/prometheus:v2.48.1
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=7d'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  # Grafanaä»ªè¡¨æ¿
  grafana:
    image: grafana/grafana:10.2.2
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=redis-datasource
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards
    restart: unless-stopped

volumes:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  default:
    driver: bridge
```

### 2. å¿«é€ŸéªŒè¯é…ç½®

```yaml
# docker-compose.minimal.yml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes

  dispatcher:
    build: .
    environment:
      - REDIS_URL=redis://redis:6379
      - TASK_FILE=/app/tasks.jsonl
    depends_on: [redis]
    volumes:
      - ./tasks.jsonl:/app/tasks.jsonl:ro
      - ./output:/app/output

  sandbox-runtime:
    build: ./runtimes/sandbox
    environment:
      - REDIS_URL=redis://redis:6379
    depends_on: [redis]
    volumes:
      - ./output:/app/output
    ports:
      - "8001:8001"
    cap_add:
      - SYS_ADMIN
    security_opt:
      - apparmor:unconfined

networks:
  default:
    driver: bridge
```

---

## ğŸ“‹ é…ç½®æ–‡ä»¶

### 1. Prometheusé…ç½®

```yaml
# config/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'agent-pipeline'
    static_configs:
      - targets: 
          - 'dispatcher:8000'
          - 'sandbox-runtime:8001'
          - 'web-runtime:8002'
    scrape_interval: 5s
    metrics_path: /metrics

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']
    scrape_interval: 30s
```

### 2. Grafanaä»ªè¡¨æ¿

```json
# config/grafana/dashboards/agent-pipeline.json
{
  "dashboard": {
    "id": null,
    "title": "Agent Data Pipeline Dashboard",
    "tags": ["agent", "pipeline"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Task Throughput",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(tasks_completed_total[5m])",
            "legendFormat": "{{runtime}} - {{status}}"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
      },
      {
        "id": 2,
        "title": "Queue Sizes",
        "type": "graph",
        "targets": [
          {
            "expr": "queue_size",
            "legendFormat": "{{queue_name}}"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
      },
      {
        "id": 3,
        "title": "Error Rate by Type",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(tasks_failed_total[5m])",
            "legendFormat": "{{runtime}} - {{error_type}}"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
      },
      {
        "id": 4,
        "title": "Cache Hit Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(cache_hits_total[5m]) / (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m]))",
            "legendFormat": "Hit Rate"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
      }
    ],
    "time": {"from": "now-1h", "to": "now"},
    "refresh": "5s"
  }
}
```

---

## ğŸ§ª æµ‹è¯•è„šæœ¬

### 1. ä¸€é”®æ„å»ºè„šæœ¬

```bash
#!/bin/bash
# scripts/build.sh

set -e

echo "ğŸ”¨ Building Agent Data Platform..."

# æ£€æŸ¥Docker
if ! command -v docker &> /dev/null; then
    echo "âŒ Docker not found. Please install Docker first."
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "âŒ Docker Compose not found. Please install Docker Compose first."
    exit 1
fi

# åˆ›å»ºå¿…è¦ç›®å½•
echo "ğŸ“ Creating directories..."
mkdir -p output/{trajectories,logs}
mkdir -p config/{grafana/dashboards}

# æ„å»ºé•œåƒ
echo "ğŸ³ Building Docker images..."
docker-compose build --parallel

echo "âœ… Build completed successfully!"
echo ""
echo "Next steps:"
echo "1. Create tasks.jsonl with your tasks"
echo "2. Run: ./scripts/deploy.sh"
```

### 2. ä¸€é”®éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# scripts/deploy.sh

set -e

echo "ğŸš€ Deploying Agent Data Platform..."

# æ£€æŸ¥tasks.jsonl
if [ ! -f "tasks.jsonl" ]; then
    echo "ğŸ“ Creating sample tasks.jsonl..."
    cat > tasks.jsonl << 'EOF'
{"task_id": "demo_fib", "task_type": "code", "description": "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ç¬¬10é¡¹", "expected_tools": ["python_executor"], "constraints": {}, "max_steps": 5}
{"task_id": "demo_search", "task_type": "web", "description": "æœç´¢Pythonç¼–ç¨‹æ•™ç¨‹", "expected_tools": ["web_search"], "constraints": {"start_url": "https://www.google.com"}, "max_steps": 3}
{"task_id": "demo_prime", "task_type": "code", "description": "æ‰¾å‡º100ä»¥å†…çš„æ‰€æœ‰è´¨æ•°", "expected_tools": ["python_executor"], "constraints": {}, "max_steps": 5}
EOF
    echo "âœ… Sample tasks created"
fi

# éƒ¨ç½²æ¨¡å¼é€‰æ‹©
MODE=${1:-"minimal"}

if [ "$MODE" = "full" ]; then
    echo "ğŸ¯ Deploying full production stack..."
    docker-compose up -d
else
    echo "ğŸ¯ Deploying minimal stack for testing..."
    docker-compose -f docker-compose.minimal.yml up -d
fi

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ Waiting for services to start..."
sleep 30

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
echo "ğŸ” Checking service status..."
docker-compose ps

# æ˜¾ç¤ºè®¿é—®ä¿¡æ¯
echo ""
echo "ğŸ‰ Deployment completed!"
echo ""
echo "ğŸ“Š Monitoring URLs:"
if [ "$MODE" = "full" ]; then
    echo "  - Prometheus: http://localhost:9090"
    echo "  - Grafana: http://localhost:3000 (admin/admin)"
fi
echo "  - Sandbox Metrics: http://localhost:8001/metrics"
if [ "$MODE" = "full" ]; then
    echo "  - Web Runtime Metrics: http://localhost:8002/metrics"
fi
echo ""
echo "ğŸ“ Output directory: ./output/trajectories/"
echo ""
echo "ğŸ” Monitor progress:"
echo "  watch -n5 'ls output/trajectories | wc -l'"
echo ""
echo "ğŸ›‘ Stop all services:"
echo "  docker-compose down"
```

### 3. å®Œæ•´å†’çƒŸæµ‹è¯•

```bash
#!/bin/bash
# scripts/smoke_test.sh

set -e

echo "ğŸ” Starting comprehensive smoke test..."

# æ¸…ç†ä¹‹å‰çš„æµ‹è¯•ç»“æœ
echo "ğŸ§¹ Cleaning up previous test results..."
rm -rf output/trajectories/test_*
rm -f test_tasks.jsonl

# åˆ›å»ºæµ‹è¯•ä»»åŠ¡
echo "ğŸ“ Creating test tasks..."
cat > test_tasks.jsonl << 'EOF'
{"task_id": "test_fib_smoke", "task_type": "code", "description": "Calculate fibonacci(5)", "expected_tools": ["python_executor"], "constraints": {}, "max_steps": 3}
{"task_id": "test_fact_smoke", "task_type": "code", "description": "Calculate factorial(4)", "expected_tools": ["python_executor"], "constraints": {}, "max_steps": 3}
EOF

# å¤‡ä»½åŸtasks.jsonl
if [ -f "tasks.jsonl" ]; then
    cp tasks.jsonl tasks.jsonl.bak
fi

# ä½¿ç”¨æµ‹è¯•ä»»åŠ¡
cp test_tasks.jsonl tasks.jsonl

# å¯åŠ¨æœ€å°é…ç½®
echo "ğŸš€ Starting minimal services..."
docker-compose -f docker-compose.minimal.yml down
docker-compose -f docker-compose.minimal.yml up -d

# å¥åº·æ£€æŸ¥å‡½æ•°
check_service() {
    local service=$1
    local url=$2
    local max_attempts=30
    
    echo "Checking $service..."
    for i in $(seq 1 $max_attempts); do
        if curl -f -s "$url" > /dev/null 2>&1; then
            echo "âœ… $service is healthy"
            return 0
        fi
        sleep 2
    done
    
    echo "âŒ $service failed health check"
    return 1
}

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ Waiting for services to initialize..."
sleep 45

# å¥åº·æ£€æŸ¥
echo "ğŸ¥ Performing health checks..."
check_service "Redis" "http://localhost:6379" || (echo "Note: Redis check via HTTP not available, assuming healthy")
check_service "Sandbox Runtime" "http://localhost:8001/metrics"

# æ£€æŸ¥Redisè¿æ¥
echo "ğŸ”— Testing Redis connection..."
if docker exec $(docker-compose -f docker-compose.minimal.yml ps -q redis) redis-cli ping | grep -q PONG; then
    echo "âœ… Redis connection successful"
else
    echo "âŒ Redis connection failed"
    exit 1
fi

# ç­‰å¾…ä»»åŠ¡æ‰§è¡Œ
echo "â³ Waiting for task execution (60 seconds)..."
for i in $(seq 1 12); do
    completed=$(ls output/trajectories/test_*.json 2>/dev/null | wc -l)
    echo "Progress: $completed/2 tasks completed"
    if [ "$completed" -eq 2 ]; then
        break
    fi
    sleep 5
done

# æ£€æŸ¥ç»“æœ
echo "ğŸ” Checking test results..."
completed_tasks=$(ls output/trajectories/test_*.json 2>/dev/null | wc -l)

if [ "$completed_tasks" -eq 2 ]; then
    echo "âœ… All test tasks completed successfully"
    
    # éªŒè¯è½¨è¿¹æ–‡ä»¶å†…å®¹
    for file in output/trajectories/test_*.json; do
        if jq -e '.success' "$file" > /dev/null 2>&1; then
            task_id=$(jq -r '.task_id' "$file")
            success=$(jq -r '.success' "$file")
            echo "âœ… Task $task_id: success=$success"
        else
            echo "âŒ Invalid trajectory file: $file"
            exit 1
        fi
    done
else
    echo "âŒ Only $completed_tasks/2 tasks completed"
    echo "ğŸ“‹ Checking logs for errors..."
    
    echo "=== Dispatcher Logs ==="
    docker-compose -f docker-compose.minimal.yml logs dispatcher | tail -20
    
    echo "=== Sandbox Runtime Logs ==="
    docker-compose -f docker-compose.minimal.yml logs sandbox-runtime | tail -20
    
    exit 1
fi

# æ£€æŸ¥metrics
echo "ğŸ“Š Checking metrics..."
if curl -s http://localhost:8001/metrics | grep -q "tasks_completed_total"; then
    echo "âœ… Metrics are working"
    
    # æ˜¾ç¤ºä¸€äº›å…³é”®æŒ‡æ ‡
    echo "ğŸ“ˆ Key metrics:"
    curl -s http://localhost:8001/metrics | grep -E "(tasks_completed_total|tasks_failed_total|task_duration)" | head -5
else
    echo "âŒ Metrics not found"
    exit 1
fi

# æ¢å¤åŸtasks.jsonl
if [ -f "tasks.jsonl.bak" ]; then
    mv tasks.jsonl.bak tasks.jsonl
else
    rm -f tasks.jsonl
fi

# æ¸…ç†æµ‹è¯•æ–‡ä»¶
rm -f test_tasks.jsonl

echo ""
echo "ğŸ‰ Smoke test completed successfully!"
echo ""
echo "ğŸ“Š Test summary:"
echo "  - Services: âœ… Healthy"
echo "  - Tasks: âœ… $completed_tasks/2 completed"
echo "  - Metrics: âœ… Working"
echo ""
echo "ğŸ›‘ To stop test services:"
echo "  docker-compose -f docker-compose.minimal.yml down"
```

### 4. è´Ÿè½½æµ‹è¯•è„šæœ¬

```bash
#!/bin/bash
# scripts/load_test.sh

TASK_COUNT=${1:-50}
RUNTIME_MINS=${2:-5}

echo "ğŸš€ Starting load test..."
echo "ğŸ“Š Parameters:"
echo "  - Task count: $TASK_COUNT"
echo "  - Runtime: $RUNTIME_MINS minutes"

# æ¸…ç†ä¹‹å‰çš„ç»“æœ
echo "ğŸ§¹ Cleaning previous results..."
rm -rf output/trajectories/load_*
rm -f load_test_tasks.jsonl

# ç”Ÿæˆè´Ÿè½½æµ‹è¯•ä»»åŠ¡
echo "ğŸ“ Generating $TASK_COUNT test tasks..."
> load_test_tasks.jsonl

for i in $(seq 1 $TASK_COUNT); do
    # æ··åˆä¸åŒç±»å‹çš„ä»»åŠ¡
    if [ $((i % 3)) -eq 0 ]; then
        # Webä»»åŠ¡
        cat >> load_test_tasks.jsonl << EOF
{"task_id": "load_web_$i", "task_type": "web", "description": "Search for information about topic $i", "expected_tools": ["web_search"], "constraints": {"start_url": "https://www.google.com"}, "max_steps": 2}
EOF
    else
        # ä»£ç ä»»åŠ¡
        cat >> load_test_tasks.jsonl << EOF
{"task_id": "load_code_$i", "task_type": "code", "description": "Calculate fibonacci($((i % 10 + 5)))", "expected_tools": ["python_executor"], "constraints": {}, "max_steps": 3}
EOF
    fi
done

echo "âœ… Generated $TASK_COUNT tasks"

# å¤‡ä»½å¹¶æ›¿æ¢tasks.jsonl
if [ -f "tasks.jsonl" ]; then
    cp tasks.jsonl tasks.jsonl.load_bak
fi
cp load_test_tasks.jsonl tasks.jsonl

# å¯åŠ¨å®Œæ•´æœåŠ¡æ ˆ
echo "ğŸš€ Starting full service stack..."
docker-compose down
docker-compose up -d --scale sandbox-runtime=3 --scale web-runtime=2

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ Waiting for services to start..."
sleep 60

# è®°å½•å¼€å§‹æ—¶é—´
START_TIME=$(date +%s)
TIMEOUT=$((START_TIME + RUNTIME_MINS * 60))

echo "ğŸ“ˆ Monitoring progress for $RUNTIME_MINS minutes..."

# ç›‘æ§å¾ªç¯
while [ $(date +%s) -lt $TIMEOUT ]; do
    COMPLETED=$(ls output/trajectories/load_*.json 2>/dev/null | wc -l)
    ELAPSED=$(($(date +%s) - START_TIME))
    
    if [ $ELAPSED -gt 0 ]; then
        THROUGHPUT=$(echo "scale=2; $COMPLETED / $ELAPSED * 60" | bc -l 2>/dev/null || echo "0")
    else
        THROUGHPUT="0"
    fi
    
    echo "$(date '+%H:%M:%S') - Progress: $COMPLETED/$TASK_COUNT tasks, Throughput: $THROUGHPUT tasks/min"
    
    # å¦‚æœæ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œæå‰é€€å‡º
    if [ "$COMPLETED" -eq "$TASK_COUNT" ]; then
        echo "ğŸ‰ All tasks completed early!"
        break
    fi
    
    sleep 10
done

# æœ€ç»ˆç»Ÿè®¡
END_TIME=$(date +%s)
TOTAL_DURATION=$((END_TIME - START_TIME))
FINAL_COMPLETED=$(ls output/trajectories/load_*.json 2>/dev/null | wc -l)

echo ""
echo "ğŸ“Š Load test results:"
echo "  Total tasks: $TASK_COUNT"
echo "  Completed: $FINAL_COMPLETED"
echo "  Duration: ${TOTAL_DURATION}s"

if [ $TOTAL_DURATION -gt 0 ]; then
    THROUGHPUT=$(echo "scale=2; $FINAL_COMPLETED / $TOTAL_DURATION * 60" | bc -l)
    echo "  Throughput: $THROUGHPUT tasks/min"
fi

# è®¡ç®—æˆåŠŸç‡
if [ $FINAL_COMPLETED -gt 0 ]; then
    SUCCESS_COUNT=0
    for file in output/trajectories/load_*.json; do
        if [ -f "$file" ]; then
            if jq -e '.success' "$file" > /dev/null 2>&1 && [ "$(jq -r '.success' "$file")" = "true" ]; then
                SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
            fi
        fi
    done
    
    SUCCESS_RATE=$(echo "scale=2; $SUCCESS_COUNT / $FINAL_COMPLETED * 100" | bc -l)
    ERROR_RATE=$(echo "scale=2; 100 - $SUCCESS_RATE" | bc -l)
    
    echo "  Success rate: $SUCCESS_RATE%"
    echo "  Error rate: $ERROR_RATE%"
    
    if [ $(echo "$ERROR_RATE > 10" | bc -l) -eq 1 ]; then
        echo "âš ï¸  High error rate detected"
    fi
fi

# æ˜¾ç¤ºmetricsæ‘˜è¦
echo ""
echo "ğŸ“ˆ Metrics summary:"
for port in 8001 8002; do
    if curl -s http://localhost:$port/metrics > /dev/null 2>&1; then
        echo "Runtime on port $port:"
        curl -s http://localhost:$port/metrics | grep "tasks_completed_total" | head -3
    fi
done

# æ¢å¤åŸé…ç½®
if [ -f "tasks.jsonl.load_bak" ]; then
    mv tasks.jsonl.load_bak tasks.jsonl
else
    rm -f tasks.jsonl
fi

# æ¸…ç†
rm -f load_test_tasks.jsonl

echo ""
if [ "$FINAL_COMPLETED" -eq "$TASK_COUNT" ] && [ $(echo "$SUCCESS_RATE > 90" | bc -l) -eq 1 ]; then
    echo "âœ… Load test PASSED"
    exit 0
else
    echo "âŒ Load test FAILED"
    exit 1
fi
```

---

## ğŸŒŸ ä¸€é”®å¯åŠ¨æŒ‡å—

### å¤åˆ¶ç²˜è´´å¯åŠ¨åºåˆ—

```bash
# ğŸš€ ä¸€é”®å¯åŠ¨Agentæ•°æ®æ„å»ºå¹³å°

# 1. å…‹éš†æˆ–åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir -p agent-data-platform && cd agent-data-platform

# 2. åˆ›å»ºå®Œæ•´é¡¹ç›®ç»“æ„ï¼ˆå¤åˆ¶ç²˜è´´è¿™æ•´ä¸ªä»£ç å—ï¼‰
mkdir -p {core,runtimes/{sandbox,web_navigator},config/{prometheus,grafana/dashboards},scripts,tests,output/{trajectories,logs}}

# 3. ä¸‹è½½æœ¬æ–‡æ¡£çš„æ‰€æœ‰ä»£ç æ–‡ä»¶
# ï¼ˆå®é™…ä½¿ç”¨æ—¶ï¼Œè¿™äº›æ–‡ä»¶åº”è¯¥æ¥è‡ªgitä»“åº“ï¼‰

# 4. åˆ›å»ºç¤ºä¾‹ä»»åŠ¡æ–‡ä»¶
cat > tasks.jsonl << 'EOF'
{"task_id": "demo_fib", "task_type": "code", "description": "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ç¬¬10é¡¹", "expected_tools": ["python_executor"], "constraints": {}, "max_steps": 5}
{"task_id": "demo_search", "task_type": "web", "description": "æœç´¢Pythonç¼–ç¨‹æ•™ç¨‹", "expected_tools": ["web_search"], "constraints": {"start_url": "https://www.google.com"}, "max_steps": 3}
{"task_id": "demo_prime", "task_type": "code", "description": "æ‰¾å‡º100ä»¥å†…çš„æ‰€æœ‰è´¨æ•°", "expected_tools": ["python_executor"], "constraints": {}, "max_steps": 5}
{"task_id": "demo_factorial", "task_type": "code", "description": "è®¡ç®—5çš„é˜¶ä¹˜", "expected_tools": ["python_executor"], "constraints": {}, "max_```bash
# 4. åˆ›å»ºç¤ºä¾‹ä»»åŠ¡æ–‡ä»¶ï¼ˆç»­ï¼‰
{"task_id": "demo_factorial", "task_type": "code", "description": "è®¡ç®—5çš„é˜¶ä¹˜", "expected_tools": ["python_executor"], "constraints": {}, "max_steps": 5}
EOF

# 5. åˆ›å»ºæ„å»ºè„šæœ¬
cat > scripts/build.sh << 'EOF'
#!/bin/bash
set -e
echo "ğŸ”¨ Building Agent Data Platform..."
mkdir -p output/{trajectories,logs}
mkdir -p config/{grafana/dashboards}
docker-compose build --parallel
echo "âœ… Build completed!"
EOF

chmod +x scripts/build.sh

# 6. åˆ›å»ºéƒ¨ç½²è„šæœ¬  
cat > scripts/deploy.sh << 'EOF'
#!/bin/bash
set -e
echo "ğŸš€ Deploying Agent Data Platform..."

MODE=${1:-"minimal"}
if [ "$MODE" = "full" ]; then
    docker-compose up -d
else
    docker-compose -f docker-compose.minimal.yml up -d
fi

sleep 30
echo "ğŸ‰ Deployment completed!"
echo "ğŸ“Š Monitor: watch -n5 'ls output/trajectories | wc -l'"
echo "ğŸ›‘ Stop: docker-compose down"
EOF

chmod +x scripts/deploy.sh

# 7. ä¸€é”®æ„å»º
./scripts/build.sh

# 8. ä¸€é”®éƒ¨ç½²ï¼ˆæœ€å°æ¨¡å¼ï¼‰
./scripts/deploy.sh minimal

# 9. ç›‘æ§æ‰§è¡Œè¿›åº¦
echo "ğŸ“Š ç›‘æ§ä»»åŠ¡æ‰§è¡Œè¿›åº¦..."
watch -n5 'echo "å®Œæˆä»»åŠ¡æ•°: $(ls output/trajectories 2>/dev/null | wc -l)" && echo "è¿è¡ŒçŠ¶æ€:" && docker-compose ps'
```

### å¿«é€ŸéªŒè¯å‘½ä»¤

```bash
# ğŸ” éªŒè¯ç³»ç»Ÿæ­£å¸¸è¿è¡Œ

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker-compose ps

# æ£€æŸ¥Redisè¿æ¥
docker exec $(docker-compose ps -q redis) redis-cli ping

# æ£€æŸ¥metrics
curl -s http://localhost:8001/metrics | grep tasks_completed_total

# æŸ¥çœ‹å®æ—¶æ—¥å¿—
docker-compose logs -f sandbox-runtime

# æŸ¥çœ‹å·²å®Œæˆçš„è½¨è¿¹
ls -la output/trajectories/

# æŸ¥çœ‹è½¨è¿¹å†…å®¹ç¤ºä¾‹
if [ -f output/trajectories/*.json ]; then
    echo "è½¨è¿¹ç¤ºä¾‹:"
    jq '.' output/trajectories/*.json | head -20
fi
```

---

## ğŸ›ï¸ è¿ç»´å‘½ä»¤æ‰‹å†Œ

### æ—¥å¸¸æ“ä½œå‘½ä»¤

```bash
# ğŸ“‹ æ—¥å¸¸è¿ç»´å‘½ä»¤é€ŸæŸ¥

# ============ æœåŠ¡ç®¡ç† ============
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# å¯åŠ¨æœ€å°é…ç½®
docker-compose -f docker-compose.minimal.yml up -d

# åœæ­¢æ‰€æœ‰æœåŠ¡
docker-compose down

# é‡å¯ç‰¹å®šæœåŠ¡
docker-compose restart sandbox-runtime

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æ‰©å®¹è¿è¡Œæ—¶
docker-compose up -d --scale sandbox-runtime=4

# ============ æ—¥å¿—æŸ¥çœ‹ ============
# æŸ¥çœ‹æ‰€æœ‰æ—¥å¿—
docker-compose logs

# æŸ¥çœ‹ç‰¹å®šæœåŠ¡æ—¥å¿—
docker-compose logs -f sandbox-runtime

# æŸ¥çœ‹æœ€è¿‘100è¡Œæ—¥å¿—
docker-compose logs --tail=100 dispatcher

# æŒç»­ç›‘æ§é”™è¯¯æ—¥å¿—
docker-compose logs -f | grep -i error

# ============ ä»»åŠ¡ç®¡ç† ============
# æŸ¥çœ‹é˜Ÿåˆ—çŠ¶æ€
docker exec $(docker-compose ps -q redis) redis-cli xlen tasks:code
docker exec $(docker-compose ps -q redis) redis-cli xlen tasks:web

# æŸ¥çœ‹æŒ‚èµ·ä»»åŠ¡
docker exec $(docker-compose ps -q redis) redis-cli xpending tasks:code workers

# æ¸…ç©ºé˜Ÿåˆ—ï¼ˆæ…ç”¨ï¼‰
docker exec $(docker-compose ps -q redis) redis-cli del tasks:code tasks:web

# æ·»åŠ å•ä¸ªæµ‹è¯•ä»»åŠ¡
docker exec $(docker-compose ps -q redis) redis-cli xadd tasks:code "*" task '{"task_id":"manual_test","task_type":"code","description":"print hello world","expected_tools":["python_executor"],"max_steps":3}'

# ============ æ•°æ®ç®¡ç† ============
# æŸ¥çœ‹è¾“å‡ºç»Ÿè®¡
echo "å®Œæˆä»»åŠ¡æ•°: $(ls output/trajectories/*.json 2>/dev/null | wc -l)"

# æŸ¥çœ‹æˆåŠŸç‡
find output/trajectories -name "*.json" -exec jq -r '.success' {} \; | sort | uniq -c

# å¤‡ä»½è½¨è¿¹æ•°æ®
tar -czf trajectories_backup_$(date +%Y%m%d_%H%M%S).tar.gz output/trajectories/

# æ¸…ç†æ—§è½¨è¿¹ï¼ˆä¿ç•™æœ€è¿‘100ä¸ªï¼‰
ls -t output/trajectories/*.json 2>/dev/null | tail -n +101 | xargs rm -f

# ============ ç›‘æ§æ£€æŸ¥ ============
# æ£€æŸ¥å¥åº·çŠ¶æ€
curl -s http://localhost:8001/health || echo "Sandbox runtime ä¸å¥åº·"
curl -s http://localhost:8002/health || echo "Web runtime ä¸å¥åº·"

# æŸ¥çœ‹å…³é”®æŒ‡æ ‡
echo "=== ä»»åŠ¡å®ŒæˆæŒ‡æ ‡ ==="
curl -s http://localhost:8001/metrics | grep tasks_completed_total

echo "=== é˜Ÿåˆ—å¤§å° ==="
curl -s http://localhost:8001/metrics | grep queue_size

echo "=== é”™è¯¯ç»Ÿè®¡ ==="
curl -s http://localhost:8001/metrics | grep tasks_failed_total

# ============ æ•…éšœæ’æŸ¥ ============
# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
docker stats --no-stream

# æ£€æŸ¥å®¹å™¨èµ„æºé™åˆ¶
docker inspect $(docker-compose ps -q sandbox-runtime) | jq '.[].HostConfig.Memory'

# é‡ç½®RedisçŠ¶æ€
docker exec $(docker-compose ps -q redis) redis-cli flushall

# é‡å»ºæ‰€æœ‰å®¹å™¨
docker-compose down
docker-compose build --no-cache
docker-compose up -d
```

### æ€§èƒ½è°ƒä¼˜å‘½ä»¤

```bash
# ğŸ“ˆ æ€§èƒ½è°ƒä¼˜æŒ‡å—

# ============ å¹¶å‘è°ƒä¼˜ ============
# åŠ¨æ€è°ƒæ•´Sandboxè¿è¡Œæ—¶æ•°é‡
docker-compose up -d --scale sandbox-runtime=6

# é™åˆ¶Webè¿è¡Œæ—¶å¹¶å‘æ•°
docker-compose exec web-runtime sh -c 'echo "MAX_CONCURRENT=2" >> /etc/environment'

# ============ å†…å­˜ä¼˜åŒ– ============
# æ£€æŸ¥å®¹å™¨å†…å­˜ä½¿ç”¨
docker stats --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}"

# è®¾ç½®Rediså†…å­˜é™åˆ¶
docker exec $(docker-compose ps -q redis) redis-cli config set maxmemory 2gb

# æ¸…ç†Dockerç³»ç»Ÿç¼“å­˜
docker system prune -f

# ============ é˜Ÿåˆ—ä¼˜åŒ– ============
# æŸ¥çœ‹é˜Ÿåˆ—å¤„ç†é€Ÿç‡
for i in {1..5}; do
    echo "æ—¶é—´: $(date), é˜Ÿåˆ—é•¿åº¦: $(docker exec $(docker-compose ps -q redis) redis-cli xlen tasks:code)"
    sleep 10
done

# è®¾ç½®é˜Ÿåˆ—æœ€å¤§é•¿åº¦ï¼ˆé˜²æ­¢å†…å­˜æº¢å‡ºï¼‰
docker exec $(docker-compose ps -q redis) redis-cli xtrim tasks:code maxlen 1000

# ============ ç¼“å­˜ä¼˜åŒ– ============
# æŸ¥çœ‹ç¼“å­˜å‘½ä¸­ç‡
curl -s http://localhost:8001/metrics | grep cache_hits_total
curl -s http://localhost:8001/metrics | grep cache_misses_total

# æ¸…ç†ç¼“å­˜ï¼ˆå¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼‰
docker exec $(docker-compose ps -q redis) redis-cli --scan --pattern "template_cache:*" | xargs docker exec $(docker-compose ps -q redis) redis-cli del
```

---

## ğŸš¨ æ•…éšœæ’æŸ¥æ‰‹å†Œ

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

```bash
# ğŸ”§ æ•…éšœæ’æŸ¥å®Œæ•´æ‰‹å†Œ

# ============ é—®é¢˜1: æœåŠ¡å¯åŠ¨å¤±è´¥ ============
echo "æ£€æŸ¥æœåŠ¡å¯åŠ¨é—®é¢˜..."

# æ£€æŸ¥ç«¯å£å ç”¨
netstat -tulpn | grep -E ":(6379|8001|8002|9090|3000)"

# æ£€æŸ¥Dockerå®ˆæŠ¤è¿›ç¨‹
systemctl status docker

# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h | grep -E "(/$|/var)"

# è§£å†³æ–¹æ¡ˆ
if [ $(df / | tail -1 | awk '{print $5}' | sed 's/%//') -gt 90 ]; then
    echo "âš ï¸ ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œæ¸…ç†Dockerèµ„æº..."
    docker system prune -af --volumes
fi

# ============ é—®é¢˜2: ä»»åŠ¡æ‰§è¡Œå¡ä½ ============
echo "æ£€æŸ¥ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€..."

# æ£€æŸ¥æŒ‚èµ·ä»»åŠ¡
PENDING_TASKS=$(docker exec $(docker-compose ps -q redis) redis-cli xpending tasks:code workers | grep -c "^1)")
echo "æŒ‚èµ·ä»»åŠ¡æ•°: $PENDING_TASKS"

if [ "$PENDING_TASKS" -gt 5 ]; then
    echo "âš ï¸ å‘ç°è¿‡å¤šæŒ‚èµ·ä»»åŠ¡ï¼Œæ‰§è¡Œæ¢å¤..."
    
    # é‡å¯æ¶ˆè´¹è€…
    docker-compose restart sandbox-runtime web-runtime
    
    # ç­‰å¾…30ç§’åæ£€æŸ¥
    sleep 30
    NEW_PENDING=$(docker exec $(docker-compose ps -q redis) redis-cli xpending tasks:code workers | grep -c "^1)")
    echo "æ¢å¤åæŒ‚èµ·ä»»åŠ¡æ•°: $NEW_PENDING"
fi

# ============ é—®é¢˜3: å†…å­˜æº¢å‡º ============
echo "æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ..."

# æ£€æŸ¥å®¹å™¨å†…å­˜ä½¿ç”¨
HIGH_MEM_CONTAINERS=$(docker stats --no-stream --format "{{.Container}} {{.MemPerc}}" | awk '$2 > 80.0 {print $1}')

if [ -n "$HIGH_MEM_CONTAINERS" ]; then
    echo "âš ï¸ å‘ç°é«˜å†…å­˜ä½¿ç”¨å®¹å™¨:"
    echo "$HIGH_MEM_CONTAINERS"
    
    # é‡å¯é«˜å†…å­˜å®¹å™¨
    for container in $HIGH_MEM_CONTAINERS; do
        echo "é‡å¯å®¹å™¨: $container"
        docker restart "$container"
    done
fi

# ============ é—®é¢˜4: Redisè¿æ¥å¤±è´¥ ============
echo "æ£€æŸ¥Redisè¿æ¥..."

if ! docker exec $(docker-compose ps -q redis) redis-cli ping > /dev/null 2>&1; then
    echo "âš ï¸ Redisè¿æ¥å¤±è´¥ï¼Œå°è¯•ä¿®å¤..."
    
    # é‡å¯Redis
    docker-compose restart redis
    
    # ç­‰å¾…å¯åŠ¨
    sleep 10
    
    # éªŒè¯è¿æ¥
    if docker exec $(docker-compose ps -q redis) redis-cli ping > /dev/null 2>&1; then
        echo "âœ… Redisè¿æ¥å·²æ¢å¤"
    else
        echo "âŒ Redisè¿æ¥ä»ç„¶å¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨æ£€æŸ¥"
        exit 1
    fi
fi

# ============ é—®é¢˜5: è½¨è¿¹æ–‡ä»¶æŸå ============
echo "æ£€æŸ¥è½¨è¿¹æ–‡ä»¶å®Œæ•´æ€§..."

CORRUPTED_FILES=0
for file in output/trajectories/*.json; do
    if [ -f "$file" ]; then
        if ! jq empty "$file" > /dev/null 2>&1; then
            echo "âš ï¸ æŸåçš„è½¨è¿¹æ–‡ä»¶: $file"
            # ç§»åŠ¨åˆ°å¤‡ä»½ç›®å½•
            mkdir -p output/corrupted
            mv "$file" output/corrupted/
            CORRUPTED_FILES=$((CORRUPTED_FILES + 1))
        fi
    fi
done

echo "å‘ç°å¹¶å¤„ç†äº† $CORRUPTED_FILES ä¸ªæŸåæ–‡ä»¶"

# ============ é—®é¢˜6: ç½‘ç»œè¿æ¥é—®é¢˜ ============
echo "æ£€æŸ¥ç½‘ç»œè¿æ¥..."

# æµ‹è¯•å¤–éƒ¨ç½‘ç»œ
if ! curl -s --max-time 10 https://www.google.com > /dev/null; then
    echo "âš ï¸ å¤–éƒ¨ç½‘ç»œè¿æ¥å¤±è´¥ï¼ŒWebä»»åŠ¡å¯èƒ½å—å½±å“"
fi

# æµ‹è¯•å®¹å™¨é—´ç½‘ç»œ
if ! docker exec $(docker-compose ps -q dispatcher) ping -c 1 redis > /dev/null 2>&1; then
    echo "âš ï¸ å®¹å™¨é—´ç½‘ç»œè¿æ¥å¤±è´¥"
    echo "å°è¯•é‡å»ºç½‘ç»œ..."
    docker-compose down
    docker network prune -f
    docker-compose up -d
fi

echo "âœ… æ•…éšœæ’æŸ¥å®Œæˆ"
```

### è‡ªåŠ¨ä¿®å¤è„šæœ¬

```bash
#!/bin/bash
# scripts/auto_fix.sh - è‡ªåŠ¨ä¿®å¤å¸¸è§é—®é¢˜

echo "ğŸ”§ å¼€å§‹è‡ªåŠ¨ä¿®å¤..."

# ä¿®å¤1: æ¸…ç†èµ„æº
echo "1. æ¸…ç†ç³»ç»Ÿèµ„æº..."
docker system prune -f
docker volume prune -f

# ä¿®å¤2: é‡ç½®RedisçŠ¶æ€
echo "2. é‡ç½®RedisçŠ¶æ€..."
docker exec $(docker-compose ps -q redis) redis-cli config set maxmemory 4gb
docker exec $(docker-compose ps -q redis) redis-cli config set maxmemory-policy allkeys-lru

# ä¿®å¤3: é‡å¯æœåŠ¡
echo "3. é‡å¯æ ¸å¿ƒæœåŠ¡..."
docker-compose restart dispatcher sandbox-runtime

# ä¿®å¤4: éªŒè¯å¥åº·çŠ¶æ€
echo "4. éªŒè¯ä¿®å¤æ•ˆæœ..."
sleep 30

HEALTH_CHECK_PASSED=true

# æ£€æŸ¥Redis
if ! docker exec $(docker-compose ps -q redis) redis-cli ping > /dev/null; then
    echo "âŒ Rediså¥åº·æ£€æŸ¥å¤±è´¥"
    HEALTH_CHECK_PASSED=false
fi

# æ£€æŸ¥è¿è¡Œæ—¶
if ! curl -s http://localhost:8001/metrics > /dev/null; then
    echo "âŒ Sandboxè¿è¡Œæ—¶å¥åº·æ£€æŸ¥å¤±è´¥"
    HEALTH_CHECK_PASSED=false
fi

if [ "$HEALTH_CHECK_PASSED" = true ]; then
    echo "âœ… è‡ªåŠ¨ä¿®å¤æˆåŠŸ"
    exit 0
else
    echo "âŒ è‡ªåŠ¨ä¿®å¤å¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨å¹²é¢„"
    exit 1
fi
```

---

## ğŸ“š æœ€ç»ˆæ£€æŸ¥æ¸…å•

### éƒ¨ç½²å‰æ£€æŸ¥

```bash
# âœ… éƒ¨ç½²å‰å®Œæ•´æ£€æŸ¥æ¸…å•

echo "ğŸ“‹ æ‰§è¡Œéƒ¨ç½²å‰æ£€æŸ¥..."

# 1. ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥
echo "1. æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ..."
CHECK_PASSED=true

# Dockerç‰ˆæœ¬
DOCKER_VERSION=$(docker --version | grep -oE '[0-9]+\.[0-9]+' | head -1)
if [ $(echo "$DOCKER_VERSION >= 20.0" | bc) -eq 0 ]; then
    echo "âŒ Dockerç‰ˆæœ¬è¿‡ä½: $DOCKER_VERSION (éœ€è¦ >= 20.0)"
    CHECK_PASSED=false
else
    echo "âœ… Dockerç‰ˆæœ¬: $DOCKER_VERSION"
fi

# Docker Composeç‰ˆæœ¬
COMPOSE_VERSION=$(docker-compose --version | grep -oE '[0-9]+\.[0-9]+' | head -1)
if [ $(echo "$COMPOSE_VERSION >= 1.28" | bc) -eq 0 ]; then
    echo "âŒ Docker Composeç‰ˆæœ¬è¿‡ä½: $COMPOSE_VERSION (éœ€è¦ >= 1.28)"
    CHECK_PASSED=false
else
    echo "âœ… Docker Composeç‰ˆæœ¬: $COMPOSE_VERSION"
fi

# ç£ç›˜ç©ºé—´
DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
if [ "$DISK_USAGE" -gt 80 ]; then
    echo "âŒ ç£ç›˜ç©ºé—´ä¸è¶³: $DISK_USAGE% (å»ºè®® < 80%)"
    CHECK_PASSED=false
else
    echo "âœ… ç£ç›˜ç©ºé—´: $DISK_USAGE%"
fi

# å†…å­˜æ£€æŸ¥
TOTAL_MEM=$(free -g | awk '/^Mem:/{print $2}')
if [ "$TOTAL_MEM" -lt 8 ]; then
    echo "âš ï¸ å†…å­˜è¾ƒå°‘: ${TOTAL_MEM}GB (å»ºè®® >= 8GB)"
else
    echo "âœ… å†…å­˜: ${TOTAL_MEM}GB"
fi

# 2. ç«¯å£å¯ç”¨æ€§æ£€æŸ¥
echo "2. æ£€æŸ¥ç«¯å£å¯ç”¨æ€§..."
REQUIRED_PORTS="6379 8001 8002 9090 3000"
for port in $REQUIRED_PORTS; do
    if netstat -tuln | grep -q ":$port "; then
        echo "âŒ ç«¯å£ $port å·²è¢«å ç”¨"
        CHECK_PASSED=false
    else
        echo "âœ… ç«¯å£ $port å¯ç”¨"
    fi
done

# 3. æ–‡ä»¶ç»“æ„æ£€æŸ¥
echo "3. æ£€æŸ¥é¡¹ç›®æ–‡ä»¶..."
REQUIRED_FILES="docker-compose.yml core/interfaces.py runtimes/sandbox/Dockerfile"
for file in $REQUIRED_FILES; do
    if [ ! -f "$file" ]; then
        echo "âŒ ç¼ºå°‘æ–‡ä»¶: $file"
        CHECK_PASSED=false
    else
        echo "âœ… æ–‡ä»¶å­˜åœ¨: $file"
    fi
done

# 4. ä»»åŠ¡æ–‡ä»¶æ£€æŸ¥
echo "4. æ£€æŸ¥ä»»åŠ¡æ–‡ä»¶..."
if [ ! -f "tasks.jsonl" ]; then
    echo "âš ï¸ æœªæ‰¾åˆ° tasks.jsonlï¼Œå°†åˆ›å»ºç¤ºä¾‹æ–‡ä»¶"
    cat > tasks.jsonl << 'EOF'
{"task_id": "example_1", "task_type": "code", "description": "Calculate fibonacci(10)", "expected_tools": ["python_executor"], "max_steps": 5}
EOF
    echo "âœ… å·²åˆ›å»ºç¤ºä¾‹ tasks.jsonl"
else
    # éªŒè¯JSONæ ¼å¼
    if jq empty tasks.jsonl > /dev/null 2>&1; then
        TASK_COUNT=$(wc -l < tasks.jsonl)
        echo "âœ… tasks.jsonl æ ¼å¼æ­£ç¡®ï¼ŒåŒ…å« $TASK_COUNT ä¸ªä»»åŠ¡"
    else
        echo "âŒ tasks.jsonl æ ¼å¼é”™è¯¯"
        CHECK_PASSED=false
    fi
fi

# æœ€ç»ˆç»“æœ
echo ""
if [ "$CHECK_PASSED" = true ]; then
    echo "ğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹éƒ¨ç½²ï¼"
    echo ""
    echo "ä¸‹ä¸€æ­¥æ“ä½œ:"
    echo "1. æ„å»ºé•œåƒ: ./scripts/build.sh"
    echo "2. å¯åŠ¨æœåŠ¡: ./scripts/deploy.sh"
    echo "3. ç›‘æ§è¿›åº¦: watch -n5 'ls output/trajectories | wc -l'"
    exit 0
else
    echo "âŒ æ£€æŸ¥æœªé€šè¿‡ï¼Œè¯·ä¿®å¤ä¸Šè¿°é—®é¢˜åé‡è¯•"
    exit 1
fi
```

### éƒ¨ç½²åéªŒè¯

```bash
# âœ… éƒ¨ç½²åå®Œæ•´éªŒè¯

echo "ğŸ” æ‰§è¡Œéƒ¨ç½²åéªŒè¯..."

# ç­‰å¾…æœåŠ¡ç¨³å®š
echo "â³ ç­‰å¾…æœåŠ¡ç¨³å®š..."
sleep 30

VALIDATION_PASSED=true

# 1. æœåŠ¡å¥åº·æ£€æŸ¥
echo "1. æœåŠ¡å¥åº·æ£€æŸ¥..."
EXPECTED_SERVICES="redis dispatcher sandbox-runtime"
for service in $EXPECTED_SERVICES; do
    if docker-compose ps | grep -q "$service.*Up"; then
        echo "âœ… $service è¿è¡Œæ­£å¸¸"
    else
        echo "âŒ $service è¿è¡Œå¼‚å¸¸"
        VALIDATION_PASSED=false
    fi
done

# 2. è¿æ¥æ£€æŸ¥
echo "2. è¿æ¥æ£€æŸ¥..."

# Redisè¿æ¥
if docker exec $(docker-compose ps -q redis) redis-cli ping | grep -q PONG; then
    echo "âœ… Redisè¿æ¥æ­£å¸¸"
else
    echo "âŒ Redisè¿æ¥å¤±è´¥"
    VALIDATION_PASSED=false
fi

# Metricsç«¯ç‚¹
if curl -s http://localhost:8001/metrics > /dev/null; then
    echo "âœ… Sandbox metricså¯è®¿é—®"
else
    echo "âŒ Sandbox metricsä¸å¯è®¿é—®"
    VALIDATION_PASSED=false
fi

# 3. é˜Ÿåˆ—æ£€æŸ¥
echo "3. é˜Ÿåˆ—æ£€æŸ¥..."
CODE_QUEUE_EXISTS=$(docker exec $(docker-compose ps -q redis) redis-cli exists tasks:code)
if [ "$CODE_QUEUE_EXISTS" -eq 1 ]; then
    QUEUE_LENGTH=$(docker exec $(docker-compose ps -q redis) redis-cli xlen tasks:code)
    echo "âœ… ä»£ç ä»»åŠ¡é˜Ÿåˆ—å­˜åœ¨ï¼Œé•¿åº¦: $QUEUE_LENGTH"
else
    echo "âš ï¸ ä»£ç ä»»åŠ¡é˜Ÿåˆ—ä¸å­˜åœ¨ï¼ˆæ­£å¸¸ï¼Œé¦–æ¬¡å¯åŠ¨æ—¶ï¼‰"
fi

# 4. ä»»åŠ¡å¤„ç†æµ‹è¯•
echo "4. ä»»åŠ¡å¤„ç†æµ‹è¯•..."
TEST_TASK='{"task_id":"validation_test","task_type":"code","description":"print('\''Hello World'\'')","expected_tools":["python_executor"],"max_steps":3}'

# æäº¤æµ‹è¯•ä»»åŠ¡
docker exec $(docker-compose ps -q redis) redis-cli xadd tasks:code "*" task "$TEST_TASK"
echo "ğŸ“ å·²æäº¤æµ‹è¯•ä»»åŠ¡"

# ç­‰å¾…å¤„ç†
echo "â³ ç­‰å¾…ä»»åŠ¡å¤„ç†..."
for i in {1..30}; do
    if [ -f "output/trajectories/validation_test.json" ]; then
        echo "âœ… æµ‹è¯•ä»»åŠ¡å¤„ç†æˆåŠŸ"
        
        # éªŒè¯ç»“æœ
        if jq -e '.success' output/trajectories/validation_test.json > /dev/null; then
            SUCCESS=$(jq -r '.success' output/trajectories/validation_test.json)
            echo "âœ… ä»»åŠ¡æ‰§è¡Œç»“æœ: success=$SUCCESS"
        else
            echo "âŒ ä»»åŠ¡ç»“æœæ ¼å¼é”™è¯¯"
            VALIDATION_PASSED=false
        fi
        break
    fi
    
    if [ $i -eq 30 ]; then
        echo "âŒ æµ‹è¯•ä»»åŠ¡å¤„ç†è¶…æ—¶"
        VALIDATION_PASSED=false
    fi
    
    sleep 2
done

# 5. æ€§èƒ½åŸºå‡†æµ‹è¯•
echo "5. æ€§èƒ½åŸºå‡†æµ‹è¯•..."
START_TIME=$(date +%s)

# æäº¤5ä¸ªç®€å•ä»»åŠ¡
for i in {1..5}; do
    TASK="{\"task_id\":\"perf_test_$i\",\"task_type\":\"code\",\"description\":\"Calculate $i + $i\",\"expected_tools\":[\"python_executor\"],\"max_steps\":3}"
    docker exec $(docker-compose ps -q redis) redis-cli xadd tasks:code "*" task "$TASK"
done

# ç­‰å¾…å®Œæˆ
echo "â³ ç­‰å¾…æ€§èƒ½æµ‹è¯•å®Œæˆ..."
COMPLETED=0
for i in {1..60}; do
    COMPLETED=$(ls output/trajectories/perf_test_*.json 2>/dev/null | wc -l)
    if [ "$COMPLETED" -eq 5 ]; then
        break
    fi
    sleep 1
done

END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))

if [ "$COMPLETED" -eq 5 ]; then
    THROUGHPUT=$(echo "scale=2; 5 / $DURATION * 60" | bc)
    echo "âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ: $COMPLETED/5 ä»»åŠ¡ï¼Œç”¨æ—¶ ${DURATION}sï¼Œååé‡ $THROUGHPUT ä»»åŠ¡/åˆ†é’Ÿ"
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    rm -f output/trajectories/perf_test_*.json output/trajectories/validation_test.json
else
    echo "âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: ä»…å®Œæˆ $COMPLETED/5 ä»»åŠ¡"
    VALIDATION_PASSED=false
fi

# æœ€ç»ˆç»“æœ
echo ""
if [ "$VALIDATION_PASSED" = true ]; then
    echo "ğŸ‰ éƒ¨ç½²éªŒè¯æˆåŠŸï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸"
    echo ""
    echo "ğŸ“Š ç›‘æ§åœ°å€:"
    echo "  - Sandbox Metrics: http://localhost:8001/metrics"
    echo "  - Prometheus: http://localhost:9090 (å¦‚æœå¯ç”¨)"
    echo "  - Grafana: http://localhost:3000 (å¦‚æœå¯ç”¨)"
    echo ""
    echo "ğŸ“ è¾“å‡ºç›®å½•: ./output/trajectories/"
    echo "ğŸ“ˆ ç›‘æ§å‘½ä»¤: watch -n5 'ls output/trajectories | wc -l'"
    echo ""
    exit 0
else
    echo "âŒ éƒ¨ç½²éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—å¹¶ä¿®å¤é—®é¢˜"
    echo ""
    echo "ğŸ” æ’æŸ¥å»ºè®®:"
    echo "1. æŸ¥çœ‹æ—¥å¿—: docker-compose logs"
    echo "2. æ£€æŸ¥èµ„æº: docker stats"
    echo "3. é‡æ–°éƒ¨ç½²: docker-compose down && docker-compose up -d"
    echo ""
    exit 1
fi
```

---

## ğŸ¯ æ€»ç»“

è¿™ä»½å®Œæ•´çš„å·¥ç¨‹è“å›¾æä¾›äº†ï¼š

### âœ… ç«‹å³å¯ç”¨çš„ç‰¹æ€§
1. **ä¸€é”®éƒ¨ç½²** - 3æ¡å‘½ä»¤å³å¯å¯åŠ¨å®Œæ•´ç³»ç»Ÿ
2. **è½»é‡æ²™ç›’** - nsjailæ›¿ä»£Docker-in-Dockerï¼Œé•œåƒä»…80MB
3. **å†…å­˜æ§åˆ¶** - Webè¿è¡Œæ—¶å¹¶å‘é™åˆ¶ï¼Œé˜²æ­¢Chromeå†…å­˜çˆ†ç‚¸
4. **æ™ºèƒ½ç¼“å­˜** - é‡å¤ä»»åŠ¡ç¼“å­˜ï¼Œå‡å°‘50%èµ„æºæ¶ˆè€—
5. **åˆ†æµé˜Ÿåˆ—** - æŒ‰ç±»å‹åˆ†å‘ï¼Œæ¶ˆé™¤ç©ºè½®è¯¢
6. **å¤±è´¥åˆ†ç±»** - è¯¦ç»†é”™è¯¯ç±»å‹æ ‡ç­¾ï¼Œä¾¿äºæ•…éšœå®šä½
7. **è‡ªåŠ¨æ¢å¤** - ä»»åŠ¡è¶…æ—¶è‡ªåŠ¨é‡æ–°å…¥é˜Ÿ
8. **å®Œæ•´ç›‘æ§** - Prometheus + Grafanaä»ªè¡¨æ¿

### ğŸ”§ ç”Ÿäº§å°±ç»ªåŠŸèƒ½
1. **å¥åº·æ£€æŸ¥** - å®¹å™¨çº§å’Œåº”ç”¨çº§åŒé‡å¥åº·ç›‘æ§
2. **ä¼˜é›…å…³é—­** - èµ„æºæ¸…ç†å’ŒçŠ¶æ€ä¿å­˜
3. **æ°´å¹³æ‰©å±•** - æ”¯æŒå¤šå®ä¾‹è‡ªåŠ¨è´Ÿè½½å‡è¡¡
4. **æ•…éšœéš”ç¦»** - å•ä¸ªè¿è¡Œæ—¶æ•…éšœä¸å½±å“å…¶ä»–ç»„ä»¶
5. **æ•°æ®æŒä¹…åŒ–** - Redis AOF + è½¨è¿¹æ–‡ä»¶åŒé‡ä¿éšœ
6. **æ€§èƒ½è°ƒä¼˜** - å†…å­˜é™åˆ¶ã€å¹¶å‘æ§åˆ¶ã€é˜Ÿåˆ—ä¼˜åŒ–

### ğŸš€ 2å‘¨MVPäº¤ä»˜è·¯å¾„
- **Week 1**: åŸºç¡€æ¡†æ¶ + Sandboxè¿è¡Œæ—¶
- **Week 2**: Webè¿è¡Œæ—¶ + ç›‘æ§å®Œå–„

**ä¸€å¥è¯æ€»ç»“**ï¼šå¤åˆ¶æœ¬æ–‡æ¡£ä»£ç ï¼Œæ‰§è¡Œ3æ¡å‘½ä»¤ï¼Œå³å¯è·å¾—ä¸€ä¸ªå®Œæ•´çš„ã€ç”Ÿäº§çº§çš„Agentæ•°æ®æ„å»ºå¹³å°ï¼Œæ”¯æŒä»å•æœºéªŒè¯åˆ°é›†ç¾¤éƒ¨ç½²çš„å…¨åœºæ™¯éœ€æ±‚ã€‚