#!/usr/bin/env python3
"""
Agent Data Platform - Main Entry Point (æ— Dockerç‰ˆæœ¬)
"""

import asyncio
import logging
import os
import sys
from pathlib import Path
import signal
import argparse
from dotenv import load_dotenv
import contextlib
import io
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / '.env')

class SafeFormatter(logging.Formatter):
    """å®‰å…¨çš„æ—¥å¿—æ ¼å¼åŒ–å™¨ï¼Œå¤„ç†Unicodeå­—ç¬¦"""
    def format(self, record):
        try:
            return super().format(record)
        except UnicodeEncodeError:
            msg = record.getMessage()
            msg = msg.replace('âœ…', '[OK]').replace('âŒ', '[ERROR]').replace('âš ï¸', '[WARN]').replace('ğŸš€', '[START]').replace('ğŸ”§', '[FIX]').replace('â³', '[WAIT]').replace('ğŸ”„', '[PROC]')
            record.msg = msg
            record.args = ()
            return super().format(record)

class UnifiedLogCapture:
    """ç»Ÿä¸€çš„æ—¥å¿—æ•è·ç³»ç»Ÿ - æ•è·æ‰€æœ‰è¾“å‡ºåˆ°å•ä¸ªæ–‡ä»¶"""
    
    def __init__(self, log_file_path: str):
        self.log_file_path = log_file_path
        self.original_stdout = sys.stdout
        self.original_stderr = sys.stderr
        self.log_file = None
        self.original_handlers = []
        
    def __enter__(self):
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        os.makedirs(os.path.dirname(self.log_file_path), exist_ok=True)
        
        # æ‰“å¼€ç»Ÿä¸€æ—¥å¿—æ–‡ä»¶
        self.log_file = open(self.log_file_path, 'a', encoding='utf-8')
        
        # åˆ›å»ºä¸€ä¸ªåŒæ—¶å†™å…¥æ§åˆ¶å°å’Œæ–‡ä»¶çš„åŒ…è£…å™¨
        class UnifiedWriter:
            def __init__(self, console, file_handle):
                self.console = console
                self.file = file_handle
                
            def write(self, text):
                # å†™å…¥æ§åˆ¶å°
                self.console.write(text)
                self.console.flush()
                
                # å†™å…¥ç»Ÿä¸€æ—¥å¿—æ–‡ä»¶
                if text.strip():  # åªå¯¹éç©ºå†…å®¹æ·»åŠ æ—¶é—´æˆ³
                    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æ—¶é—´æˆ³ï¼Œé¿å…é‡å¤
                    if not text.startswith(timestamp[:10]):  # ç®€å•æ£€æŸ¥æ—¥æœŸéƒ¨åˆ†
                        self.file.write(f"[{timestamp}] {text}")
                    else:
                        self.file.write(text)
                else:
                    self.file.write(text)
                self.file.flush()
                
            def flush(self):
                self.console.flush()
                self.file.flush()
                
            def isatty(self):
                return self.console.isatty() if hasattr(self.console, 'isatty') else False
                
        # æ›¿æ¢stdoutå’Œstderr
        sys.stdout = UnifiedWriter(self.original_stdout, self.log_file)
        sys.stderr = UnifiedWriter(self.original_stderr, self.log_file)
        
        # é‡æ–°é…ç½®æ‰€æœ‰ç°æœ‰çš„logging handlersï¼Œè®©å®ƒä»¬ä¹Ÿè¾“å‡ºåˆ°ç»Ÿä¸€æ—¥å¿—
        self._reconfigure_logging()
        
        return self
        
    def _reconfigure_logging(self):
        """é‡æ–°é…ç½®loggingç³»ç»Ÿï¼Œè®©æ‰€æœ‰æ—¥å¿—éƒ½é€šè¿‡ç»Ÿä¸€è¾“å‡º"""
        # ä¿å­˜åŸå§‹handlers
        root_logger = logging.getLogger()
        self.original_handlers = root_logger.handlers.copy()
        
        # æ¸…é™¤æ‰€æœ‰ç°æœ‰handlers
        for handler in root_logger.handlers[:]:
            root_logger.removeHandler(handler)
            
        # åˆ›å»ºä¸€ä¸ªæ–°çš„StreamHandlerï¼Œå®ƒä¼šå†™å…¥æˆ‘ä»¬é‡å®šå‘çš„stdout
        # è¿™æ ·æ‰€æœ‰loggingè¾“å‡ºéƒ½ä¼šé€šè¿‡æˆ‘ä»¬çš„UnifiedWriter
        unified_handler = logging.StreamHandler(sys.stdout)
        unified_handler.setFormatter(SafeFormatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        
        # æ·»åŠ æ–°çš„handler
        root_logger.addHandler(unified_handler)
        
        # ç¡®ä¿æ—¥å¿—çº§åˆ«
        root_logger.setLevel(logging.DEBUG)
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        # æ¢å¤åŸå§‹çš„logging handlers
        root_logger = logging.getLogger()
        root_logger.handlers.clear()
        for handler in self.original_handlers:
            root_logger.addHandler(handler)
            
        # æ¢å¤åŸå§‹çš„stdoutå’Œstderr
        sys.stdout = self.original_stdout
        sys.stderr = self.original_stderr
        
        # å…³é—­æ—¥å¿—æ–‡ä»¶
        if self.log_file:
            self.log_file.close()

# ä¿æŒå‘åå…¼å®¹
TerminalOutputCapture = UnifiedLogCapture

# å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from core.config_manager import ConfigManager
from core.metrics import EnhancedMetrics
from core.redis_manager import RedisManager
from core.system_monitor import SystemMonitor
from core.toolscore.toolscore_client import ToolScoreClient as CoreToolScoreClient
from runtimes.reasoning.toolscore_client import ToolScoreClient as RuntimeToolScoreClient
from core.task_processing.task_loader import TaskLoader
from core.task_processing.task_enhancer import TaskEnhancer
from core.task_processing.task_distributor import TaskDistributor
from core.monitoring.queue_monitor import QueueMonitor
from core.llm_client import LLMClient # å¯¼å…¥LLMClient
from core.interfaces import TaskType # å¯¼å…¥TaskType
from core.dispatcher_enhanced import TaskProcessingCoordinator # å¯¼å…¥TaskProcessingCoordinator

from services.service_manager import ServiceManager
from services import (
    redis_service,
    toolscore_service,
    task_api_service,
    runtime_service,
    mcp_server_launcher,
    synthesis_service
)

from core.utils.path_utils import ensure_output_structure
import subprocess

def cleanup_ports():
    """å¢å¼ºçš„ç«¯å£å’Œè¿›ç¨‹æ¸…ç†åŠŸèƒ½ï¼Œå¯å®‰å…¨åœ°æ¸…ç†æ—§è¿›ç¨‹è€Œä¸ä¼šè‡ªæˆ‘ç»ˆæ­¢ã€‚"""
    current_pid = os.getpid()
    print(f"ğŸ§¹ å¼€å§‹å¢å¼ºç«¯å£æ¸…ç†... (å½“å‰è¿›ç¨‹PID: {current_pid})")

    # å®šä¹‰è¦æ¸…ç†çš„è¿›ç¨‹æ¨¡å¼
    process_patterns = [
        'main.py', 
        'mcp_servers', 
        'microsandbox_server', 
        'browser_use_server', 
        'search_tool_server', 
        'deepsearch_server'
    ]
    
    print("ğŸ” æœç´¢å¹¶æ¸…ç†ç›¸å…³çš„æ—§Pythonè¿›ç¨‹...")
    for pattern in process_patterns:
        try:
            # ä½¿ç”¨pgrepæŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„è¿›ç¨‹ID
            result = subprocess.run(['pgrep', '-f', pattern], capture_output=True, text=True)
            if result.returncode == 0:
                pids_to_kill = [pid for pid in result.stdout.strip().split('\n') if pid and int(pid) != current_pid]
                
                if pids_to_kill:
                    print(f"  - å‘ç°ä¸ '{pattern}' ç›¸å…³çš„æ—§è¿›ç¨‹: {', '.join(pids_to_kill)}. æ­£åœ¨ç»ˆæ­¢...")
                    # æ€æ­»è¿‡æ»¤åçš„è¿›ç¨‹
                    subprocess.run(['kill', '-9'] + pids_to_kill, check=False)
                else:
                    print(f"  - æœªå‘ç°ä¸ '{pattern}' ç›¸å…³çš„æ—§è¿›ç¨‹ã€‚")
            else:
                print(f"  - æœªå‘ç°ä¸ '{pattern}' ç›¸å…³çš„è¿›ç¨‹ã€‚")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†æ¨¡å¼ '{pattern}' æ—¶å‡ºé”™: {e}")
    
    print("âœ… è¿›ç¨‹æ¸…ç†å®Œæˆã€‚")

    # ç«¯å£æ¸…ç†éƒ¨åˆ†ä¿æŒä¸å˜
    ports = [8088, 8089, 8090, 8091, 8092, 5555, 8081, 8082, 8080, 8084, 8085, 8086, 8087, 8000]
    print("ğŸ§¹ å¼€å§‹æ¸…ç†ç½‘ç»œç«¯å£...")
    import time
    time.sleep(1) # ç­‰å¾…è¿›ç¨‹ç»ˆæ­¢

    for port in ports:
        try:
            result = subprocess.run(
                ['lsof', '-ti', f':{port}'], 
                capture_output=True, text=True, timeout=5
            )
            
            if result.returncode == 0 and result.stdout.strip():
                pids = result.stdout.strip().split('\n')
                for pid in pids:
                    # å†æ¬¡ç¡®è®¤ä¸ä¼šæ€æ­»è‡ªå·±
                    if pid and int(pid) != current_pid:
                        try:
                            subprocess.run(['kill', '-9', pid], timeout=3, check=False)
                            print(f"ğŸ”¥ å¼ºåˆ¶æ¸…ç†ç«¯å£ {port} çš„è¿›ç¨‹ {pid}")
                        except Exception as e:
                            print(f"âš ï¸ æ¸…ç†è¿›ç¨‹ {pid} å¤±è´¥: {e}")
            else:
                print(f"âœ… ç«¯å£ {port} ç©ºé—²")
                
        except Exception as e:
            print(f"âš ï¸ æ£€æŸ¥ç«¯å£ {port} æ—¶å‡ºé”™: {e}")
    
    time.sleep(1)
    print("âœ… å¢å¼ºç«¯å£æ¸…ç†å®Œæˆ")

# åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„
ensure_output_structure()
os.makedirs(project_root / 'logs', exist_ok=True)
os.makedirs(project_root / 'config', exist_ok=True)
os.makedirs(project_root / 'data', exist_ok=True)

# é…ç½®æ—¥å¿—

# åˆå§‹çš„åŸºç¡€loggingé…ç½® - å°†è¢«UnifiedLogCaptureé‡æ–°é…ç½®
logging.basicConfig(
    level=logging.DEBUG,  # å¯ç”¨DEBUGçº§åˆ«æ—¥å¿—
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        # æ³¨é‡Šæ‰å•ç‹¬çš„æ–‡ä»¶handlerï¼Œå°†ç”±UnifiedLogCaptureç»Ÿä¸€å¤„ç†
        # logging.FileHandler('logs/toolscore.log', mode='a', encoding='utf-8')
    ]
)

for handler in logging.root.handlers:
    handler.setFormatter(SafeFormatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))

logger = logging.getLogger(__name__)

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    # è·å–CPUæ ¸å¿ƒæ•°ä½œä¸ºé»˜è®¤å·¥ä½œè¿›ç¨‹æ•°
    try:
        default_workers = os.cpu_count() or 1
    except NotImplementedError:
        default_workers = 1
        
    parser = argparse.ArgumentParser(description="Agent Data Platform")
    parser.add_argument('--config-dir', type=str, default="config", help='é…ç½®æ–‡ä»¶ç›®å½•è·¯å¾„')
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--start-services', action='store_true', help='å¯åŠ¨æ‰€æœ‰æœåŠ¡ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰')
    parser.add_argument('--xml-streaming', action='store_true', default=True, help='å¯ç”¨XML streamingè¾“å‡ºæ ¼å¼ï¼ˆæ˜¾ç¤ºåŸå§‹çš„<think>ã€<search>ã€<answer>æ ‡ç­¾ï¼‰[é»˜è®¤å¯ç”¨]')
    parser.add_argument('--simple-runtime', action='store_true', default=True, help='ä½¿ç”¨ç®€åŒ–è¿è¡Œæ—¶ï¼ˆå‡å°‘å†—ä½™ä»£ç ï¼Œä¸“æ³¨æ ¸å¿ƒåŠŸèƒ½ï¼‰[é»˜è®¤å¯ç”¨]')
    parser.add_argument('--trajectory-storage', type=str, default='daily_grouped', 
                       choices=['individual', 'daily_grouped', 'weekly_grouped', 'monthly_grouped'],
                       help='è½¨è¿¹å­˜å‚¨æ¨¡å¼ï¼šindividual(å•ç‹¬æ–‡ä»¶), daily_grouped(æŒ‰æ—¥åˆ†ç»„), weekly_grouped(æŒ‰å‘¨åˆ†ç»„), monthly_grouped(æŒ‰æœˆåˆ†ç»„)')
    parser.add_argument('--enable-synthesis', action='store_true', default=False, 
                       help='å¯ç”¨TaskCraftè½¨è¿¹è‡ªåŠ¨ç›‘æ§å’Œä»»åŠ¡åˆæˆåŠŸèƒ½ï¼ˆé»˜è®¤å…³é—­ï¼‰')
    parser.add_argument('--num-workers', type=int, default=default_workers, 
                       help=f'å¯åŠ¨çš„å¹¶è¡Œä»»åŠ¡å¤„ç†å·¥ä½œè¿›ç¨‹æ•°é‡ (é»˜è®¤: {default_workers})')
    return parser.parse_args()

def setup_signal_handlers(service_manager):
    """è®¾ç½®ä¿¡å·å¤„ç†å™¨ä»¥ä¼˜é›…å…³é—­"""
    shutdown_requested = False
    
    def signal_handler(sig, frame):
        nonlocal shutdown_requested
        
        if shutdown_requested:
            logger.warning("âš ï¸ æ”¶åˆ°ç¬¬äºŒæ¬¡ä¸­æ–­ä¿¡å·ï¼Œå¼ºåˆ¶é€€å‡º...")
            force_cleanup()
            os._exit(1)
        
        shutdown_requested = True
        logger.info(f"ğŸ›‘ æ”¶åˆ°ä¿¡å· {sig} (Ctrl+C)ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­æœåŠ¡...")
        logger.info("ğŸ’¡ æç¤ºï¼šå†æ¬¡æŒ‰ Ctrl+C å¯å¼ºåˆ¶é€€å‡º")
        
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡æ¥ä¼˜é›…å…³é—­
        try:
            import asyncio
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # å¦‚æœäº‹ä»¶å¾ªç¯æ­£åœ¨è¿è¡Œï¼Œå®‰æ’ä¼˜é›…å…³é—­ä»»åŠ¡
                asyncio.create_task(graceful_shutdown(service_manager))
            else:
                # å¦‚æœæ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œç›´æ¥åŒæ­¥å…³é—­
                sync_graceful_shutdown(service_manager)
        except Exception as e:
            logger.error(f"âŒ ä¼˜é›…å…³é—­å¤±è´¥ï¼Œæ‰§è¡Œå¼ºåˆ¶å…³é—­: {e}")
            force_cleanup()
            os._exit(1)
    
    async def graceful_shutdown(service_manager):
        """å¼‚æ­¥ä¼˜é›…å…³é—­"""
        try:
            logger.info("ğŸ”„ å¼€å§‹ä¼˜é›…å…³é—­æ‰€æœ‰æœåŠ¡...")
            
            # ğŸ”§ æ–°å¢ï¼šä¼˜é›…å…³é—­æ—¶ä¹Ÿæ¸…ç†é˜Ÿåˆ—
            await clear_all_task_queues()
            
            await asyncio.wait_for(service_manager.stop_all(), timeout=15)
            logger.info("âœ… æ‰€æœ‰æœåŠ¡å·²ä¼˜é›…å…³é—­")
        except asyncio.TimeoutError:
            logger.warning("âš ï¸ æœåŠ¡å…³é—­è¶…æ—¶ï¼Œæ‰§è¡Œå¼ºåˆ¶æ¸…ç†")
            force_cleanup()
        except Exception as e:
            logger.error(f"âŒ ä¼˜é›…å…³é—­å¤±è´¥: {e}")
            force_cleanup()
        finally:
            logger.info("ğŸ‘‹ Agent Data Platform å·²å®‰å…¨é€€å‡º")
            os._exit(0)
    
    def sync_graceful_shutdown(service_manager):
        """åŒæ­¥ä¼˜é›…å…³é—­"""
        try:
            logger.info("ğŸ”„ å¼€å§‹ä¼˜é›…å…³é—­æ‰€æœ‰æœåŠ¡...")
            
            # ğŸ”§ æ–°å¢ï¼šåŒæ­¥ç‰ˆæœ¬çš„é˜Ÿåˆ—æ¸…ç†
            sync_clear_all_task_queues()
            
            service_manager.force_stop_all()  # åŒæ­¥åœæ­¢
            logger.info("âœ… æ‰€æœ‰æœåŠ¡å·²å…³é—­")
        except Exception as e:
            logger.error(f"âŒ å…³é—­å¤±è´¥: {e}")
            force_cleanup()
        finally:
            logger.info("ğŸ‘‹ Agent Data Platform å·²é€€å‡º")
            os._exit(0)

    async def clear_all_task_queues():
        """å¼‚æ­¥æ¸…ç†æ‰€æœ‰ä»»åŠ¡é˜Ÿåˆ—"""
        try:
            logger.info("ğŸ—‘ï¸ å¼€å§‹æ¸…ç†ä»»åŠ¡é˜Ÿåˆ—...")
            import redis.asyncio as async_redis
            
            # è¿æ¥åˆ°Redis
            try:
                redis_url = config_manager.get_redis_url() if 'config_manager' in globals() else "redis://localhost:6379"
                r = async_redis.from_url(redis_url)
                
                # æ¸…ç†ä¸»è¦é˜Ÿåˆ—
                queue_cleared = await r.delete('tasks:reasoning')
                logger.info(f"æ¸…ç†ä¸»ä»»åŠ¡é˜Ÿåˆ—: {queue_cleared} ä¸ªé˜Ÿåˆ—")
                
                # æ¸…ç†ä»»åŠ¡çŠ¶æ€
                task_keys = await r.keys('task:*')
                if task_keys:
                    task_status_cleared = await r.delete(*task_keys)
                    logger.info(f"æ¸…ç†ä»»åŠ¡çŠ¶æ€: {task_status_cleared} ä¸ªçŠ¶æ€")
                
                # æ¸…ç†ä»»åŠ¡çŠ¶æ€APIé”®
                task_status_keys = await r.keys('task_status:*')
                if task_status_keys:
                    api_status_cleared = await r.delete(*task_status_keys)
                    logger.info(f"æ¸…ç†APIä»»åŠ¡çŠ¶æ€: {api_status_cleared} ä¸ªçŠ¶æ€")
                
                # æ¸…ç†æ­»ä¿¡é˜Ÿåˆ—
                dead_letter_cleared = await r.delete('tasks:dead_letter')
                if dead_letter_cleared:
                    logger.info(f"æ¸…ç†æ­»ä¿¡é˜Ÿåˆ—: {dead_letter_cleared} ä¸ªé˜Ÿåˆ—")
                
                await r.close()
                logger.info("âœ… Redisé˜Ÿåˆ—æ¸…ç†å®Œæˆ")
                
            except Exception as redis_error:
                logger.warning(f"Redisé˜Ÿåˆ—æ¸…ç†å¤±è´¥: {redis_error}")
            
            # æ¸…ç†æœ¬åœ°ä»»åŠ¡æ–‡ä»¶
            _clear_local_task_file()
                
        except Exception as e:
            logger.error(f"ä»»åŠ¡é˜Ÿåˆ—æ¸…ç†å¤±è´¥: {e}")

    def sync_clear_all_task_queues():
        """åŒæ­¥æ¸…ç†æ‰€æœ‰ä»»åŠ¡é˜Ÿåˆ—"""
        try:
            logger.info("ğŸ—‘ï¸ å¼€å§‹æ¸…ç†ä»»åŠ¡é˜Ÿåˆ—...")
            import redis
            
            # è¿æ¥åˆ°Redis
            try:
                redis_url = config_manager.get_redis_url() if 'config_manager' in globals() else "redis://localhost:6379"
                r = redis.from_url(redis_url)
                
                # æ¸…ç†ä¸»è¦é˜Ÿåˆ—
                queue_cleared = r.delete('tasks:reasoning')
                logger.info(f"æ¸…ç†ä¸»ä»»åŠ¡é˜Ÿåˆ—: {queue_cleared} ä¸ªé˜Ÿåˆ—")
                
                # æ¸…ç†ä»»åŠ¡çŠ¶æ€
                task_keys = r.keys('task:*')
                if task_keys:
                    task_status_cleared = r.delete(*task_keys)
                    logger.info(f"æ¸…ç†ä»»åŠ¡çŠ¶æ€: {task_status_cleared} ä¸ªçŠ¶æ€")
                
                # æ¸…ç†ä»»åŠ¡çŠ¶æ€APIé”®
                task_status_keys = r.keys('task_status:*')
                if task_status_keys:
                    api_status_cleared = r.delete(*task_status_keys)
                    logger.info(f"æ¸…ç†APIä»»åŠ¡çŠ¶æ€: {api_status_cleared} ä¸ªçŠ¶æ€")
                
                # æ¸…ç†æ­»ä¿¡é˜Ÿåˆ—
                dead_letter_cleared = r.delete('tasks:dead_letter')
                if dead_letter_cleared:
                    logger.info(f"æ¸…ç†æ­»ä¿¡é˜Ÿåˆ—: {dead_letter_cleared} ä¸ªé˜Ÿåˆ—")
                
                r.close()
                logger.info("âœ… Redisé˜Ÿåˆ—æ¸…ç†å®Œæˆ")
                
            except Exception as redis_error:
                logger.warning(f"Redisé˜Ÿåˆ—æ¸…ç†å¤±è´¥: {redis_error}")
            
            # æ¸…ç†æœ¬åœ°ä»»åŠ¡æ–‡ä»¶
            _clear_local_task_file()
                
        except Exception as e:
            logger.error(f"ä»»åŠ¡é˜Ÿåˆ—æ¸…ç†å¤±è´¥: {e}")

    def _clear_local_task_file():
        """æ¸…ç†æœ¬åœ°ä»»åŠ¡æ–‡ä»¶çš„é€šç”¨å‡½æ•°"""
        try:
            task_file_path = 'tasks.jsonl'
            if 'config_manager' in globals():
                task_file_path = config_manager.get_task_file_path()
            
            with open(task_file_path, 'w', encoding='utf-8') as f:
                f.write('')  # æ¸…ç©ºæ–‡ä»¶
            logger.info(f"âœ… æ¸…ç©ºæœ¬åœ°ä»»åŠ¡æ–‡ä»¶: {task_file_path}")
            
        except Exception as file_error:
            logger.warning(f"æœ¬åœ°ä»»åŠ¡æ–‡ä»¶æ¸…ç†å¤±è´¥: {file_error}")

    async def emergency_shutdown(service_manager):
        """ç´§æ€¥å…³é—­æµç¨‹"""
        try:
            # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
            await asyncio.wait_for(service_manager.stop_all(), timeout=10)
        except asyncio.TimeoutError:
            logger.warning("æœåŠ¡åœæ­¢è¶…æ—¶ï¼Œæ‰§è¡Œå¼ºåˆ¶æ¸…ç†")
            force_cleanup()
        except Exception as e:
            logger.error(f"ç´§æ€¥å…³é—­å¤±è´¥: {e}")
            force_cleanup()
    
    def force_cleanup():
        """å¼ºåˆ¶æ¸…ç†æ‰€æœ‰èµ„æº"""
        logger.info("æ‰§è¡Œå¼ºåˆ¶æ¸…ç†...")
        
        # ğŸ”§ è°ƒç”¨é€šç”¨é˜Ÿåˆ—æ¸…ç†å‡½æ•°
        sync_clear_all_task_queues()
        
        # é¦–å…ˆå°è¯•ä½¿ç”¨ç³»ç»Ÿå‘½ä»¤å¼ºåˆ¶æ¸…ç†MCPæœåŠ¡å™¨è¿›ç¨‹
        try:
            import subprocess
            # æ¸…ç†æ‰€æœ‰MCPæœåŠ¡å™¨ç›¸å…³è¿›ç¨‹
            subprocess.run(['pkill', '-f', 'mcp_servers'], timeout=5, check=False)
            subprocess.run(['pkill', '-f', 'microsandbox_server'], timeout=3, check=False)
            subprocess.run(['pkill', '-f', 'browser_use_server'], timeout=3, check=False)
            subprocess.run(['pkill', '-f', 'search_tool_server'], timeout=3, check=False)
            logger.info("å·²å°è¯•æ¸…ç†MCPæœåŠ¡å™¨è¿›ç¨‹")
        except Exception as e:
            logger.warning(f"æ¸…ç†MCPæœåŠ¡å™¨è¿›ç¨‹å¤±è´¥: {e}")
        
        # å¼ºåˆ¶æ€æ­»æ‰€æœ‰ç›¸å…³è¿›ç¨‹
        try:
            import psutil
            current_pid = os.getpid()
            current_process = psutil.Process(current_pid)
            
            # æ€æ­»æ‰€æœ‰å­è¿›ç¨‹
            for child in current_process.children(recursive=True):
                try:
                    child.terminate()
                    child.wait(timeout=2)
                except:
                    try:
                        child.kill()
                    except:
                        pass
        except ImportError:
            # å¦‚æœæ²¡æœ‰psutilï¼Œä½¿ç”¨ç³»ç»Ÿå‘½ä»¤
            try:
                import subprocess
                subprocess.run(['pkill', '-f', 'python.*main.py'], timeout=5, check=False)
            except:
                pass
        
        # é‡Šæ”¾ç«¯å£
        release_ports([8088, 8089, 8100, 8081, 8082, 8080])
    
    def release_ports(ports):
        """å¼ºåˆ¶é‡Šæ”¾ç«¯å£"""
        for port in ports:
            try:
                import subprocess
                # åœ¨macOSä¸ŠæŸ¥æ‰¾å¹¶æ€æ­»å ç”¨ç«¯å£çš„è¿›ç¨‹
                result = subprocess.run(['lsof', '-ti', f':{port}'], 
                                      capture_output=True, text=True, timeout=5)
                if result.returncode == 0 and result.stdout.strip():
                    pids = result.stdout.strip().split('\n')
                    for pid in pids:
                        try:
                            subprocess.run(['kill', '-9', pid], timeout=3)
                            logger.info(f"å¼ºåˆ¶é‡Šæ”¾ç«¯å£ {port}ï¼Œæ€æ­»è¿›ç¨‹ {pid}")
                        except:
                            pass
            except:
                pass

    # ğŸ”§ æ³¨å†Œä¿¡å·å¤„ç†å™¨
    import signal
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

async def main_async():
    """å¼‚æ­¥ä¸»å‡½æ•°ï¼Œåº”ç”¨å…¥å£ç‚¹"""
    # å¯åŠ¨å‰å…ˆæ¸…ç†ç«¯å£
    cleanup_ports()
    
    logger.info("=== Agent Data Platform å¯åŠ¨ä¸­ ===")
    logger.debug("ğŸ”§ å¼€å§‹ç³»ç»Ÿåˆå§‹åŒ–æµç¨‹...")
    
    args = parse_arguments()
    logger.debug(f"ğŸ“ å‘½ä»¤è¡Œå‚æ•°: {vars(args)}")
    
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.debug("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨ã€‚")

    # 1. åˆå§‹åŒ– ConfigManager å¹¶è®¾ä¸ºå…¨å±€å˜é‡
    logger.debug("ğŸ”§ æ­¥éª¤1: åˆå§‹åŒ–ConfigManager...")
    global config_manager  # ğŸ”§ è®¾ä¸ºå…¨å±€å˜é‡ï¼Œä¾›æ¸…ç†å‡½æ•°ä½¿ç”¨
    config_manager = ConfigManager(config_dir=args.config_dir)
    logger.debug(f"âœ… ConfigManageråˆå§‹åŒ–å®Œæˆï¼Œé…ç½®ç›®å½•: {args.config_dir}")
    
    # 2. åŠ è½½æ‰€æœ‰å¿…è¦é…ç½®
    logger.debug("ğŸ”§ æ­¥éª¤2: åŠ è½½ç³»ç»Ÿé…ç½®...")
    redis_url = config_manager.get_redis_url()
    logger.debug(f"ğŸ“¡ Redis URL: {redis_url}")
    
    task_file = config_manager.get_task_file_path()
    logger.debug(f"ğŸ“‹ ä»»åŠ¡æ–‡ä»¶è·¯å¾„: {task_file}")
    
    routing_config = config_manager.load_routing_config()
    logger.debug(f"ğŸš¦ è·¯ç”±é…ç½®åŠ è½½å®Œæˆï¼Œä»»åŠ¡ç±»å‹æ˜ å°„: {routing_config.task_type_mapping}")
    
    queue_mapping = {
        TaskType(task_type_str.lower()): queue_name
        for task_type_str, queue_name in routing_config.task_type_mapping.items()
    }
    logger.debug(f"ğŸ“‹ é˜Ÿåˆ—æ˜ å°„: {queue_mapping}")
    
    # 3. å®ä¾‹åŒ–æ ¸å¿ƒç»„ä»¶
    logger.debug("ğŸ”§ æ­¥éª¤3: åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶...")
    metrics = EnhancedMetrics() # Metricså®ä¾‹
    logger.debug("âœ… EnhancedMetricsåˆå§‹åŒ–å®Œæˆ")
    
    redis_manager = RedisManager(redis_url) # RedisManagerå®ä¾‹
    logger.debug("âœ… RedisManageråˆå§‹åŒ–å®Œæˆ")
    
    # ToolScoreæœåŠ¡å¯åŠ¨åï¼Œè·å–å…¶å®é™…ç«¯å£
    ports_config = config_manager.get_ports_config()
    toolscore_mcp_port = ports_config['mcp_servers']['toolscore_mcp']['port']
    toolscore_http_port = ports_config['mcp_servers']['toolscore_http']['port']
    toolscore_http_endpoint = f"http://localhost:{toolscore_http_port}"
    toolscore_websocket_endpoint = f"ws://localhost:{toolscore_mcp_port}/websocket"
    
    # ä¸ºè¿è¡Œæ—¶å®ä¾‹åŒ–ä¸“ç”¨çš„ToolScoreå®¢æˆ·ç«¯
    runtime_toolscore_client = RuntimeToolScoreClient(toolscore_http_endpoint)
    
    # ä¸ºå…¶ä»–æ ¸å¿ƒç»„ä»¶å®ä¾‹åŒ–æ ¸å¿ƒToolScoreå®¢æˆ·ç«¯
    core_toolscore_client = CoreToolScoreClient(config_manager)
    
    # å®ä¾‹åŒ–ç»Ÿä¸€å·¥å…·ç®¡ç†å™¨å’ŒLLMå®¢æˆ·ç«¯
    from core.unified_tool_manager import UnifiedToolManager
    unified_tool_manager = UnifiedToolManager()
    llm_client = LLMClient(config_manager.get_llm_config(), tool_manager=unified_tool_manager)
    
    # Task Processing Components
    task_loader = TaskLoader(task_file)
    task_enhancer = TaskEnhancer(core_toolscore_client, simple_mode=args.simple_runtime)
    task_distributor = TaskDistributor(redis_url, metrics)
    
    # Monitoring Components
    queue_monitor = QueueMonitor(redis_url)
    system_monitor = SystemMonitor(redis_url, config_manager)

    # å®ä¾‹åŒ–Orchestrator
    from core.orchestrator import Orchestrator
    orchestrator = Orchestrator(
        tool_manager=unified_tool_manager,
        llm_client=llm_client,
        redis_manager=redis_manager,
        metrics_manager=metrics
    )

    # åˆ›å»ºæœåŠ¡ç®¡ç†å™¨
    service_manager = ServiceManager()
    
    # æ³¨å†ŒTaskProcessingCoordinator
    service_manager.register_service(
        name="task_processing_coordinator",
        initialize_fn=lambda config: None,
        start_fn=lambda: asyncio.create_task(
            TaskProcessingCoordinator(
                redis_url=redis_url,
                config_manager=config_manager,
                toolscore_client=core_toolscore_client,
                queue_monitor=queue_monitor,
                task_loader=task_loader,
                task_enhancer=task_enhancer,
                task_distributor=task_distributor,
                orchestrator=orchestrator,
                queue_mapping=queue_mapping
            ).start()
        ),
        stop_fn=lambda: logger.info("TaskProcessingCoordinator åœæ­¢ä¸­..."),
        health_check_fn=lambda: True,
        dependencies=["redis", "toolscore"]
    )

    # æ³¨å†Œå…¶ä»–æœåŠ¡
    service_manager.register_service(
        name="redis",
        initialize_fn=lambda config: redis_service.initialize(redis_manager),
        start_fn=redis_service.start,
        stop_fn=redis_service.stop,
        health_check_fn=redis_service.health_check,
        dependencies=[]
    )
    
    service_manager.register_service(
        name="toolscore",
        initialize_fn=lambda config: toolscore_service.initialize(config_manager),
        start_fn=toolscore_service.start,
        stop_fn=toolscore_service.stop,
        health_check_fn=toolscore_service.health_check,
        dependencies=["redis"]
    )
    
    service_manager.register_service(
        name="mcp_servers",
        initialize_fn=lambda config: mcp_server_launcher.initialize(
            config_manager, service_manager, unified_tool_manager
        ),
        start_fn=mcp_server_launcher.start,
        stop_fn=mcp_server_launcher.stop,
        health_check_fn=mcp_server_launcher.health_check,
        dependencies=["toolscore"]
    )
    
    service_manager.register_service(
        name="task_api",
        initialize_fn=lambda config: task_api_service.initialize(config_manager), # ä¼ å…¥config_managerå®ä¾‹
        start_fn=task_api_service.start,
        stop_fn=task_api_service.stop,
        health_check_fn=task_api_service.health_check,
        dependencies=["redis", "toolscore"]
    )
    
    service_manager.register_service(
        name="runtime",
        initialize_fn=lambda config: runtime_service.initialize(
            config or {},
            config_manager,
            llm_client,
            runtime_toolscore_client,
            unified_tool_manager,
            toolscore_websocket_endpoint,
            redis_manager,
            args.trajectory_storage,
            num_workers=args.num_workers  # ä¼ é€’å·¥ä½œè¿›ç¨‹æ•°é‡
        ),
        start_fn=runtime_service.start,
        stop_fn=runtime_service.stop,
        health_check_fn=runtime_service.health_check,
        dependencies=["redis", "toolscore", "mcp_servers"]
    )
    
    # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦å¯ç”¨TaskCraftè½¨è¿¹ç›‘æ§å’Œä»»åŠ¡åˆæˆåŠŸèƒ½
    if args.enable_synthesis:
        logger.info("âœ… å¯ç”¨TaskCraftè½¨è¿¹è‡ªåŠ¨ç›‘æ§å’Œä»»åŠ¡åˆæˆåŠŸèƒ½")
        # è·å–LLMé…ç½®å¹¶åˆå¹¶åˆ°æœåŠ¡é…ç½®ä¸­
        llm_config = config_manager.get_llm_config()
        synthesis_config = {
            'redis_url': redis_url,
            'TRAJECTORIES_DIR': str(project_root / 'output' / 'trajectories'),
            **llm_config  # åˆå¹¶LLMé…ç½®
        }
        
        service_manager.register_service(
            name="synthesis",
            initialize_fn=lambda config: synthesis_service.initialize(
                synthesis_config, 
                tool_manager=unified_tool_manager
            ),
            start_fn=synthesis_service.start,
            stop_fn=synthesis_service.stop,
            health_check_fn=synthesis_service.health_check,
            dependencies=["redis"]
        )
    else:
        logger.info("âšª TaskCraftè½¨è¿¹è‡ªåŠ¨ç›‘æ§å’Œä»»åŠ¡åˆæˆåŠŸèƒ½å·²ç¦ç”¨")
        logger.info("ğŸ’¡ æç¤ºï¼šä½¿ç”¨ --enable-synthesis å‚æ•°å¯ç”¨æ­¤åŠŸèƒ½")
    
    # ğŸ”§ è®¾ç½®ä¿¡å·å¤„ç†å™¨ - ç”¨äºç³»ç»Ÿé€€å‡ºæ—¶è‡ªåŠ¨æ¸…ç†é˜Ÿåˆ—
    setup_signal_handlers(service_manager)
    
    try:
        logger.debug("ğŸ”§ å¼€å§‹åˆå§‹åŒ–æ‰€æœ‰æœåŠ¡...")
        service_manager.initialize_all({}) # configå‚æ•°å¯èƒ½ä¸å†éœ€è¦ï¼Œå› ä¸ºç»„ä»¶å·²ç›´æ¥å®ä¾‹åŒ–
        logger.debug("âœ… æ‰€æœ‰æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        
        logger.debug("ğŸš€ å¼€å§‹å¯åŠ¨æ‰€æœ‰æœåŠ¡...")
        await service_manager.start_all()
        logger.debug("âœ… æ‰€æœ‰æœåŠ¡å¯åŠ¨å®Œæˆ")
        
        logger.info("ğŸ‰ æ‰€æœ‰æœåŠ¡å·²å¯åŠ¨ï¼Œç³»ç»Ÿè¿è¡Œä¸­...")
        logger.info("ğŸ“Š ç³»ç»ŸçŠ¶æ€ç›‘æ§å·²å¯ç”¨ï¼ŒæŒ‰ Ctrl+C åœæ­¢")
        
        # ä¿æŒä¸»äº‹ä»¶å¾ªç¯è¿è¡Œ
        startup_time = asyncio.get_event_loop().time()
        while True:
            await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡å¹¶è¾“å‡ºçŠ¶æ€
            current_time = asyncio.get_event_loop().time()
            uptime = int(current_time - startup_time)
            logger.debug(f"â° ç³»ç»Ÿè¿è¡Œæ—¶é—´: {uptime//3600}h {(uptime%3600)//60}m {uptime%60}s")
        
    except Exception as e:
        logger.error(f"å¯åŠ¨è¿‡ç¨‹ä¸­å‡ºé”™: {e}", exc_info=True)
        await service_manager.stop_all() # ç¡®ä¿è¿™é‡Œä¹Ÿawait
        sys.exit(1)

def main():
    # è®¾ç½®ç»Ÿä¸€æ—¥å¿—æ•è·
    unified_log_path = str(project_root / 'logs' / 'System.log')
    
    # åœ¨å¼€å§‹æ—¶å†™å…¥åˆ†éš”ç¬¦
    # The log directory is created at the top level of the script.
    with open(unified_log_path, 'a', encoding='utf-8') as f:
        f.write(f"\n{'='*80}\n")
        f.write(f"[ç³»ç»Ÿå¯åŠ¨] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"{'='*80}\n")
    
    # ä½¿ç”¨ç»Ÿä¸€æ—¥å¿—æ•è·ç³»ç»Ÿæ•è·æ‰€æœ‰è¾“å‡º
    with UnifiedLogCapture(unified_log_path):
        try:
            print(f"ğŸš€ Agent Data Platform å¯åŠ¨ä¸­... (æ‰€æœ‰æ—¥å¿—å°†ç»Ÿä¸€è®°å½•åˆ° {unified_log_path})")
            asyncio.run(main_async())
        except KeyboardInterrupt:
            print("\nâš¡ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­...")
        except Exception as e:
            print(f"âŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
            raise
        finally:
            print("ğŸ“ ç»ˆç«¯è¾“å‡ºæ•è·ç»“æŸ")

if __name__ == "__main__":
    main()