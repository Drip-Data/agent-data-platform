#!/usr/bin/env python3
"""
DeepSearch Server å¿«é€Ÿæµ‹è¯•
ä½¿ç”¨mockæ¥é¿å…å®é™…LLMè°ƒç”¨ï¼Œä¸“æ³¨äºæµ‹è¯•æ ¸å¿ƒé€»è¾‘
"""

import asyncio
import pytest
import json
import time
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
from unittest.mock import Mock, AsyncMock, patch, MagicMock
import sys
import os
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from mcp_servers.deepsearch_server.main import DeepSearchMCPServer
from core.config_manager import ConfigManager

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DeepSearchQuickTester:
    """DeepSearch å¿«é€Ÿæµ‹è¯•å™¨ï¼ˆä½¿ç”¨mockï¼‰"""
    
    def __init__(self):
        self.config_manager = ConfigManager()
        self.server = None
        self.test_results = []
        self.start_time = None
        
    async def setup_test_environment(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        logger.info("ğŸ”§ Setting up DeepSearch quick test environment...")
        
        # Initialize server
        self.server = DeepSearchMCPServer(self.config_manager)
        
        logger.info("âœ… DeepSearch quick test environment setup complete")
        
    def log_test_result(self, test_name: str, test_case: str, success: bool, 
                       duration: float, details: Dict[str, Any] = None, 
                       error: str = None):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        self.test_results.append({
            'test_name': test_name,
            'test_case': test_case,
            'success': success,
            'duration': duration,
            'details': details or {},
            'error': error,
            'timestamp': datetime.now().isoformat()
        })
        
        status = "âœ… PASS" if success else "âŒ FAIL"
        logger.info(f"{status} {test_name} - {test_case} ({duration:.2f}s)")
        if error:
            logger.error(f"    Error: {error}")
    
    async def test_json_parsing_fix(self):
        """æµ‹è¯•JSONè§£æä¿®å¤"""
        logger.info("ğŸ§ª Testing JSON parsing fix...")
        
        # æ¨¡æ‹ŸLLMè¿”å›çš„å„ç§æ ¼å¼
        test_cases = [
            {
                "name": "markdown_json_format",
                "mock_response": '''```json
{
  "queries": ["æµ‹è¯•æŸ¥è¯¢1", "æµ‹è¯•æŸ¥è¯¢2"],
  "rationale": "æµ‹è¯•åŸç†"
}
```''',
                "description": "Markdown JSONæ ¼å¼"
            },
            {
                "name": "plain_json_format",
                "mock_response": '{"queries": ["æµ‹è¯•æŸ¥è¯¢"], "rationale": "æµ‹è¯•"}',
                "description": "çº¯JSONæ ¼å¼"
            },
            {
                "name": "invalid_format",
                "mock_response": "è¿™ä¸æ˜¯JSONæ ¼å¼çš„å“åº”",
                "description": "æ— æ•ˆæ ¼å¼å¤„ç†"
            }
        ]
        
        for test_case in test_cases:
            start_time = time.time()
            try:
                # Mock LLMå“åº”
                with patch('core.llm_client.LLMClient._call_api', return_value=test_case["mock_response"]):
                    result = await self.server.handle_tool_action(
                        "research",
                        {"question": "æµ‹è¯•JSONè§£æ"}
                    )
                
                # å¯¹äºå‰ä¸¤ç§æƒ…å†µåº”è¯¥æˆåŠŸï¼Œç¬¬ä¸‰ç§ä¼šå›é€€åˆ°åŸé—®é¢˜
                expected_success = test_case["name"] != "invalid_format"
                actual_success = result.get("success", False)
                
                # å³ä½¿æ˜¯invalid_formatï¼Œä¹Ÿåº”è¯¥gracefullyå¤„ç†è€Œä¸æ˜¯å´©æºƒ
                success = actual_success if expected_success else not result.get("crashed", False)
                
                duration = time.time() - start_time
                
                self.log_test_result(
                    "JSON Parsing Fix",
                    test_case["description"],
                    success,
                    duration,
                    {
                        "case": test_case["name"],
                        "expected_success": expected_success,
                        "actual_success": actual_success,
                        "mock_response_type": type(test_case["mock_response"]).__name__
                    }
                )
                
            except Exception as e:
                duration = time.time() - start_time
                # å¯¹äºæµ‹è¯•æ¥è¯´ï¼Œå¼‚å¸¸ä¸åº”è¯¥å‘ç”Ÿ
                self.log_test_result(
                    "JSON Parsing Fix",
                    test_case["description"],
                    False,
                    duration,
                    error=str(e)
                )
    
    async def test_parameter_mapping_with_mock(self):
        """æµ‹è¯•å‚æ•°æ˜ å°„ï¼ˆä½¿ç”¨mockï¼‰"""
        logger.info("ğŸ§ª Testing parameter mapping with mock...")
        
        mock_research_response = {
            "answer": "è¿™æ˜¯æ¨¡æ‹Ÿçš„ç ”ç©¶ç»“æœ",
            "sources": [{"title": "æµ‹è¯•æ¥æº", "url": "https://test.com"}],
            "query_count": 1
        }
        
        test_cases = [
            {
                "parameters": {"question": "ç›´æ¥é—®é¢˜å‚æ•°"},
                "description": "ç›´æ¥questionå‚æ•°"
            },
            {
                "parameters": {"query": "æŸ¥è¯¢å‚æ•°æ˜ å°„"},
                "description": "query->questionæ˜ å°„"
            },
            {
                "parameters": {"task_description": "ä»»åŠ¡æè¿°æ˜ å°„"},
                "description": "task_description->questionæ˜ å°„"
            }
        ]
        
        for test_case in test_cases:
            start_time = time.time()
            try:
                # Mockæ•´ä¸ªç ”ç©¶è¿‡ç¨‹
                with patch.object(self.server.deepsearch_tool, 'research', return_value=mock_research_response):
                    result = await self.server.handle_tool_action(
                        "research",
                        test_case["parameters"]
                    )
                
                success = result.get("success", False)
                duration = time.time() - start_time
                
                self.log_test_result(
                    "Parameter Mapping",
                    test_case["description"],
                    success,
                    duration,
                    {
                        "parameters": test_case["parameters"],
                        "result_success": success
                    }
                )
                
            except Exception as e:
                duration = time.time() - start_time
                self.log_test_result(
                    "Parameter Mapping",
                    test_case["description"],
                    False,
                    duration,
                    error=str(e)
                )
    
    async def test_action_routing(self):
        """æµ‹è¯•åŠ¨ä½œè·¯ç”±"""
        logger.info("ğŸ§ª Testing action routing...")
        
        mock_response = {
            "answer": "æ¨¡æ‹Ÿå›ç­”",
            "sources": [],
            "query_count": 1
        }
        
        test_actions = [
            {"action": "research", "description": "åŸºç¡€ç ”ç©¶åŠ¨ä½œ"},
            {"action": "quick_research", "description": "å¿«é€Ÿç ”ç©¶åŠ¨ä½œ"},
            {"action": "comprehensive_research", "description": "å…¨é¢ç ”ç©¶åŠ¨ä½œ"}
        ]
        
        for test_action in test_actions:
            start_time = time.time()
            try:
                # Mockæ‰€æœ‰researchæ–¹æ³•
                with patch.object(self.server.deepsearch_tool, 'research', return_value=mock_response), \
                     patch.object(self.server.deepsearch_tool, 'quick_research', return_value=mock_response), \
                     patch.object(self.server.deepsearch_tool, 'comprehensive_research', return_value=mock_response):
                    
                    result = await self.server.handle_tool_action(
                        test_action["action"],
                        {"question": "æµ‹è¯•åŠ¨ä½œè·¯ç”±"}
                    )
                
                success = result.get("success", False)
                duration = time.time() - start_time
                
                self.log_test_result(
                    "Action Routing",
                    test_action["description"],
                    success,
                    duration,
                    {
                        "action": test_action["action"],
                        "result_success": success
                    }
                )
                
            except Exception as e:
                duration = time.time() - start_time
                self.log_test_result(
                    "Action Routing",
                    test_action["description"],
                    False,
                    duration,
                    error=str(e)
                )
    
    async def test_error_handling_scenarios(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†åœºæ™¯"""
        logger.info("ğŸ§ª Testing error handling scenarios...")
        
        error_scenarios = [
            {
                "scenario": "tool_method_not_found",
                "action": "nonexistent_research",
                "parameters": {"question": "æµ‹è¯•"},
                "description": "ä¸å­˜åœ¨çš„åŠ¨ä½œ",
                "expect_failure": True
            },
            {
                "scenario": "missing_required_parameter",
                "action": "research",
                "parameters": {},  # ç¼ºå°‘questionå‚æ•°
                "description": "ç¼ºå°‘å¿…éœ€å‚æ•°",
                "expect_failure": True
            },
            {
                "scenario": "tool_exception",
                "action": "research",
                "parameters": {"question": "æµ‹è¯•å¼‚å¸¸"},
                "description": "å·¥å…·æ‰§è¡Œå¼‚å¸¸",
                "mock_exception": Exception("æ¨¡æ‹Ÿå·¥å…·å¼‚å¸¸"),
                "expect_failure": True
            }
        ]
        
        for scenario in error_scenarios:
            start_time = time.time()
            try:
                if "mock_exception" in scenario:
                    # Mockå¼‚å¸¸
                    with patch.object(self.server.deepsearch_tool, 'research', side_effect=scenario["mock_exception"]):
                        result = await self.server.handle_tool_action(
                            scenario["action"],
                            scenario["parameters"]
                        )
                else:
                    # æ­£å¸¸è°ƒç”¨
                    result = await self.server.handle_tool_action(
                        scenario["action"],
                        scenario["parameters"]
                    )
                
                expect_failure = scenario.get("expect_failure", False)
                actual_success = result.get("success", False)
                
                # å¦‚æœæœŸæœ›å¤±è´¥ï¼Œé‚£ä¹ˆå®é™…å¤±è´¥å°±æ˜¯æˆåŠŸ
                success = (not actual_success) if expect_failure else actual_success
                
                duration = time.time() - start_time
                
                self.log_test_result(
                    "Error Handling",
                    scenario["description"],
                    success,
                    duration,
                    {
                        "scenario": scenario["scenario"],
                        "expect_failure": expect_failure,
                        "actual_success": actual_success,
                        "error_message": result.get("error_message", "")
                    }
                )
                
            except Exception as e:
                duration = time.time() - start_time
                expect_failure = scenario.get("expect_failure", False)
                
                # å¯¹äºæœŸæœ›å¤±è´¥çš„åœºæ™¯ï¼Œå¼‚å¸¸ä¹Ÿå¯ä»¥æ¥å—
                success = expect_failure
                
                self.log_test_result(
                    "Error Handling",
                    scenario["description"],
                    success,
                    duration,
                    error=str(e) if not expect_failure else None
                )
    
    async def test_performance_with_mock(self):
        """æµ‹è¯•æ€§èƒ½ï¼ˆä½¿ç”¨mockï¼‰"""
        logger.info("ğŸ§ª Testing performance with mock...")
        
        mock_response = {
            "answer": "å¿«é€Ÿæ¨¡æ‹Ÿå›ç­”" * 100,  # æ¨¡æ‹Ÿè¾ƒé•¿å›ç­”
            "sources": [{"title": f"æ¥æº{i}", "url": f"https://example{i}.com"} for i in range(5)],
            "query_count": 3
        }
        
        performance_tests = [
            {
                "test": "quick_response_time",
                "action": "quick_research",
                "max_time": 1.0,  # ä½¿ç”¨mockåº”è¯¥åœ¨1ç§’å†…å®Œæˆ
                "description": "å¿«é€Ÿå“åº”æ—¶é—´æµ‹è¯•"
            },
            {
                "test": "concurrent_requests",
                "action": "research",
                "max_time": 2.0,  # å¹¶å‘è¯·æ±‚æµ‹è¯•
                "description": "å¹¶å‘è¯·æ±‚å¤„ç†æµ‹è¯•",
                "concurrent": True
            }
        ]
        
        for perf_test in performance_tests:
            start_time = time.time()
            try:
                with patch.object(self.server.deepsearch_tool, 'research', return_value=mock_response), \
                     patch.object(self.server.deepsearch_tool, 'quick_research', return_value=mock_response):
                    
                    if perf_test.get("concurrent"):
                        # å¹¶å‘æµ‹è¯•
                        tasks = [
                            self.server.handle_tool_action(
                                perf_test["action"],
                                {"question": f"å¹¶å‘æµ‹è¯•{i}"}
                            ) for i in range(3)
                        ]
                        results = await asyncio.gather(*tasks)
                        success = all(r.get("success", False) for r in results)
                    else:
                        # å•ä¸ªè¯·æ±‚æµ‹è¯•
                        result = await self.server.handle_tool_action(
                            perf_test["action"],
                            {"question": "æ€§èƒ½æµ‹è¯•"}
                        )
                        success = result.get("success", False)
                
                duration = time.time() - start_time
                within_time_limit = duration <= perf_test["max_time"]
                
                overall_success = success and within_time_limit
                
                self.log_test_result(
                    "Performance",
                    perf_test["description"],
                    overall_success,
                    duration,
                    {
                        "test": perf_test["test"],
                        "max_time": perf_test["max_time"],
                        "within_limit": within_time_limit,
                        "result_success": success
                    }
                )
                
            except Exception as e:
                duration = time.time() - start_time
                self.log_test_result(
                    "Performance",
                    perf_test["description"],
                    False,
                    duration,
                    error=str(e)
                )
    
    async def run_all_quick_tests(self):
        """è¿è¡Œæ‰€æœ‰å¿«é€Ÿæµ‹è¯•"""
        logger.info("ğŸš€ Starting DeepSearch quick tests...")
        self.start_time = time.time()
        
        try:
            # è®¾ç½®ç¯å¢ƒ
            await self.setup_test_environment()
            
            # è¿è¡Œæ‰€æœ‰æµ‹è¯•
            await self.test_json_parsing_fix()
            await self.test_parameter_mapping_with_mock()
            await self.test_action_routing()
            await self.test_error_handling_scenarios()
            await self.test_performance_with_mock()
            
        finally:
            pass  # æ— éœ€æ¸…ç†
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_quick_test_report()
        
        return self.calculate_overall_success_rate()
    
    def calculate_overall_success_rate(self):
        """è®¡ç®—æ€»ä½“æˆåŠŸç‡"""
        if not self.test_results:
            return False
            
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r['success'])
        
        return passed_tests / total_tests >= 0.8  # 80%æˆåŠŸç‡
    
    def generate_quick_test_report(self):
        """ç”Ÿæˆå¿«é€Ÿæµ‹è¯•æŠ¥å‘Š"""
        total_time = time.time() - self.start_time if self.start_time else 0
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r['success'])
        failed_tests = total_tests - passed_tests
        
        print("\n" + "="*80)
        print("âš¡ DEEPSEARCH SERVER QUICK TEST REPORT")
        print("="*80)
        print(f"ğŸ“Š Total Tests: {total_tests}")
        print(f"âœ… Passed: {passed_tests} ({passed_tests/total_tests*100:.1f}%)")
        print(f"âŒ Failed: {failed_tests} ({failed_tests/total_tests*100:.1f}%)")
        print(f"â±ï¸  Total Time: {total_time:.2f}s")
        print()
        
        # æŒ‰æµ‹è¯•ç±»å‹åˆ†ç»„
        test_groups = {}
        for result in self.test_results:
            group = result['test_name']
            if group not in test_groups:
                test_groups[group] = []
            test_groups[group].append(result)
        
        # è¯¦ç»†ç»“æœ
        for group_name, group_results in test_groups.items():
            group_passed = sum(1 for r in group_results if r['success'])
            group_total = len(group_results)
            print(f"ğŸ“‹ {group_name}: {group_passed}/{group_total} passed")
            
            for result in group_results:
                status = "âœ…" if result['success'] else "âŒ"
                print(f"  {status} {result['test_case']} ({result['duration']:.2f}s)")
                if not result['success'] and result['error']:
                    print(f"      Error: {result['error']}")
        
        # å…³é”®å‘ç°
        print("\nğŸ” Key Findings:")
        
        # JSONè§£æä¿®å¤æ•ˆæœ
        json_results = [r for r in self.test_results if "JSON Parsing" in r['test_name']]
        if json_results:
            json_success_rate = sum(1 for r in json_results if r['success']) / len(json_results)
            print(f"  ğŸ”§ JSON parsing fix success rate: {json_success_rate*100:.1f}%")
        
        # æ€§èƒ½æ”¹è¿›
        perf_results = [r for r in self.test_results if "Performance" in r['test_name']]
        if perf_results:
            avg_time = sum(r['duration'] for r in perf_results) / len(perf_results)
            print(f"  âš¡ Average mock response time: {avg_time:.2f}s (vs ~27s real)")
        
        # é”™è¯¯å¤„ç†
        error_results = [r for r in self.test_results if "Error Handling" in r['test_name']]
        if error_results:
            error_success_rate = sum(1 for r in error_results if r['success']) / len(error_results)
            print(f"  ğŸ›¡ï¸  Error handling robustness: {error_success_rate*100:.1f}%")
        
        # æ¨èæ”¹è¿›
        print(f"\nğŸ’¡ Recommendations:")
        if passed_tests == total_tests:
            print("  ğŸ‰ All tests passed! Core logic is working correctly.")
            print("  ğŸ“ˆ Consider implementing real search API integration")
            print("  ğŸ”§ JSON parsing fix should resolve LLM response issues")
        else:
            failed_groups = set(r['test_name'] for r in self.test_results if not r['success'])
            for group in failed_groups:
                print(f"  âš ï¸  Fix issues in {group} test group")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = project_root / "test_results_deepsearch_quick.json"
        with open(results_file, 'w') as f:
            json.dump({
                'test_summary': {
                    'total_tests': total_tests,
                    'passed_tests': passed_tests,
                    'failed_tests': failed_tests,
                    'success_rate': passed_tests/total_tests*100,
                    'total_time': total_time,
                    'test_type': 'quick_mock_tests',
                    'timestamp': datetime.now().isoformat()
                },
                'detailed_results': self.test_results
            }, f, indent=2)
        
        print(f"\nğŸ“ Detailed results saved to: {results_file}")
        print("="*80)


# Pytest integration
@pytest.mark.asyncio
async def test_deepsearch_quick():
    """Pytest wrapper for quick DeepSearch tests"""
    tester = DeepSearchQuickTester()
    success = await tester.run_all_quick_tests()
    assert success, "Some DeepSearch quick tests failed"


# Standalone execution
async def main():
    """Main function for standalone execution"""
    tester = DeepSearchQuickTester()
    await tester.run_all_quick_tests()


if __name__ == "__main__":
    asyncio.run(main())