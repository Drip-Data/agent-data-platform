#!/usr/bin/env python3
"""
æ­¥éª¤çº§æ—¥å¿—ç³»ç»Ÿæµ‹è¯•

éªŒè¯æ­¥éª¤æ—¥å¿—è®°å½•å™¨çš„åŠŸèƒ½å®Œæ•´æ€§
"""

import pytest
import sys
import os
import json
import tempfile
import asyncio
from unittest.mock import AsyncMock, MagicMock
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.step_logger import StepDiagnosticLogger
from core.interfaces import TaskSpec


class TestStepDiagnosticLogger:
    """æµ‹è¯•æ­¥éª¤è¯Šæ–­æ—¥å¿—è®°å½•å™¨"""
    
    def setup_method(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        # ä½¿ç”¨ä¸´æ—¶ç›®å½•è¿›è¡Œæµ‹è¯•
        self.temp_dir = tempfile.mkdtemp()
        self.logger = StepDiagnosticLogger(base_path=self.temp_dir)
    
    def test_task_initialization(self):
        """æµ‹è¯•ä»»åŠ¡åˆå§‹åŒ–"""
        task_id = "test-task-123"
        task_description = "æµ‹è¯•ä»»åŠ¡æè¿°"
        
        self.logger.start_task(task_id, task_description)
        
        assert self.logger.current_task_data is not None
        assert self.logger.current_task_data["task_id"] == task_id
        assert self.logger.current_task_data["task_description"] == task_description
        assert "task_start_time" in self.logger.current_task_data
        assert self.logger.current_task_data["steps"] == []
    
    def test_step_initialization(self):
        """æµ‹è¯•æ­¥éª¤åˆå§‹åŒ–"""
        self.logger.start_task("test-task", "test description")
        
        step_index = 0
        self.logger.start_step(step_index)
        
        assert self.logger.current_step_data is not None
        assert self.logger.current_step_data["step_index"] == step_index
        assert "step_start_time" in self.logger.current_step_data
        assert "llm_interaction" in self.logger.current_step_data
        assert "parsing_stage" in self.logger.current_step_data
        assert "tool_executions" in self.logger.current_step_data
    
    def test_llm_call_logging(self):
        """æµ‹è¯•LLMè°ƒç”¨æ—¥å¿—è®°å½•"""
        self.logger.start_task("test-task", "test description")
        self.logger.start_step(0)
        
        prompt = [{"role": "user", "content": "è®¡ç®—1+1"}]
        raw_response = "<think>éœ€è¦è®¡ç®—1+1</think><answer>2</answer>"
        stop_sequence = "</answer>"
        start_time = 1000.0
        end_time = 1002.5
        token_usage = {"prompt_tokens": 10, "completion_tokens": 5, "total_tokens": 15}
        
        self.logger.log_llm_call(
            prompt=prompt,
            raw_response=raw_response,
            stop_sequence=stop_sequence,
            start_time=start_time,
            end_time=end_time,
            token_usage=token_usage
        )
        
        llm_data = self.logger.current_step_data["llm_interaction"]
        assert llm_data["prompt_sent_to_llm"] == prompt
        assert llm_data["raw_llm_response"] == raw_response
        assert llm_data["stop_sequence_triggered"] == stop_sequence
        assert llm_data["llm_call_duration_seconds"] == 2.5
        assert llm_data["token_usage"] == token_usage
    
    def test_parsing_result_logging(self):
        """æµ‹è¯•è§£æç»“æœæ—¥å¿—è®°å½•"""
        self.logger.start_task("test-task", "test description")
        self.logger.start_step(0)
        
        think_content = "éœ€è¦è®¡ç®—1+1"
        execution_block = "<microsandbox><run_code>print(1+1)</run_code></microsandbox>"
        answer_content = None
        actions = [{"service": "microsandbox", "tool": "run_code", "input": "print(1+1)"}]
        parsing_errors = []
        start_time = 2000.0
        end_time = 2000.1
        
        self.logger.log_parsing_result(
            think_content=think_content,
            execution_block=execution_block,
            answer_content=answer_content,
            actions=actions,
            parsing_errors=parsing_errors,
            start_time=start_time,
            end_time=end_time
        )
        
        parsing_data = self.logger.current_step_data["parsing_stage"]
        assert parsing_data["extracted_think_content"] == think_content
        assert parsing_data["extracted_execution_block"] == execution_block
        assert parsing_data["extracted_answer_content"] == answer_content
        assert parsing_data["parsed_actions"] == actions
        assert parsing_data["parsing_errors"] == parsing_errors
        assert parsing_data["parsing_duration_seconds"] == 0.1
    
    def test_tool_execution_logging(self):
        """æµ‹è¯•å·¥å…·æ‰§è¡Œæ—¥å¿—è®°å½•"""
        self.logger.start_task("test-task", "test description")
        self.logger.start_step(0)
        
        execution_index = 0
        action = {"service": "microsandbox", "tool": "run_code", "input": "print(1+1)"}
        toolscore_request = {
            "endpoint": "http://127.0.0.1:8090/execute_tool",
            "method": "POST",
            "payload": {"tool_id": "microsandbox", "action": "run_code", "parameters": {"code": "print(1+1)"}}
        }
        raw_response = {
            "success": True,
            "data": {"stdout": "2\n", "stderr": "", "exit_code": 0}
        }
        formatted_result = "2"
        start_time = 3000.0
        end_time = 3000.5
        execution_status = "success"
        
        self.logger.log_tool_execution(
            execution_index=execution_index,
            action=action,
            toolscore_request=toolscore_request,
            raw_response=raw_response,
            formatted_result=formatted_result,
            start_time=start_time,
            end_time=end_time,
            execution_status=execution_status
        )
        
        tool_executions = self.logger.current_step_data["tool_executions"]
        assert len(tool_executions) == 1
        
        execution_data = tool_executions[0]
        assert execution_data["execution_index"] == execution_index
        assert execution_data["action"] == action
        assert execution_data["toolscore_request"] == toolscore_request
        assert execution_data["toolscore_raw_response"] == raw_response
        assert execution_data["formatted_result_for_llm"] == formatted_result
        assert execution_data["execution_status"] == execution_status
        assert execution_data["execution_timing"]["execution_duration_seconds"] == 0.5
    
    def test_step_completion(self):
        """æµ‹è¯•æ­¥éª¤å®Œæˆ"""
        self.logger.start_task("test-task", "test description")
        self.logger.start_step(0)
        
        step_conclusion = "task_completed"
        self.logger.finish_step(step_conclusion)
        
        # æ£€æŸ¥æ­¥éª¤æ˜¯å¦å·²æ·»åŠ åˆ°ä»»åŠ¡æ•°æ®ä¸­
        assert len(self.logger.current_task_data["steps"]) == 1
        completed_step = self.logger.current_task_data["steps"][0]
        
        assert completed_step["step_index"] == 0
        assert "step_end_time" in completed_step
        assert "step_duration_seconds" in completed_step
        assert completed_step["step_conclusion"] == step_conclusion
        
        # æ£€æŸ¥å½“å‰æ­¥éª¤æ•°æ®æ˜¯å¦å·²é‡ç½®
        assert self.logger.current_step_data is None
    
    @pytest.mark.asyncio
    async def test_task_finalization(self):
        """æµ‹è¯•ä»»åŠ¡å®Œæˆå’Œæ–‡ä»¶å†™å…¥"""
        self.logger.start_task("test-task-456", "å®Œæ•´çš„æµ‹è¯•ä»»åŠ¡")
        
        # æ¨¡æ‹Ÿä¸€ä¸ªå®Œæ•´çš„æ­¥éª¤
        self.logger.start_step(0)
        self.logger.log_llm_call(
            prompt=[{"role": "user", "content": "test"}],
            raw_response="<answer>test result</answer>",
            stop_sequence="</answer>",
            start_time=1000.0,
            end_time=1001.0
        )
        self.logger.log_parsing_result(
            think_content=None,
            execution_block=None,
            answer_content="test result",
            actions=[],
            parsing_errors=[],
            start_time=1001.0,
            end_time=1001.1
        )
        self.logger.finish_step("completed_with_answer")
        
        # å®Œæˆä»»åŠ¡
        final_status = "success"
        final_result = "test result"
        await self.logger.finalize_task(final_status, final_result)
        
        # éªŒè¯ä»»åŠ¡æ•°æ®å®Œæ•´æ€§
        assert self.logger.current_task_data is None  # åº”è¯¥å·²é‡ç½®
        
        # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦åˆ›å»º
        date_str = datetime.now().strftime("%Y-%m-%d")
        log_file = os.path.join(self.temp_dir, "grouped", date_str, f"step_logs_{date_str}.jsonl")
        
        assert os.path.exists(log_file), f"æ—¥å¿—æ–‡ä»¶åº”è¯¥å­˜åœ¨: {log_file}"
        
        # éªŒè¯æ—¥å¿—æ–‡ä»¶å†…å®¹
        with open(log_file, 'r', encoding='utf-8') as f:
            log_content = f.read().strip()
            
        # åº”è¯¥åªæœ‰ä¸€è¡Œï¼ˆä¸€ä¸ªJSONå¯¹è±¡ï¼‰
        assert len(log_content.split('\n')) == 1
        
        # è§£æJSONå†…å®¹
        log_data = json.loads(log_content)
        
        assert log_data["task_id"] == "test-task-456"
        assert log_data["task_description"] == "å®Œæ•´çš„æµ‹è¯•ä»»åŠ¡"
        assert log_data["task_final_status"] == final_status
        assert log_data["task_final_result"] == final_result
        assert "task_duration_seconds" in log_data
        assert len(log_data["steps"]) == 1
        
        # éªŒè¯æ­¥éª¤æ•°æ®
        step_data = log_data["steps"][0]
        assert step_data["step_index"] == 0
        assert "llm_interaction" in step_data
        assert "parsing_stage" in step_data
        assert "tool_executions" in step_data
        assert step_data["step_conclusion"] == "completed_with_answer"
    
    def test_content_extraction_methods(self):
        """æµ‹è¯•å†…å®¹æå–æ–¹æ³•"""
        response_with_think = "<think>æˆ‘éœ€è¦è®¡ç®—è¿™ä¸ªé—®é¢˜</think><answer>42</answer>"
        response_with_execution = "<execute_tools><microsandbox><run>print('hello')</run></microsandbox></execute_tools>"
        response_with_answer = "<answer>æœ€ç»ˆç­”æ¡ˆæ˜¯42</answer>"
        
        # æµ‹è¯•æ€è€ƒå†…å®¹æå–
        think_content = self.logger._extract_think_content(response_with_think)
        assert think_content == "æˆ‘éœ€è¦è®¡ç®—è¿™ä¸ªé—®é¢˜"
        
        # æµ‹è¯•ç­”æ¡ˆå†…å®¹æå–
        answer_content = self.logger._extract_answer_content(response_with_think)
        assert answer_content == "42"
        
        answer_content2 = self.logger._extract_answer_content(response_with_answer)
        assert answer_content2 == "æœ€ç»ˆç­”æ¡ˆæ˜¯42"
        
        # æµ‹è¯•æ‰§è¡Œå—æå–
        exec_block = self.logger._extract_execution_block(response_with_execution)
        assert "<microsandbox>" in exec_block
        assert "print('hello')" in exec_block
        
        # æµ‹è¯•æ²¡æœ‰åŒ¹é…çš„æƒ…å†µ
        no_think = self.logger._extract_think_content("æ²¡æœ‰æ€è€ƒå†…å®¹")
        assert no_think is None
        
        no_answer = self.logger._extract_answer_content("æ²¡æœ‰ç­”æ¡ˆå†…å®¹")
        assert no_answer is None
        
        no_exec = self.logger._extract_execution_block("æ²¡æœ‰æ‰§è¡Œå—")
        assert no_exec is None
    
    def teardown_method(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        import shutil
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)


class TestStepLoggingIntegration:
    """æµ‹è¯•æ­¥éª¤æ—¥å¿—ä¸Runtimeçš„é›†æˆ"""
    
    def test_step_logger_initialization_in_runtime(self):
        """æµ‹è¯•Runtimeä¸­æ­¥éª¤æ—¥å¿—è®°å½•å™¨çš„åˆå§‹åŒ–"""
        from runtimes.reasoning.enhanced_runtime import EnhancedReasoningRuntime
        from core.step_logger import StepDiagnosticLogger
        
        # åˆ›å»ºmockå¯¹è±¡
        mock_config = MagicMock()
        mock_llm_client = MagicMock()
        mock_toolscore_client = MagicMock()
        mock_tool_manager = MagicMock()
        
        # åˆ›å»ºruntimeå®ä¾‹
        runtime = EnhancedReasoningRuntime(
            config_manager=mock_config,
            llm_client=mock_llm_client,
            toolscore_client=mock_toolscore_client,
            tool_manager=mock_tool_manager
        )
        
        # éªŒè¯æ­¥éª¤æ—¥å¿—è®°å½•å™¨å·²åˆå§‹åŒ–
        assert hasattr(runtime, 'step_logger')
        assert isinstance(runtime.step_logger, StepDiagnosticLogger)
    
    def test_detect_triggered_stop_sequence(self):
        """æµ‹è¯•åœæ­¢åºåˆ—æ£€æµ‹"""
        from runtimes.reasoning.enhanced_runtime import EnhancedReasoningRuntime
        
        # åˆ›å»ºmock runtime
        runtime = EnhancedReasoningRuntime(
            config_manager=MagicMock(),
            llm_client=MagicMock(),
            toolscore_client=MagicMock(),
            tool_manager=MagicMock()
        )
        
        stop_sequences = ["</answer>", "<execute_tools />", "<execute_tools></execute_tools>"]
        
        # æµ‹è¯•æ£€æµ‹ç­”æ¡ˆç»“æŸæ ‡ç­¾
        response_with_answer = "è¿™æ˜¯ç­”æ¡ˆ</answer>"
        detected = runtime._detect_triggered_stop_sequence(response_with_answer, stop_sequences)
        assert detected == "</answer>"
        
        # æµ‹è¯•æ£€æµ‹å·¥å…·æ‰§è¡Œæ ‡ç­¾
        response_with_tools = "å¼€å§‹æ‰§è¡Œ<execute_tools />"
        detected = runtime._detect_triggered_stop_sequence(response_with_tools, stop_sequences)
        assert detected == "<execute_tools />"
        
        # æµ‹è¯•æœªæ£€æµ‹åˆ°çš„æƒ…å†µ
        response_without_stop = "æ™®é€šçš„å“åº”æ–‡æœ¬"
        detected = runtime._detect_triggered_stop_sequence(response_without_stop, stop_sequences)
        assert detected == "unknown"


if __name__ == "__main__":
    """è¿è¡Œæ­¥éª¤æ—¥å¿—ç³»ç»Ÿæµ‹è¯•"""
    print("ğŸ”§ å¼€å§‹æµ‹è¯•æ­¥éª¤çº§æ—¥å¿—ç³»ç»Ÿ...")
    
    # è¿è¡Œæµ‹è¯•
    pytest.main([__file__, "-v", "--tb=short"])
    
    print("\nâœ… æ­¥éª¤çº§æ—¥å¿—ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“‹ æµ‹è¯•éªŒè¯å†…å®¹:")
    print("- âœ… æ—¥å¿—è®°å½•å™¨åˆå§‹åŒ–å’Œä»»åŠ¡ç®¡ç†")
    print("- âœ… LLMè°ƒç”¨æ—¥å¿—è®°å½•")
    print("- âœ… è§£æç»“æœæ—¥å¿—è®°å½•") 
    print("- âœ… å·¥å…·æ‰§è¡Œæ—¥å¿—è®°å½•")
    print("- âœ… æ­¥éª¤å®Œæˆå’Œä»»åŠ¡ç»ˆç»“")
    print("- âœ… æ—¥å¿—æ–‡ä»¶åˆ›å»ºå’ŒJSONæ ¼å¼éªŒè¯")
    print("- âœ… å†…å®¹æå–è¾…åŠ©æ–¹æ³•")
    print("- âœ… Runtimeé›†æˆéªŒè¯")