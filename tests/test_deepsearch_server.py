#!/usr/bin/env python3
"""
DeepSearch Server ç»¼åˆæµ‹è¯•å¥—ä»¶
æµ‹è¯•æ·±åº¦æœç´¢æœåŠ¡å™¨çš„æ‰€æœ‰åŠŸèƒ½ã€é”™è¯¯å¤„ç†å’Œé…ç½®ä¸€è‡´æ€§
"""

import asyncio
import pytest
import json
import time
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
from unittest.mock import Mock, AsyncMock, patch, MagicMock
import sys
import os
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from mcp_servers.deepsearch_server.main import DeepSearchMCPServer
    from mcp_servers.deepsearch_server.deepsearch_tool_unified import DeepSearchToolUnified
    from core.config_manager import ConfigManager
    from core.llm_client import LLMClient
except ImportError as e:
    print(f"Import error: {e}")
    print("Please ensure all required modules are available")
    sys.exit(1)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DeepSearchServerTester:
    """DeepSearch Server ç»¼åˆæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.config_manager = ConfigManager()
        self.server = None
        self.test_results = []
        self.start_time = None
        
    async def setup_test_environment(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        logger.info("ğŸ”§ Setting up DeepSearch server test environment...")
        
        try:
            # Initialize server
            self.server = DeepSearchMCPServer(self.config_manager)
            logger.info("âœ… DeepSearch server initialized successfully")
            
        except Exception as e:
            logger.error(f"âŒ Failed to initialize DeepSearch server: {e}")
            raise
        
        logger.info("âœ… DeepSearch test environment setup complete")
        
    async def cleanup_test_environment(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        logger.info("ğŸ§¹ Cleaning up DeepSearch test environment...")
        
        # ç›®å‰æ— éœ€ç‰¹æ®Šæ¸…ç†æ“ä½œ
        
        logger.info("âœ… DeepSearch test environment cleanup complete")
        
    def log_test_result(self, test_name: str, test_case: str, success: bool, 
                       duration: float, details: Dict[str, Any] = None, 
                       error: str = None):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        self.test_results.append({
            'test_name': test_name,
            'test_case': test_case,
            'success': success,
            'duration': duration,
            'details': details or {},
            'error': error,
            'timestamp': datetime.now().isoformat()
        })
        
        status = "âœ… PASS" if success else "âŒ FAIL"
        logger.info(f"{status} {test_name} - {test_case} ({duration:.2f}s)")
        if error:
            logger.error(f"    Error: {error}")
    
    async def test_server_initialization(self):
        """æµ‹è¯•æœåŠ¡å™¨åˆå§‹åŒ–"""
        logger.info("ğŸ§ª Testing server initialization...")
        
        test_cases = [
            {
                "case": "basic_initialization",
                "description": "åŸºç¡€åˆå§‹åŒ–æµ‹è¯•"
            },
            {
                "case": "config_loading",
                "description": "é…ç½®åŠ è½½æµ‹è¯•"
            },
            {
                "case": "llm_client_setup",
                "description": "LLMå®¢æˆ·ç«¯è®¾ç½®æµ‹è¯•"
            }
        ]
        
        for test_case in test_cases:
            start_time = time.time()
            try:
                case_name = test_case["case"]
                
                if case_name == "basic_initialization":
                    # æ£€æŸ¥æœåŠ¡å™¨åŸºæœ¬å±æ€§
                    success = (
                        hasattr(self.server, 'config_manager') and
                        hasattr(self.server, 'server_name') and
                        hasattr(self.server, 'server_id') and
                        self.server.server_id == "deepsearch"
                    )
                    
                elif case_name == "config_loading":
                    # æ£€æŸ¥é…ç½®åŠ è½½
                    llm_config = self.config_manager.get_llm_config()
                    success = (
                        llm_config is not None and
                        hasattr(llm_config, 'provider')
                    )
                    
                elif case_name == "llm_client_setup":
                    # æµ‹è¯•LLMå®¢æˆ·ç«¯åˆ›å»º
                    llm_config = self.config_manager.get_llm_config()
                    llm_client = LLMClient(llm_config)
                    success = llm_client is not None
                    
                duration = time.time() - start_time
                
                self.log_test_result(
                    "Server Initialization",
                    test_case["description"],
                    success,
                    duration,
                    {"case": case_name}
                )
                
            except Exception as e:
                duration = time.time() - start_time
                self.log_test_result(
                    "Server Initialization",
                    test_case["description"],
                    False,
                    duration,
                    error=str(e)
                )
    
    async def test_action_execution(self):
        """æµ‹è¯•åŠ¨ä½œæ‰§è¡Œ"""
        logger.info("ğŸ§ª Testing action execution...")
        
        test_actions = [
            {
                "action": "research",
                "parameters": {"question": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"},
                "description": "åŸºç¡€ç ”ç©¶æµ‹è¯•"
            },
            {
                "action": "quick_research", 
                "parameters": {"question": "Pythonç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹"},
                "description": "å¿«é€Ÿç ”ç©¶æµ‹è¯•"
            },
            {
                "action": "comprehensive_research",
                "parameters": {
                    "question": "æœºå™¨å­¦ä¹ åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨",
                    "topic_focus": "åŒ»ç–—è¯Šæ–­"
                },
                "description": "å…¨é¢ç ”ç©¶æµ‹è¯•"
            }
        ]
        
        for test_action in test_actions:
            start_time = time.time()
            try:
                result = await self.server.handle_tool_action(
                    test_action["action"],
                    test_action["parameters"]
                )
                
                # æ£€æŸ¥ç»“æœç»“æ„
                success = (
                    isinstance(result, dict) and
                    "success" in result and
                    "data" in result
                )
                
                if success and result.get("success"):
                    # æ£€æŸ¥è¿”å›æ•°æ®è´¨é‡
                    data = result.get("data", {})
                    success = (
                        isinstance(data, dict) and
                        "answer" in data and
                        len(str(data.get("answer", ""))) > 50  # ç¡®ä¿æœ‰å®è´¨å†…å®¹
                    )
                
                duration = time.time() - start_time
                
                self.log_test_result(
                    "Action Execution",
                    test_action["description"],
                    success,
                    duration,
                    {
                        "action": test_action["action"],
                        "parameters": test_action["parameters"],
                        "result_success": result.get("success"),
                        "result_size": len(str(result.get("data", {})))
                    }
                )
                
            except Exception as e:
                duration = time.time() - start_time
                self.log_test_result(
                    "Action Execution",
                    test_action["description"],
                    False,
                    duration,
                    error=str(e)
                )
    
    async def test_parameter_handling(self):
        """æµ‹è¯•å‚æ•°å¤„ç†"""
        logger.info("ğŸ§ª Testing parameter handling...")
        
        test_cases = [
            {
                "case": "parameter_mapping",
                "action": "research",
                "parameters": {"query": "æµ‹è¯•æŸ¥è¯¢"},  # ä½¿ç”¨queryè€Œéquestion
                "description": "å‚æ•°æ˜ å°„æµ‹è¯•(query->question)"
            },
            {
                "case": "parameter_mapping_task",
                "action": "research", 
                "parameters": {"task_description": "åˆ†æAIå‘å±•è¶‹åŠ¿"},  # ä½¿ç”¨task_description
                "description": "å‚æ•°æ˜ å°„æµ‹è¯•(task_description->question)"
            },
            {
                "case": "optional_parameters",
                "action": "research",
                "parameters": {
                    "question": "æœºå™¨å­¦ä¹ ç®—æ³•",
                    "max_loops": 2,
                    "reasoning_model": "gemini-1.5-pro"
                },
                "description": "å¯é€‰å‚æ•°æµ‹è¯•"
            },
            {
                "case": "missing_required_param",
                "action": "research",
                "parameters": {},  # ç¼ºå°‘å¿…éœ€å‚æ•°
                "description": "ç¼ºå°‘å¿…éœ€å‚æ•°æµ‹è¯•",
                "expect_failure": True
            },
            {
                "case": "invalid_action",
                "action": "invalid_research_action",
                "parameters": {"question": "æµ‹è¯•"},
                "description": "æ— æ•ˆåŠ¨ä½œæµ‹è¯•",
                "expect_failure": True
            }
        ]
        
        for test_case in test_cases:
            start_time = time.time()
            try:
                result = await self.server.handle_tool_action(
                    test_case["action"],
                    test_case["parameters"]
                )
                
                expect_failure = test_case.get("expect_failure", False)
                
                if expect_failure:
                    # åº”è¯¥å¤±è´¥çš„æµ‹è¯•
                    success = not result.get("success", False)
                else:
                    # åº”è¯¥æˆåŠŸçš„æµ‹è¯•
                    success = result.get("success", False)
                
                duration = time.time() - start_time
                
                self.log_test_result(
                    "Parameter Handling",
                    test_case["description"],
                    success,
                    duration,
                    {
                        "case": test_case["case"],
                        "expect_failure": expect_failure,
                        "actual_success": result.get("success"),
                        "error_message": result.get("error_message", "")
                    }
                )
                
            except Exception as e:
                duration = time.time() - start_time
                expect_failure = test_case.get("expect_failure", False)
                
                # å¯¹äºé¢„æœŸå¤±è´¥çš„æµ‹è¯•ï¼Œå¼‚å¸¸ä¹Ÿç®—æˆåŠŸ
                success = expect_failure
                
                self.log_test_result(
                    "Parameter Handling",
                    test_case["description"],
                    success,
                    duration,
                    error=str(e) if not expect_failure else None
                )
    
    async def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        logger.info("ğŸ§ª Testing error handling...")
        
        # æ¨¡æ‹Ÿå„ç§é”™è¯¯åœºæ™¯
        error_scenarios = [
            {
                "scenario": "llm_client_failure",
                "description": "LLMå®¢æˆ·ç«¯å¤±è´¥å¤„ç†",
                "mock_target": "core.llm_client.LLMClient.call",
                "mock_side_effect": Exception("Simulated LLM failure")
            },
            {
                "scenario": "json_parsing_error",
                "description": "JSONè§£æé”™è¯¯å¤„ç†",
                "mock_target": "json.loads",
                "mock_side_effect": json.JSONDecodeError("Invalid JSON", "", 0)
            }
        ]
        
        for scenario in error_scenarios:
            start_time = time.time()
            try:
                with patch(scenario["mock_target"], side_effect=scenario["mock_side_effect"]):
                    result = await self.server.handle_tool_action(
                        "research",
                        {"question": "æµ‹è¯•é”™è¯¯å¤„ç†"}
                    )
                    
                    # é”™è¯¯åº”è¯¥è¢«æ­£ç¡®å¤„ç†ï¼Œè¿”å›å¤±è´¥ç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
                    success = (
                        isinstance(result, dict) and
                        not result.get("success", True) and
                        "error_message" in result
                    )
                
                duration = time.time() - start_time
                
                self.log_test_result(
                    "Error Handling",
                    scenario["description"],
                    success,
                    duration,
                    {
                        "scenario": scenario["scenario"],
                        "error_caught": not result.get("success", True),
                        "error_message": result.get("error_message", "")
                    }
                )
                
            except Exception as e:
                duration = time.time() - start_time
                # å¦‚æœè¿˜æ˜¯æŠ›å‡ºå¼‚å¸¸ï¼Œè¯´æ˜é”™è¯¯å¤„ç†ä¸å¤Ÿå¥½
                self.log_test_result(
                    "Error Handling",
                    scenario["description"],
                    False,
                    duration,
                    error=str(e)
                )
    
    async def test_configuration_consistency(self):
        """æµ‹è¯•é…ç½®ä¸€è‡´æ€§"""
        logger.info("ğŸ§ª Testing configuration consistency...")
        
        start_time = time.time()
        try:
            # è¯»å–service.json
            service_json_path = project_root / "mcp_servers" / "deepsearch_server" / "service.json"
            unified_config_path = project_root / "config" / "unified_tool_definitions.yaml"
            
            consistency_checks = {}
            
            if service_json_path.exists():
                with open(service_json_path, 'r', encoding='utf-8') as f:
                    service_config = json.load(f)
                
                # æ£€æŸ¥åŸºæœ¬é…ç½®
                consistency_checks.update({
                    "service_id_matches": service_config.get("service_id") == "deepsearch",
                    "port_defined": "port" in service_config,
                    "capabilities_defined": "capabilities" in service_config,
                    "actions_count": len(service_config.get("capabilities", [])) >= 3
                })
                
                # æ£€æŸ¥åŠ¨ä½œå®šä¹‰
                capabilities = service_config.get("capabilities", [])
                action_names = [cap.get("name") for cap in capabilities]
                expected_actions = ["research", "quick_research", "comprehensive_research"]
                
                consistency_checks.update({
                    "all_actions_present": all(action in action_names for action in expected_actions)
                })
            else:
                consistency_checks["service_json_exists"] = False
            
            if unified_config_path.exists():
                import yaml
                with open(unified_config_path, 'r', encoding='utf-8') as f:
                    unified_config = yaml.safe_load(f)
                
                deepsearch_config = unified_config.get("tools", {}).get("deepsearch", {})
                
                consistency_checks.update({
                    "unified_config_exists": True,
                    "unified_id_matches": deepsearch_config.get("id") == "deepsearch",
                    "unified_actions_defined": "actions" in deepsearch_config
                })
            else:
                consistency_checks["unified_config_exists"] = False
            
            # æ€»ä½“ä¸€è‡´æ€§è¯„åˆ†
            consistency_score = sum(consistency_checks.values()) / len(consistency_checks)
            success = consistency_score >= 0.8
            
            duration = time.time() - start_time
            
            self.log_test_result(
                "Configuration Consistency",
                "é…ç½®æ–‡ä»¶ä¸€è‡´æ€§æ£€æŸ¥",
                success,
                duration,
                {
                    "consistency_score": consistency_score,
                    "checks": consistency_checks,
                    "missing_checks": [k for k, v in consistency_checks.items() if not v]
                }
            )
            
        except Exception as e:
            duration = time.time() - start_time
            self.log_test_result(
                "Configuration Consistency",
                "é…ç½®æ–‡ä»¶ä¸€è‡´æ€§æ£€æŸ¥",
                False,
                duration,
                error=str(e)
            )
    
    async def test_tool_implementation_consistency(self):
        """æµ‹è¯•å·¥å…·å®ç°ä¸€è‡´æ€§"""
        logger.info("ğŸ§ª Testing tool implementation consistency...")
        
        start_time = time.time()
        try:
            # æ£€æŸ¥å·¥å…·å®ç°æ–‡ä»¶
            tool_files_checks = {}
            
            # æ£€æŸ¥ç»Ÿä¸€å·¥å…·å®ç°
            unified_tool_path = project_root / "mcp_servers" / "deepsearch_server" / "deepsearch_tool_unified.py"
            if unified_tool_path.exists():
                tool_files_checks["unified_tool_exists"] = True
                
                # åˆ›å»ºå·¥å…·å®ä¾‹æµ‹è¯•
                try:
                    llm_config = self.config_manager.get_llm_config()
                    llm_client = LLMClient(llm_config)
                    tool = DeepSearchToolUnified(llm_client)
                    
                    tool_files_checks.update({
                        "tool_instantiation": True,
                        "has_research_method": hasattr(tool, 'research'),
                        "has_quick_research_method": hasattr(tool, 'quick_research'),
                        "has_comprehensive_research_method": hasattr(tool, 'comprehensive_research')
                    })
                except Exception as e:
                    tool_files_checks["tool_instantiation"] = False
                    tool_files_checks["instantiation_error"] = str(e)
            else:
                tool_files_checks["unified_tool_exists"] = False
            
            # æ£€æŸ¥åŸå§‹å·¥å…·å®ç°
            original_tool_path = project_root / "mcp_servers" / "deepsearch_server" / "deepsearch_tool.py"
            tool_files_checks["original_tool_exists"] = original_tool_path.exists()
            
            consistency_score = sum(v for v in tool_files_checks.values() if isinstance(v, bool)) / sum(1 for v in tool_files_checks.values() if isinstance(v, bool))
            success = consistency_score >= 0.8
            
            duration = time.time() - start_time
            
            self.log_test_result(
                "Tool Implementation Consistency",
                "å·¥å…·å®ç°ä¸€è‡´æ€§æ£€æŸ¥",
                success,
                duration,
                {
                    "consistency_score": consistency_score,
                    "checks": tool_files_checks
                }
            )
            
        except Exception as e:
            duration = time.time() - start_time
            self.log_test_result(
                "Tool Implementation Consistency",
                "å·¥å…·å®ç°ä¸€è‡´æ€§æ£€æŸ¥",
                False,
                duration,
                error=str(e)
            )
    
    async def test_performance_benchmarks(self):
        """æµ‹è¯•æ€§èƒ½åŸºå‡†"""
        logger.info("ğŸ§ª Testing performance benchmarks...")
        
        benchmark_tests = [
            {
                "test": "simple_research_speed",
                "action": "quick_research",
                "parameters": {"question": "ä»€ä¹ˆæ˜¯Python?"},
                "max_time": 30.0,  # 30ç§’å†…å®Œæˆ
                "description": "ç®€å•ç ”ç©¶é€Ÿåº¦æµ‹è¯•"
            },
            {
                "test": "complex_research_speed", 
                "action": "comprehensive_research",
                "parameters": {
                    "question": "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨å’ŒæŒ‘æˆ˜",
                    "topic_focus": "åŒ»ç–—AI"
                },
                "max_time": 60.0,  # 60ç§’å†…å®Œæˆ
                "description": "å¤æ‚ç ”ç©¶é€Ÿåº¦æµ‹è¯•"
            }
        ]
        
        for benchmark in benchmark_tests:
            start_time = time.time()
            try:
                result = await self.server.handle_tool_action(
                    benchmark["action"],
                    benchmark["parameters"]
                )
                
                duration = time.time() - start_time
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´é™åˆ¶å†…å®Œæˆä¸”æˆåŠŸ
                success = (
                    duration <= benchmark["max_time"] and
                    result.get("success", False)
                )
                
                self.log_test_result(
                    "Performance Benchmarks",
                    benchmark["description"],
                    success,
                    duration,
                    {
                        "test": benchmark["test"],
                        "max_time": benchmark["max_time"],
                        "actual_time": duration,
                        "within_limit": duration <= benchmark["max_time"],
                        "result_success": result.get("success")
                    }
                )
                
            except Exception as e:
                duration = time.time() - start_time
                self.log_test_result(
                    "Performance Benchmarks",
                    benchmark["description"],
                    False,
                    duration,
                    error=str(e)
                )
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ Starting comprehensive DeepSearch server tests...")
        self.start_time = time.time()
        
        try:
            # è®¾ç½®ç¯å¢ƒ
            await self.setup_test_environment()
            
            # è¿è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶
            await self.test_server_initialization()
            await self.test_action_execution()
            await self.test_parameter_handling()
            await self.test_error_handling()
            await self.test_configuration_consistency()
            await self.test_tool_implementation_consistency()
            await self.test_performance_benchmarks()
            
        finally:
            # æ¸…ç†ç¯å¢ƒ
            await self.cleanup_test_environment()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_test_report()
        
        return self.calculate_overall_success_rate()
    
    def calculate_overall_success_rate(self):
        """è®¡ç®—æ€»ä½“æˆåŠŸç‡"""
        if not self.test_results:
            return False
            
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r['success'])
        
        return passed_tests / total_tests >= 0.7  # 70%æˆåŠŸç‡
    
    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        total_time = time.time() - self.start_time if self.start_time else 0
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r['success'])
        failed_tests = total_tests - passed_tests
        
        print("\n" + "="*80)
        print("ğŸ” DEEPSEARCH SERVER TEST REPORT")
        print("="*80)
        print(f"ğŸ“Š Total Tests: {total_tests}")
        print(f"âœ… Passed: {passed_tests} ({passed_tests/total_tests*100:.1f}%)")
        print(f"âŒ Failed: {failed_tests} ({failed_tests/total_tests*100:.1f}%)")
        print(f"â±ï¸  Total Time: {total_time:.2f}s")
        print()
        
        # æŒ‰æµ‹è¯•ç±»å‹åˆ†ç»„
        test_groups = {}
        for result in self.test_results:
            group = result['test_name']
            if group not in test_groups:
                test_groups[group] = []
            test_groups[group].append(result)
        
        # è¯¦ç»†ç»“æœ
        for group_name, group_results in test_groups.items():
            group_passed = sum(1 for r in group_results if r['success'])
            group_total = len(group_results)
            print(f"ğŸ“‹ {group_name}: {group_passed}/{group_total} passed")
            
            for result in group_results:
                status = "âœ…" if result['success'] else "âŒ"
                print(f"  {status} {result['test_case']} ({result['duration']:.2f}s)")
                if not result['success'] and result['error']:
                    print(f"      Error: {result['error']}")
        
        # å…³é”®å‘ç°
        print("\nğŸ” Key Findings:")
        
        # æ€§èƒ½åˆ†æ
        perf_results = [r for r in self.test_results if "Performance" in r['test_name']]
        if perf_results:
            avg_time = sum(r['duration'] for r in perf_results) / len(perf_results)
            print(f"  âš¡ Average response time: {avg_time:.2f}s")
            
        # é…ç½®é—®é¢˜
        config_results = [r for r in self.test_results if "Configuration" in r['test_name']]
        config_issues = [r for r in config_results if not r['success']]
        if config_issues:
            print(f"  âš ï¸  Configuration issues detected: {len(config_issues)}")
            
        # é”™è¯¯å¤„ç†
        error_results = [r for r in self.test_results if "Error Handling" in r['test_name']]
        error_success_rate = sum(1 for r in error_results if r['success']) / len(error_results) if error_results else 0
        print(f"  ğŸ›¡ï¸  Error handling success rate: {error_success_rate*100:.1f}%")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = project_root / "test_results_deepsearch_server.json"
        with open(results_file, 'w') as f:
            json.dump({
                'test_summary': {
                    'total_tests': total_tests,
                    'passed_tests': passed_tests,
                    'failed_tests': failed_tests,
                    'success_rate': passed_tests/total_tests*100,
                    'total_time': total_time,
                    'test_timestamp': datetime.now().isoformat()
                },
                'detailed_results': self.test_results
            }, f, indent=2)
        
        print(f"\nğŸ“ Detailed results saved to: {results_file}")
        print("="*80)


# Pytest integration
@pytest.mark.asyncio
async def test_deepsearch_server_comprehensive():
    """Pytest wrapper for comprehensive DeepSearch tests"""
    tester = DeepSearchServerTester()
    success = await tester.run_all_tests()
    assert success, "Some DeepSearch server tests failed"


# Standalone execution
async def main():
    """Main function for standalone execution"""
    tester = DeepSearchServerTester()
    await tester.run_all_tests()


if __name__ == "__main__":
    asyncio.run(main())