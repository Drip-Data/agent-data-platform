#!/usr/bin/env python3
"""
ç»“æ„åŒ–å·¥å…·ç³»ç»Ÿæµ‹è¯•è„šæœ¬
éªŒè¯Pydanticå·¥å…·å®šä¹‰ã€JSON Schemaæ ¡éªŒå’Œé¢„æ ¡éªŒä¸­é—´ä»¶
"""

import sys
import os
import json
import pytest
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.toolscore.structured_tools import tool_registry, LLMRequest, ToolValidationError
from core.toolscore.tool_definitions import *
from core.llm.validation_middleware import validation_middleware


def test_tool_registration():
    """æµ‹è¯•å·¥å…·æ³¨å†Œ"""
    print("=== æµ‹è¯•å·¥å…·æ³¨å†Œ ===")
    
    tools = tool_registry.get_all_tools()
    print(f"å·²æ³¨å†Œå·¥å…·æ•°é‡: {len(tools)}")
    
    expected_tools = ["mcp-deepsearch", "microsandbox-mcp-server", "browser-use-mcp-server", "mcp-search-tool"]
    registered_tool_ids = [tool.id for tool in tools]
    
    for expected in expected_tools:
        if expected in registered_tool_ids:
            print(f"âœ… {expected} å·²æ³¨å†Œ")
        else:
            print(f"âŒ {expected} æœªæ³¨å†Œ")
    
    return len(tools) >= 4


def test_valid_request_validation():
    """æµ‹è¯•æœ‰æ•ˆè¯·æ±‚çš„æ ¡éªŒ"""
    print("\n=== æµ‹è¯•æœ‰æ•ˆè¯·æ±‚æ ¡éªŒ ===")
    
    valid_request = {
        "thinking": "ç”¨æˆ·è¦æ±‚ç ”ç©¶Python asyncioï¼Œåº”è¯¥ä½¿ç”¨deepsearchå·¥å…·",
        "action": "research",
        "tool_id": "mcp-deepsearch",
        "parameters": {
            "query": "Python asyncioæœ€ä½³å®è·µ",
            "max_results": 10
        },
        "confidence": 0.9
    }
    
    try:
        is_valid, validated_data, error = validation_middleware.validate_before_llm_call(valid_request)
        if is_valid:
            print("âœ… æœ‰æ•ˆè¯·æ±‚æ ¡éªŒé€šè¿‡")
            print(f"   æ ¡éªŒåæ•°æ®: {json.dumps(validated_data, ensure_ascii=False, indent=2)}")
            return True
        else:
            print(f"âŒ æœ‰æ•ˆè¯·æ±‚æ ¡éªŒå¤±è´¥: {error}")
            return False
    except Exception as e:
        print(f"âŒ æ ¡éªŒè¿‡ç¨‹å‡ºé”™: {e}")
        return False


def test_invalid_request_correction():
    """æµ‹è¯•æ— æ•ˆè¯·æ±‚çš„è‡ªåŠ¨çº æ­£"""
    print("\n=== æµ‹è¯•æ— æ•ˆè¯·æ±‚è‡ªåŠ¨çº æ­£ ===")
    
    # è¿™æ˜¯æˆ‘ä»¬ä¹‹å‰é‡åˆ°çš„å…¸å‹é”™è¯¯ï¼šsearch_and_install_tools è¢«é”™è¯¯åˆ†é…ç»™ mcp-deepsearch
    invalid_request = {
        "thinking": "éœ€è¦æœç´¢å’Œå®‰è£…å·¥å…·",
        "action": "search_and_install_tools",  # é”™è¯¯ï¼šè¿™ä¸ªactionä¸å±äºmcp-deepsearch
        "tool_id": "mcp-deepsearch",          # é”™è¯¯çš„å·¥å…·ID
        "parameters": {
            "task_description": "éœ€è¦å¤„ç†PDFæ–‡ä»¶"
        },
        "confidence": 0.8
    }
    
    try:
        is_valid, validated_data, error = validation_middleware.validate_before_llm_call(invalid_request)
        if is_valid:
            print("âœ… æ— æ•ˆè¯·æ±‚å·²è‡ªåŠ¨çº æ­£")
            print(f"   åŸå§‹: {invalid_request['tool_id']}.{invalid_request['action']}")
            print(f"   çº æ­£: {validated_data['tool_id']}.{validated_data['action']}")
            return True
        else:
            print(f"âŒ æ— æ•ˆè¯·æ±‚æ— æ³•çº æ­£: {error}")
            return False
    except Exception as e:
        print(f"âŒ çº æ­£è¿‡ç¨‹å‡ºé”™: {e}")
        return False


def test_parameter_validation():
    """æµ‹è¯•å‚æ•°æ ¡éªŒ"""
    print("\n=== æµ‹è¯•å‚æ•°æ ¡éªŒ ===")
    
    # æµ‹è¯•ç¼ºå°‘å¿…éœ€å‚æ•°çš„æƒ…å†µ
    invalid_params_request = {
        "thinking": "è¦æ‰§è¡Œä»£ç ä½†æ²¡æä¾›codeå‚æ•°",
        "action": "microsandbox_execute",
        "tool_id": "microsandbox-mcp-server",
        "parameters": {
            # ç¼ºå°‘å¿…éœ€çš„ "code" å‚æ•°
            "timeout": 30
        },
        "confidence": 0.7
    }
    
    try:
        is_valid, validated_data, error = validation_middleware.validate_before_llm_call(invalid_params_request)
        if not is_valid and "å‚æ•°æ ¡éªŒå¤±è´¥" in error:
            print("âœ… å‚æ•°æ ¡éªŒæ­£ç¡®è¯†åˆ«äº†ç¼ºå¤±çš„å¿…éœ€å‚æ•°")
            print(f"   é”™è¯¯ä¿¡æ¯: {error}")
            return True
        else:
            print(f"âŒ å‚æ•°æ ¡éªŒæœªèƒ½è¯†åˆ«é”™è¯¯")
            return False
    except Exception as e:
        print(f"âŒ å‚æ•°æ ¡éªŒè¿‡ç¨‹å‡ºé”™: {e}")
        return False


def test_tool_description_generation():
    """æµ‹è¯•å·¥å…·æè¿°ç”Ÿæˆ"""
    print("\n=== æµ‹è¯•å·¥å…·æè¿°ç”Ÿæˆ ===")
    
    try:
        description = tool_registry.generate_llm_tools_description()
        print("âœ… å·¥å…·æè¿°ç”ŸæˆæˆåŠŸ")
        print("--- ç”Ÿæˆçš„å·¥å…·æè¿° ---")
        print(description[:500] + "..." if len(description) > 500 else description)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®ä¿¡æ¯
        checks = [
            "mcp-deepsearch" in description,
            "microsandbox-mcp-server" in description,
            "research" in description,
            "microsandbox_execute" in description
        ]
        
        if all(checks):
            print("âœ… å·¥å…·æè¿°åŒ…å«æ‰€æœ‰å¿…è¦ä¿¡æ¯")
            return True
        else:
            print("âŒ å·¥å…·æè¿°ç¼ºå°‘æŸäº›å…³é”®ä¿¡æ¯")
            return False
            
    except Exception as e:
        print(f"âŒ å·¥å…·æè¿°ç”Ÿæˆå¤±è´¥: {e}")
        return False


def test_validation_stats():
    """æµ‹è¯•æ ¡éªŒç»Ÿè®¡"""
    print("\n=== æµ‹è¯•æ ¡éªŒç»Ÿè®¡ ===")
    
    # é‡ç½®ç»Ÿè®¡
    validation_middleware.reset_stats()
    
    # æ‰§è¡Œå‡ ä¸ªæµ‹è¯•è¯·æ±‚
    test_requests = [
        {"thinking": "test1", "action": "research", "tool_id": "mcp-deepsearch", "parameters": {"question": "test"}},
        {"thinking": "test2", "action": "search_and_install_tools", "tool_id": "mcp-deepsearch", "parameters": {"task_description": "test"}},  # éœ€è¦çº æ­£
        {"thinking": "test3", "action": "invalid_action", "tool_id": "invalid_tool", "parameters": {}},  # æ— æ•ˆ
    ]
    
    for req in test_requests:
        try:
            validation_middleware.validate_before_llm_call(req)
        except:
            pass  # å¿½ç•¥é”™è¯¯ï¼Œåªå…³æ³¨ç»Ÿè®¡
    
    stats = validation_middleware.get_validation_stats()
    print(f"âœ… æ ¡éªŒç»Ÿè®¡:")
    print(f"   æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
    print(f"   æœ‰æ•ˆè¯·æ±‚: {stats['valid_requests']}")
    print(f"   è‡ªåŠ¨çº æ­£: {stats['auto_corrected']}")
    print(f"   å¤±è´¥è¯·æ±‚: {stats['invalid_requests']}")
    
    return stats['total_requests'] == 3


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹ç»“æ„åŒ–å·¥å…·ç³»ç»Ÿæµ‹è¯•\n")
    
    tests = [
        ("å·¥å…·æ³¨å†Œ", test_tool_registration),
        ("æœ‰æ•ˆè¯·æ±‚æ ¡éªŒ", test_valid_request_validation),
        ("æ— æ•ˆè¯·æ±‚è‡ªåŠ¨çº æ­£", test_invalid_request_correction),
        ("å‚æ•°æ ¡éªŒ", test_parameter_validation),
        ("å·¥å…·æè¿°ç”Ÿæˆ", test_tool_description_generation),
        ("æ ¡éªŒç»Ÿè®¡", test_validation_stats),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                print(f"âœ… {test_name} - é€šè¿‡")
                passed += 1
            else:
                print(f"âŒ {test_name} - å¤±è´¥")
        except Exception as e:
            print(f"ğŸ’¥ {test_name} - å¼‚å¸¸: {e}")
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç»“æ„åŒ–å·¥å…·ç³»ç»Ÿå·¥ä½œæ­£å¸¸ã€‚")
        return 0
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥å®ç°ã€‚")
        return 1


if __name__ == "__main__":
    exit(main())